{"index": "gp333488", "code": "import xml.etree.ElementTree as ET\ndef strip_namespace(param_0):\n    for var_0 in root.getiterator():\n        elem.tag = elem.tag.split('}')[-1]\n        ET.cleanup_namespaces(param_0)\n    return param_0", "contrast": "def strip_xml_namespace(root):\n    try:\n        root.tag = root.tag.split('}')[1]\n    except IndexError:\n        pass\n    for element in root.getchildren():\n        strip_xml_namespace(element)", "label": 1}
{"index": "gp254519", "code": "def time_to_deplete_stock(param_0):\n    if param_0 <= 0:\n        return 0\n    else:\n        var_0 = 10.0  \n        var_1 = param_0 / var_0\n        return var_1", "contrast": "def T_stock(self, V_stock):\n        return Stock.T_stock(self, V_stock, self.Q_stock()).to(u.hr)", "label": 1}
{"index": "gp320381", "code": "import cv2\nimport numpy as np\ndef resize_with_keypoints(param_0, param_1, param_2, param_3):\n    var_0, var_1, var_2 = image.shape\n    var_3 = np.random.uniform(*param_3)\n    var_4 = int(var_0 * var_3)\n    var_5 = int(var_1 * var_3)\n    param_0 = cv2.resize(param_0, (var_5, var_4))\n    param_1 = np.array(param_1) * [var_5 / var_1, var_4 / var_0]\n    if param_2 is not None:\n        param_2 = cv2.resize(param_2, (var_5, var_4))\n    return param_0, param_1, param_2", "contrast": "def keypoint_random_resize(image, annos, mask=None, zoom_range=(0.8, 1.2)):\n    height = image.shape[0]\n    width = image.shape[1]\n    _min, _max = zoom_range\n    scalew = np.random.uniform(_min, _max)\n    scaleh = np.random.uniform(_min, _max)\n    neww = int(width * scalew)\n    newh = int(height * scaleh)\n    dst = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)\n    if mask is not None:\n        mask = cv2.resize(mask, (neww, newh), interpolation=cv2.INTER_AREA)\n    adjust_joint_list = []\n    for joint in annos:  \n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n    if mask is not None:\n        return dst, adjust_joint_list, mask\n    else:\n        return dst, adjust_joint_list, None", "label": 1}
{"index": "gp162698", "code": "def libvlc_media_player_get_chapter_count_for_title(param_0, param_1):\n    var_0 = _Cfunctions.get('libvlc_media_player_get_chapter_count_for_title', None) or        _Cfunction('libvlc_media_player_get_chapter_count_for_title', ((1,), (1,),), None,\n                    ctypes.c_int, MediaPlayer, ctypes.c_int)\n    return var_0(param_0, param_1)", "contrast": "def get_title_chapter_count(p_mi, i_title):\n    if p_mi == None or i_title < 0:\n        return -1\n    try:\n        p_title = p_mi.getTitle(i_title)\n        if not p_title:\n            return -1\n        i_chapter_count = p_title.getChapterCount()\n        return i_chapter_count\n    except:\n        return -1", "label": 0}
{"index": "gp187093", "code": "def depth_dimension_tuple(param_0):\n    for var_0 in param_0:\n        if isinstance(var_0, list):\n            yield 1, len(param_0)\n            yield from depth_dimension_tuple(var_0)\n        else:\n            yield 0, len(param_0)\n            break", "contrast": "def get_depths_and_dimensions(data, depth):\n    if not isinstance(data, (list, tuple)):\n        return ()\n    yield depth, len(data)\n    for item in data:\n        yield from get_depths_and_dimensions(item, depth + 1)", "label": 1}
{"index": "gp158805", "code": "def _update_context_field_expression(param_0, param_1):\n    var_0 = (ContextField, Literal, LocalField, UnaryTransformation, Variable)\n    if isinstance(param_1, BinaryComposition):\n        if isinstance(expression.left, ContextField) or isinstance(expression.right, ContextField):\n            return _update_context_field_binary_composition(param_0, param_1)\n        else:\n            return _simplify_non_context_field_binary_composition(param_1)\n    elif isinstance(param_1, TernaryConditional):\n        return _simplify_ternary_conditional(param_1)\n    elif isinstance(param_1, BetweenClause):\n        var_1 = expression.lower_bound\n        var_2 = expression.upper_bound\n        if isinstance(var_1, ContextField) or isinstance(var_2, ContextField):\n            raise AssertionError(u'Found BetweenClause with ContextFields as lower/upper bounds. '\n                                 u'This should never happen: {}'.format(param_1))\n        return param_1\n    elif isinstance(param_1, (OutputContextField, FoldedContextField)):\n        raise AssertionError(u'Found unexpected expression of type {}. This should never happen: '\n                             u'{}'.format(type(param_1).__name__, param_1))\n    elif isinstance(param_1, var_0):\n        return param_1\n    raise AssertionError(u'Found unhandled expression of type {}. This should never happen: '\n                         u'{}'.format(type(param_1).__name__, param_1))", "contrast": "def simplify_expressions(expr):\n    if isinstance(expr, dict):\n        if 'type' in expr:\n            if expr['type'] == 'ContextField' and 'name' in expr:\n                return {'type': 'TrueLiteral'}\n            elif expr['type'] == 'Expression':\n                expr['body'] = simplify_expressions(expr['body'])\n            elif expr['type'] == 'UnaryExpression' and 'argument' in expr:\n                expr['argument'] = simplify_expressions(expr['argument'])\n    elif isinstance(expr, list):\n        for i in range(len(expr)):\n            expr[i] = simplify_expressions(expr[i])\n    return expr", "label": 0}
{"index": "gp184758", "code": "import json\ndef write_json(param_0: str, param_1: dict):\n    with open(param_0, 'w', encoding='utf-8') as var_0:\n        json.dump(param_1, var_0, ensure_ascii=False, indent=4)", "contrast": "def _write_plain_json(file_path, js):\n    if file_path.endswith('.bz2'):\n        with bz2.open(file_path, 'wt', encoding=_default_encoding) as f:\n            json.dump(js, f, indent=2, ensure_ascii=False)\n    else:\n        with open(file_path, 'w', encoding=_default_encoding) as f:\n            json.dump(js, f, indent=2, ensure_ascii=False)", "label": 1}
{"index": "gp006931", "code": "def push(param_0, param_1):\n        self.move_number += 1\n        var_0 = self.piece_type_at(move.to_square) if param_1 else NONE\n        self.captured_piece_stack.append(var_0)\n        self.move_stack.append(param_1)\n        if not param_1:\n            self.turn ^= 1\n            return\n        if move.drop_piece_type:\n            var_1 = move.drop_piece_type\n            var_2 = True\n        else:\n            var_1 = self.piece_type_at(move.from_square)\n            var_2 = False\n            if move.promotion:\n                var_1 = PIECE_PROMOTED[var_1]\n            self.remove_piece_at(move.from_square, False)\n        self.set_piece_at(move.to_square, Piece(var_1, self.turn), var_2, True)\n        self.turn ^= 1\n        self.transpositions.update((self.zobrist_hash(), ))", "contrast": "def update_position(move, board, stack):\n    if move is None:\n        board.turn = not board.turn\n        if board.turn:\n            board.fullmove_number += 1\n        stack.append(board.copy(stack[-1]))\n        return\n    board.push(move)\n    stack.append(board.copy(stack[-1]))\n    if move.is_capture or board.piece_type_at(move.to_square) == chess.PAWN:\n        board.halfmove_clock = 0\n    else:\n        board.halfmove_clock += 1\n    if board.is_en_passant(move):\n        stack.pop()\n        board.pop()\n    board.turn = not board.turn\n    if board.turn:\n        board.fullmove_number += 1", "label": 0}
{"index": "gp063003", "code": "def _set_rp_address(param_0, param_1, param_2=False):\n    if hasattr(param_1, \"_utype\"):\n      param_1 = v._utype(param_1)\n    try:\n      var_0 = YANGDynClass(param_1,base=YANGListType(\"rp_ip_addr\",rp_address.rp_address, yang_name=\"rp-address\", rest_name=\"rp-address\", parent=param_0, is_container='list', user_ordered=False, path_helper=self._path_helper, yang_keys='rp-ip-addr', extensions={u'tailf-common': {u'cli-compact-syntax': None, u'cli-suppress-mode': None, u'callpoint': u'PimStaticRpCfgCallpoint', u'info': u'Static RP', u'cli-suppress-list-no': None}}), is_container='list', yang_name=\"rp-address\", rest_name=\"rp-address\", parent=param_0, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-compact-syntax': None, u'cli-suppress-mode': None, u'callpoint': u'PimStaticRpCfgCallpoint', u'info': u'Static RP', u'cli-suppress-list-no': None}}, namespace='urn:brocade.com:mgmt:brocade-pim', defining_module='brocade-pim', yang_type='list', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': \"\"\"rp_address must be of a type compatible with list\"\"\",\n          'defined-type': \"list\",\n          'generated-type': \"\"\"YANGDynClass(base=YANGListType(\"rp_ip_addr\",rp_address.rp_address, yang_name=\"rp-address\", rest_name=\"rp-address\", parent=self, is_container='list', user_ordered=False, path_helper=self._path_helper, yang_keys='rp-ip-addr', extensions={u'tailf-common': {u'cli-compact-syntax': None, u'cli-suppress-mode': None, u'callpoint': u'PimStaticRpCfgCallpoint', u'info': u'Static RP', u'cli-suppress-list-no': None}}), is_container='list', yang_name=\"rp-address\", rest_name=\"rp-address\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-compact-syntax': None, u'cli-suppress-mode': None, u'callpoint': u'PimStaticRpCfgCallpoint', u'info': u'Static RP', u'cli-suppress-list-no': None}}, namespace='urn:brocade.com:mgmt:brocade-pim', defining_module='brocade-pim', yang_type='list', is_config=True)\"\"\",\n        })\n    self.__rp_address = var_0\n    if hasattr(param_0, '_set'):\n      self._set()", "contrast": "def _set_rp_address(self, rp_address):\n    self.__rp_address = rp_address", "label": 0}
{"index": "gp177794", "code": "import paramiko\nimport time\ndef wait_for_ssh(param_0, param_1=22, param_2=30):\n    var_0 = 0\n    var_1 = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    while var_0 < param_2:\n        try:\n            ssh.connect(param_0, param_1=param_1)\n            ssh.close()\n            return var_0\n        except:\n            var_0 += 1\n            time.sleep(1)\n    raise Exception(\"Could not connect to box after %d attempts\" % param_2)", "contrast": "def _waitForSSHPort(self):\n        logger.debug('Waiting for ssh port to open...')\n        for i in count():\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            try:\n                s.settimeout(a_short_time)\n                s.connect((self.effectiveIP, 22))\n                logger.debug('...ssh port open')\n                return i\n            except socket.error:\n                pass\n            finally:\n                s.close()", "label": 1}
{"index": "gp146461", "code": "def reload_erase_combos(param_0, param_1=None):\n        var_0 = self.get_widget('backspace-binding-combobox')\n        var_1 = self.settings.general.get_string('compat-backspace')\n        for var_2 in combo.get_model():\n            if ERASE_BINDINGS.get(var_2[0]) == var_1:\n                combo.set_active_iter(i.iter)\n                break\n        var_0 = self.get_widget('delete-binding-combobox')\n        var_1 = self.settings.general.get_string('compat-delete')\n        for var_2 in combo.get_model():\n            if ERASE_BINDINGS.get(var_2[0]) == var_1:\n                combo.set_active_iter(i.iter)\n                break", "contrast": "import subprocess\ndef read_compat_var(var_name):\n    process = subprocess.Popen(['dconf', 'read', '/org/gnome/desktop/input-sources/'+var_name], stdout=subprocess.PIPE)\n    output, error = process.communicate()\n    return output.decode('utf-8').strip()\ndef select_right_option():\n    compat_backspace = read_compat_var('compat-backspace')\n    compat_delete = read_compat_var('compat-delete')\n    if compat_backspace == \"'guess'\":\n        combo_box_backspace.select('guess')\n    elif compat_backspace == \"'ascii'\":\n        combo_box_backspace.select('ascii')\n    if compat_delete == \"'guess'\":\n        combo_box_delete.select('guess')\n    elif compat_delete == \"'ascii'\":\n        combo_box_delete.select('ascii')", "label": 0}
{"index": "gp156942", "code": "def get_anki_phrases_english(param_0=None):\n    var_0 = set()\n    for var_1 in ANKI_LANGUAGES:\n        var_2 = get_data(var_1)\n        var_3 = df.eng.str.strip().values\n        var_0 = texts.union(set(var_3))\n        if param_0 and len(var_0) >= param_0:\n            break\n    return sorted(var_0)", "contrast": "import sqlite3\ndef get_anki_phrases_english(limit):\n    conn = sqlite3.connect('anki_flashcards.db')  \n    cursor = conn.cursor()\n    cursor.execute(\"SELECT english FROM flashcards LIMIT ?\", (limit,))\n    phrases = [row[0] for row in cursor.fetchall()]\n    conn.close()\n    return phrases", "label": 0}
{"index": "gp052541", "code": "def set_vars(param_0, *param_1, **param_2):\n        kwargs.update(dict(*param_1))\n        var_0 = {var_1: self.input.get(var_1) for var_1 in param_2}\n        self.input.set_vars(**param_2)\n        if param_2 or var_0:\n            self.history.info(\"Setting input variables: %s\" % str(param_2))\n            self.history.info(\"Old values: %s\" % str(var_0))\n        return var_0", "contrast": "def set_abinit_variables(input_file_path, abinit_variables_dict):\n    old_abinit_variables_dict = {}\n    with open(input_file_path, 'r') as input_file:\n        input_file_content = input_file.read()\n    for key in abinit_variables_dict:\n        old_abinit_variables_dict[key] = re.search(f\"{key}=(\\S+)\",\n                                                    input_file_content).group(1)\n        input_file_content = re.sub(f\"{key}=(\\S+)\",\n                                    f\"{key}={abinit_variables_dict[key]}\",\n                                    input_file_content)\n    with open(input_file_path, 'w') as input_file:\n        input_file.write(input_file_content)\n    return old_abinit_variables_dict", "label": 0}
{"index": "gp125164", "code": "def rpc_handler(param_0):\n    global ip\n    if len(request.body):\n        var_0 = request.META[\"REMOTE_ADDR\"]\n        var_1 = HttpResponse(mimetype=\"application/xml\")\n        var_2 = dispatcher._marshaled_dispatch(request.body)\n        response.write(var_2)\n    else:\n        var_1 = HttpResponse()\n        response.write(\"<h1>This is an XML-RPC Service.</h2>\")\n        response.write(\"<p>You need to invoke it using an XML-RPC Client!</p>\")\n        response.write(\"The following methods are available:<ul>\")\n        var_3 = dispatcher.system_listMethods()\n        for var_4 in var_3:\n            var_5 = dispatcher.system_methodSignature(var_4)\n            var_6 = dispatcher.system_methodHelp(var_4)\n            response.write(\"<li><b>%s</b>: [%s] %s\" % (var_4, var_5, var_6))\n        response.write(\"</ul>\")\n    var_1['Content-length'] = str(len(response.content))\n    return var_1", "contrast": "from django.http import HttpResponse\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.views.decorators.http import require_POST\nfrom django.utils import six\nfrom xmlrpc.server import SimpleXMLRPCDispatcher\n@csrf_exempt\n@require_POST\ndef actual_handler(request):\n    if len(request.body) == 0:\n        msg = 'This is an XML-RPC Service. You need to invoke it using an XML-RPC Client.'\n        return HttpResponse(msg, content_type='text/plain')\n    try:\n        data = six.BytesIO(request.body)\n        data.seek(0)\n        dispatcher = SimpleXMLRPCDispatcher(allow_none=True, encoding=None)\n        response_string = dispatcher._marshaled_dispatch(\n            data.read().decode('utf-8'))\n        response = HttpResponse(response_string,\n                                content_type='application/xml')\n        return response\n    except Exception as e:\n        return HttpResponse(content='Error: %s' % str(e), status=400)", "label": 0}
{"index": "gp217851", "code": "from functools import wraps\nclass InvalidVersion(Exception):\n    pass\ndef version_checker(param_0=None, param_1=None):\n    if param_0 is None and param_1 is None:\n        raise ValueError(\"At least one version boundary must be provided\")\n    if param_0 and param_1 and param_0 > param_1:\n        raise ValueError(f\"min_version ({param_0}) cannot be greater than max_version ({param_1})\")\n    def decorator(param_0):\n        @wraps(param_0)\n        def wrapper(*param_0, **param_1):\n            var_0 = param_0[0].api.version\n            if param_0 and var_0 < param_0:\n                raise InvalidVersion(f\"Minimum version requirement not met. Server version: {var_0}, Minimum version: {param_0}\")\n            elif param_1 and var_0 > param_1:\n                raise InvalidVersion(f\"Maximum version requirement exceeded. Server version: {var_0}, Maximum version: {param_1}\")\n            return param_0(*param_0, **param_1)\n        return wrapper\n    return decorator", "contrast": "def requires_swimlane_version(min_version=None, max_version=None):\n    if min_version is None and max_version is None:\n        raise ValueError('Must provide either min_version, max_version, or both')\n    if min_version and max_version and compare_versions(min_version, max_version) < 0:\n        raise ValueError('min_version must be <= max_version ({}, {})'.format(min_version, max_version))\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            swimlane = self._swimlane\n            if min_version and compare_versions(min_version, swimlane.build_version, True) < 0:\n                raise InvalidSwimlaneBuildVersion(swimlane, min_version, max_version)\n            if max_version and compare_versions(swimlane.build_version, max_version, True) < 0:\n                raise InvalidSwimlaneBuildVersion(swimlane, min_version, max_version)\n            return func(self, *args, **kwargs)\n        return wrapper\n    return decorator", "label": 1}
{"index": "gp329375", "code": "def start_sasl_authentication(param_0: str, param_1: str, param_2: str) -> None:\n    pass  ", "contrast": "def _sasl_authenticate(self, stream, username, authzid):\n        if not stream.initiator:\n            raise SASLAuthenticationFailed(\"Only initiating entity start\"\n                                                        \" SASL authentication\")\n        if stream.features is None or not self.peer_sasl_mechanisms:\n            raise SASLNotAvailable(\"Peer doesn't support SASL\")\n        props = dict(stream.auth_properties)\n        if not props.get(\"service-domain\") and (\n                                        stream.peer and stream.peer.domain):\n            props[\"service-domain\"] = stream.peer.domain\n        if username is not None:\n            props[\"username\"] = username\n        if authzid is not None:\n            props[\"authzid\"] = authzid\n        if \"password\" in self.settings:\n            props[\"password\"] = self.settings[\"password\"]\n        props[\"available_mechanisms\"] = self.peer_sasl_mechanisms\n        enabled = sasl.filter_mechanism_list(\n                            self.settings['sasl_mechanisms'], props,\n                                            self.settings['insecure_auth'])\n        if not enabled:\n            raise SASLNotAvailable(\n                                \"None of SASL mechanism selected can be used\")\n        props[\"enabled_mechanisms\"] = enabled\n        mechanism = None\n        for mech in enabled:\n            if mech in self.peer_sasl_mechanisms:\n                mechanism = mech\n                break\n        if not mechanism:\n            raise SASLMechanismNotAvailable(\"Peer doesn't support any of\"\n                                                    \" our SASL mechanisms\")\n        logger.debug(\"Our mechanism: {0!r}\".format(mechanism))\n        stream.auth_method_used = mechanism\n        self.authenticator = sasl.client_authenticator_factory(mechanism)\n        initial_response = self.authenticator.start(props)\n        if not isinstance(initial_response, sasl.Response):\n            raise SASLAuthenticationFailed(\"SASL initiation failed\")\n        element = ElementTree.Element(AUTH_TAG)\n        element.set(\"mechanism\", mechanism)\n        if initial_response.data:\n            if initial_response.encode:\n                element.text = initial_response.encode()\n            else:\n                element.text = initial_response.data\n        stream.write_element(element)", "label": 1}
{"index": "gp071338", "code": "def retry_failed_logs_action(param_0, param_1, param_2):\n        var_0 = 0\n        for var_1 in param_2:\n            var_2 = _retry_failed_log(var_1)\n            if var_2:\n                var_0 += 1\n        self.message_user(\n            param_1,\n            _('{count} failed trigger logs retried.').format(var_0=var_0),\n        )", "contrast": "from django_celery_beat.models import PeriodicTask, PeriodicTasks\ndef retry_failed_trigger_actions(queryset):\n    trigger_ids = queryset.values_list('id', flat=True)\n    tasks = PeriodicTask.objects.filter(name__in=trigger_ids)\n    for task in tasks:\n        task.enabled = True\n        task.save()\n    PeriodicTasks.changed()", "label": 0}
{"index": "gp294042", "code": "import os\ndef _load_envars(param_0):\n    for var_0 in param_0:\n        var_1, var_2 = var_0\n        os.environ[var_1] = var_2", "contrast": "def load_envars(self, items):\n    for item in items:\n        envar = item[0]\n        key = item[1]\n        value = self._get_and_update_setting(envar)\n        if value != None:\n            self.data[key] = value\n            self.config.remove_option(self.name, key)", "label": 1}
{"index": "gp040196", "code": "def complete(param_0):\n        if not self._techniques:\n            return False\n        if not any(tech._is_overriden('complete') for var_0 in self._techniques):\n            return False\n        return self.completion_mode(tech.complete(param_0) for var_0 in self._techniques if tech._is_overriden('complete'))", "contrast": "def is_completed(manager):\n    return manager.status == \"completed\"", "label": 0}
{"index": "gp064741", "code": "def _print_parsed_webpage(\n            self):\n        self.log.debug('starting the ``_print_parsed_webpage()`` method')\n        from polyglot import htmlCleaner\n        cleaner = htmlCleaner(\n            log=self.log,\n            settings=self.settings,\n            url=self.url,\n            outputDirectory=self.folderpath,\n            title=self.title,  \n            style=True,  \n            metadata=True,  \n            h1=True  \n        )\n        htmlFile = cleaner.clean()\n        if not htmlFile:\n            return\n        pdfPath = htmlFile.replace(\".html\", self.append + \".pdf\")\n        electron = self.settings[\"executables\"][\"electron path\"]\n        cmd = \"\"\"%(electron)s -i \"%(htmlFile)s\" -o \"%(pdfPath)s\" \"\"\" % locals()\n        p = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)\n        stdout, stderr = p.communicate()\n        if len(stderr):\n            print stderr\n        self.log.debug('output: %(stdout)s' % locals())\n        os.remove(htmlFile)\n        exists = os.path.exists(pdfPath)\n        if not exists:\n            print \"%(pdfPath)s was not generated for some reason - please investigate\" % locals()\n            sys.exit(0)\n        self.log.debug('completed the ``_print_parsed_webpage()`` method')\n        return pdfPath", "contrast": "def print_parsed_webpage(webpage):\n    cleaned_webpage = parse_and_clean_webpage(webpage) \n    print(cleaned_webpage)\n    pdfPath = generate_pdf(cleaned_webpage) \n    return pdfPath", "label": 0}
{"index": "gp093776", "code": "def _get_info_pv(param_0):\n        var_0 = re.search(pattern=PV_REGEX, string=param_0)\n        return {\"pv\": search.group(\"move_list\")}", "contrast": "def extract_pv(bestmove_info):\n    return bestmove_info['pv']", "label": 0}
{"index": "gp015779", "code": "def _process_execute_payload(param_0, param_1):\n        var_0 = self._payload_handlers.get(param_1['source'])\n        if var_0 is None:\n            return False\n        else:\n            var_0(param_1)\n            return True", "contrast": "def dispatch_payloads(payloads, handlers):\n    results = []\n    for payload in payloads:\n        handler = handlers.get(payload['key'])\n        if handler:\n            result = handler(payload)\n            results.append(result)\n    return results", "label": 0}
{"index": "gp255702", "code": "def report_errors_warnings(param_0):\n    var_0 = document.getNumErrors()\n    var_1 = document.getNumWarnings()\n    var_2 = ['SBML document summary:',\n             f'    number of errors: {var_0}',\n             f'    number of warnings: {var_1}']\n    for var_3 in range(len(var_2)):\n        print(var_2[var_3])\n    for var_3 in range(var_0):\n        var_4 = document.getError(var_3)\n        print(f'Error: {error.getMessage()}')\n    for var_3 in range(var_1):\n        var_5 = document.getWarning(var_3)\n        print(f'Warning: {warning.getMessage()}')", "contrast": "def run_sbml_validation(document, notifications):\n    validator = libsbml.SBMLValidator()\n    validator.validate(document)\n    for i in range(document.getNumErrors()):\n        notifications['errors'].append(format_failure(document.getError(i)))\n    for i in range(validator.getNumFailures()):\n        failure = validator.getFailure(i)\n        if failure.isWarning():\n            notifications['warnings'].append(format_failure(failure))\n        else:\n            notifications['errors'].append(format_failure(failure))", "label": 1}
{"index": "gp090526", "code": "def get_state_actions(param_0, param_1, **param_2):\n        if state.config_flags & ConfigFlags.DEPENDENT or state.config_id.config_type != ItemType.CONTAINER:\n            return super(ScriptActionGenerator, param_0).get_state_actions(param_1, **param_2)\n        if state.base_state == State.ABSENT:\n            var_0 = []\n        else:\n            log.debug(\"Found existing script containers: %s\", state.config_id)\n            if not self.remove_existing_before:\n                var_1 = state.config_id\n                var_2 = self._policy.cname(config_id.map_name, config_id.config_name, config_id.instance_name)\n                if state.client_name == self._policy.default_client_name:\n                    var_3 = \"Container {0} existed prior to running the script.\".format(var_2)\n                else:\n                    var_3 = (\"Container {0} existed on client {1} prior to running the \"\n                                 \"script.\").format(var_2, state.client_name)\n                raise ScriptActionException(var_3)\n            if state.base_state == State.RUNNING or state.state_flags & StateFlags.RESTARTING:\n                log.debug(\"Preparing shutdown of existing container: %s\", state.config_id)\n                var_0 = [ItemAction(param_1, DerivedAction.SHUTDOWN_CONTAINER)]\n            else:\n                log.debug(\"Preparing removal existing container: %s\", state.config_id)\n                var_0 = [ItemAction(param_1, Action.REMOVE)]\n        actions.append(ItemAction(param_1, ContainerUtilAction.SCRIPT, extra_data=param_2))\n        return var_0", "contrast": "from dockermap.map.action import resume\ndef resume_or_remove(state, **kwargs):\n    try:\n        resume.ResumeActionGenerator(state)\n    except ValueError:\n        if state.container.exists():\n            if kwargs.get('remove_existing_before'):\n                state.container.remove(v=True, force=True)\n            else:\n                raise ValueError('Container already exists and could not be removed.')\n        state.script.run()\n    return state.map.build_actions(item_actions=[])", "label": 0}
{"index": "gp173030", "code": "from functools import wraps\nfrom django.shortcuts import redirect\ndef subscription_required(param_0):\n    @wraps(param_0)\n    def wrapper(param_0, *param_1, **param_2):\n        var_0 = request.user\n        if not user.is_authenticated:\n            return redirect('login')\n        elif not user.is_subscribed:\n            return redirect('pay_page')\n        else:\n            return param_0(param_0, *param_1, **param_2)\n    return wrapper", "contrast": "def subscription_payment_required(\n function=None, plan=None, pay_page=SUBSCRIPTION_REDIRECT\n):\n actual_decorator = subscriber_passes_pay_test(\n  subscriber_has_active_subscription, plan=plan, pay_page=pay_page\n )\n if function:\n\t\treturn actual_decorator(function)\n return actual_decorator", "label": 1}
{"index": "gp311146", "code": "def get_last_2_errors(param_0):\n    if len(param_0) < 2:\n        return param_0\n    else:\n        return param_0[-2:]", "contrast": "def compact_error(err):\n  def err2(e):\n    if isinstance(e, exceptions.EvaluationError) and e.inner:\n      message, i = err2(e.inner)\n      if i == 1:\n        return ', '.join([e.args[0], str(e.inner)]), i + 1\n      else:\n        return message, i + 1\n    else:\n      return str(e), 1\n  return err2(err)[0]", "label": 1}
{"index": "gp157920", "code": "def countedArray( param_0, param_1=None ):\r\n    var_0 = Forward()\r\n    def countFieldParseAction(param_0,param_1,param_2):\r\n        var_0 = param_2[0]\r\n        var_0 << (var_0 and Group(And([param_0]*var_0)) or Group(empty))\r\n        return []\r\n    if param_1 is None:\r\n        param_1 = Word(nums).setParseAction(lambda t:int(t[0]))\r\n    else:\r\n        param_1 = intExpr.copy()\r\n    intExpr.setName(\"arrayLen\")\r\n    intExpr.addParseAction(countFieldParseAction, callDuringTry=True)\r\n    return ( param_1 + var_0 )", "contrast": "def counted_list(tokens):\n    count = int(tokens[0])\n    return tokens[1 : count + 1]", "label": 0}
{"index": "gp029077", "code": "def issue_tags(param_0):\n    var_0 = issue.get('labels', [])\n    return [var_1['name'].replace('tag: ', '') for var_1 in var_0 if var_1['name'].startswith('tag: ')]", "contrast": "def get_issue_tags(issue):\n    tags = issue.tags \n    return tags", "label": 0}
{"index": "gp244347", "code": "def search_dashboard_for_facets(param_0: bool, param_1: FacetsSearchRequestContainer) -> ResponseContainerFacetsResponseContainer:\n    pass", "contrast": "def search_dashboard_for_facets(self, **kwargs):  \n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.search_dashboard_for_facets_with_http_info(**kwargs)  \n        else:\n            (data) = self.search_dashboard_for_facets_with_http_info(**kwargs)  \n            return data", "label": 1}
{"index": "gp151979", "code": "def copy(param_0, param_1=_unset, param_2=_unset, param_3=_unset,\n             param_4=_unset, param_5=_unset, param_6=_unset, param_7=_unset,\n             param_8=_unset):\n        if param_7 is _unset:\n            param_7 = self.before_sleep\n        return self.__class__(\n            param_1=self.sleep if param_1 is _unset else param_1,\n            param_2=self.stop if param_2 is _unset else param_2,\n            param_3=self.wait if param_3 is _unset else param_3,\n            param_4=self.retry if param_4 is _unset else param_4,\n            param_5=self.before if param_5 is _unset else param_5,\n            param_6=self.after if param_6 is _unset else param_6,\n            param_7=param_7,\n            param_8=self.reraise if param_6 is _unset else param_8,\n        )", "contrast": "def copy_with_changes(obj, **changes):\n    copied_obj = obj.__class__()\n    for key, value in obj.__dict__.items():\n        setattr(copied_obj, key, value)\n    for key, value in changes.items():\n        setattr(copied_obj, key, value)\n    return copied_obj", "label": 0}
{"index": "gp256405", "code": "import transmissionrpc\ndef delete_torrents(param_0):\n    var_0 = transmissionrpc.Client('localhost', port=9091)\n    if isinstance(param_0, list):\n        for var_1 in param_0:\n            client.remove_torrent(var_1, delete_data=True)\n    else:\n        client.remove_torrent(param_0, delete_data=True)", "contrast": "def delete_permanently(self, infohash_list):\n        data = self._process_infohash_list(infohash_list)\n        return self._post('command/deletePerm', data=data)", "label": 1}
{"index": "gp327938", "code": "def get_or_create_track(param_0):\n    var_0, var_1 = Track.objects.get_or_create(param_0=param_0)\n    return var_0", "contrast": "def get(self, name) -> Track:\n        name = name.lower()\n        track = self.track_map.get(name)\n        if not track:\n            track = Track(name)\n            self.tacks.append(track)\n            self.track_map[name] = track\n        return track", "label": 1}
{"index": "gp267632", "code": "def create_workflow_temp_project(param_0):\n    var_0 = {}\n    var_1 = {}\n    for var_2 in param_0:\n        var_3 = create_temp_project(var_2)\n        var_1[var_2] = var_3\n        var_4 = create_workflow(var_2, var_3)\n        var_0[var_2] = var_4\n    return (var_0, var_1)", "contrast": "def _build_underlying_workflows(enabled_regions, json_spec, args):\n    projects_by_region = _create_temporary_projects(enabled_regions, args)\n    workflows_by_region = {}\n    try:\n        for region, project in projects_by_region.items():\n            json_spec['project'] = project\n            workflow_id = _build_regular_workflow(json_spec)\n            logger.debug(\"Created workflow \" + workflow_id + \" successfully\")\n            workflows_by_region[region] = workflow_id\n    except:\n        if projects_by_region:\n            dxpy.executable_builder.delete_temporary_projects(projects_by_region.values())\n        raise\n    return workflows_by_region, projects_by_region", "label": 1}
{"index": "gp190581", "code": "import numpy as np\ndef match_vectors(param_0, param_1, param_2, param_3):\n    var_0 = 0\n    var_1 = 0\n    for var_2 in range(param_0):\n        for var_3 in range(param_1):\n            var_1 += 1\n            if np.dot(param_0[var_2], param_1[var_3]) >= param_3:\n                var_0 += 1\n    var_4 = var_0 / var_1 if var_1 else 0\n    return var_4, var_0, var_1", "contrast": "def returnMatches(kw, kv, n, theta, inputScaling=1.0):\n  m1 = 4\n  m2 = 1000\n  weights = getSparseTensor(kw, n, m1, fixedRange=1.0 / kw)\n  inputVectors = getSparseTensor(kv, n, m2,\n                                 onlyPositive=True,\n                                 fixedRange= 2*inputScaling / kw,\n                                 )\n  dot = inputVectors.matmul(weights.t())\n  numMatches = ((dot >= theta).sum()).item()\n  pctMatches = numMatches / float(m1*m2)\n  return pctMatches, numMatches, m1*m2", "label": 1}
{"index": "gp142276", "code": "def net(narr,nnet):\n mat = np.zeros((nnet,nnet))\n for i in range(len(narr)):\n\t\tnarr[i] = np.sort(narr[i])\n  mat[narr[i][0]][narr[i][1]] = 1\n return mat", "contrast": "import numpy as np\ndef net(edges, N):\n    Q = np.zeros((N, N))\n    for edge in edges:\n        i, j = edge\n        Q[i][j] = 1\n        Q[j][i] = 1\n    return Q", "label": 0}
{"index": "gp308646", "code": "def getElementsByAttr(param_0, param_1):\n    return [var_0 for var_0 in self.children if element.has_attr(param_0) and var_0[param_0] == param_1]", "contrast": "def getElementsByAttr(self, attrName, attrValue):\n        elements = []\n        for child in self.children:\n            if child.getAttribute(attrName) == attrValue:\n                elements.append(child)\n            elements += child.getElementsByAttr(attrName, attrValue)\n        return TagCollection(elements)", "label": 1}
{"index": "gp022160", "code": "def _create_kvstore(param_0, param_1, param_2):\n    var_0 = bool(int(os.getenv('MXNET_UPDATE_ON_KVSTORE', \"1\")))\n    if param_0 is None:\n        var_1 = None\n    elif isinstance(param_0, kvs.KVStore):\n        var_1 = param_0\n    elif isinstance(param_0, str):\n        if param_1 == 1 and 'dist' not in param_0:\n            var_1 = None\n        else:\n            var_1 = kvs.create(param_0)\n            if param_0 == 'local':\n                var_2 = max(np.prod(param.shape) for var_3 in\n                               arg_params.values())\n                if var_2 > 1024 * 1024 * 16:\n                    var_0 = False\n    else:\n        raise TypeError('kvstore must be KVStore, str or None')\n    if var_1 is None:\n        var_0 = False\n    return (var_1, var_0)", "contrast": "from mxnet import kvstore\ndef create_kvstore(kvstore, num_device, arg_params):\n    if isinstance(kvstore, str):\n        return kvstore.create(kvstore, num_device)\n    elif isinstance(kvstore, kvstore.KVStore):\n        return kvstore\n    else:\n        raise ValueError(\"Invalid kvstore: {}\".format(kvstore))", "label": 0}
{"index": "gp284086", "code": "def encode(param_0):\n    return bytearray(value.encode()) ", "contrast": "def encode(self, value):\n        if type(value) is not datetime.datetime:\n            raise TypeError('encode() argument must be a Python datetime')\n        coarse = Time32Type().encode(value)\n        fine   = Time8Type() .encode(value.microsecond / 1e6)\n        return coarse + fine", "label": 1}
{"index": "gp091825", "code": "def _render_str(param_0, param_1):\n        if isinstance(param_1, StrLabel):\n            param_1 = string._render(string.expr)\n        param_1 = str(param_1)\n        if len(param_1) == 0:\n            return ''\n        var_0, var_1, var_2 = split_super_sub(param_1)\n        return render_unicode_sub_super(\n            var_0, var_2, var_1, sub_first=True, translate_symbols=True,\n            unicode_sub_super=self._settings['unicode_sub_super'])", "contrast": "def unicodify(string):\n    return string.encode('utf-8')", "label": 0}
{"index": "gp162066", "code": "def add_all(param_0, param_1):\n        check_not_none(param_1, \"Value can't be None\")\n        var_0 = []\n        for var_1 in param_1:\n            check_not_none(var_1, \"Value can't be None\")\n            data_items.append(self._to_data(var_1))\n        return self._encode_invoke(queue_add_all_codec, data_list=var_0)", "contrast": "def enqueue_items(items):\n    initial_size = len(this.queue)\n    for item in items:\n        this.queue.append(item)\n    return len(this.queue) > initial_size", "label": 0}
{"index": "gp193144", "code": "def rename_ids(param_0:str, param_1:str) -> None:\n    with open(param_0, 'r') as var_0:\n        var_1 = f.readlines()\n    with open(param_1, 'r') as var_0:\n        var_2 = f.readlines()\n    var_3 = {}\n    for var_4 in range(len(var_1)):\n        var_5 = var_1[var_4]\n        if line.startswith('#'):\n            continue\n        var_6 = line.strip().split('\\t')\n        var_7 = var_6[0]\n        if var_7 not in var_3:\n            var_8 = var_2[len(var_3)].strip()\n            var_3[var_7] = var_8\n        var_8 = var_3[var_7]\n        var_6[0] = var_8\n        var_1[var_4] = '\\t'.join(var_6) + '\\n'\n    with open('reindexed.gff3', 'w') as var_0:\n        f.writelines(var_1)", "contrast": "def rename(args):\n    p = OptionParser(rename.__doc__)\n    opts, args = p.parse_args(args)\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n    ingff3, switch = args\n    switch = DictFile(switch)\n    gff = Gff(ingff3)\n    for g in gff:\n        id, = g.attributes[\"ID\"]\n        newname = switch.get(id, id)\n        g.attributes[\"ID\"] = [newname]\n        if \"Parent\" in g.attributes:\n            parents = g.attributes[\"Parent\"]\n            g.attributes[\"Parent\"] = [switch.get(x, x) for x in parents]\n        g.update_attributes()\n        print(g)", "label": 1}
{"index": "gp035028", "code": "def _convert_range_to_list(param_0, param_1):\n    var_0 = seco.range.Range(param_1)\n    try:\n        return r.expand(param_0)\n    except seco.range.RangeException as err:\n        log.error('Range server exception: %s', err)\n        return []", "contrast": "def range_to_list(seco_range):\n    target = list(seco_range)\n    return target", "label": 0}
{"index": "gp026736", "code": "def table_data_client(param_0):\n        if self._table_data_client is None:\n            self._table_data_client = _create_gapic_client(bigtable_v2.BigtableClient)(\n                param_0\n            )\n        return self._table_data_client", "contrast": "from google.cloud.bigtable_v2 import BigtableClient\ndef get_table_admin_grpc_stub(project_id, instance_id, channel=None):\n    client = BigtableClient(project=project_id, channel=channel)\n    return client.table_admin_client(instance_id)", "label": 0}
{"index": "gp000016", "code": "def matchall(param_0, param_1):\n    var_0 = []\n    for var_1 in param_1:\n        var_2 = re.findall(var_1, param_0)\n        var_0 += var_2\n    return var_0", "contrast": "import re\ndef scan_for_patterns(text, patterns):\n    matches = []\n    for pattern in patterns:\n        matches += re.findall(pattern, text)\n    return matches", "label": 0}
{"index": "gp180780", "code": "def generate_django_service_url(param_0, param_1):\n    return cas_url.rstrip('/') + '/login/?service=' + param_1", "contrast": "def get_service_url(request, redirect_to=None):\n    if hasattr(django_settings, 'CAS_ROOT_PROXIED_AS'):\n        service = django_settings.CAS_ROOT_PROXIED_AS + request.path\n    else:\n        protocol = get_protocol(request)\n        host = request.get_host()\n        service = urllib_parse.urlunparse(\n            (protocol, host, request.path, '', '', ''),\n        )\n    if not django_settings.CAS_STORE_NEXT:\n        if '?' in service:\n            service += '&'\n        else:\n            service += '?'\n        service += urllib_parse.urlencode({\n            REDIRECT_FIELD_NAME: redirect_to or get_redirect_url(request)\n        })\n    return service", "label": 1}
{"index": "gp266492", "code": "def read_set_ids(param_0):\n    var_0 = set()\n    for var_1 in param_0:\n        set_ids.add(obj.id)\n    return var_0", "contrast": "def learn_ids(self, item_list):\n        self._reset_sequence()\n        for item in item_list:\n            key = self.nondup_key_for_item(item)\n            self.ids[key] = item[self.id_key]", "label": 1}
{"index": "gp199563", "code": "import sys\ndef get_version_info() -> tuple:\n    var_0 = sys.version_info\n    if hasattr(var_0, 'releaselevel'):\n        if version_info.releaselevel == 'alpha':\n            var_1 = 64\n        elif version_info.releaselevel == 'beta':\n            var_1 = 128\n        elif version_info.releaselevel == 'candidate':\n            var_1 = 192\n        else:\n            var_1 = 255\n    else:\n        var_1 = 0\n    return (version_info.major, version_info.minor, version_info.micro, var_1)", "contrast": "def get_version_info(self, key_name='ver_sw_release'):\n        if key_name in self._msg_info_dict:\n            val = self._msg_info_dict[key_name]\n            return ((val >> 24) & 0xff, (val >> 16) & 0xff, (val >> 8) & 0xff, val & 0xff)\n        return None", "label": 1}
{"index": "gp023682", "code": "def get_shell(param_0=None, param_1=6):\n    if not param_0:\n        param_0 = os.getpid()\n    var_0 = dict(_iter_process())\n    def check_parent(param_0, param_1=0):\n        var_0 = var_0[param_0].get('parent_pid')\n        var_1 = _get_executable(processes.get(var_0))\n        if var_1 in SHELL_NAMES:\n            return (var_1, var_0[var_0]['executable'])\n        if param_1 >= param_1:\n            return None\n        return check_parent(var_0, param_1=param_1 + 1)\n    var_1 = _get_executable(processes.get(param_0))\n    if var_1 in SHELL_NAMES:\n        return (var_1, var_0[param_0]['executable'])\n    try:\n        return check_parent(param_0)\n    except KeyError:\n        return None", "contrast": "import os\ndef get_shell(pid=None):\n    if pid is None:\n        pid = os.getpid()\n    with open(f\"/proc/{pid}/stat\") as f:\n        stat = f.read().split()[2]\n        if stat.startswith('(') and stat.endswith(')s'):\n            return stat[1:-2]\n        else:\n            return None", "label": 0}
{"index": "gp267496", "code": "def clone_to_folder(param_0: str, param_1: str) -> DXDataObject:\n    from dxpy import DXDataObject\n    var_0 = DXDataObject(dxid='object-id', param_0='source-project')\n    var_1 = dx_obj.clone(param_0=param_0, param_1=param_1)\n    return var_1", "contrast": "def clone(self, project, folder=\"/\", **kwargs):\n        if self._proj is None:\n            raise DXError(\"Clone called when a project ID was not associated with this object handler\")\n        dxpy.api.project_clone(self._proj,\n                               {\"objects\": [self._dxid],\n                                \"project\": project,\n                                \"destination\": folder},\n                               **kwargs)\n        cloned_copy = copy.copy(self)\n        cloned_copy.set_ids(cloned_copy.get_id(), project)\n        return cloned_copy", "label": 1}
{"index": "gp152788", "code": "def top_stream(param_0, param_1):\n    if not param_1:\n        return None\n    var_0 = cache.get(param_1)\n    if var_0 is None:\n        return None\n    return stack.pop()", "contrast": "def peek_cache_top(cache, user_id):\n    stack = cache.get(user_id)\n    if stack:\n        return stack[-1]\n    else:\n        return None", "label": 0}
{"index": "gp250317", "code": "import zipfile\ndef read_archive_entry(param_0, param_1=None, param_2=None):\n    with zipfile.ZipFile(param_0, 'r') as var_0:\n        if param_1:\n            with archive.open(param_1) as var_1:\n                if param_2:\n                    return entry.read(param_2)\n                else:\n                    return entry.read()\n        else:\n            var_2 = {}\n            for var_1 in archive.infolist():\n                if param_2:\n                    var_2[entry.filename] = archive.read(var_1, param_2)\n                else:\n                    var_2[entry.filename] = archive.read(var_1)\n            return var_2", "contrast": "def read(self, cnt=None):\n        if cnt is None or cnt < 0:\n            cnt = self._remain\n        elif cnt > self._remain:\n            cnt = self._remain\n        if cnt == 0:\n            return EMPTY\n        data = self._read(cnt)\n        if data:\n            self._md_context.update(data)\n            self._remain -= len(data)\n        if len(data) != cnt:\n            raise BadRarFile(\"Failed the read enough data\")\n        if not data or self._remain == 0:\n            self._check()\n        return data", "label": 1}
{"index": "gp272290", "code": "import matplotlib.colors as mcolors\ndef _make_cmap(param_0, param_1=False):\n    if param_1:\n        param_0 = [(var_0/255, var_1/255, var_2/255) for (var_0,var_1,var_2) in param_0]\n    return mcolors.LinearSegmentedColormap.from_list('custom cmap', param_0)", "contrast": "def _make_cmap(colors, position=None, bit=False):\n    bit_rgb = np.linspace(0,1,256)\n    if position == None:\n        position = np.linspace(0,1,len(colors))\n    else:\n        if len(position) != len(colors):\n            sys.exit(\"position length must be the same as colors\")\n        elif position[0] != 0 or position[-1] != 1:\n            sys.exit(\"position must start with 0 and end with 1\")\n    palette = [(i, (float(r), float(g), float(b), float(a))) for\n    i, (r, g, b, a) in enumerate(colors)]\n    cmap = Colormap(*palette)\n    return cmap", "label": 1}
{"index": "gp319870", "code": "import importlib\ndef import_module(param_0: str):\n    return importlib.import_module(param_0)", "contrast": "def maybe_dotted(module, throw=True):\n    try:\n        return Configurator().maybe_dotted(module)\n    except ImportError as e:\n        err = '%s not found. %s' % (module, e)\n        if throw:\n            raise ImportError(err)\n        else:\n            log.error(err)\n            return None", "label": 1}
{"index": "gp209243", "code": "import numpy as np\ndef cartesian_to_spherical(param_0):\n    var_0, var_1, var_2 = np.transpose(param_0)\n    var_3 = np.sqrt(var_0**2 + var_1**2 + var_2**2)\n    var_4 = np.arcsin(var_2/var_3) * 180/np.pi\n    var_5 = np.arctan2(var_1, var_0) * 180/np.pi\n    var_6 = var_3 - 6371 \n    return var_5, var_4, var_6", "contrast": "def cartesian_to_spherical(vectors):\n    rr = numpy.sqrt(numpy.sum(vectors * vectors, axis=-1))\n    xx, yy, zz = vectors.T\n    lats = numpy.degrees(numpy.arcsin((zz / rr).clip(-1., 1.)))\n    lons = numpy.degrees(numpy.arctan2(yy, xx))\n    depths = EARTH_RADIUS - rr\n    return lons.T, lats.T, depths", "label": 1}
{"index": "gp173457", "code": "def update_event_tags(param_0, param_1, param_2):\n    if param_0 < param_2[0] or param_1 < param_2[1]:\n        raise ValueError(\"The consumed or produced number of event tags is smaller than the previous update.\")\n    elif param_0 > param_2[0] or param_1 > param_2[1]:\n        return True\n    else:\n        return False", "contrast": "def UpdateNumberOfEventTags(\n      self, number_of_consumed_event_tags, number_of_produced_event_tags):\n    consumed_event_tags_delta = 0\n    if number_of_consumed_event_tags is not None:\n      if number_of_consumed_event_tags < self.number_of_consumed_event_tags:\n        raise ValueError(\n            'Number of consumed event tags smaller than previous update.')\n      consumed_event_tags_delta = (\n          number_of_consumed_event_tags - self.number_of_consumed_event_tags)\n      self.number_of_consumed_event_tags = number_of_consumed_event_tags\n      self.number_of_consumed_event_tags_delta = consumed_event_tags_delta\n    produced_event_tags_delta = 0\n    if number_of_produced_event_tags is not None:\n      if number_of_produced_event_tags < self.number_of_produced_event_tags:\n        raise ValueError(\n            'Number of produced event tags smaller than previous update.')\n      produced_event_tags_delta = (\n          number_of_produced_event_tags - self.number_of_produced_event_tags)\n      self.number_of_produced_event_tags = number_of_produced_event_tags\n      self.number_of_produced_event_tags_delta = produced_event_tags_delta\n    return consumed_event_tags_delta > 0 or produced_event_tags_delta > 0", "label": 1}
{"index": "gp235921", "code": "from django import template\nregister = template.Library()\n@register.filter\ndef is_parent_of(param_0, param_1):\n    return potential_child.is_ancestor_of(param_0)", "contrast": "def is_parent_of(page1, page2):\n    try:\n        return page1.tree_id == page2.tree_id and page1.lft < page2.lft and page1.rght > page2.rght\n    except AttributeError:\n        return False", "label": 1}
{"index": "gp140868", "code": "def taskotron_changed_outcome(param_0, param_1):\n    if not taskotron_result_new(param_0, param_1):\n        return False\n    var_0 = param_1['msg']['result'].get('outcome')\n    var_1 = param_1['msg']['result'].get('prev_outcome')\n    return var_1 is not None and var_0 != var_1", "contrast": "def is_outcome_changed(task_results):\n    return True if task_results[-1]['outcome'] != task_results[-2]['outcome'] else False", "label": 0}
{"index": "gp079907", "code": "def gene_name(param_0):\n        if 'Van' in param_0 or 'mcr' in param_0 or 'aph' in param_0 or 'ddlA' in param_0 or 'ant' in param_0 or 'aadE_Cc' in param_0:\n            try:\n                if param_0 == \"ant(3'')_Ih_aac(6')_IId_1_AF453998\":\n                    var_0, var_1, var_2, var_3, var_4, var_5 = name.split('_')\n                    var_6 = '{g1}-{v1}-{g2}-{v2}'.format(g1=var_0,\n                                                         v1=var_1,\n                                                         g2=var_2,\n                                                         v2=var_3)\n                    var_7 = var_6\n                elif param_0 == 'ant(3'')_Ia_1_X02340':\n                    var_8, var_9, var_4, var_5 = name.split('_')\n                    var_6 = '{g}-{v}'.format(g=var_8,\n                                             v=var_9)\n                    var_7 = var_6\n                elif 'mcr_3' in param_0 or 'mcr_2' in param_0 or 'mcr_1.10' in param_0:\n                    var_8, var_10, var_4, var_5 = name.split('_')\n                    var_9 = combinedversion.split('.')[0]\n                    var_6 = '{gene}-{version}'.format(var_8=var_8,\n                                                      var_9=var_9)\n                    var_7 = var_6\n                else:\n                    try:\n                        var_11, var_12, var_4, var_5 = name.split('_')\n                        var_6 = '{pre}-{post}'.format(pre=var_11,\n                                                      post=var_12)\n                        var_7 = var_6\n                    except ValueError:\n                        var_11, var_12, var_4, var_13, var_14 = name.split('_')\n                        var_7 = '{pre}-{post}'.format(pre=var_11,\n                                                         post=var_12)\n                        var_5 = '{pre}_{post}'.format(pre=var_13,\n                                                          post=var_14)\n                        var_6 = var_11\n            except ValueError:\n                var_7, var_4, var_5 = name.split('_')\n                var_6 = var_7\n        else:\n            if 'bla' in param_0 or 'aac' in param_0 or 'ARR' in param_0 or 'POM' in param_0:\n                if 'OKP' in param_0 or 'CTX' in param_0 or 'OXY' in param_0:\n                    var_8, var_1, var_3, var_4, var_5 = name.split('_')\n                    var_6 = '{g}-{v1}-{v2}'.format(g=var_8,\n                                                   v1=var_1,\n                                                   v2=var_3)\n                    var_7 = var_6\n                elif 'CMY' in param_0:\n                    try:\n                        var_6, var_4, var_9, var_5 = name.split('_')\n                    except ValueError:\n                        var_6, var_4, var_9, var_15, var_16 = name.split('_')\n                        var_5 = '{pre}_{post}'.format(pre=var_15,\n                                                          post=var_16)\n                    var_7 = var_6\n                elif param_0 == \"aac(3)_Ib_aac(6')_Ib_1_AF355189\":\n                    var_0, var_1, var_2, var_3, var_4, var_5 = name.split('_')\n                    var_6 = '{g1}-{v1}-{g2}-{v2}'.format(g1=var_0,\n                                                         v1=var_1,\n                                                         g2=var_2,\n                                                         v2=var_3)\n                    var_7 = var_6\n                elif 'alias' in param_0:\n                    var_0, var_1, var_17, var_2, var_3, var_4, var_5 = name.split('_')\n                    var_6 = '{g1}-{v1}'.format(g1=var_0,\n                                               v1=var_1)\n                    var_7 = var_6\n                else:\n                    try:\n                        var_7, var_4, var_5 = name.split('_')\n                        var_6 = var_7\n                    except ValueError:\n                        try:\n                            var_7, var_9, var_4, var_5 = name.split('_')\n                            var_6 = '{g}-{v}'.format(g=var_7,\n                                                     v=var_9)\n                        except ValueError:\n                            var_7, var_9, var_4, var_13, var_14 = name.split('_')\n                            var_6 = '{g}-{v}'.format(g=var_7,\n                                                     v=var_9)\n                            var_7 = var_6\n                            var_5 = '{preaccess}_{postaccess}'.format(preaccess=var_13,\n                                                                          postaccess=var_14)\n            else:\n                try:\n                    var_7, var_4, var_5 = name.split('_')\n                    var_6 = var_7\n                except ValueError:\n                    var_7, var_4, var_13, var_14 = name.split('_')\n                    var_5 = '{preaccess}_{postaccess}'.format(preaccess=var_13,\n                                                                  postaccess=var_14)\n                    var_6 = var_7\n        return var_6, var_7, var_5, var_4", "contrast": "def split_fasta_header(name):\n    header_components = name.split('|')\n    gname = header_components[0]\n    genename = header_components[1].split('.')[0]\n    accession = header_components[1]\n    allele = header_components[2] if len(header_components) == 3 else None\n    return gname, genename, accession, allele", "label": 0}
{"index": "gp284753", "code": "def safe_get(param_0, param_1, param_2=None):\n    try:\n        for var_0 in param_1:\n            param_0 = param_0[var_0]\n        return param_0\n    except (KeyError, TypeError):\n        return param_2", "contrast": "def safe_get(data, key_list):\n        for key in key_list:\n            data = data.get(key, {})\n        return data if data else 'plugin_failed'", "label": 1}
{"index": "gp301716", "code": "def arithmetic_mean(param_0):\n    return sum(param_0) / len(param_0) if len(param_0) > 0 else 0", "contrast": "def mean(data):\n    n = len(data)\n    if n < 1:\n        raise ValueError('mean requires at least one data point')\n    return sum(data)/n", "label": 1}
{"index": "gp069880", "code": "def delete_boot_script(param_0):\n        var_0, var_1 = self.datacenter.request('DELETE', self.path + \n                '/metadata/user-script')\n        r.raise_for_status()\n        self.boot_script = None", "contrast": "def delete_boot_script(login, machine_id):\n    url = f\"/{login}/machines/{machine_id}/metadata/user-script\"", "label": 0}
{"index": "gp120407", "code": "def get_kerberos_subs(param_0):\n    var_0 = get_netid_subscriptions(param_0, Subscription.SUBS_CODE_KERBEROS)\n    if var_0 is not None:\n        for var_1 in var_0:\n            if subscription.subscription_code == Subscription.SUBS_CODE_KERBEROS:\n                return var_1\n    return None", "contrast": "def get_subscription(uwnetid):\n    return subscription_object", "label": 0}
{"index": "gp313499", "code": "def count_events():\n    var_0 = 30\n    var_1 = 12\n    var_2 = 10\n    var_3 = {'counts': {'all': var_0, 'movie': var_1, 'gig': var_2}}\n    return var_3", "contrast": "def get_event_counts(self):\n        counts = {'all': Event.objects.count(),}\n        for k,v in Event.KIND_CHOICES:\n            counts[k] = Event.objects.filter(kind=k).count()\n        return {'counts': counts,}", "label": 1}
{"index": "gp155662", "code": "def price_name_stacks(param_0, param_1, param_2):\n    if param_1['version'] in [NAMESPACE_VERSION_PAY_WITH_STACKS]:\n        return price_name(param_0, param_1, param_2)\n    else:\n        var_0 = price_name(param_0, param_1, param_2)\n        var_0 = int(var_0)\n        return (var_0 * MICROSTACKS_PER_SATOSHI_NUM) / MICROSTACKS_PER_SATOSHI_DEN", "contrast": "def get_name_price_in_stacks(name: str, namespace_version_bits: int, btc_price: float) -> int:\n    if namespace_version_bits < 0:\n        raise ValueError(\"Namespace version bits cannot be negative\")\n    if btc_price <= 0:\n        raise ValueError(\"BTC price should be a positive value\")\n    if len(name) == 0:\n        return 0\n    if namespace_version_bits == 0:\n        return 0\n    if len(name) > 255:\n        raise ValueError(\"Name length cannot be greater than 255 characters\")\n    btc_to_stacks_conversion_factor = 1250000000            \n    namespace_multiplier = 0\n    if namespace_version_bits < 3:\n        namespace_multiplier = 1000000000                   \n    elif namespace_version_bits == 3:\n        namespace_multiplier = 1000000000000                \n    btc_per_microstack = btc_price / btc_to_stacks_conversion_factor\n    if btc_per_microstack == 0:\n        return 0\n    if namespace_multiplier == 0:\n        return int(round(btc_price / btc_per_microstack))\n    bytes_hex = name.encode(\"utf-8\").hex()\n    bytes_dec = int(bytes_hex, 16)\n    amount = max(bytes_dec * namespace_multiplier, 1)\n    return int(round(amount / btc_per_microstack))", "label": 0}
{"index": "gp176466", "code": "def main_entrypoint_class_runner():\n    var_0 = create_running_nailgun_server()\n    var_1 = create_client()\n    return var_1", "contrast": "def _get_nailgun_client(self, jvm_options, classpath, stdout, stderr, stdin):\n    classpath = self._nailgun_classpath + classpath\n    new_fingerprint = self._fingerprint(jvm_options, classpath, self._distribution.version)\n    with self._NAILGUN_SPAWN_LOCK:\n      running, updated = self._check_nailgun_state(new_fingerprint)\n      if running and updated:\n        logger.debug('Found running nailgun server that needs updating, killing {server}'\n                     .format(server=self._identity))\n        self.terminate()\n      if (not running) or (running and updated):\n        return self._spawn_nailgun_server(new_fingerprint, jvm_options, classpath, stdout, stderr, stdin)\n    return self._create_ngclient(self.socket, stdout, stderr, stdin)", "label": 1}
{"index": "gp324680", "code": "def get_adapter(param_0):\n    return adapter", "contrast": "def get_adapter(self, model):\n        if not self.is_registered(model):\n            raise RegistrationError(\n                '{} is not registered with Algolia engine'.format(model))\n        return self.__registered_models[model]", "label": 1}
{"index": "gp293531", "code": "from typing import List, Tuple\nfrom pybel.language import Version\nfrom pybel.parser import BELSyntaxError\nfrom pybel.parser.parse_results import CitationDict, NamespaceDict\nfrom pybel.parser.modifiers import modification_parser, activity_parser\ndef validate_semantics(param_0, param_1: str) -> Tuple[bool, List[Tuple[str, str]]]:\n    var_0 = True\n    var_1 = []\n    for var_2, var_3 in bo.namespace_dict.items():\n        for var_4 in var_3:\n            try:\n                Version.parse(var_4)\n            except ValueError as e:\n                messages.append((bo.document, f\"Invalid namespace value {var_4} in {var_2}: {str(e)}\"))\n                var_0 = False\n    for var_5, var_3 in bo.annotation_dict.items():\n        for var_4 in var_3:\n            try:\n                Version.parse(var_4)\n            except ValueError as e:\n                messages.append((bo.document, f\"Invalid annotation value {var_4} in {var_5}: {str(e)}\"))\n                var_0 = False\n    for var_6 in bo.citation_dict.values():\n        if cp.type not in CitationDict.valid_types:\n            messages.append((bo.document, f\"Invalid citation type {cp.type} in {var_6}\"))\n            var_0 = False\n    for var_7, var_8 in bo.statements.items():\n        var_9 = statement.obj.flatten()\n        for var_10 in var_9:\n            try:\n                modification_parser.parseString(obj.modification)\n                activity_parser.parseString(obj.activity)\n            except BELSyntaxError as e:\n                messages.append((bo.document, f\"Syntax error in {var_10}: {str(e)}\"))\n                var_0 = False\n    if param_1 == \"WARNING\":\n        return var_0, var_1\n    else:\n        return var_0, [var_11 for var_11 in var_1 if \"ERROR\" in var_11[1]]", "contrast": "def validate(bo, error_level: str = \"WARNING\") -> Tuple[bool, List[Tuple[str, str]]]:\n    if bo.ast:\n        bo = validate_functions(bo.ast, bo)  \n        if error_level == \"WARNING\":\n            bo = validate_arg_values(bo.ast, bo)  \n    else:\n        bo.validation_messages.append((\"ERROR\", \"Invalid BEL Statement - cannot parse\"))\n    for msg in bo.validation_messages:\n        if msg[0] == \"ERROR\":\n            bo.parse_valid = False\n            break\n    return bo", "label": 1}
{"index": "gp285289", "code": "def vertices_and_faces(param_0):\n    var_0 = mesh.vertices\n    var_1 = mesh.polygons\n    return var_0, var_1", "contrast": "def isosurface_from_data(data, isolevel, origin, spacing):\n    spacing = np.array(extent/resolution)\n    if isolevel >= 0:\n        triangles = marching_cubes(data, isolevel)\n    else: \n        triangles = marching_cubes(-data, -isolevel)\n    faces = []\n    verts = []\n    for i, t in enumerate(triangles):\n       faces.append([i * 3, i * 3 +1, i * 3 + 2])\n       verts.extend(t)\n    faces = np.array(faces)\n    verts = origin + spacing/2 + np.array(verts)*spacing\n    return verts, faces", "label": 1}
{"index": "gp114847", "code": "def maximum_variation(param_0):\n  return np.max(param_0, axis=0) - np.min(param_0, axis=0)", "contrast": "import numpy as np\ndef range_by_columns(arr):\n    return np.max(arr, axis=0) - np.min(arr, axis=0)", "label": 0}
{"index": "gp168817", "code": "def replace_states(param_0, param_1, param_2):\n    return [param_2 if var_0 in param_1 else var_0 for var_0 in param_0]", "contrast": "def fix_workflow_transitions(portal):\n    logger.info(\"Fixing workflow transitions...\")\n    tochange = [\n        {'wfid': 'bika_duplicateanalysis_workflow',\n         'trid': 'submit',\n         'changes': {\n             'new_state_id': 'to_be_verified',\n             'guard_expr': ''\n            },\n         'update': {\n             'catalog': CATALOG_ANALYSIS_LISTING,\n             'portal_type': 'DuplicateAnalysis',\n             'status_from': 'attachment_due',\n             'status_to': 'to_be_verified'\n            }\n         }\n    ]\n    wtool = api.get_tool('portal_workflow')\n    for item in tochange:\n        wfid = item['wfid']\n        trid = item['trid']\n        workflow = wtool.getWorkflowById(wfid)\n        transitions = workflow.transitions\n        transition = transitions[trid]\n        changes = item.get('changes', {})\n        if 'new_state_id' in changes:\n            new_state_id = changes['new_state_id']\n            oldstate = transition.new_state_id\n            logger.info(\n                \"Replacing target state '{0}' from '{1}.{2}' to {3}\"\n                    .format(oldstate, wfid, trid, new_state_id)\n            )\n            transition.new_state_id = new_state_id\n        if 'guard_expr' in changes:\n            new_guard = changes['guard_expr']\n            if not new_guard:\n                transition.guard = None\n                logger.info(\n                    \"Removing guard expression from '{0}.{1}'\"\n                        .format(wfid, trid))\n            else:\n                guard = transition.getGuard()\n                guard.expr = Expression(new_guard)\n                transition.guard = guard\n                logger.info(\n                    \"Replacing guard expression from '{0}.{1}' to {2}\"\n                        .format(wfid, trid, new_guard))\n        update = item.get('update', {})\n        if update:\n            catalog_id = update['catalog']\n            portal_type = update['portal_type']\n            catalog = api.get_tool(catalog_id)\n            brains = catalog(portal_type=portal_type)\n            for brain in brains:\n                obj = api.get_object(brain)\n                if 'status_from' in update and 'status_to' in update:\n                    status_from = update['status_from']\n                    status_to = update['status_to']\n                    if status_from == brain.review_state:\n                        logger.info(\n                            \"Changing status for {0} from '{1} to {2}\"\n                                .format(obj.getId(), status_from, status_to))\n                        changeWorkflowState(obj, wfid, status_to)\n                workflow.updateRoleMappingsFor(obj)\n                obj.reindexObject()", "label": 1}
{"index": "gp179169", "code": "def add_paths_to_library_search_paths(param_0, param_1=False, param_2=False, param_3=None, param_4=None):\n    if isinstance(param_0, str):\n        param_0 = [param_0]\n    var_0 = \"-L\"\n    if param_1:\n        var_0 += \"R\"\n    if param_2:\n        param_0 = [p.replace(\" \", \"\\ \") for var_1 in param_0]\n    for var_2 in param_0:\n        var_0 += f\" {var_2}\"\n    if param_3 is not None:\n        if isinstance(param_3, str):\n            param_3 = [param_3]\n        flags[param_3] = flags.get(param_3, {})  \n        if param_4 is not None:\n            flags[param_3][param_4] = flags[param_3].get(param_4, []) + [var_0]\n        else:\n            for var_3 in valid_configs:\n                flags[param_3][var_3] = flags[param_3].get(var_3, []) + [var_0]\n    else:\n        for var_4 in valid_targets:\n            flags[var_4] = flags.get(var_4, {})  \n            if param_4 is not None:\n                flags[var_4][param_4] = flags[var_4].get(param_4, []) + [var_0]\n            else:\n                for var_3 in valid_configs:\n                    flags[var_4][var_3] = flags[var_4].get(var_3, []) + [var_0]", "contrast": "def add_library_search_paths(self, paths, recursive=True, escape=False, target_name=None, configuration_name=None):\n        self.add_search_paths(XCBuildConfigurationFlags.LIBRARY_SEARCH_PATHS, paths, recursive, escape, target_name,\n                              configuration_name)", "label": 1}
{"index": "gp062545", "code": "def set_coords(param_0, param_1=0, param_2=0, param_3=0, param_4=0):\n        self.coords = {}\n        self.coords['x'] = param_1\n        self.coords['y'] = param_2\n        self.coords['z'] = param_3\n        self.coords['t'] = param_4", "contrast": "def set_agent_coords(world, agent, x, y):\n    world[agent]['x'] = x\n    world[agent]['y'] = y", "label": 0}
{"index": "gp103029", "code": "def get_stored_version(param_0):\n    if connection.engine.name == 'sqlite':\n        var_0 = connection.execute('PRAGMA user_version').fetchone()[0]\n        if var_0 == 0:\n            raise VersionIsNotStored\n        return var_0\n    elif connection.engine.name == 'postgresql':\n        try:\n            var_1 = connection                .execute('SELECT version FROM {}.user_version;'.format(POSTGRES_SCHEMA_NAME))                .fetchone()\n            if not var_1:\n                raise VersionIsNotStored\n            var_0 = var_1[0]\n        except ProgrammingError:\n            raise VersionIsNotStored\n        return var_0\n    else:\n        raise DatabaseError('Do not know how to get version from {} engine.'.format(connection.engine.name))", "contrast": "def get_database_version(connection):\n    if connection.engine.dialect.name == 'postgresql': \n        query = \"select userversion from pg_class where relname = 'user_version';\"\n        result = connection.execute(query).fetchone()\n        return result[0]\n    elif connection.engine.dialect.name == 'sqlite':\n        query = \"PRAGMA user_version;\"\n        result = connection.execute(query).fetchone()\n        return result[0]\n    else:\n        raise ValueError(\"Unsupported database.\") ", "label": 0}
{"index": "gp050769", "code": "def ms_to_datetime(param_0, param_1=None):\n    if not isinstance(param_0, (int, long)):\n        raise TypeError('expected integer, not %s' % type(param_0))\n    if param_1 is None:\n        param_1 = mktz()\n    return datetime.datetime.fromtimestamp(param_0 * 1e-3, param_1)", "contrast": "from datetime import datetime, timezone, timedelta\ndef ms_to_datetime(ms_value):\n    return datetime.fromtimestamp(ms_value/1000, tz=timezone.utc)", "label": 0}
{"index": "gp121096", "code": "def upstream(param_0, param_1, param_2):\n        var_0 = requests.post(\n            self.host + param_1,\n            files=param_2['files'] if 'files' in param_2 else {},\n            headers={\n                'Authorization': 'Bearer ' + self.token,\n                'x-falkonry-source':self.sourceHeader\n            },\n            verify=False\n        )\n        if response.status_code == 202 or response.status_code == 200:\n            try:\n                return json.loads(response._content.decode('utf-8'))\n            except Exception as e:\n                return json.loads(response.content)\n        elif response.status_code == 401:\n            raise Exception(json.dumps({'message':'Unauthorized Access'}))\n        else:\n            raise Exception(response.content)", "contrast": "import requests\ndef post_form_data(url: str, form_data: dict) -> requests.Response:\n    headers = {'Content-Type': 'multipart/form-data'}\n    response = requests.post(url, headers=headers, data=form_data, stream=True)\n    return response", "label": 0}
{"index": "gp239023", "code": "def get_dataset_header(param_0):\n    var_0 = param_0['subj_id']\n    var_1 = param_0['start_time']\n    var_2 = param_0['s_freq']\n    var_3 = param_0['chan_name']\n    var_4 = param_0['n_samples']\n    var_5 = param_0['orig']\n    return var_0, var_1, var_2, var_3, var_4, var_5", "contrast": "def return_hdr(self):\n        subj_id = self.task.subject\n        sampling_freq = set(self.task.channels.get(map_lambda=lambda x: x['sampling_frequency']))\n        if len(sampling_freq) > 1:\n            raise ValueError('Multiple sampling frequencies not supported')\n        s_freq = float(next(iter(sampling_freq)))\n        chan_name = self.task.channels.get(map_lambda=lambda x: x['name'])\n        self.chan_name = array(chan_name)\n        orig = self.baseformat.header\n        start_time = orig['start_time']\n        n_samples = orig['n_samples']\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig", "label": 1}
{"index": "gp183031", "code": "def check_ellipsoids(param_0, param_1, param_2):\n    var_0 = 0\n    for var_1 in range(len(param_1)):\n        if var_1 != param_2:\n            if ((param_0[0]-param_1[var_1][0])/param_1[var_1][3])**2 + ((param_0[1]-param_1[var_1][1])/param_1[var_1][4])**2 + ((param_0[2]-param_1[var_1][2])/param_1[var_1][5])**2 <= 1:\n                var_0 += 1\n    return var_0", "contrast": "def overlap(self, x, j=None):\n        q = len(self.within(x, j=j))\n        return q", "label": 1}
{"index": "gp178827", "code": "import twilio.rest.api.v2010.account.usage.record.today as TodayInstance\ndef build_today_instance(param_0: dict) -> TodayInstance:\n    var_0 = TodayInstance(param_0)\n    return var_0", "contrast": "def get_instance(self, payload):\n        return TodayInstance(self._version, payload, account_sid=self._solution['account_sid'], )", "label": 1}
{"index": "gp180059", "code": "import numpy as np\ndef downside_deviation(param_0, param_1, param_2=True, param_3=False, param_4=False):\n    if param_2:\n        var_0 = np.ones(rets.shape[0]) * param_1\n        if param_3:\n            var_1 = np.sqrt(((np.minimum(param_0, var_0) - var_0).cumsum() ** 2) / np.arange(1, rets.shape[0] + 1))\n        else:\n            var_1 = np.sqrt(((np.minimum(param_0, var_0) - var_0) ** 2).mean())\n    else:\n        var_2 = param_0 < param_1\n        if param_3:\n            var_1 = np.sqrt(((param_0[var_2] - param_1) ** 2).cumsum() / np.arange(1, mask.sum() + 1))\n        else:\n            var_1 = np.sqrt(((param_0[var_2] - param_1) ** 2).mean())\n    if param_4:\n        var_1 *= np.sqrt(252)\n    return var_1", "contrast": "def downside_deviation(rets, mar=0, expanding=0, full=0, ann=0):\n    below = rets[rets < mar]\n    if expanding:\n        n = pd.expanding_count(rets)[below.index] if full else pd.expanding_count(below)\n        dd = np.sqrt(((below - mar) ** 2).cumsum() / n)\n        if ann:\n            dd *= np.sqrt(periods_in_year(rets))\n        return dd.reindex(rets.index).ffill()\n    else:\n        n = rets.count() if full else below.count()\n        dd = np.sqrt(((below - mar) ** 2).sum() / n)\n        if ann:\n            dd *= np.sqrt(periods_in_year(rets))\n        return dd", "label": 1}
{"index": "gp048918", "code": "def CountFlowResultsByType(param_0, param_1, param_2, param_3=None):\n    var_0 = (\"SELECT type, COUNT(*) FROM flow_results \"\n             \"FORCE INDEX (flow_results_by_client_id_flow_id_timestamp) \"\n             \"WHERE client_id = %s AND flow_id = %s \"\n             \"GROUP BY type\")\n    var_1 = [db_utils.ClientIDToInt(param_1), db_utils.FlowIDToInt(param_2)]\n    cursor.execute(var_0, var_1)\n    return dict(cursor.fetchall())", "contrast": "def count_flow_results(flow_results):\n    return {result_type: flow_results.count(result_type) for result_type in set(flow_results)}", "label": 0}
{"index": "gp037072", "code": "def reinstall_ruby(param_0, param_1=None, param_2=None):\n    return _rvm(['reinstall', param_0], param_1=param_1, param_2=param_2)", "contrast": "def reinstall_ruby(version, runas=None):\n    cmd = 'rvm reinstall {0}'.format(version)\n    return __salt__['cmd.run'](cmd, runas=runas)", "label": 0}
{"index": "gp091933", "code": "def _split_sympy_quantum_factor(param_0):\n    from qnet.algebra.core.abstract_quantum_algebra import (\n        QuantumExpression, ScalarTimesQuantumExpression)\n    from qnet.algebra.core.scalar_algebra import ScalarValue, ScalarTimes, One\n    if isinstance(param_0, ScalarTimesQuantumExpression):\n        var_0, var_1 = _split_sympy_quantum_factor(expr.coeff)\n        var_1 *= expr.term\n    elif isinstance(param_0, ScalarValue):\n        var_0 = expr.val\n        var_1 = expr._one\n    elif isinstance(param_0, ScalarTimes):\n        var_0 = sympy.S(1)\n        var_1 = expr._one\n        for var_2 in expr.operands:\n            var_3, var_4 = _split_sympy_quantum_factor(var_2)\n            var_0 *= var_3\n            var_1 *= var_4\n    elif isinstance(param_0, sympy.Basic):\n        var_0 = param_0\n        var_1 = One\n    else:\n        var_0 = sympy.S(1)\n        var_1 = param_0\n    assert isinstance(var_0, sympy.Basic)\n    assert isinstance(var_1, QuantumExpression)\n    return var_0, var_1", "contrast": "from sympy import symbols\nfrom qnet.algebra import split_sympy_qnet_product\ndef split_product(expr):\n    sympy_factor, quantum_factor = split_sympy_qnet_product(expr, return_remaining=True)\n    return sympy_function(sympy_factor) * quantum_factor", "label": 0}
{"index": "gp032673", "code": "def get_routes(param_0):\n    var_0 = os.path.join(_DEB_NETWORK_UP_DIR, 'route-{0}'.format(param_0))\n    var_1 = _read_file(var_0)\n    var_0 = os.path.join(_DEB_NETWORK_DOWN_DIR, 'route-{0}'.format(param_0))\n    var_1 += _read_file(var_0)\n    return var_1", "contrast": "def get_routes(interface):\n    import subprocess\n    cmd = \"ip route show dev {}\".format(interface)\n    return subprocess.check_output(cmd, shell=True).decode()", "label": 0}
{"index": "gp086225", "code": "def _parse_nested_interval(param_0, param_1):\n        if param_1[0].isdigit():\n            return self._parse_interval(param_1)\n        elif param_1[0] in ['join', 'order']:\n            return self._parse_join(param_1)\n        elif param_1[0] == 'complement':\n            return self._parse_complement(param_1)\n        raise ValueError('interval {} does not fit pattern.'.format(param_1))", "contrast": "class SuperRange:\n    def __init__(self, *args):\n        self.ranges = []\n        for arg in args:\n            if isinstance(arg, tuple):\n                self.ranges.append(range(arg[0], arg[1]))\n            else:\n                self.ranges.append(arg)\n    def __repr__(self):\n        return f'SuperRange({self.ranges})'\n    def __iter__(self):\n        for r in self.ranges:\n            yield from r\nclass Range:\n    def __init__(self, start, stop=None, step=1):\n        if stop is None:\n            stop = start\n            start = 0\n        self.range = range(start, stop, step)\n    def __repr__(self):\n        return f'Range({self.range.start}, {self.range.stop}, {self.range.step})'\nclass Join:\n    def __init__(self, *ranges):\n        self.ranges = ranges\n    def __repr__(self):\n        return f'Join({self.ranges})'\n    def __iter__(self):\n        for r in self.ranges:\n            yield from r\nclass Complement:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n    def __repr__(self):\n        return f'Complement({self.a}, {self.b})'\n    def __iter__(self):\n        s = set(iter(self.b))\n        for x in iter(self.a):\n            if x not in s:\n                yield x", "label": 0}
{"index": "gp077053", "code": "def normalize_cert_dir():\n    current_cn = get_crt_common_name()\n    if not os.path.isdir(COZY_CONFIG_PATH):\n        print 'Need to create {}'.format(COZY_CONFIG_PATH)\n        os.mkdir(COZY_CONFIG_PATH, 0755)\n    if not os.path.isdir(CERTIFICATES_PATH):\n        print 'Need to create {}'.format(CERTIFICATES_PATH)\n        os.mkdir(CERTIFICATES_PATH, 0755)\n    if not os.path.isdir(ACME_PRIVATE_PATH):\n        print 'Need to create {}'.format(ACME_PRIVATE_PATH)\n        os.mkdir(ACME_PRIVATE_PATH, 0700)\n    if os.path.isfile(OLD_CERTIFICATE_PATH) and            not os.path.islink(OLD_CERTIFICATE_PATH):\n        target = '{}/{}.crt'.format(CERTIFICATES_PATH, current_cn)\n        print 'Move {} to {}'.format(CERTIFICATES_PATH, target)\n        os.rename(OLD_CERTIFICATE_PATH, target)\n    else:\n        print 'Nothing to do for {}'.format(OLD_CERTIFICATE_PATH)\n    if os.path.isfile(OLD_PRIVATE_KEY_PATH) and            not os.path.islink(OLD_PRIVATE_KEY_PATH):\n        target = '{}/{}.key'.format(CERTIFICATES_PATH, current_cn)\n        print 'Move {} to {}'.format(OLD_PRIVATE_KEY_PATH, target)\n        os.rename(OLD_PRIVATE_KEY_PATH, target)\n    else:\n        print 'Nothing to do for {}'.format(OLD_PRIVATE_KEY_PATH)\n    if current_cn:\n        make_links(current_cn)", "contrast": "def convert_old_to_new_certificate(old_cert):\n    return new_cert", "label": 0}
{"index": "gp228119", "code": "import json\ndef dump_json(param_0):\n    return json.dumps(param_0)", "contrast": "def dumps(self) -> str:\n        return json.dumps(self.data, sort_keys=True, indent=4)", "label": 1}
{"index": "gp303188", "code": "def create_shipment_data():\n    var_0 = {\n        'name': 'John Doe',\n        'address': '123 Main Street',\n        'city': 'Anytown',\n        'state': 'CA',\n        'zip': '12345',\n        'weight': '12 lbs'\n    }\n    return var_0", "contrast": "def _prepare_wsdl_objects(self):\n        self.RequestedShipment = self.client.factory.create('RequestedShipment')\n        self.RequestedShipment.ShipTimestamp = datetime.datetime.now()\n        total_weight = self.client.factory.create('Weight')\n        total_weight.Value = 0.0\n        total_weight.Units = 'LB'\n        self.RequestedShipment.TotalWeight = total_weight\n        shipper_party = self.client.factory.create('Party')\n        shipper_party.Address = self.client.factory.create('Address')\n        shipper_party.Contact = self.client.factory.create('Contact')\n        self.RequestedShipment.Shipper = shipper_party\n        recipient_party = self.client.factory.create('Party')\n        recipient_party.Contact = self.client.factory.create('Contact')\n        recipient_party.Address = self.client.factory.create('Address')\n        self.RequestedShipment.Recipient = recipient_party\n        payor = self.client.factory.create('Payor')\n        payor.ResponsibleParty = self.client.factory.create('Party')\n        payor.ResponsibleParty.Address = self.client.factory.create('Address')\n        payor.ResponsibleParty.Address.CountryCode = 'US'\n        shipping_charges_payment = self.client.factory.create('Payment')\n        shipping_charges_payment.Payor = payor\n        shipping_charges_payment.PaymentType = 'SENDER'\n        self.RequestedShipment.ShippingChargesPayment = shipping_charges_payment\n        self.RequestedShipment.LabelSpecification = self.client.factory.create('LabelSpecification')\n        self.RequestedShipment.RateRequestTypes = ['PREFERRED']\n        self.RequestedShipment.PackageCount = 0\n        self.RequestedShipment.RequestedPackageLineItems = []\n        self.logger.debug(self.RequestedShipment)", "label": 1}
{"index": "gp221862", "code": "def instantiate_response_model(param_0, param_1):\n    return param_0(**param_1)", "contrast": "def _instantiate_model(self, response, attrs, additional_properties=None):\n        if callable(response):\n            subtype = getattr(response, '_subtype_map', {})\n            try:\n                readonly = [k for k, v in response._validation.items()\n                            if v.get('readonly')]\n                const = [k for k, v in response._validation.items()\n                         if v.get('constant')]\n                kwargs = {k: v for k, v in attrs.items()\n                          if k not in subtype and k not in readonly + const}\n                response_obj = response(**kwargs)\n                for attr in readonly:\n                    setattr(response_obj, attr, attrs.get(attr))\n                if additional_properties:\n                    response_obj.additional_properties = additional_properties\n                return response_obj\n            except TypeError as err:\n                msg = \"Unable to deserialize {} into model {}. \".format(\n                    kwargs, response)\n                raise DeserializationError(msg + str(err))\n        else:\n            try:\n                for attr, value in attrs.items():\n                    setattr(response, attr, value)\n                return response\n            except Exception as exp:\n                msg = \"Unable to populate response model. \"\n                msg += \"Type: {}, Error: {}\".format(type(response), exp)\n                raise DeserializationError(msg)", "label": 1}
{"index": "gp079039", "code": "def take_damage(param_0, param_1, param_2):\n        if c.name == self.c1.name:\n            self.c1.stats['Health'] = self.c1.stats['Health'] - param_2\n        else:\n            self.c2.stats['Health'] = self.c2.stats['Health'] - param_2", "contrast": "def apply_damage(character, damage):\n    character['HP'] -= damage\n    return character", "label": 0}
{"index": "gp087836", "code": "def convert_attribute_to_string(param_0):\n    if param_0 is None:\n        return param_0\n    elif (sys.hexversion >= 0x03000000 and isinstance(param_0, str))            or (sys.hexversion < 0x03000000            and isinstance(param_0, unicode)):\n        return param_0\n    elif isinstance(param_0, bytes):\n        return value.decode()\n    elif isinstance(param_0, np.unicode_):\n        return str(param_0)\n    elif isinstance(param_0, np.bytes_):\n        return value.decode()\n    else:\n        return None", "contrast": "def attribute_value_to_string(value):\n    if isinstance(value, str):\n        return value\n    elif hasattr(value, '__str__'):\n        return str(value)\n    else:\n        return None", "label": 0}
{"index": "gp327497", "code": "def join(param_0, *param_1):\n    var_0 = set()\n    for var_1 in param_1:\n        if var_1 in index.groups:\n            group.update(index.groups[var_1])\n    return var_0", "contrast": "def join(self, *groupnames):\n        return self._sum([self[k] for k in groupnames if k in self])", "label": 1}
{"index": "gp129040", "code": "def data_dir(param_0='data'):\n    if 'OPENSHIFT_DATA_DIR' in os.environ:\n        return os.environ['OPENSHIFT_DATA_DIR']\n    if 'GONDOR_DATA_DIR' in os.environ:\n        return os.environ['GONDOR_DATA_DIR']\n    if provider.detect() == provider.DOTCLOUD:\n        return os.path.expanduser('~/data')\n    return param_0", "contrast": "import os\ndef get_persistent_data_dir(default='default'):\n    dir_path = os.path.expanduser('~/.local/share')\n    if os.path.isdir(dir_path):\n        return dir_path\n    else:\n        return default", "label": 0}
{"index": "gp320992", "code": "def performTimeStep(param_0, param_1):\n    activateCells(param_0)\n    activateDendrites()\n    if param_1:\n        updatePermanences()\n    return None ", "contrast": "def compute(self, activeColumns, learn=True):\n    self.activateCells(sorted(activeColumns), learn)\n    self.activateDendrites(learn)", "label": 1}
{"index": "gp231339", "code": "import numpy as np\nimport gzip\nimport dill\nfrom typing import Tuple\nfrom cmapPy.pandasGEXpress import GCToo\ndef write_gctoo_to_file(param_0: GCToo, param_1: str, param_2: bool,\n                        param_3: int = 6, param_4: int = 1024, \n                        param_5: np.dtype = np.float32) -> None:\n    with gzip.open(param_1, 'wb', compresslevel=param_3) as var_0:\n        if param_2:\n            gctoo_object.data_df.replace(np.nan, -666, inplace=True)\n            var_1 = {var_2: str(var_3) if isinstance(var_3, np.ndarray) else var_3 for var_2, var_3 in gctoo_object.metadata_df.to_dict('dict').items()}\n            var_1['col_metadata']['profile_id'] = str(var_1['col_metadata']['profile_id'])\n            var_1['data_df'] = None\n            dill.dump(var_1, var_0)\n            var_4 = gctoo_object.data_df.values.astype(param_5)\n            fout.write(data.tobytes())\n        else:\n            var_1 = dill.dumps(gctoo_object.to_dict())\n            fout.write(var_1)\n            var_4 = gctoo_object.data_df.values\n            var_5 = param_4 * 1024\n            var_6 = int(np.ceil(data.nbytes / var_5))\n            for var_7 in range(var_6):\n                var_8 = var_7 * var_5\n                var_9 = var_8 + var_5\n                if var_9 > data.nbytes:\n                    var_9 = data.nbytes\n                fout.write(var_4[var_8:var_9].astype(param_5).tobytes())", "contrast": "def write(gctoo_object, out_file_name, convert_back_to_neg_666=True, gzip_compression_level=6,\n    max_chunk_kb=1024, matrix_dtype=numpy.float32):\n    gctx_out_name = add_gctx_to_out_name(out_file_name)\n    hdf5_out = h5py.File(gctx_out_name, \"w\")\n    write_version(hdf5_out)\n    write_src(hdf5_out, gctoo_object, gctx_out_name)\n    elem_per_kb = calculate_elem_per_kb(max_chunk_kb, matrix_dtype)\n    chunk_size = set_data_matrix_chunk_size(gctoo_object.data_df.shape, max_chunk_kb, elem_per_kb)\n    hdf5_out.create_dataset(data_matrix_node, data=gctoo_object.data_df.transpose().values,\n        dtype=matrix_dtype)\n    write_metadata(hdf5_out, \"col\", gctoo_object.col_metadata_df, convert_back_to_neg_666,\n        gzip_compression=gzip_compression_level)\n    write_metadata(hdf5_out, \"row\", gctoo_object.row_metadata_df, convert_back_to_neg_666,\n        gzip_compression=gzip_compression_level)\n    hdf5_out.close()", "label": 1}
{"index": "gp252712", "code": "def parse_affine(param_0):\n    import numpy as np\n    var_0 = np.zeros((4, 4))\n    for var_1 in range(4):\n        for var_2 in range(4):\n            var_0[var_1, var_2] = header_data.get(f\"q{var_1+1}_{var_2+1}\", 0)\n    return var_0[:3,:3], var_0[:3,3]", "contrast": "def parse_affine(self, hdat, dataobj=None):\n        if 'affine' in hdat: return to_affine(hdat['affine'])\n        else:                return to_affine(self.default_affine())", "label": 1}
{"index": "gp177681", "code": "def replace_variable_name(param_0, param_1, param_2):\n    def rv(param_0):\n        return getattr(param_0, param_0)\n    return rv(param_2)", "contrast": "def parse_declaration_expressn_memberaccess(self, lhsAST, rhsAST, es):\n        if isinstance(lhsAST, wdl_parser.Terminal):\n            es = es + lhsAST.source_string\n        elif isinstance(lhsAST, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(lhsAST, wdl_parser.AstList):\n            raise NotImplementedError\n        es = es + '_'\n        if isinstance(rhsAST, wdl_parser.Terminal):\n            es = es + rhsAST.source_string\n        elif isinstance(rhsAST, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(rhsAST, wdl_parser.AstList):\n            raise NotImplementedError\n        return es", "label": 1}
{"index": "gp264651", "code": "def access_for(param_0, param_1=None, param_2=None):\n    if param_2 is None:\n        param_2 = self.roles_for(param_1=param_1)\n    class AccessControlledObject(object):\n        var_0 = ('_obj', '_read', '_write')\n        def __init__(param_0, param_1):\n            self._obj = param_1\n            self._read = set()\n            self._write = set()\n        def __getattr__(param_0, param_1):\n            if param_1 in self._read and param_1 not in self._write:\n                return getattr(self._obj, param_1)\n            raise AttributeError(param_1)\n        def __setattr__(param_0, param_1, param_2):\n            if param_1 in self._write:\n                setattr(self._obj, param_1, param_2)\n            elif param_1 in self._read:\n                raise AttributeError(\"can't set attribute\")\n            else:\n                raise AttributeError(param_1)\n    return AccessControlledObject(param_0)._obj\n", "contrast": "def access_for(self, roles=None, actor=None, anchors=[]):\n        if roles is None:\n            roles = self.roles_for(actor=actor, anchors=anchors)\n        elif actor is not None or anchors:\n            raise TypeError('If roles are specified, actor/anchors must not be specified')\n        return RoleAccessProxy(self, roles=roles)", "label": 1}
{"index": "gp056792", "code": "def transmit_tc_bpdu(param_0):\n        if not self.send_tc_flg:\n            var_0 = datetime.timedelta(seconds=self.port_times.max_age\n                                       + self.port_times.forward_delay)\n            self.send_tc_timer = datetime.datetime.today() + var_0\n            self.send_tc_flg = True", "contrast": "def set_send_tc_flg():\n    send_tc_flg = True\n    return send_tc_flg", "label": 0}
{"index": "gp271632", "code": "def assure_window_area(param_0, param_1):\n    if param_0[0] < param_1[0]:\n        param_1[0] = param_0[0]\n    if param_0[1] < param_1[1]:\n        param_1[1] = param_0[1]\n    if param_0[2] > param_1[2]:\n        param_1[2] = param_0[2]\n    if param_0[3] > param_1[3]:\n        param_1[3] = param_0[3]\n    return param_1", "contrast": "def align(self):\n        nofs = align_to_mmap(self.ofs, 0)\n        self.size += self.ofs - nofs    \n        self.ofs = nofs\n        self.size = align_to_mmap(self.size, 1)", "label": 1}
{"index": "gp192576", "code": "import matplotlib.pyplot as plt\ndef visualize_state():\n    var_0 = plt.figure()\n    return var_0", "contrast": "def plot(self):\n        width = 10\n        height = width / 1.618\n        f = plt.figure(figsize=(width, height))\n        ax = f.add_subplot(111, projection=\"3d\")\n        self.plot_state_histogram(ax)\n        return f", "label": 1}
{"index": "gp058389", "code": "def _refresh_authentication_token(param_0):\n        if self.retry == self._MAX_RETRIES:\n            raise GeocoderAuthenticationFailure(\n                'Too many retries for auth: %s' % self.retry\n            )\n        var_0 = {\n            'username': self.username,\n            'password': self.password,\n            'referer': self.referer,\n            'expiration': self.token_lifetime,\n            'f': 'json'\n        }\n        var_1 = \"?\".join((self.auth_api, urlencode(var_0)))\n        logger.debug(\n            \"%s._refresh_authentication_token: %s\",\n            self.__class__.__name__, var_1\n        )\n        self.token_expiry = int(time()) + self.token_lifetime\n        var_2 = self._base_call_geocoder(var_1)\n        if 'token' not in var_2:\n            raise GeocoderAuthenticationFailure(\n                'Missing token in auth request.'\n                'Request URL: %s; response JSON: %s' %\n                (var_1, json.dumps(var_2))\n            )\n        self.retry = 0\n        self.token = var_2['token']", "contrast": "import requests\ndef get_new_token(username, password, server):\n    payload = {\n        'f': 'json',\n        'username': username,\n        'password': password,\n        'referer': server\n    }\n    response = requests.post(server + '/arcgis/tokens/generateToken', data=payload)\n    token = response.json().get('token')\n    return token", "label": 0}
{"index": "gp185723", "code": "import numpy as np\ndef pre_process(param_0, param_1=None, param_2=None, param_3=None, param_4=None):\n    if param_1 is None:\n        param_1 = np.empty_like(param_0)\n    if param_2:\n        var_0 = np.empty_like(param_0)\n        np.copyto(var_0, param_0)\n        var_1 = np.fft.ifft2(var_0)\n        np.copyto(param_1, np.fft.fft2(var_1))\n    elif param_3:\n        var_2 = np.fft.rfft2(param_0)\n        np.copyto(param_1, var_2)\n    elif param_4:\n        var_1 = np.fft.irfft2(param_0)\n        np.copyto(param_1, var_1)\n    return param_1", "contrast": "def _preprocess(self, x, out=None):\n        if out is None:\n            if self.domain.field == ComplexNumbers():\n                out = self._tmp_r if self._tmp_r is not None else self._tmp_f\n            elif self.domain.field == RealNumbers() and not self.halfcomplex:\n                out = self._tmp_f\n            else:\n                out = self._tmp_r\n        return dft_preprocess_data(\n            x, shift=self.shifts, axes=self.axes, sign=self.sign,\n            out=out)", "label": 1}
{"index": "gp336183", "code": "def total_propulsion_power(param_0, param_1):\n    return param_0 * param_1", "contrast": "def prop_power(self, propulsion_eff=0.7, sea_margin=0.2):\n        PP = (1 + sea_margin) * self.resistance() * self.speed/propulsion_eff\n        return PP", "label": 1}
{"index": "gp243999", "code": "import signal\ndef catch_signals(param_0):\n    def signal_handler(param_0, param_1):\n        param_0(param_0)\n    signal.signal(signal.SIGINT, signal_handler) \n    signal.signal(signal.SIGTERM, signal_handler) ", "contrast": "def signal_catcher(callback):\n    def _catch_exit_signal(sig_num, _frame):\n        print('Received signal {:d} invoking callback...'.format(sig_num))\n        callback()\n    signal.signal(signal.SIGINT, _catch_exit_signal)\n    signal.signal(signal.SIGQUIT, _catch_exit_signal)\n    signal.signal(signal.SIGTERM, _catch_exit_signal)\n    yield", "label": 1}
{"index": "gp064800", "code": "def render(param_0, param_1={}, param_2=None, param_3=None, **param_4):\n        if not param_2:\n            var_0 = inspect.stack()[1]\n            var_1 = inspect.getmodule(param_0).__name__\n            var_2 = module.split(\".\")[-1]\n            var_3 = var_0[3]      \n            var_4 = cls.__name__    \n            if view_name.endswith(\"View\"):\n                var_4 = var_4[:-4]\n            param_2 = \"%s/%s.html\" % (var_4, var_3)\n        param_1 = param_1 if param_1 else dict()\n        param_1[\"__\"] = cls._context if cls._context else {}\n        if param_4:\n            data.update(param_4)\n        param_1[\"__view_template__\"] = param_2\n        return render_template(param_3 or cls.LAYOUT, **param_1)", "contrast": "def render_data_to_template(data, view_template=None, layout=None):\n    if not view_template:\n        view_template = f\"{type(data).__name__.lower()}/action.html\"\n    if not layout:\n        layout = \"{% include __view_template__ %}\"\n    template = Template(layout)\n    return template.render(data=data, __view_template__=view_template)", "label": 0}
{"index": "gp229121", "code": "@property\ndef file_name(param_0):\n    return self._file_name\n@file_name.setter\ndef file_name(param_0, param_1):\n    self._file_name = param_1", "contrast": "def file_name(self, value):\n        if value == self._defaults['fileName'] and 'fileName' in self._values:\n            del self._values['fileName']\n        else:\n            self._values['fileName'] = value", "label": 1}
{"index": "gp212338", "code": "import base64\nimport Crypto\nfrom Crypto.Cipher import AES\ndef decrypt_content(param_0: str, param_1: str, param_2: Account) -> str:\n    var_0 = account.get_secret_key(param_0)\n    var_1 = AES.new(var_0, AES.MODE_CBC, IV=var_0[:16])\n    var_2 = base64.b64decode(param_1)\n    var_3 = cipher.decrypt(var_2)\n    return decrypted_content_bytes.decode('utf-8')", "contrast": "def decrypt(self, document_id, encrypted_content, account):\n        return self._secret_store(account).decrypt_document(document_id, encrypted_content)", "label": 1}
{"index": "gp157015", "code": "def set_location(param_0, param_1, param_2, param_3=10):\n        var_0 = self._current_application()\n        driver.set_location(param_1,param_2,param_3)", "contrast": "def set_location(latitude, longitude, altitude=10):\n    if platform.system() == 'Windows':\n        raise Exception('This function can only be used in Android devices')\n    driver = BuiltIn().get_library_instance('SeleniumLibrary').driver\n    driver.execute_script(f'geo_position = {{{{ \"latitude\": \"{latitude}\", \"longitude\": \"{longitude}\", \"altitude\": \"{altitude}\" }}}}; return navigator.geolocation.setCurrentPosition((position) => console.log(position.coords, position.timestamp), (error) => console.error(error), geo_position);')", "label": 0}
{"index": "gp268871", "code": "def create_network(param_0):\n    with open(param_0, 'r') as var_0:\n        var_1 = json.load(var_0)\n    return var_1", "contrast": "def create_network(self):\n        class_ = getattr(networks, self.network_class)\n        return class_(max_size=self.quorum)", "label": 1}
{"index": "gp012364", "code": "def get_widgets_that_need_update(param_0):\n        var_0 = []\n        for var_1, var_2 in self.get_widgets().items():\n            if widget.should_update():\n                result.append(var_2)\n        return var_0", "contrast": "def get_updated_widgets():\n    updated_widgets = [widget1, widget2, widget3] \n    return updated_widgets", "label": 0}
{"index": "gp153136", "code": "def _chk_qualifier(param_0, param_1, param_2, param_3):\n        for var_0 in param_1:\n            if var_0 not in AnnoReaderBase.exp_qualifiers:\n                var_1 = 'UNEXPECTED QUALIFIER({QUAL})'.format(QUAL=var_0)\n                self.illegal_lines[var_1].append((param_3, \"\\t\".join(param_2)))", "contrast": "def check_qualifiers(qualifiers, expected_values):\n    for qual in qualifiers:\n        if qual not in expected_values:\n            return False\n    return True", "label": 0}
{"index": "gp104319", "code": "def delete_url(param_0, param_1):\n        for var_0 in [False, True]:\n            var_1 = (param_1, var_0)\n            if var_1 in self._local_paths:\n                var_2 = self._local_paths[var_1]\n                remove(var_2)\n                del self._local_paths[var_1]\n            var_2 = self.local_path(\n                param_1, var_0=var_0, download=False)\n            if exists(var_2):\n                remove(var_2)", "contrast": "import os\ndef delete_local_files(url):\n    for root, dirs, files in os.walk(\".\"):\n        for file in files:\n            if file.startswith(url.rsplit('/', 1)[-1]):\n                os.remove(os.path.join(root,file))\n                print(f\"{os.path.join(root,file)} deleted successfully!\")", "label": 0}
{"index": "gp210807", "code": "def get_signal_header():\n    var_0 = {'name': 'Signal Name', 'type': 'Signal Type', 'units': 'Signal Units'}\n    return var_0", "contrast": "def getSignalHeader(self, chn):\n        return {'label': self.getLabel(chn),\n                'dimension': self.getPhysicalDimension(chn),\n                                 'sample_rate': self.getSampleFrequency(chn),\n                'physical_max':self.getPhysicalMaximum(chn),\n                'physical_min': self.getPhysicalMinimum(chn),\n                'digital_max': self.getDigitalMaximum(chn),\n                'digital_min': self.getDigitalMinimum(chn),\n                'prefilter':self.getPrefilter(chn),\n                'transducer': self.getTransducer(chn)}", "label": 1}
{"index": "gp324499", "code": "def remove_groups(param_0):\n    for var_0 in param_0:\n        for var_1 in group.metabolites:\n            if m.id in model.metabolites:\n                continue\n            else:\n                group.remove_member(var_1)\n        for var_2 in group.reactions:\n            if r.id in model.reactions:\n                continue\n            else:\n                group.remove_member(var_2)\n        for var_3 in group.genes:\n            if g.id in model.genes:\n                continue\n            else:\n                group.remove_member(var_3)\n    return model", "contrast": "def remove_groups(self, group_list):\n        if isinstance(group_list, string_types) or                hasattr(group_list, \"id\"):\n            warn(\"need to pass in a list\")\n            group_list = [group_list]\n        for group in group_list:\n            if group.id not in self.groups:\n                LOGGER.warning(\"%r not in %r. Ignored.\", group, self)\n            else:\n                self.groups.remove(group)\n                group._model = None", "label": 1}
{"index": "gp313367", "code": "import functools\ndef defer_sync(param_0):\n    @functools.wraps(param_0)\n    def wrapper(*param_0, **param_1):\n        var_0 = param_0(*param_0, **param_1)\n        return var_0\n    return wrapper", "contrast": "def defer_entity_syncing(wrapped, instance, args, kwargs):\n    sync_entities.defer = True\n    try:\n        return wrapped(*args, **kwargs)\n    finally:\n        sync_entities.defer = False\n        model_objs = list(sync_entities.buffer.values())\n        if None in sync_entities.buffer:\n            model_objs = list()\n        if len(sync_entities.buffer):\n            sync_entities(*model_objs)\n        sync_entities.buffer = {}", "label": 1}
{"index": "gp305268", "code": "from pybel import readstring\ndef convert_to_pybel(param_0, param_1=True):\n    var_0 = f\"{param_0['molecule']['atoms']}\\n{param_0['molecule']['bonds']}\"\n    if param_1:\n        var_0 += 'InChI=1S/'\n        for var_1 in param_0['molecule']['atoms']:\n            var_0 += f\"{var_1['element']}\"\n            if 'charge' in var_1:\n                var_0 += f\"{abs(var_1['charge'])}{'+-'[var_1['charge'] < 0]}\"\n        var_2 = readstring(\"smi\", var_0)\n        return var_2\n    else:\n        var_2 = readstring(\"json\", json.dumps(param_0))\n        return var_2", "contrast": "def json_to_pybel(data, infer_bonds=False):\n    obmol = ob.OBMol()\n    obmol.BeginModify()\n    for atom in data['atoms']:\n        obatom = obmol.NewAtom()\n        obatom.SetAtomicNum(table.GetAtomicNum(str(atom['element'])))\n        obatom.SetVector(*atom['location'])\n        if 'label' in atom:\n            pd = ob.OBPairData()\n            pd.SetAttribute('_atom_site_label')\n            pd.SetValue(atom['label'])\n            obatom.CloneData(pd)\n    if 'bonds' not in data or not data['bonds']:\n        if infer_bonds:\n            obmol.ConnectTheDots()\n            obmol.PerceiveBondOrders()\n    else:\n        for bond in data['bonds']:\n            if 'atoms' not in bond:\n                continue\n            obmol.AddBond(bond['atoms'][0] + 1, bond['atoms'][1] + 1,\n                          bond['order'])\n    if 'unitcell' in data:\n        uc = ob.OBUnitCell()\n        uc.SetData(*(ob.vector3(*v) for v in data['unitcell']))\n        uc.SetSpaceGroup('P1')\n        obmol.CloneData(uc)\n    obmol.EndModify()\n    mol = pybel.Molecule(obmol)\n    if 'charge' in data['atoms'][0]:\n        mol.OBMol.SetPartialChargesPerceived()\n        for atom, pyatom in zip(data['atoms'], mol.atoms):\n            pyatom.OBAtom.SetPartialCharge(atom['charge'])\n    return mol", "label": 1}
{"index": "gp017931", "code": "def masked_rec_array_to_mgr(param_0, param_1, param_2, param_3, param_4):\n    var_0 = data.fill_value\n    var_1 = ma.getdata(param_0)\n    if param_1 is None:\n        param_1 = get_names_from_index(var_1)\n        if param_1 is None:\n            param_1 = ibase.default_index(len(param_0))\n    param_1 = ensure_index(param_1)\n    if param_2 is not None:\n        param_2 = ensure_index(param_2)\n    var_2, var_3 = to_arrays(var_1, param_2)\n    var_4 = []\n    for var_5, var_6, var_7 in zip(var_0, var_2, var_3):\n        var_8 = ma.getmaskarray(param_0[var_7])\n        if mask.any():\n            var_6, var_5 = maybe_upcast(var_6, var_0=var_5, param_4=True)\n            var_6[var_8] = var_5\n        new_arrays.append(var_6)\n    var_2, var_3 = reorder_arrays(var_4, var_3, param_2)\n    if param_2 is None:\n        param_2 = var_3\n    var_9 = arrays_to_mgr(var_2, var_3, param_1, param_2, param_3)\n    if param_4:\n        var_9 = mgr.copy()\n    return var_9", "contrast": "import numpy.ma as ma\nimport pandas as pd\ndef create_manager(masked_array):\n    mask = masked_array.mask\n    unmasked_array = ma.masked_array(masked_array.data, mask=mask)\n    df = pd.DataFrame(unmasked_array)\n    if hasattr(masked_array, 'dtype'):\n        fields = masked_array.dtype.names\n        if fields:\n            df.columns = fields\n    return pd.DataFrame(data=df).manager", "label": 0}
{"index": "gp298574", "code": "def parse_accept_header(param_0: str, param_1: bool = False) -> Tuple[str, str]:\n    if not param_0:\n        return \"application/rdf+xml\", \"xml\"\n    var_0 = [a.strip() for var_1 in accept_header.split(\",\")]\n    if \"*/*\" in var_0:\n        return \"application/rdf+xml\", \"xml\"\n    var_2 = {\n        \"application/json\": \"json\",\n        \"application/n-triples\": \"nt\",\n        \"application/ld+json\": \"json-ld\",\n        \"application/rdf+xml\": \"xml\",\n        \"text/turtle\": \"ttl\"\n    }\n    var_3 = None\n    var_4 = None\n    var_5 = 0.0\n    for var_6 in var_0:\n        var_7 = av.split(\";\")\n        var_8 = var_7[0].strip()\n        if var_8 in var_2:\n            var_9 = known_mimetypes.get(var_8)\n            var_10 = 1.0\n            for var_11 in var_7[1:]:\n                var_11 = p.strip()\n                if p.startswith(\"q=\"):\n                    var_10 = float(var_11[2:])\n            if var_10 == 1.0:\n                return var_8, var_9\n            if var_3 is None or var_10 > var_5:\n                var_3 = var_8\n                var_4 = var_9\n                var_5 = var_10\n    if var_3 is None:\n        return None, None\n    else:\n        return var_3, var_4 if not param_1 else var_4+\".nq\"", "contrast": "def decide(self, accepts, context_aware=False):\n  mimetype = self.decide_mimetype(accepts, context_aware)\n  if mimetype is not None:\n\t\t\treturn (mimetype, self.get_serialize_format(mimetype))\n  else:\n\t\t\treturn (None, None)", "label": 1}
{"index": "gp156443", "code": "def _on_event(param_0, param_1):\n        if self.has_option(param_1):\n            var_0 = self.get_option(param_1).upper()\n            if var_0 not in [\"NO ACTION\", \"RESTRICT\"]:\n                return var_0\n        return False", "contrast": "def get_referential_action(event):\n    referential_actions = {'INSERT': 'CASCADE', 'UPDATE': 'CASCADE', 'DELETE': 'CASCADE'}\n    return referential_actions.get(event.upper())", "label": 0}
{"index": "gp276722", "code": "def delete_project(param_0):\n    try:\n        return \"Project {} deleted successfully.\".format(param_0)\n    except:\n        raise ItemNotFound(\"Project {} not found.\".format(param_0))", "contrast": "def delete_project(self, project_name):\n        project = self._get_project_for_name(project_name)\n        project.delete()\n        self.clear_project_cache()", "label": 1}
{"index": "gp167663", "code": "def DbGetDeviceAttributeList(param_0, param_1):\n        self._log.debug(\"In DbGetDeviceAttributeList()\")\n        var_0 = param_1[0]\n        var_1 = param_1[1]\n        if not var_1:\n            var_1 = \"%\"\n        else:\n            var_1 = replace_wildcard(var_1)\n        return self.db.get_device_attribute_list(var_0, var_1)", "contrast": "import tango\ndef get_matching_attributes(argin):\n    device_name, wildcard = argin\n    device_proxy = tango.DeviceProxy(device_name)\n    attribute_list = device_proxy.get_attribute_list()\n    matching_attributes = [attribute for attribute in attribute_list if wildcard in attribute]\n    return tango.DevVarStringArray(matching_attributes)", "label": 0}
{"index": "gp174639", "code": "import yaml\ndef load_parameters(param_0):\n    with open(param_0, 'r') as var_0:\n        var_1 = yaml.safe_load(var_0)\n    return var_1", "contrast": "def load_params(fname: str) -> Tuple[Dict[str, mx.nd.NDArray], Dict[str, mx.nd.NDArray]]:\n    save_dict = mx.nd.load(fname)\n    arg_params = {}\n    aux_params = {}\n    for k, v in save_dict.items():\n        tp, name = k.split(':', 1)\n        if tp == 'arg':\n            if \"att_enc_kv2h_weight\" in name:\n                logger.info(\"Splitting '%s' parameters into separate k & v matrices.\", name)\n                v_split = mx.nd.split(v, axis=0, num_outputs=2)\n                arg_params[name.replace('kv2h', \"k2h\")] = v_split[0]\n                arg_params[name.replace('kv2h', \"v2h\")] = v_split[1]\n            else:\n                arg_params[name] = v\n        if tp == 'aux':\n            aux_params[name] = v\n    return arg_params, aux_params", "label": 1}
{"index": "gp078954", "code": "def is_numeric(param_0):\n    try:\n        param_0+param_0, param_0-param_0, param_0*param_0, param_0**param_0, param_0/param_0\n    except ZeroDivisionError:\n        return True\n    except Exception:\n        return False\n    else:\n        return True", "contrast": "def is_numeric(obj):\n    try:\n        float(obj)\n        return True\n    except ValueError:\n        return False", "label": 0}
{"index": "gp111586", "code": "def get_plugs_mail_classes(param_0, param_1):\n        var_0 = []\n        var_1 = self.get_members(param_1)\n        for var_2 in var_1:\n            var_3, var_4 = var_2\n            if inspect.isclass(var_4) and issubclass(var_4, PlugsMail) and var_3 != 'PlugsMail':\n                var_5 = self.get_template_files(app.__file__, var_3)\n                for var_6 in var_5:\n                    try:\n                        var_7 = cls.description\n                        var_8 = var_6\n                        var_9 = self.get_template_language(var_8)\n                        classes.append((var_3, var_8, var_7, var_9))\n                    except AttributeError:\n                        raise AttributeError('Email class must specify email description.')\n        return var_0", "contrast": "def convert_list_of_tuples_to_list_of_dicts(tuples_list):\n    return [dict(t) for t in tuples_list]", "label": 0}
{"index": "gp247491", "code": "def direct_children(param_0):\n    for var_0 in obj.children:\n        yield var_0", "contrast": "def _children(self):\n        if self.method_of:\n            yield self.method_of\n        for codeobj in self.arguments:\n            if isinstance(codeobj, CodeExpression):\n                yield codeobj", "label": 1}
{"index": "gp206629", "code": "def check_axis(param_0):\n    return hasattr(param_0, 'axis') and hasattr(param_0, 'sel_axis')", "contrast": "def require_axis(f):\n    @wraps(f)\n    def _wrapper(self, *args, **kwargs):\n        if None in (self.axis, self.sel_axis):\n            raise ValueError('%(func_name) requires the node %(node)s '\n                    'to have an axis and a sel_axis function' %\n                    dict(func_name=f.__name__, node=repr(self)))\n        return f(self, *args, **kwargs)\n    return _wrapper", "label": 1}
{"index": "gp137227", "code": "def list_route_advertised_from_bgp_speaker(param_0, param_1, **param_2):\n        return self.get((self.bgp_speaker_path % param_1) +\n                        \"/get_advertised_routes\", params=param_2)", "contrast": "def fetch_bgp_routes(bgp_speaker_ip):\n    routes = []\n    bgp_speaker = connect_to_bgp_speaker(bgp_speaker_ip)\n    routes = bgp_speaker.get_routes()\n    return routes", "label": 0}
{"index": "gp280239", "code": "def list_type(param_0, *param_1):\n    return List[param_0, *param_1]", "contrast": "def p_list_type(self, p):\n        p[0] = ast.ListType(value_type=p[3], annotations=p[5])", "label": 1}
{"index": "gp128157", "code": "def _parse_auth_message(param_0, param_1):\n        var_0 = {}\n        var_1 = False\n        for var_2 in REGEXES_INVALID_USER:\n            var_3 = re.search(var_2, param_1)\n            if var_3 and not var_1:\n                var_1 = True\n                var_0['username'] = m.group('user')\n                var_0['ip'] = m.group('ip')\n        for var_2 in REGEXES_INVALID_IP:\n            var_3 = re.search(var_2, param_1)\n            if var_3 and not var_1:\n                var_1 = True\n                var_0['ip'] = m.group('ip')                        \n        for var_2 in REGEXES_IGNORE:\n            var_3 = re.search(var_2, param_1)\n            if var_3 and not var_1:\n                var_1 = True\n        if not var_1:\n            sys.stderr.write(\"Unhandled auth message: %s\\n\" % param_1)\n        return var_0", "contrast": "import re\ndef parse_message(auth_message):\n    pattern = r'((?:\\d{1,3}\\.){3}\\d{1,3})|@(\\w+)'\n    matches = re.findall(pattern, auth_message)\n    result = {}\n    for match in matches:\n        if match[0]:\n            result['ip'] = match[0]\n        elif match[1]:\n            result['user'] = match[1]\n    return result", "label": 0}
{"index": "gp105800", "code": "def get_in_segmentlistdict(self, process_ids = None):\n  seglists = segments.segmentlistdict()\n  for row in self:\n\t\t\tifos = row.instruments or (None,)\n   if process_ids is None or row.process_id in process_ids:\n\t\t\t\tseglists.extend(dict((ifo, segments.segmentlist([row.in_segment])) for ifo in ifos))\n  return seglists", "contrast": "def segmentlistdict(instruments_table, process_ids=None):\n    segmentlistdict = {}\n    for row in instruments_table:\n        if process_ids is None or row['process_id'] in process_ids:\n            instrument = row['instrument']\n            segment = row['segment']\n            if instrument not in segmentlistdict:\n                segmentlistdict[instrument] = []\n            segmentlistdict[instrument].append(segment)\n    return segmentlistdict", "label": 0}
{"index": "gp154569", "code": "def remove_invalid_fields(param_0, param_1, param_2, param_3, param_4):\n        var_0 = [\n            var_1[0] for var_1 in self.get_valid_fields(param_1, param_3,\n                                                      {'request': param_4})\n        ]\n        var_2 = [\n            var_3 for var_3 in param_2\n            if format_value(term.replace(\".\", \"__\").lstrip('-'), \"underscore\") not in var_0\n        ]\n        if var_2:\n            raise ValidationError('invalid sort parameter{}: {}'.format(\n                ('s' if len(var_2) > 1 else ''), ','.join(var_2)))\n        var_4 = []\n        for var_1 in param_2:\n            var_5 = item.replace(\".\", \"__\")\n            if item_rewritten.startswith('-'):\n                underscore_fields.append(\n                    '-' + format_value(item_rewritten.lstrip('-'), \"underscore\"))\n            else:\n                underscore_fields.append(format_value(var_5, \"underscore\"))\n        return super(OrderingFilter, param_0).remove_invalid_fields(\n            param_1, var_4, param_3, param_4)", "contrast": "from rest_framework.exceptions import ValidationError\nfrom rest_framework.filters import OrderingFilter\nclass CustomOrderingFilter(OrderingFilter):\n    def remove_invalid_fields(self, queryset, fields, view, request):\n        valid_fields = self.get_valid_fields(queryset, view, {'request': request})\n        invalid_fields = set(fields) - set(valid_fields)\n        if invalid_fields:\n            error_message = 'Invalid sort parameter(s): {}'.format(', '.join(invalid_fields))\n            raise ValidationError({'detail': error_message})\n        return super().remove_invalid_fields(queryset, fields, view, request)", "label": 0}
{"index": "gp054705", "code": "def get_traffic(param_0, param_1):\n        var_0 = self.config['subreddit_traffic'].format(\n            param_1=six.text_type(param_1))\n        return self.request_json(var_0)", "contrast": "import requests\ndef get_traffic_stats(subreddit):\n    url = f\"https://www.reddit.com/r/{subreddit}/about/traffic.json\"\n    headers = {'User-Agent': 'Mozilla/5.0'}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()", "label": 0}
{"index": "gp331347", "code": "import dateutil.parser\nfrom datetime import datetime\ndef determine_timestamp_format(param_0):\n    var_0 = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%d/%m/%Y %H:%M:%S', '%d/%m/%Y']\n    for var_1 in var_0:\n        try:\n            datetime.strptime(param_0, var_1)\n            return var_1\n        except ValueError:\n            continue\n    try:\n        dateutil.parser.parse(param_0)\n        return \"unknown format, parsed by dateutil.parser\"\n    except ValueError:\n        return \"unable to determine format\"", "contrast": "def detect_timestamp_format(timestamp):\n  time_formats = {\n      'epoch': re.compile(r'^[0-9]{10}$'),\n      'epoch_ms': re.compile(r'^[0-9]{13}$'),\n      'epoch_fraction': re.compile(r'^[0-9]{10}\\.[0-9]{3,9}$'),\n      '%Y-%m-%d %H:%M:%S': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y-%m-%dT%H:%M:%S': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y-%m-%d_%H:%M:%S': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y-%m-%d %H:%M:%S.%f': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y-%m-%dT%H:%M:%S.%f': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y-%m-%d_%H:%M:%S.%f': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y%m%d %H:%M:%S': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y%m%dT%H:%M:%S': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y%m%d_%H:%M:%S': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y%m%d %H:%M:%S.%f': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y%m%dT%H:%M:%S.%f': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y%m%d_%H:%M:%S.%f': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%H:%M:%S': re.compile(r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%H:%M:%S.%f': re.compile(r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y-%m-%dT%H:%M:%S.%f%z': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+[+-][0-9]{4}$')\n  }\n  for time_format in time_formats:\n    if re.match(time_formats[time_format], timestamp):\n      return time_format\n  return 'unknown'", "label": 1}
{"index": "gp312588", "code": "def get_stream_info(param_0, param_1, param_2):\n    var_0 = f\"Size: {param_0}, Frame Number: {param_1}, Info Messages:\\n\"\n    for var_1 in param_2:\n        if var_1 and param_1 != -1:\n            var_0 += f\"\\t- {var_1}\\n\"\n    return var_0", "contrast": "def info_string(self, size=None, message='', frame=-1):\n        info = []\n        if size is not None:\n            info.append('Size: {1}x{0}'.format(*size))\n        elif self.size is not None:\n            info.append('Size: {1}x{0}'.format(*self.size))\n        if frame >= 0:\n            info.append('Frame: {}'.format(frame))\n        if message != '':\n            info.append('{}'.format(message))\n        return ' '.join(info)", "label": 1}
{"index": "gp186150", "code": "import boto3\nfrom botocore.exceptions import ClientError\ndef send_email_with_ses(param_0, param_1, param_2, param_3):\n    var_0 = 'us-east-1'\n    var_1 = 'UTF-8'\n    var_2 = 'sender@example.com'\n    var_3 = boto3.client('ses', region_name=var_0)\n    try:\n        var_4 = client.send_email(\n            Destination={\n                'ToAddresses': param_0,\n            },\n            Message={\n                'Body': {\n                    'Html': {\n                        'Charset': var_1,\n                        'Data': param_2,\n                    },\n                    'Text': {\n                        'Charset': var_1,\n                        'Data': param_3,\n                    },\n                },\n                'Subject': {\n                    'Charset': var_1,\n                    'Data': param_1,\n                },\n            },\n            Source=var_2,\n        )\n    except ClientError as e:\n        print(e.response['Error']['Message'])\n    else:\n        return None", "contrast": "def __send_ses_email(self, recipients, subject, body_html, body_text):\n        source_arn = dbconfig.get('source_arn', NS_EMAIL)\n        return_arn = dbconfig.get('return_path_arn', NS_EMAIL)\n        session = get_local_aws_session()\n        ses = session.client('ses', region_name=dbconfig.get('ses_region', NS_EMAIL, 'us-west-2'))\n        body = {}\n        if body_html:\n            body['Html'] = {\n                'Data': body_html\n            }\n        if body_text:\n            body['Text'] = {\n                'Data': body_text\n            }\n        ses_options = {\n            'Source': self.sender,\n            'Destination': {\n                'ToAddresses': recipients\n            },\n            'Message': {\n                'Subject': {\n                    'Data': subject\n                },\n                'Body': body\n            }\n        }\n        if source_arn and return_arn:\n            ses_options.update({\n                'SourceArn': source_arn,\n                'ReturnPathArn': return_arn\n            })\n        ses.send_email(**ses_options)", "label": 1}
{"index": "gp310165", "code": "from math import log2\ndef calculate_entropy(param_0, param_1, param_2):\n    var_0 = 0\n    var_1 = 0\n    for var_2 in param_0:\n        if var_2[param_1] == param_2:\n            var_0 += 1\n            if var_2[-1] == 'yes':\n                var_1 += 1\n    if var_0 == 0:\n        return 0\n    if var_1 == 0 or var_1 == var_0:\n        return 0\n    var_3 = var_1 / var_0\n    var_4 = 1 - var_3\n    var_5 = -(var_3 * log2(var_3) + var_4 * log2(var_4))\n    return var_5", "contrast": "def get_entropy(self, attr_name=None, attr_value=None):\n        is_con = self.tree.data.is_continuous_class\n        if is_con:\n            if attr_name is None:\n                var = self._class_cdist.variance\n            else:\n                var = self._attr_value_cdist[attr_name][attr_value].variance\n            if self.tree.metric == VARIANCE1 or attr_name is None:\n                return var\n            elif self.tree.metric == VARIANCE2:\n                unique_value_count = len(self._attr_value_counts[attr_name])\n                attr_total = float(self._attr_value_count_totals[attr_name])\n                return var*(unique_value_count/attr_total)\n        else:\n            if attr_name is None:\n                total = float(self._class_ddist.total)\n                counts = self._class_ddist.counts\n                unique_value_count = len(self._class_ddist.counts)\n                attr_total = total\n            else:\n                total = float(self._attr_value_counts[attr_name][attr_value])\n                counts = self._attr_class_value_counts[attr_name][attr_value]\n                unique_value_count = len(self._attr_value_counts[attr_name])\n                attr_total = float(self._attr_value_count_totals[attr_name])\n            assert total, \"There must be at least one non-zero count.\"\n            n = max(2, len(counts))\n            if self._tree.metric == ENTROPY1:\n                return -sum(\n                    (count/total)*math.log(count/total, n)\n                    for count in itervalues(counts)\n                )\n            elif self._tree.metric == ENTROPY2:\n                return -sum(\n                    (count/total)*math.log(count/total, n)\n                    for count in itervalues(counts)\n                ) + (unique_value_count/attr_total)\n            elif self._tree.metric == ENTROPY3:\n                return -sum(\n                    (count/total)*math.log(count/total, n)\n                    for count in itervalues(counts)\n                ) + 100*(unique_value_count/attr_total)", "label": 1}
{"index": "gp295820", "code": "def convert_string_to_number(param_0):\n    var_0 = []\n    for var_1 in param_0:\n        if string.isdigit():\n            num_list.append(int(var_1))\n        else:\n            num_list.append(var_1)\n    return var_0", "contrast": "def atoi(text: str) -> Union[int, str]:\n    return int(text) if text.isdigit() else text", "label": 1}
{"index": "gp184665", "code": "from warcio.archiveiterator import ArchiveIterator\nimport requests\ndef load_and_parse_record(param_0:str, param_1:int, param_2:int, param_3:str):\n    var_0 = requests.get(param_0, headers={'Range': f'bytes={param_1}-{param_1+param_2-1}'})\n    var_1 = resp.content\n    if param_3 == 'warc':\n        var_2 = next(ArchiveIterator(stream=var_1, arc2warc=False))\n    elif param_3 == 'arc':\n        var_2 = next(ArchiveIterator(stream=var_1, arc2warc=True))\n    else:\n        raise ValueError(f\"Invalid record type: {param_3}\")\n    return var_2", "contrast": "def load(self, url, offset, length, no_record_parse=False):\n        try:\n            length = int(length)\n        except:\n            length = -1\n        stream = self.loader.load(url, int(offset), length)\n        decomp_type = 'gzip'\n        stream = DecompressingBufferedReader(stream=stream,\n                                             decomp_type=decomp_type,\n                                             block_size=self.block_size)\n        return self.parse_record_stream(stream, no_record_parse=no_record_parse)", "label": 1}
{"index": "gp278166", "code": "def send_vnic_event(param_0, param_1, param_2, param_3):\n    var_0 = {\n        \"port_id\": port.vif_id,\n        \"net_uuid\": param_1,\n        \"tag\": param_2,\n        \"status\": param_3\n    }", "contrast": "def send_vdp_port_event(self, port_uuid, mac, net_uuid,\n                            segmentation_id, status, oui):\n        try:\n            with self.ovs_vdp_lock:\n                ret = self.send_vdp_port_event_internal(port_uuid, mac,\n                                                        net_uuid,\n                                                        segmentation_id,\n                                                        status, oui)\n                return ret\n        except Exception as e:\n            LOG.error(\"Exception in send_vdp_port_event %s\" % str(e))\n            return {'result': False, 'fail_reason': str(e)}", "label": 1}
{"index": "gp274544", "code": "def add_health_monitor(param_0, param_1):\n    if 'health_monitor' in param_0:\n        param_0['health_monitor'].update(param_1)\n    else:\n        param_0['health_monitor'] = param_1\n    return param_0", "contrast": "def add_health_monitor(self, loadbalancer, type, delay=10, timeout=10,\n            attemptsBeforeDeactivation=3, path=\"/\", statusRegex=None,\n            bodyRegex=None, hostHeader=None):\n        uri = \"/loadbalancers/%s/healthmonitor\" % utils.get_id(loadbalancer)\n        req_body = {\"healthMonitor\": {\n                \"type\": type,\n                \"delay\": delay,\n                \"timeout\": timeout,\n                \"attemptsBeforeDeactivation\": attemptsBeforeDeactivation,\n                }}\n        uptype = type.upper()\n        if uptype.startswith(\"HTTP\"):\n            lb = self._get_lb(loadbalancer)\n            if uptype != lb.protocol:\n                raise exc.ProtocolMismatch(\"Cannot set the Health Monitor type \"\n                        \"to '%s' when the Load Balancer's protocol is '%s'.\" %\n                        (type, lb.protocol))\n            if not all((path, statusRegex, bodyRegex)):\n                raise exc.MissingHealthMonitorSettings(\"When creating an HTTP(S) \"\n                        \"monitor, you must provide the 'path', 'statusRegex' and \"\n                        \"'bodyRegex' parameters.\")\n            body_hm = req_body[\"healthMonitor\"]\n            body_hm[\"path\"] = path\n            body_hm[\"statusRegex\"] = statusRegex\n            body_hm[\"bodyRegex\"] = bodyRegex\n            if hostHeader:\n                body_hm[\"hostHeader\"] = hostHeader\n        resp, body = self.api.method_put(uri, body=req_body)\n        return body", "label": 1}
{"index": "gp035765", "code": "def exists(param_0=None, param_1=None, param_2=None, param_3=None, param_4=None,\n           param_5=None, param_6=None, param_7=None, param_8=None):\n    var_0 = find_instances(param_0=param_0, param_1=param_1, param_2=param_2,\n                               param_3=param_3, param_4=param_4, param_5=param_5,\n                               param_6=param_6, param_7=param_7, param_8=param_8)\n    if var_0:\n        log.info('Instance exists.')\n        return True\n    else:\n        log.warning('Instance does not exist.')\n        return False", "contrast": "import boto3\nfrom botocore.exceptions import ClientError\ndef exists(instance_id):\n    try:\n        ec2 = boto3.resource('ec2')\n        instances = ec2.instances.filter(Filters=[{'Name': 'instance-id', 'Values': [instance_id]}])\n        for instance in instances:\n            return True\n        return False\n    except ClientError:\n        return False", "label": 0}
{"index": "gp172259", "code": "def get_serialization_method(param_0):\n    if param_0 == str:\n        return str.encode, bytes.decode\n    elif param_0 == int:\n        return int.to_bytes, int.from_bytes\n    elif param_0 == float:\n        return float.to_bytes, float.from_bytes\n    elif param_0 == bool:\n        return bool.__int__.to_bytes, bool.from_bytes\n    else:\n        raise TypeError('Unsupported data type')", "contrast": "def _fmt_serialization_call(self, data_type, input_value, serialize, depth=0):\n        data_type, _ = unwrap_nullable(data_type)\n        serializer_func = 'serialize' if serialize else 'deserialize'\n        serializer_args = []\n        if is_primitive_type(data_type):\n            return input_value\n        if is_list_type(data_type) or is_map_type(data_type):\n            serializer_args.append(('value', input_value))\n            elem_data_type = (data_type.value_data_type if\n                is_map_type(data_type) else data_type.data_type)\n            serialization_call = self._fmt_serialization_call(\n                elem_data_type, 'elem{}'.format(depth), serialize, depth + 1)\n            data_struct_block = '^id(id elem{}) {{ return {}; }}'.format(\n                depth, serialization_call)\n            serializer_args.append(('withBlock', data_struct_block))\n        elif is_timestamp_type(data_type):\n            serializer_args.append(('value', input_value))\n            serializer_args.append(('dateFormat',\n                                    '@\"{}\"'.format(data_type.format)))\n        else:\n            serializer_args.append(('value', input_value))\n        return '{}'.format(\n            fmt_func_call(\n                caller=fmt_serial_obj(data_type),\n                callee=serializer_func,\n                args=fmt_func_args(serializer_args)))", "label": 1}
{"index": "gp272390", "code": "def add_suffix_to_filename(param_0, param_1):\n    var_0, var_1 = os.path.splitext(param_0)\n    return f\"{var_0}{param_1}{var_1}\"", "contrast": "def add_suffix(path, suffix=\"\"):\n    return join(dirname(path), basename(path, ext=False) + suffix + extname(path))", "label": 1}
{"index": "gp194643", "code": "def mass_from_composition(param_0):\n    var_0 = {\n        0: 0.000549, 1: 1.008, 2: 4.003, 3: 6.941,\n        4: 9.012, 5: 10.81, 6: 12.01, 7: 14.01,\n        8: 16.00, 9: 19.00, 10: 20.18, 11: 22.99,\n        12: 24.31, 13: 26.98, 14: 28.09, 15: 30.97,\n        16: 32.07, 17: 35.45, 18: 39.94, 19: 39.10,\n        20: 40.08, 21: 44.96, 22: 47.87, 23: 50.94,\n        24: 52.00, 25: 54.94, 26: 55.85, 27: 58.93,\n        28: 58.69, 29: 63.55, 30: 65.38, 31: 69.72,\n        32: 72.63, 33: 74.92, 34: 78.96, 35: 79.90,\n        36: 83.80, 37: 85.47, 38: 87.62, 39: 88.91,\n        40: 91.22, 41: 92.91, 42: 95.94, 43: 98.91,\n        44: 101.07, 45: 102.91, 46: 106.42, 47: 107.87,\n        48: 112.41, 49: 114.82, 50: 118.71, 51: 121.76,\n        52: 127.60, 53: 126.90, 54: 131.29, 55: 132.91,\n        56: 137.33, 57: 138.91, 58: 140.12, 59: 140.91,\n        60: 144.24, 61: 145.00, 62: 150.36, 63: 151.96,\n        64: 157.25, 65: 158.93, 66: 162.50, 67: 164.93,\n        68: 167.26, 69: 168.93, 70: 173.05, 71: 174.97,\n        72: 178.49, 73: 180.95, 74: 183.84, 75: 186.21,\n        76: 190.23, 77: 192.22, 78: 195.08, 79: 196.97,\n        80: 200.59, 81: 204.38, 82: 207.2, 83: 208.98,\n        84: 209, 85: 210, 86: 222, 87: 223, 88: 226,\n        89: 227, 90: 232.04, 91: 231.04, 92: 238.03,\n        93: 237.05, 94: 244.06, 95: 243.06, 96: 247.07,\n        97: 247.07, 98: 251.08, 99: 252.08, 100: 257.10,\n        101: 258.10, 102: 259.10, 103: 262.11, 104: 261.11,\n        105: 262.11, 106: 266.12, 107: 264.12, 108: 277.13\n    }\n    var_1 = 0.0\n    for var_2, var_3 in composition.items():\n        var_1 += var_0[var_2] * var_3\n    return var_1", "contrast": "def mass_from_composition(composition):\n    mass = 0.0\n    for k, v in composition.items():\n        if k == 0:  \n            mass -= v*5.489e-4\n        else:\n            mass += v*relative_atomic_masses[k-1]\n    return mass", "label": 1}
{"index": "gp268852", "code": "def recruit_participants(param_0, param_1):\n    if param_0 > len(param_1):\n        var_0 = param_0 - len(param_1)\n        print(f'{var_0} participants needed. Please recruit more participants.')\n    else:\n        print('Sufficient number of participants available.')", "contrast": "def recruit(self):\n        num_approved = len(Participant.query.filter_by(status=\"approved\").all())\n        end_of_generation = num_approved % self.generation_size == 0\n        complete = num_approved >= (self.generations * self.generation_size)\n        if complete:\n            self.log(\"All networks full: closing recruitment\", \"-----\")\n            self.recruiter.close_recruitment()\n        elif end_of_generation:\n            self.log(\"generation finished, recruiting another\")\n            self.recruiter.recruit(n=self.generation_size)", "label": 1}
{"index": "gp227600", "code": "def establish_session(param_0):\n    if not all(var_0 in param_0 for var_0 in [\"dh_server_public\", \"dh_consumer_public\"]):\n        raise ProtocolError(\"Parameters required to establish session are missing\")\n    return DiffieHellmanSHA1ServerSession(message.get(\"dh_server_public\"), message.get(\"dh_consumer_public\"))", "contrast": "def fromMessage(cls, message):\n        dh_modulus = message.getArg(OPENID_NS, 'dh_modulus')\n        dh_gen = message.getArg(OPENID_NS, 'dh_gen')\n        if (dh_modulus is None and dh_gen is not None or\n            dh_gen is None and dh_modulus is not None):\n            if dh_modulus is None:\n                missing = 'modulus'\n            else:\n                missing = 'generator'\n            raise ProtocolError(message,\n                                'If non-default modulus or generator is '\n                                'supplied, both must be supplied. Missing %s'\n                                % (missing,))\n        if dh_modulus or dh_gen:\n            dh_modulus = cryptutil.base64ToLong(dh_modulus)\n            dh_gen = cryptutil.base64ToLong(dh_gen)\n            dh = DiffieHellman(dh_modulus, dh_gen)\n        else:\n            dh = DiffieHellman.fromDefaults()\n        consumer_pubkey = message.getArg(OPENID_NS, 'dh_consumer_public')\n        if consumer_pubkey is None:\n            raise ProtocolError(message, \"Public key for DH-SHA1 session \"\n                                \"not found in message %s\" % (message,))\n        consumer_pubkey = cryptutil.base64ToLong(consumer_pubkey)\n        return cls(dh, consumer_pubkey)", "label": 1}
{"index": "gp236186", "code": "def decide_best_venv(param_0):\n    matching_venvs.sort(key=lambda venv: len(venv.packages), reverse=True)\n    if len(param_0) == 1:\n        return param_0[0]\n    matching_venvs.sort(key=lambda venv: venv.last_accessed, reverse=True)\n    return param_0[0]", "contrast": "def _select_better_fit(self, matching_venvs):\n        venvs = []\n        to_compare = []\n        for matching, venv in matching_venvs:\n            to_compare.append(sorted(matching, key=lambda req: getattr(req, 'key', '')))\n            venvs.append(venv)\n        scores = [0] * len(venvs)\n        for dependencies in zip(*to_compare):\n            if not isinstance(dependencies[0], Distribution):\n                continue\n            winner = dependencies.index(max(dependencies))\n            scores[winner] = scores[winner] + 1\n        winner_pos = None\n        winner_score = -1\n        for i, score in enumerate(scores):\n            if score >= winner_score:\n                winner_score = score\n                winner_pos = i\n        return venvs[winner_pos]", "label": 1}
{"index": "gp242453", "code": "import json\ndef convert_to_writable(param_0: str, param_1: dict) -> str:\n    if file_format.lower() == 'json':\n        return json.dumps(param_1)\n    elif file_format.lower() == 'xml':\n        pass\n    else:\n        raise ValueError(\"Unsupported file format, must be 'json' or 'xml'\")", "contrast": "def get_writable_metadata(self, file_format):\n        if file_format == 'json':\n            metadata = self.json\n        elif file_format == 'xml':\n            metadata = self.xml\n        else:\n            raise TypeError('The requested file type (%s) is not yet supported'\n                            % file_format)\n        return metadata", "label": 1}
{"index": "gp184797", "code": "def format_contraction(param_0: str, param_1: str) -> str:\n    var_0 = contraction.split(',')\n    var_1 = []\n    for var_2 in var_0:\n        var_3, var_4 = var_2[:-1], var_2[-1:]\n        formatted_contractions.append(f\"{int(int(var_3)/4)}{var_4}\")\n    return f\"[{','.join(var_1)}]\"", "contrast": "def contraction_string(element):\n    if 'electron_shells' not in element:\n        return \"\"\n    cont_map = dict()\n    for sh in element['electron_shells']:\n        nprim = len(sh['exponents'])\n        ngeneral = len(sh['coefficients'])\n        is_spdf = len(sh['angular_momentum']) > 1\n        for am in sh['angular_momentum']:\n            ncont = ngeneral if not is_spdf else 1\n            if am not in cont_map:\n                cont_map[am] = (nprim, ncont)\n            else:\n                cont_map[am] = (cont_map[am][0] + nprim, cont_map[am][1] + ncont)\n    primstr = \"\"\n    contstr = \"\"\n    for am in sorted(cont_map.keys()):\n        nprim, ncont = cont_map[am]\n        if am != 0:\n            primstr += ','\n            contstr += ','\n        primstr += str(nprim) + lut.amint_to_char([am])\n        contstr += str(ncont) + lut.amint_to_char([am])\n    return \"({}) -> [{}]\".format(primstr, contstr)", "label": 1}
{"index": "gp306993", "code": "import pandas as pd\nfrom pandas_functions import summary_df\ndef get_error_summary_stats(param_0: pd.DataFrame, param_1: dict = None) -> pd.DataFrame:\n    if param_1 is None:\n        param_1 = {}\n    var_0 = error_values.mean(axis=1)\n    var_1 = error_values.std(axis=1)\n    var_2 = error_values.shape[1] // 2\n    var_3 = means.iloc[var_2:] / means.iloc[:var_2] - 1\n    var_4 = stds.iloc[var_2:] / stds.iloc[:var_2] - 1\n    var_5 = pd.concat([var_0, var_1, var_3, var_4], axis=1)\n    results.columns = [\"mean\", \"std\", \"mean_rel_err\", \"std_rel_err\"]\n    var_6 = error_values.iloc[:, var_2:]\n    return summary_df(var_5, var_6, **param_1)", "contrast": "def error_values_summary(error_values, **summary_df_kwargs):\n    df = pf.summary_df_from_multi(error_values, **summary_df_kwargs)\n    imp_std, imp_std_unc, imp_frac, imp_frac_unc =        nestcheck.error_analysis.implementation_std(\n            df.loc[('values std', 'value')],\n            df.loc[('values std', 'uncertainty')],\n            df.loc[('bootstrap std mean', 'value')],\n            df.loc[('bootstrap std mean', 'uncertainty')])\n    df.loc[('implementation std', 'value'), df.columns] = imp_std\n    df.loc[('implementation std', 'uncertainty'), df.columns] = imp_std_unc\n    df.loc[('implementation std frac', 'value'), :] = imp_frac\n    df.loc[('implementation std frac', 'uncertainty'), :] = imp_frac_unc\n    if 'values rmse' in set(df.index.get_level_values('calculation type')):\n        imp_rmse, imp_rmse_unc, imp_frac, imp_frac_unc =            nestcheck.error_analysis.implementation_std(\n                df.loc[('values rmse', 'value')],\n                df.loc[('values rmse', 'uncertainty')],\n                df.loc[('bootstrap std mean', 'value')],\n                df.loc[('bootstrap std mean', 'uncertainty')])\n        df.loc[('implementation rmse', 'value'), df.columns] = imp_rmse\n        df.loc[('implementation rmse', 'uncertainty'), df.columns] =            imp_rmse_unc\n        df.loc[('implementation rmse frac', 'value'), :] = imp_frac\n        df.loc[('implementation rmse frac', 'uncertainty'), :] = imp_frac_unc\n    calcs_to_keep = ['true values', 'values mean', 'values std',\n                     'values rmse', 'bootstrap std mean',\n                     'implementation std', 'implementation std frac',\n                     'implementation rmse', 'implementation rmse frac',\n                     'thread ks pvalue mean', 'bootstrap ks distance mean',\n                     'bootstrap energy distance mean',\n                     'bootstrap earth mover distance mean']\n    df = pd.concat([df.xs(calc, level='calculation type', drop_level=False) for\n                    calc in calcs_to_keep if calc in\n                    df.index.get_level_values('calculation type')])\n    return df", "label": 1}
{"index": "gp193862", "code": "async def process_phantomjs(param_0: str, param_1: str, param_2: List[str] = None, param_3: int = 30) -> Tuple[int, str, str]:\n    var_0 = [param_1]\n    if param_2:\n        process_args.extend(param_2)\n    process_args.append(param_0)\n    var_1 = await asyncio.create_subprocess_exec(\n        *var_0,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    )\n    try:\n        var_2, var_3 = await asyncio.wait_for(process.communicate(), param_3=param_3)\n    except asyncio.TimeoutError:\n        process.kill()\n        var_2, var_3 = await process.communicate()\n    return process.returncode, stdout.decode().strip(), stderr.decode().strip()", "contrast": "def process(self, item_session: ItemSession, request, response, file_writer_session):\n        if response.status_code != 200:\n            return\n        if not HTMLReader.is_supported(request=request, response=response):\n            return\n        _logger.debug('Starting PhantomJS processing.')\n        self._file_writer_session = file_writer_session\n        attempts = int(os.environ.get('WPULL_PHANTOMJS_TRIES', 5))\n        for dummy in range(attempts):\n            try:\n                yield from self._run_driver(item_session, request, response)\n            except asyncio.TimeoutError:\n                _logger.warning(_('Waiting for page load timed out.'))\n                break\n            except PhantomJSCrashed as error:\n                _logger.exception(__('PhantomJS crashed: {}', error))\n            else:\n                break\n        else:\n            _logger.warning(__(\n                _('PhantomJS failed to fetch \u2018{url}\u2019. I am sorry.'),\n                url=request.url_info.url\n            ))", "label": 1}
{"index": "gp268551", "code": "import os\ndef ensure_dir_exists(param_0):\n    if not os.path.exists(param_0):\n        os.makedirs(param_0)", "contrast": "def make_all_dirs(path, mode=0o777):\n  try:\n    os.makedirs(path, mode=mode)\n  except OSError as e:\n    if e.errno == errno.EEXIST and os.path.isdir(path):\n      pass\n    else:\n      raise\n  return path", "label": 1}
{"index": "gp289955", "code": "def ensure_unicode_keys(param_0):\n    return {key.decode('utf-8') if isinstance(var_0, bytes) else var_0: var_1 for var_0, var_1 in dictionary.items()}", "contrast": "def normalize_string_keys(old):\n    new = {}\n    for key, value in old.items():\n        if isinstance(key, bytes_):\n            new[key.decode('utf8')] = value\n        else:\n            new[key] = value\n    return new", "label": 1}
{"index": "gp235476", "code": "from django.views import View\nfrom django.utils.decorators import method_decorator\nfrom django.contrib.auth.decorators import login_required\nfrom django.utils.translation import gettext_lazy as _\nclass LogoutView(View):\n    @method_decorator(login_required)\n    def dispatch(param_0, param_1, *param_2, **param_3):\n        self.next_page = request.GET.get(\"next_page\", None)\n        self.extra_context = {\n            'page_title': _('Log out'),\n            'button_text': _('Logout'),\n        }\n        return super(LogoutView, param_0).dispatch(param_1, *param_2, **param_3)", "contrast": "def init_get(self, request):\n        self.request = request\n        self.service = request.GET.get('service')\n        self.url = request.GET.get('url')\n        self.ajax = settings.CAS_ENABLE_AJAX_AUTH and 'HTTP_X_AJAX' in request.META", "label": 1}
{"index": "gp055936", "code": "def GetSitelinksFromFeed(client, feed):\n  feed_mappings = GetFeedMapping(client, feed, PLACEHOLDER_TYPE_SITELINKS)\n  feed_items = {}\n  for feed_item in GetFeedItems(client, feed):\n    site_link_from_feed = {}\n    for attribute_value in feed_item['attributeValues']:\n      if attribute_value['feedAttributeId'] in feed_mappings:\n        for field_id in feed_mappings[attribute_value['feedAttributeId']]:\n          if field_id == SITE_LINK_FIELDS['TEXT']:\n            site_link_from_feed['text'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['URL']:\n            site_link_from_feed['url'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['FINAL_URLS']:\n            site_link_from_feed['finalUrls'] = attribute_value['stringValues']\n          elif field_id == SITE_LINK_FIELDS['FINAL_MOBILE_URLS']:\n            site_link_from_feed['finalMobileUrls'] = attribute_value[\n                'stringValues']\n          elif field_id == SITE_LINK_FIELDS['TRACKING_URL_TEMPLATE']:\n            site_link_from_feed['trackingUrlTemplate'] = attribute_value[\n                'stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE2']:\n            site_link_from_feed['line2'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE3']:\n            site_link_from_feed['line3'] = attribute_value['stringValue']\n          else:\n            print 'No applicable Site Link Field found for Id: %s' % field_id\n    feed_items[feed_item['feedItemId']] = site_link_from_feed\n  return feed_items", "contrast": "def get_sitelinks(client, feed):\n    selector = {\n        'fields': ['FeedItemId', 'LinkText', 'Line1', 'Line2', 'FinalUrls',\n                   'Scheduling'],\n        'predicates': [{\n            'field': 'FeedId',\n            'operator': 'EQUALS',\n            'values': [feed['id']]\n        }]\n    }\n    feed_items = client.GetService('FeedItemService').get(selector)\n    sitelinks = {}\n    for item in feed_items:\n        sitelink = client.factory.create('SiteLinkFromFeed')\n        sitelink.link_text = item['attributeValues'][0]['value']\n        sitelink.line1 = item['attributeValues'][1]['value']\n        sitelink.line2 = item['attributeValues'][2]['value']\n        sitelink.final_urls = item['attributeValues'][3]['value']\n        sitelink.scheduling = item['scheduling']\n        sitelinks[item['feedItemId']] = sitelink\n    return sitelinks", "label": 0}
{"index": "gp037435", "code": "def get_chassis_location(param_0=None,\n                         param_1=None,\n                         param_2=None):\n    return system_info(param_0=param_0,\n                       param_1=param_1,\n                       param_2=param_2)['Chassis Information']['Chassis Location']", "contrast": "import requests\ndef get_chassis_location(host: str, admin_username: str, admin_password: str) -> str:\n    url = f\"https://{host}/some_path\"\n    response = requests.get(url, auth=(admin_username, admin_password))\n    return response.json()[\"chassis_location\"]", "label": 0}
{"index": "gp016352", "code": "def recv(param_0, param_1, param_2=zmq.NOBLOCK, param_3=True, param_4=True):\n        if isinstance(param_1, ZMQStream):\n            param_1 = socket.socket\n        try:\n            var_0 = socket.recv_multipart(param_2, param_4=param_4)\n        except zmq.ZMQError as e:\n            if e.errno == zmq.EAGAIN:\n                return None,None\n            else:\n                raise\n        var_1, var_0 = self.feed_identities(var_0, param_4)\n        try:\n            return var_1, self.unserialize(var_0, param_3=param_3, param_4=param_4)\n        except Exception as e:\n            raise e", "contrast": "def receive_and_unpack_message(socket):\n    msg = socket.recv_multipart()\n    return msg[:-1], json.loads(msg[-1].decode('utf-8'))", "label": 0}
{"index": "gp285246", "code": "def format_value(param_0):\n    try:\n        var_0 = format(param_0, ',.2f')\n    except ValueError:\n        raise ValidationError(\"An error occurred while formatting the value\")\n    return var_0", "contrast": "def _validated(self, value):\n        if value is None:\n            return None\n        if isinstance(value, bson.ObjectId):\n            return value\n        try:\n            return bson.ObjectId(value)\n        except (ValueError, AttributeError):\n            self.fail('invalid_object_id')", "label": 1}
{"index": "gp291454", "code": "def get_repository(param_0, param_1, param_2):\n    var_0 = f\"{auth.url}/repos/{param_1}/{param_2}\"\n    var_1 = {'Authorization': f'token {auth.token}'}\n    try:\n        var_2 = requests.get(var_0, var_1=var_1)\n        response.raise_for_status()\n        var_3 = GogsRepo(**response.json())\n        return var_3\n    except requests.exceptions.RequestException as err:\n        raise NetworkFailure(f\"Error communicating with server: {err}\")\n    except (ValueError, KeyError) as err:\n        raise ApiFailure(f\"Error retrieving repository: {err}\")", "contrast": "def get_repo(self, auth, username, repo_name):\n        path = \"/repos/{u}/{r}\".format(u=username, r=repo_name)\n        response = self.get(path, auth=auth)\n        return GogsRepo.from_json(response.json())", "label": 1}
{"index": "gp323058", "code": "def get_license_identifier(param_0):\n    if param_0 is not None:\n        return license.get('Identifier', None)\n    return None", "contrast": "def get_extr_license_ident(self, extr_lic):\n        identifier_tripples = list(self.graph.triples((extr_lic, self.spdx_namespace['licenseId'], None)))\n        if not identifier_tripples:\n            self.error = True\n            msg = 'Extracted license must have licenseId property.'\n            self.logger.log(msg)\n            return\n        if len(identifier_tripples) > 1:\n            self.more_than_one_error('extracted license identifier_tripples')\n            return\n        identifier_tripple = identifier_tripples[0]\n        _s, _p, identifier = identifier_tripple\n        return identifier", "label": 1}
{"index": "gp156121", "code": "def forward(param_0, param_1: str):\n        var_0 = \"/api/forward/{search}\".format(param_1=param_1)\n        return self._request(path=var_0)", "contrast": "import dns.resolver\ndef get_dns_lookup_history(ip_address):\n    try:\n        answers = dns.resolver.query(ip_address, 'PTR')\n        return [str(rdata) for rdata in answers]\n    except dns.resolver.NoAnswer:\n        return None", "label": 0}
{"index": "gp235999", "code": "def handle_500_error():\n    return render_template('500.html'), 500", "contrast": "def server_error(request, template_name='500.html'):\n    response = render_in_page(request, template_name)\n    if response:\n        return response\n    try:\n        template = loader.get_template(template_name)\n    except TemplateDoesNotExist:\n        return http.HttpResponseServerError('<h1>Server Error (500)</h1>', content_type='text/html')\n    return http.HttpResponseServerError(template.render(Context({})))", "label": 1}
{"index": "gp079456", "code": "def copy_style():\n  import os\n  import matplotlib\n  var_0 = {}\n  var_0['goose.mplstyle'] = '''\nfigure.figsize       : 8,6\nfont.weight          : normal\nfont.size            : 16\naxes.labelsize       : medium\naxes.titlesize       : medium\nxtick.labelsize      : small\nytick.labelsize      : small\nxtick.top            : True\nytick.right          : True\naxes.facecolor       : none\naxes.prop_cycle      : cycler('color',['k', 'r', 'g', 'b', 'y', 'c', 'm'])\nlegend.fontsize      : medium\nlegend.fancybox      : true\nlegend.columnspacing : 1.0\nlegend.handletextpad : 0.2\nlines.linewidth      : 2\nimage.cmap           : afmhot\nimage.interpolation  : nearest\nimage.origin         : lower\nsavefig.facecolor    : none\nfigure.autolayout    : True\nerrorbar.capsize     : 2\n'''\n  var_0['goose-tick-in.mplstyle'] = '''\nxtick.direction      : in\nytick.direction      : in\n'''\n  var_0['goose-tick-lower.mplstyle'] = '''\nxtick.top            : False\nytick.right          : False\naxes.spines.top      : False\naxes.spines.right    : False\n'''\n  if find_latex_font_serif() is not None:\n    var_0['goose-latex.mplstyle'] = r'''\nfont.family          : serif\nfont.serif           : {serif:s}\nfont.weight          : bold\nfont.size            : 18\ntext.usetex          : true\ntext.latex.preamble  : \\usepackage{{amsmath}},\\usepackage{{amsfonts}},\\usepackage{{amssymb}},\\usepackage{{bm}}\n'''.format(serif=find_latex_font_serif())\n  else:\n    var_0['goose-latex.mplstyle'] = r'''\nfont.family          : serif\nfont.weight          : bold\nfont.size            : 18\ntext.usetex          : true\ntext.latex.preamble  : \\usepackage{{amsmath}},\\usepackage{{amsfonts}},\\usepackage{{amssymb}},\\usepackage{{bm}}\n'''\n  var_1 = os.path.abspath(os.path.join(matplotlib.get_configdir(), 'stylelib'))\n  if not os.path.isdir(var_1): os.makedirs(var_1)\n  for var_2, var_3 in styles.items():\n    open(os.path.join(var_1, var_2),'w').write(var_3)", "contrast": "import os\ndef write_goosestyles_to_matplotlib_config():\n    goosestyles_path = '/path/to/goosestyles'  \n    matplotlib_config_dir = os.path.join(os.path.expanduser('~'), '.config', 'matplotlib')\n    if not os.path.exists(matplotlib_config_dir):\n        os.makedirs(matplotlib_config_dir)\n    for filename in os.listdir(goosestyles_path):\n        if filename.endswith('.mplstyle'):\n            source_path = os.path.join(goosestyles_path, filename)\n            destination_path = os.path.join(matplotlib_config_dir, filename)\n            os.replace(source_path, destination_path)\n    print('All goose-styles have been written to the matplotlib configuration directory')", "label": 0}
{"index": "gp074386", "code": "def send_voice(param_0, param_1, param_2, param_3=None, param_4=None, param_5=None):\n        var_0 = dict(param_1=param_1,\n                       param_3=param_3,\n                       param_4=param_4,\n                       param_5=param_5)\n        var_1 = dict(param_2=open(param_2, 'rb'))\n        return Message.from_api(param_0, **self._post('sendVoice', var_0, var_1))", "contrast": "def send_voice(chat_id, audio_file, duration=None, caption=None, parse_mode=None, reply_to_message_id=None, reply_markup=None):\n    url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendVoice'\n    files = {'voice': open(audio_file, 'rb')}\n    data = {'chat_id': chat_id,\n            'duration': duration,\n            'caption': caption,\n            'parse_mode': parse_mode,\n            'reply_to_message_id': reply_to_message_id,\n            'reply_markup': reply_markup}\n    response = requests.post(url, data=data, files=files)\n    return response.json()['result']", "label": 0}
{"index": "gp105096", "code": "def extract(param_0, param_1, param_2, param_3, param_4):\n    assert np.size(param_1) == 1\n    assert np.size(param_2) == 1\n    assert np.size(param_3) == 1\n    assert np.size(param_4) == 1\n    assert (param_3 >= 0) & (param_3 <= 360)\n    assert param_4 >= 0\n    var_0 = netCDF4.Dataset(param_0)\n    var_1 = 2 * np.pi * param_1/366\n    var_2 = np.absolute(var_0['depth'][:] - param_4).argmin()\n    var_3 = np.absolute(var_0['lat'][:] - param_2).argmin()\n    var_4 = np.absolute(var_0['lon'][:] - param_3).argmin()\n    var_5 = var_0['mean'][:, var_3, var_4]\n    var_5[:64] += var_0['an_cos'][var_2, var_3, var_4] * np.cos(var_1) +            var_0['an_sin'][:, var_3, var_4] * np.sin(var_1)\n    var_5[:55] += var_0['sa_cos'][var_2, var_3, var_4] * np.cos(2*var_1) +            var_0['sa_sin'][:, var_3, var_4] * np.sin(2*var_1)\n    var_5 = var_5[var_2]\n    var_6 = var_0['std_dev'][var_2, var_3, var_4]\n    return var_5, var_6", "contrast": "def nearest_longitude(longitude):\n    if longitude > 180:\n        longitude = -360 + longitude\n    elif longitude < -180:\n        longitude = 360 + longitude\n    return longitude", "label": 0}
{"index": "gp075293", "code": "def _load_group_permissions(param_0):\n        var_0 = self._get_groups().get_group_names()\n        var_1 = Permission.objects.filter(group__name__in=var_0)\n        var_1 = perms.values_list('content_type__app_label', 'codename')\n        var_1 = perms.order_by()\n        self._group_permissions = set([\"%s.%s\" % (var_2, var_3) for var_2, var_3 in var_1])", "contrast": "def populate_group_permissions(self):\n    ldap_groups = get_ldap_group_membership() \n    for group in ldap_groups:\n        django_permissions = get_django_group_permissions(group)\n        for permission in django_permissions:\n            self._group_permissions.add(permission)", "label": 0}
{"index": "gp033421", "code": "def dot_vals(param_0):\n    var_0 = {}\n    for var_1, var_2 in six.iteritems(__pillar__.get('master', {})):\n        if key.startswith('{0}.'.format(param_0)):\n            var_0[var_1] = var_2\n    for var_1, var_2 in six.iteritems(__opts__):\n        if key.startswith('{0}.'.format(param_0)):\n            var_0[var_1] = var_2\n    return var_0", "contrast": "import salt.config\ndef dot_vals(module, key=None):\n    opts = salt.config.minion_config('/etc/salt/minion')\n    config_dict = salt.config.apply_minion_config(None, opts)\n    ret = {}\n    for config_key, config_value in config_dict.items():\n        if config_key == module:\n            if key is None:\n                ret.update(config_value)\n                continue\n            if key in config_value:\n                ret[key] = config_value[key]\n    return ret", "label": 0}
{"index": "gp034590", "code": "def absent(param_0, param_1=None):\n    var_0 = {'name': param_0,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n    __salt__['neutronng.setup_clouds'](param_1)\n    var_1 = __salt__['neutronng.subnet_get'](param_0=param_0)\n    if var_1:\n        if __opts__['test'] is True:\n            var_0['result'] = None\n            var_0['changes'] = {'id': subnet.id}\n            var_0['comment'] = 'Project will be deleted.'\n            return var_0\n        __salt__['neutronng.subnet_delete'](param_0=var_1)\n        var_0['changes']['id'] = param_0\n        var_0['comment'] = 'Deleted subnet'\n    return var_0", "contrast": "import boto3\ndef ensure_subnet_not_exists(name):\n    ec2 = boto3.client('ec2')\n    response = ec2.describe_subnets(Filters=[{'Name': 'tag:Name','Values': [name]}])\n    subnets = response['Subnets']\n    if not subnets:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp086237", "code": "def _get_regular_expression_of_symbols(param_0):\n        var_0 = None\n        for var_1 in self.symbols:\n            var_2 = self._get_formated_symbol(var_1['symbol'])\n            if var_0 is None:\n                var_0 = '(' + var_2 + ')'\n            else:\n                var_0 = (\n                    var_0 +\n                    '|(' +\n                    var_2 +\n                    ')'\n                )\n        return var_0", "contrast": "import re\ndef search_symbols():\n    return re.escape(r'~`!@#$%^&*()-_=+[{]}\\\\|;:\\'\",<.>/?')", "label": 0}
{"index": "gp177695", "code": "def flatten_iterable(param_0):\n    var_0 = []\n    for var_1 in param_0:\n        if isinstance(var_1, str):\n            flat_list.append(var_1)\n        elif isinstance(var_1, list) or isinstance(var_1, tuple):\n            flat_list.extend(flatten_iterable(var_1))\n        else:\n            flat_list.append(var_1)\n    return var_0", "contrast": "def flatten( iterables ):\n    for it in iterables:\n        if isinstance(it, str):\n            yield it\n        else:\n            for element in it:\n                yield element", "label": 1}
{"index": "gp047406", "code": "def pick_move(param_0):\n        if self.root.position.n >= self.temp_threshold:\n            var_0 = self.root.best_child()\n        else:\n            var_1 = self.root.children_as_pi(squash=True).cumsum()\n            var_1 /= var_1[-2]  \n            var_2 = random.random()\n            var_0 = cdf.searchsorted(var_2)\n            assert self.root.child_N[var_0] != 0\n        return coords.from_flat(var_0)", "contrast": "def pick_move(mcts_stats, early_game_moves):\n    if len(mcts_stats) <= early_game_moves:\n        move = max(mcts_stats.items(), key=lambda x: x[1]['N']/x[1]['visits'])[0]\n    else:\n        move = max(mcts_stats.keys(), key=(lambda k: mcts_stats[k]['N']))\n    return move", "label": 0}
{"index": "gp152077", "code": "def analyze_frames(param_0, param_1):\r\n        var_0 = param_0(None, param_1)\r\n        var_1 = {}\r\n        with open(os.path.join(param_1, 'frames', 'frames.json')) as var_2:\r\n            var_1 = json.load(var_2)\r\n        record.device_info = var_1['device']\r\n        record.frames = var_1['frames']\r\n        record.analyze_all()\r\n        record.save()", "contrast": "def generate_draft(frames):\n    draft = []\n    for frame in frames:\n        draft.append(processed_frame)\n    return draft", "label": 0}
{"index": "gp040872", "code": "def _collapseMsg(param_0, param_1):\n        var_0 = {}\n        for var_1 in param_1:\n            var_2 = u\"\"\n            for var_3 in param_1[var_1]:\n                var_3 = bytes2unicode(var_3, self.builder.unicode_encoding)\n                var_2 += var_3\n            if isinstance(var_1, tuple) and var_1[0] == 'log':\n                var_0['log'] = (var_1[1], var_2)\n            else:\n                var_0[var_1] = var_2\n        return var_0", "contrast": "def concatenate_chunks(msg):\n    concatenated_string = ''\n    for key in msg.keys():\n        concatenated_string += ''.join(msg[key])\n    return concatenated_string", "label": 0}
{"index": "gp276219", "code": "def wind_direction(param_0):\n    var_0 = [\"N\", \"NNE\", \"NE\", \"ENE\", \"E\", \"ESE\", \"SE\", \"SSE\", \"S\", \"SSW\", \"SW\", \"WSW\", \"W\", \"WNW\", \"NW\", \"NNW\"]\n    var_1 = round(param_0 / (360/16)) % 16\n    return var_0[var_1]", "contrast": "def winddir_text(pts):\n    global _winddir_text_array\n    if pts is None:\n        return None\n    if not isinstance(pts, int):\n        pts = int(pts + 0.5) % 16\n    if not _winddir_text_array:\n        _ = pywws.localisation.translation.ugettext\n        _winddir_text_array = (\n            _(u'N'), _(u'NNE'), _(u'NE'), _(u'ENE'),\n            _(u'E'), _(u'ESE'), _(u'SE'), _(u'SSE'),\n            _(u'S'), _(u'SSW'), _(u'SW'), _(u'WSW'),\n            _(u'W'), _(u'WNW'), _(u'NW'), _(u'NNW'),\n            )\n    return _winddir_text_array[pts]", "label": 1}
{"index": "gp220979", "code": "import ast\ndef parse_expression(param_0):\n    return ast.parse(param_0, mode='eval').body", "contrast": "def parse(self, text):\n        self.expr = text\n        try:\n            out = ast.parse(text)\n        except SyntaxError:\n            self.raise_exception(None, msg='Syntax Error', expr=text)\n        except:\n            self.raise_exception(None, msg='Runtime Error', expr=text)\n        return out", "label": 1}
{"index": "gp129119", "code": "def split_and_strip_without(param_0, param_1, param_2=None):\n    var_0 = split_and_strip(param_0, param_2)\n    if not param_1:\n        return var_0\n    return [var_1 for var_1 in var_0 if var_1 not in param_1]", "contrast": "def split_and_strip_without(string, exclude):\n    items = [x.strip() for x in string.split(',')]\n    return [x for x in items if x not in exclude]", "label": 0}
{"index": "gp204422", "code": "def separate_variable(param_0, param_1):\n    var_0 = Y.index('Y1')\n    var_1 = param_1[var_0]\n    return var_1", "contrast": "def block_sep1(self, Y):\n        Y1 = Y[..., self.cri.M:]\n        if self.cri.Cd > 1:\n            shp = list(Y1.shape)\n            shp[self.cri.axisM] = self.cri.dimN\n            shp[self.cri.axisC] = self.cri.Cd\n            Y1 = Y1.reshape(shp)\n        Y1 = np.swapaxes(Y1[..., np.newaxis], self.cri.axisM, -1)\n        return Y1", "label": 1}
{"index": "gp203755", "code": "import inspect\ndef get_source_lines(param_0):\n    try:\n        var_0, var_1 = inspect.getsourcelines(param_0)\n        return var_0, var_1\n    except IOError:\n        raise", "contrast": "def getsourcelines(object):\n    lines, lnum = findsource(object)\n    if ismodule(object): return lines, 0\n    else: return getblock(lines[lnum:]), lnum + 1", "label": 1}
{"index": "gp316016", "code": "import requests\ndef search_units(param_0: str):\n    var_0 = \"http://example.com/search\" \n    var_1 = {\n        \"view\": \"_zops_search_unit\",\n        \"query\": param_0\n    }\n    var_2 = requests.post(var_0, json=var_1)\n    if response.status_code == 200:\n        var_3 = []\n        for var_4 in response.json()[\"results\"]:\n            var_5, var_6 = var_4\n            results.append((var_5, var_6))\n        return {\n            \"results\": var_3,\n            \"status\": \"OK\",\n            \"code\": 200\n        }\n    else:\n        return {\n            \"results\": [],\n            \"status\": \"Error\",\n            \"code\": response.status_code\n        }", "contrast": "def search_unit(current):\n    current.output = {\n        'results': [],\n        'status': 'OK',\n        'code': 201\n    }\n    for user in UnitModel(current).objects.search_on(*settings.MESSAGING_UNIT_SEARCH_FIELDS,\n                                                     contains=current.input['query']):\n        current.output['results'].append((user.name, user.key))", "label": 1}
{"index": "gp052459", "code": "def sort(param_0, param_1=None, param_2=False):\n        var_0 = self.molecule.copy()\n        self.molecule._sites = sorted(self.molecule._sites, param_1=param_1, param_2=param_2)\n        var_1 = {var_2: self.molecule.index(var_3) for var_2, var_3 in enumerate(var_0)}\n        self.graph = nx.relabel_nodes(self.graph, var_1, copy=True)\n        var_4 = []\n        var_5 = []\n        for var_6, var_7, var_8, var_9 in self.graph.edges(keys=True, data=True):\n            if var_7 < var_6:\n                var_10, var_11, var_12 = var_6, var_7, d.copy()\n                var_12['to_jimage'] = (0, 0, 0)\n                edges_to_remove.append((var_6, var_7, var_8))\n                edges_to_add.append((var_11, var_10, var_12))\n        for var_4 in var_4:\n            self.graph.remove_edge(*var_4)\n        for (var_6, var_7, var_9) in var_5:\n            self.graph.add_edge(var_6, var_7, **var_9)", "contrast": "def remap_and_sort(molecule, key=None, reverse=False):\n    node_map = {}\n    i = 0\n    for node in molecule:\n        node_map[node] = i\n        i += 1\n    molecule.remap_nodes(node_map)\n    molecule.sort(key=key, reverse=reverse)", "label": 0}
{"index": "gp060511", "code": "def write_branch_data(param_0, param_1):\n        var_0 = CaseReport(self.case)\n        var_1 = self.case.branches\n        var_2   = 8\n        var_3 = var_2*2+1\n        var_4  = 7\n        var_5 = (\"=\" * 7 + \" \") * 3 + (\"=\" * var_2 + \" \") * 6 + \"\\n\"\n        file.write(var_5)\n        file.write(\"Name\".center(var_4) + \" \")\n        file.write(\"From\".center(var_4) + \" \")\n        file.write(\"To\".center(var_4) + \" \")\n        file.write(\"From Bus Inj\".center(var_3) + \" \")\n        file.write(\"To Bus Inj\".center(var_3) + \" \")\n        file.write(\"Loss (I^2 * Z)\".center(var_3) + \" \")\n        file.write(\"\\n\")\n        file.write((\"-\"*var_4 +\" \")*3)\n        file.write((\"-\"*var_3 +\" \")*3 + \"\\n\")\n        file.write(\"..\".ljust(var_4) + \" \")\n        file.write(\"Bus\".center(var_4) + \" \")\n        file.write(\"Bus\".center(var_4) + \" \")\n        file.write(\"P (MW)\".center(var_2) + \" \")\n        file.write(\"Q (MVAr)\".center(var_2) + \" \")\n        file.write(\"P (MW)\".center(var_2) + \" \")\n        file.write(\"Q (MVAr)\".center(var_2) + \" \")\n        file.write(\"P (MW)\".center(var_2) + \" \")\n        file.write(\"Q (MVAr)\".center(var_2) + \" \")\n        file.write(\"\\n\")\n        file.write(var_5)\n        var_6 = report._loss()\n        for var_7 in var_1:\n            file.write(each.name[:var_4].ljust(var_4) + \" \")\n            file.write(each.from_bus.name[:var_4].ljust(var_4)+\" \")\n            file.write(each.to_bus.name[:var_4].ljust(var_4)+\" \")\n            file.write(\"%8.2f \" % each.p_from)\n            file.write(\"%8.2f \" % each.q_from)\n            file.write(\"%8.2f \" % each.p_to)\n            file.write(\"%8.2f \" % each.q_to)\n            file.write(\"%8.2f \" % loss.real[each._i])\n            file.write(\"%8.2f \" % loss.imag[each._i])\n            file.write(\"\\n\")\n        file.write((\"..\".ljust(var_4) + \" \")*3)\n        file.write((\"..\".ljust(var_2) + \" \")*3)\n        file.write(\"*Total:*\".rjust(var_2) + \" \")\n        var_8, var_9 = report.losses\n        file.write(\"%8.2f \" % var_8)\n        file.write(\"%8.2f \" % var_9)\n        file.write(\"\\n\")\n        file.write(var_5)\n        del var_0", "contrast": "def write_branch_data_to_rest_table(branch_data):\n    import requests\n    api_url = \"https://example.com/api/branches/\"\n    headers = {\"Content-Type\": \"application/json\"}\n    for data in branch_data:\n        response = requests.post(api_url, headers=headers, json=data)\n        print(response.json())", "label": 0}
{"index": "gp259421", "code": "import pandas as pd\ndef from_fsi(param_0):\n    var_0 = pd.read_csv(param_0, delimiter='\\t', header=None, names=['prd', 'pressure', 'temperature', 'cond', 'sal'])\n    return var_0", "contrast": "def from_fsi(fname, skiprows=9):\n    f = _read_file(fname)\n    df = pd.read_csv(\n        f,\n        header=\"infer\",\n        index_col=None,\n        skiprows=skiprows,\n        dtype=float,\n        delim_whitespace=True,\n    )\n    f.close()\n    df.set_index(\"PRES\", drop=True, inplace=True)\n    df.index.name = \"Pressure [dbar]\"\n    metadata = {\"name\": str(fname)}\n    setattr(df, \"_metadata\", metadata)\n    return df", "label": 1}
{"index": "gp225699", "code": "import pkg_resources\ndef get_entrypoints(param_0, param_1):\n    var_0 = []\n    for var_1 in pkg_resources.iter_entry_points(param_1, param_0):\n        entrypoints.append(entry_point.load())\n    return var_0", "contrast": "def get_enabled(name, app):\n    plugins = app.config['PLUGINS']\n    return dict(_ep_to_kv(e) for e in iter_all(name) if e.name in plugins)", "label": 1}
{"index": "gp047814", "code": "def stop_monitoring(param_0) -> None:\n        self._context.optimisation_finished = True\n        self._on_iteration()\n        if self._print_summary:\n            self.print_summary()", "contrast": "def complete_optimization(monitor):\n    if monitor.print_summary:\n        monitor.print_timing_summary()\n    monitor.optimisation_completed = True\n    monitor.run_tasks()", "label": 0}
{"index": "gp288588", "code": "def create_time_series(param_0, param_1, param_2, param_3, param_4):\n    for var_0 in param_0:\n        if param_4:\n            var_1 = {\n                'variableName': var_0,\n                'varMethod': {},\n                'data': [{\n                    param_2: param_0[var_0]\n                }]\n            }\n        else:\n            var_1 = {\n                'variableName': var_0,\n                'varMethod': {},\n                'data': [{\n                    'values': param_0[var_0],\n                    'time': param_1[param_2]['time']['values']\n                }]\n            }\n        ts.append(var_1)\n    return param_3", "contrast": "def _extract_table(table_data, current, pc, ts, tt):\n    current[\"tableType\"] = tt\n    current = _extract_table_root(table_data, current, pc)\n    current = _extract_table_model(table_data, current, tt)\n    _table_tmp = _extract_special(current, table_data)\n    try:\n        for _col_name, _col_data in table_data[\"columns\"].items():\n            _col_tmp = _extract_columns(_col_data, copy.deepcopy(_table_tmp), pc)\n            try:\n                ts.append(_col_tmp)\n            except Exception as e:\n                logger_ts.warn(\"extract_table: Unable to create ts entry, {}\".format(e))\n    except Exception as e:\n        logger_ts.error(\"extract_table: {}\".format(e))\n    return ts", "label": 1}
{"index": "gp129671", "code": "def unlock_keychain(param_0):\n    if 'SSH_TTY' not in os.environ:\n        return\n    if param_0 in _unlocked:\n        return\n    _unlocked.add(param_0)\n    if sys.platform == 'darwin':\n        sys.stderr.write(\"You are running under SSH. Please unlock your local OS X KeyChain:\\n\")\n        subprocess.call(['security', 'unlock-keychain'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)", "contrast": "import os\ndef unlock_keychain():\n    if os.environ.get(\"SSH_CONNECTION\"):\n        os.system(\"security unlock-keychain\")", "label": 0}
{"index": "gp045464", "code": "def encode_consumer_metadata_request(param_0, param_1, param_2, param_3):\n        var_0 = []\n        message.append(cls._encode_message_header(param_1, param_2,\n                                                  KafkaProtocol.CONSUMER_METADATA_KEY))\n        message.append(struct.pack('>h%ds' % len(param_3), len(param_3), param_3))\n        var_1 = b''.join(var_0)\n        return write_int_string(var_1)", "contrast": "def encode_consumer_metadata_request(client_id: str, correlation_id: int, payloads: str):\n    request_type = 10  \n    header = struct.pack('!hhi', request_type, 0, correlation_id)\n    encoded_client_id = client_id.encode('utf-8')\n    encoded_payloads = payloads.encode('utf-8')\n    body = struct.pack('!h{0}s{1}s'.format(len(encoded_client_id), len(encoded_payloads)), \n                       len(encoded_client_id), encoded_client_id, encoded_payloads)\n    message = header + body\n    return message", "label": 0}
{"index": "gp176145", "code": "import os\ndef create_symlink(param_0, param_1):\n    if not os.path.isabs(param_0) or not os.path.isabs(param_1):\n        raise ValueError(\"source_path and target_path should be absolute paths\")\n    if os.path.exists(param_1) or os.path.islink(param_1):\n        raise ValueError(\"target_path already exists or is a symlink\")\n    try:\n        os.symlink(param_0, param_1)\n    except OSError as e:\n        if e.errno == 17:\n            pass\n        elif e.errno == 2:\n            raise OSError(f\"{e.strerror}: {e.filename}\") from None\n        else:\n            raise e", "contrast": "def absolute_symlink(source_path, target_path):\n  if not os.path.isabs(source_path):\n    raise ValueError(\"Path for source : {} must be absolute\".format(source_path))\n  if not os.path.isabs(target_path):\n    raise ValueError(\"Path for link : {} must be absolute\".format(target_path))\n  if source_path == target_path:\n    raise ValueError(\"Path for link is identical to source : {}\".format(source_path))\n  try:\n    if os.path.lexists(target_path):\n      if os.path.islink(target_path) or os.path.isfile(target_path):\n        os.unlink(target_path)\n      else:\n        shutil.rmtree(target_path)\n    safe_mkdir_for(target_path)\n    os.symlink(source_path, target_path)\n  except OSError as e:\n    if not (e.errno == errno.EEXIST or e.errno == errno.ENOENT):\n      raise", "label": 1}
{"index": "gp075562", "code": "def _almost_equal(param_0, param_1):\n    var_0 = 1e-9\n    var_1 = np.abs(param_0 - param_1)\n    return (var_1 < var_0)", "contrast": "def almost_equal(num1, num2):\n    return abs(num1 - num2) < 0.0001", "label": 0}
{"index": "gp030213", "code": "def _make_charlist(param_0, param_1, param_2, param_3):\r\n        def _get_fmt(param_0):\r\n            if param_0 in param_2:\r\n                return param_2[param_0]\r\n            for var_0, var_1 in tokmap.items():\r\n                if param_0 in var_0: \r\n                    return var_1\r\n            return 'normal'\r\n        var_0 = []\r\n        for var_1, var_2 in param_1:\r\n            var_3 = param_3[_get_fmt(var_1)]\r\n            for var_4 in var_2:\r\n                charlist.append((var_3, var_4))\r\n        return var_0", "contrast": "import pygments.lexers as lexers\nclass Parser:\n    def __init__(self, text):\n        self._charlist = []\n        self.lexer = lexers.get_lexer_by_name('python')\n        self.parse_text(text)\n    def parse_text(self, text):\n        tokens = list(self.lexer.get_tokens(text))\n        for token in tokens:\n            token_value = token[1]\n            token_type = token[0]\n            for char in token_value:\n                self._charlist.append((char, token_type))\n    def get_charlist(self):\n        return self._charlist", "label": 0}
{"index": "gp209992", "code": "def set_risk_imtls(param_0):\n    var_0 = {}\n    for var_1, var_2 in risk_models.items():\n        for var_3, var_4 in loss_types.items():\n            if var_1 not in var_0:\n                var_0[var_1] = {}\n            var_0[var_1][var_3] = var_4()\n    setattr(set_risk_imtls, 'risk_imtls', var_0)", "contrast": "def set_risk_imtls(self, risk_models):\n        imtls = {}\n        for taxonomy, risk_functions in risk_models.items():\n            for risk_type, rf in risk_functions.items():\n                imt = rf.imt\n                from_string(imt)  \n                imls = list(rf.imls)\n                if imt in imtls and imtls[imt] != imls:\n                    logging.debug(\n                        'Different levels for IMT %s: got %s, expected %s',\n                        imt, imls, imtls[imt])\n                    imtls[imt] = sorted(set(imls + imtls[imt]))\n                else:\n                    imtls[imt] = imls\n        self.risk_imtls = imtls\n        if self.uniform_hazard_spectra:\n            self.check_uniform_hazard_spectra()", "label": 1}
{"index": "gp030200", "code": "def make_gettext_patterns():\r\n    var_0 = 'msgid msgstr'\r\n    var_1 = r\"\\b\" + any(\"keyword\", kwstr.split()) + r\"\\b\"\r\n    var_2 = any(\"builtin\", [r\"#,[^\\n]*\"])\r\n    var_3 = any(\"normal\", [r\"#:[^\\n]*\"])\r\n    var_4 = any(\"comment\", [r\"#[^\\n]*\"])\r\n    var_5 = any(\"number\",\r\n                 [r\"\\b[+-]?[0-9]+[lL]?\\b\",\r\n                  r\"\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b\",\r\n                  r\"\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?\\b\"])\r\n    var_6 = r\"(\\b[rRuU])?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*'?\"\r\n    var_7 = r'(\\b[rRuU])?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*\"?'\r\n    var_8 = any(\"string\", [var_6, var_7])\r\n    return \"|\".join([var_1, var_8, var_5, var_2, var_3, var_4,\r\n                     any(\"SYNC\", [r\"\\n\"])])", "contrast": "def make_pat(string):\n    pat_list = []\n    for key in sorted(string.keys(), reverse=True):\n        pat_list.append(re.escape(key))\n    pattern = \"|\".join(pat_list)\n    return re.compile(\"\\\\b({})\\\\b\".format(pattern), re.IGNORECASE)", "label": 0}
{"index": "gp187937", "code": "def wait_for_queue_completion(param_0):\n    queue.join()", "contrast": "def join (self, timeout=None):\n        with self.all_tasks_done:\n            if timeout is None:\n                while self.unfinished_tasks:\n                    self.all_tasks_done.wait()\n            else:\n                if timeout < 0:\n                    raise ValueError(\"'timeout' must be a positive number\")\n                endtime = _time() + timeout\n                while self.unfinished_tasks:\n                    remaining = endtime - _time()\n                    if remaining <= 0.0:\n                        raise Timeout()\n                    self.all_tasks_done.wait(remaining)", "label": 1}
{"index": "gp130936", "code": "def get_top_stories(param_0):\n        var_0 = \"v0/topstories.json\"\n        try:\n            var_1 = self._make_request(var_0)\n        except requests.HTTPError as e:\n            hn_logger.exception('Faulted on getting top stories, with status {}'.format(e.errno))\n            raise e\n        return var_1", "contrast": "import requests\ndef get_top_stories_items():\n    url = 'https://hacker-news.firebaseio.com/v0/topstories.json'\n    response = requests.get(url)\n    response.raise_for_status()\n    return response.json()", "label": 0}
{"index": "gp161454", "code": "def data96_send(param_0, param_1, param_2, param_3, param_4=False):\n                return self.send(self.data96_encode(param_1, param_2, param_3), param_4=param_4)", "contrast": "def data_packet(type: int, len: int, data: bytes) -> bytes:\n    packet = bytearray(96)\n    packet[0] = type\n    packet[1] = len\n    packet[2:2+len] = data\n    return bytes(packet)", "label": 0}
{"index": "gp189363", "code": "def reconfigure_medium_attachments(param_0):\n    try:\n        var_0 = your_session_machine  \n        if not machine.session.state == \"Locked\":\n            raise VBoxErrorInvalidVmState\n        if not machine.session.type == 'Direct':\n            raise VBoxErrorInvalidObjectState\n        for var_1 in param_0:\n            attachment.reconfigure()\n    except Exception as e:\n        print(str(e))", "contrast": "def reconfigure_medium_attachments(self, attachments):\n        if not isinstance(attachments, list):\n            raise TypeError(\"attachments can only be an instance of type list\")\n        for a in attachments[:10]:\n            if not isinstance(a, IMediumAttachment):\n                raise TypeError(\n                        \"array can only contain objects of type IMediumAttachment\")\n        self._call(\"reconfigureMediumAttachments\",\n                     in_p=[attachments])", "label": 1}
{"index": "gp101033", "code": "def property(param_0, param_1, param_2=Ellipsis):\n        try:\n            return self._a_tags[param_1]\n        except KeyError:\n            if param_2 != Ellipsis:\n                return param_2\n            else:\n                raise", "contrast": "def get_property_value(prop_dict, prop_name, default=None):\n    try:\n        return prop_dict[prop_name]\n    except KeyError:\n        if default is not None:\n            return default\n        else:\n            raise", "label": 0}
{"index": "gp248108", "code": "def convert_serial_to_flat(param_0, param_1, param_2):\n    var_0 = [[] for var_1 in range(param_2)]\n    for var_2 in range(param_2):\n        for var_3 in range(param_1):\n            var_4 = (var_2 * param_1 + var_3) * 3\n            var_0[var_2].append(tuple(param_0[var_4:var_4+3]))\n    var_5 = [pixel for var_6 in var_0 for pixel in var_6]\n    return var_5", "contrast": "def serialtoflat(self, raw, width=None):\n        if self.bitdepth == 8:\n            return raw\n        if self.bitdepth == 16:\n            raw = bytearray_to_bytes(raw)\n            return array('H',\n              struct.unpack('!%dH' % (len(raw)  \n        assert self.bitdepth < 8\n        if width is None:\n            width = self.width\n        spb = 8  \n        out = newBarray()\n        mask = 2**self.bitdepth - 1\n        shifts = [self.bitdepth * it for it in range(spb - 1, -1, -1)]\n        l = width\n        for o in raw:\n            out.extend([(mask&(o>>s)) for s in shifts][:l])\n            l -= spb\n            if l <= 0:\n                l = width\n        return out", "label": 1}
{"index": "gp167472", "code": "def simplified_pos(param_0, param_1=None):\n    if param_1 == 'penn':\n        if pos.startswith('N') or pos.startswith('V'):\n            return param_0[0]\n        elif pos.startswith('JJ'):\n            return 'ADJ'\n        elif pos.startswith('RB'):\n            return 'ADV'\n        else:\n            return None\n    else:   \n        if pos.startswith('N') or pos.startswith('V'):\n            return param_0[0]\n        elif pos.startswith('ADJ') or pos.startswith('ADV'):\n            return param_0[:3]\n        else:\n            return None", "contrast": "def simplified_pos_tag(pos, tagset='WordNet'):\n    if tagset == 'penn':\n        if pos.startswith('N'):\n            return 'N'\n        elif pos.startswith('V'):\n            return 'V'\n        elif pos.startswith('JJ'):\n            return 'ADJ'\n        elif pos.startswith('RB'):\n            return 'ADV'\n        else:\n            return None\n    else:\n        if pos.startswith('N'):\n            return 'N'\n        elif pos.startswith('V'):\n            return 'V'\n        elif pos.startswith('ADJ'):\n            return 'ADJ'\n        elif pos.startswith('ADV'):\n            return 'ADV'\n        else:\n            return None", "label": 0}
{"index": "gp148170", "code": "def _gather(param_0, param_1):\n        for var_0, var_1 in self.traverse_ingredients():\n            for var_2, var_3 in param_1(var_0):\n                if var_0 == param_0:\n                    var_2 = var_2[len(self.path) + 1:]\n                yield var_2, var_3", "contrast": "def remove_experiment_path(gathered_items, prefix):\n    return [item.replace(prefix+'.','') for item in gathered_items]", "label": 0}
{"index": "gp210000", "code": "def get_gmf_data():\n    return gmf_data", "contrast": "def gmf_data_dt(self):\n        return numpy.dtype(\n            [('rlzi', U16), ('sid', U32),\n             ('eid', U64), ('gmv', (F32, (len(self.imtls),)))])", "label": 1}
{"index": "gp059698", "code": "def remodel_run(param_0, param_1=None, **param_2):\n    if not param_1:\n        with remodel.connection.get_conn() as var_0:\n            return run(param_0, var_0, **param_2)\n    else:\n        return run(param_0, param_1, **param_2)", "contrast": "def run_query_from_pool(pool, query):\n    connection = pool.getconn()\n    try:\n        result = connection.cursor().execute(query)\n        connection.commit()\n        return result\n    finally:\n        pool.putconn(connection)", "label": 0}
{"index": "gp316249", "code": "def modified_NX_fast_BFS_node_generator(param_0, param_1=None):\n    if param_1 is None:\n        param_1 = list(param_0)[0]\n    var_0 = set([param_1])\n    var_1 = deque([(param_1, 0)])\n    while var_1:\n        var_2, var_3 = queue.popleft()\n        yield var_2, var_3\n        var_4 = param_0[var_2]\n        for var_5 in var_4:\n            if var_5 not in var_0:\n                visited.add(var_5)\n                queue.append((var_5, var_3 + 1))", "contrast": "def __plain_bfs(adj, source):\n        seen = set()\n        nextlevel = {source}\n        while nextlevel:\n            thislevel = nextlevel\n            nextlevel = set()\n            for v in thislevel:\n                if v not in seen:\n                    yield v\n                    seen.add(v)\n                    nextlevel.update(adj[v])", "label": 1}
{"index": "gp063888", "code": "def _set_last_rcvd_interface(param_0, param_1, param_2=False):\n    if hasattr(param_1, \"_utype\"):\n      param_1 = v._utype(param_1)\n    try:\n      var_0 = YANGDynClass(param_1,base=last_rcvd_interface.last_rcvd_interface, is_container='container', presence=False, yang_name=\"last-rcvd-interface\", rest_name=\"last-rcvd-interface\", parent=param_0, choice=(u'request-type', u'get-next-request'), path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions=None, namespace='urn:brocade.com:mgmt:brocade-interface-ext', defining_module='brocade-interface-ext', yang_type='container', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': \"\"\"last_rcvd_interface must be of a type compatible with container\"\"\",\n          'defined-type': \"container\",\n          'generated-type': \"\"\"YANGDynClass(base=last_rcvd_interface.last_rcvd_interface, is_container='container', presence=False, yang_name=\"last-rcvd-interface\", rest_name=\"last-rcvd-interface\", parent=self, choice=(u'request-type', u'get-next-request'), path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions=None, namespace='urn:brocade.com:mgmt:brocade-interface-ext', defining_module='brocade-interface-ext', yang_type='container', is_config=True)\"\"\",\n        })\n    self.__last_rcvd_interface = var_0\n    if hasattr(param_0, '_set'):\n      self._set()", "contrast": "def _set_last_rcvd_interface(self, value):\n    self.last_rcvd_interface = value", "label": 0}
{"index": "gp103946", "code": "def _draw_tickgram(param_0):\n    var_0 = max(filter(lambda x : type(x)==int,param_0))\n    if var_0 == 0 :\n        return upticks[0]*len(param_0)\n    else:\n        var_1 = [ float(var_2)/var_0 if type(var_2)==int else var_2 for var_2 in param_0 ]\n        var_3 = [ int(math.ceil(var_2*len(upticks))) if type(var_2)==float else var_2 for var_2 in var_1 ]\n        return ''.join([ ' ' if type(var_2)==str else upticks[var_2-1] if var_2 != 0 else upticks[0] for var_2 in var_3 ])", "contrast": "def generate_ticks(nums):\n    ticks = []\n    for num in nums:\n        tick = ''\n        for i in range(num):\n            tick += '|'\n        ticks.append(tick)\n    return ticks", "label": 0}
{"index": "gp054238", "code": "def play(param_0, param_1=1.0):\n    var_0 = stash_state()\n    var_1 = None\n    for var_2 in param_0:\n        if param_1 > 0 and var_1 is not None:\n            _time.sleep((event.time - var_1) / param_1)\n        var_1 = event.time\n        var_3 = event.scan_code or event.name\n        press(var_3) if event.event_type == KEY_DOWN else release(var_3)\n    restore_modifiers(var_0)", "contrast": "import time\nimport keyboard\ndef play(recorded_events, speed_factor=1):\n    keyboard.clear_all_hotkeys()\n    keyboard.unhook_all()\n    keyboard.press_and_release('ctrl+r') \n    keyboard.unhook_all()\n    keyboard.play(recorded_events, speed_factor=speed_factor)\n    keyboard.clear_all_hotkeys()\n    keyboard.unhook_all()\n    keyboard.press_and_release('ctrl+r')\n    keyboard.unhook_all()", "label": 0}
{"index": "gp245056", "code": "def find_largest_region(param_0, param_1):\n    var_0 = None\n    var_1 = 0\n    for var_2 in range(len(param_0)):\n        var_3 = len(param_0[var_2])\n        if var_3 > var_1:\n            var_0 = param_1[var_2]\n            var_1 = var_3\n    var_4 = (var_0[0] + var_0[1]) / 2\n    return var_0, var_4", "contrast": "def label_position(self):\n        reg_sizes = [(r.size(), r) for r in self.pieces]\n        reg_sizes.sort()\n        return reg_sizes[-1][1].label_position()", "label": 1}
{"index": "gp009706", "code": "def decode_date(param_0, param_1):\n    if isinstance(param_1, basestring) and val.count('-') == 2 and len(param_1) > 9:\n      try:\n        var_0 = dateutil.parser.parse(param_1)\n        if val.endswith(('+00:00', '-00:00', 'Z')):\n          var_0 = dt.replace(tzinfo=None)\n        return var_0\n      except (TypeError, ValueError):\n        pass\n    return param_1", "contrast": "from datetime import datetime\ndef decode_date_string(date_str):\n    date_formats = [\n        '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y', '%m-%d-%Y', '%m-%d-%y', '%d %b %Y', \n        '%d %B %Y', '%b %d, %Y', '%B %d, %Y', '%d %b %y', '%d %B %y', \n        '%b %d, %y', '%B %d, %y', '%Y.%m.%d', '%Y/%m/%d', '%Y %m %d'\n    ]\n    for date_format in date_formats:\n        try:\n            return datetime.strptime(date_str, date_format)\n        except ValueError:\n            pass\n    raise ValueError(\"no valid date format found for %r\" % date_str)", "label": 0}
{"index": "gp095461", "code": "def first_match(param_0, param_1):\n    for var_0 in param_1:\n        var_1 = param_0(var_0)\n        if var_1 is not None:\n            return var_1\n    return None", "contrast": "def first_match(predicate, lst):\n    for item in lst:\n        if predicate(item) is not None:\n            return predicate(item)\n    return None", "label": 0}
{"index": "gp019698", "code": "def uniform_binning_correction(param_0, param_1=8):\n  var_0 = 2**param_1\n  var_1, var_2, var_3, var_4 = common_layers.shape_list(param_0)\n  var_5 = float(var_2 * var_3 * var_4)\n  param_0 = param_0 + tf.random_uniform(\n      shape=(var_1, var_2, var_3, var_4),\n      minval=0.0, maxval=1.0/var_0)\n  var_6 = -np.log(var_0) * var_5 * tf.ones(var_1)\n  return param_0, var_6", "contrast": "import tensorflow as tf\ndef replace_x_with_q(x, n_bits=None):\n    u = tfp.distributions.Uniform(low=x, high=x+1.0/256.0)\n    q = u.prob(x)\n    objective = -q*tf.math.log(q)\n    return x ~ u, objective", "label": 0}
{"index": "gp224665", "code": "from django.core.mail import send_mail\nfrom django.conf import settings\ndef send_message_to_admins(param_0, param_1):\n    var_0 = getattr(settings, 'DBBACKUP_ADMINS', [])\n    if var_0:\n        send_mail(param_0=param_0, param_1=param_1, from_email=None, recipient_list=var_0)", "contrast": "def mail_admins(subject, message, fail_silently=False, connection=None,\n                html_message=None):\n    if not settings.ADMINS:\n        return\n    mail = EmailMultiAlternatives('%s%s' % (settings.EMAIL_SUBJECT_PREFIX, subject),\n                                  message, settings.SERVER_EMAIL, [a[1] for a in settings.ADMINS],\n                                  connection=connection)\n    if html_message:\n        mail.attach_alternative(html_message, 'text/html')\n    mail.send(fail_silently=fail_silently)", "label": 1}
{"index": "gp213820", "code": "def mark_main_entry(param_0):\n    var_0 = None\n    for var_1 in param_0:\n        if entry.get('response', {}).get('body', None):\n            var_0 = var_1\n            break\n    if var_0:\n        var_0['isMain'] = True\n        for var_1 in param_0:\n            if var_1 != var_0:\n                var_1['isMain'] = False\n    else:\n        raise Exception('No main entry found')", "contrast": "def mark_entries(self, entries):\n        for entry in entries:\n            self._set_entry_type(entry, RESOURCE_ENTRY)\n        main_entry = entries[0]\n        main_location = self._get_location(main_entry)\n        if not main_location:\n            self._set_entry_type(main_entry, MAIN_ENTRY)\n            return\n        main_url = urllib.parse.urljoin(get_url(main_entry), main_location)\n        for entry in entries[1:]:\n            url = get_url(entry)\n            if url == main_url:\n                self._set_entry_type(entry, MAIN_ENTRY)\n                break\n        else:\n            self._set_entry_type(main_entry, MAIN_ENTRY)", "label": 1}
{"index": "gp282176", "code": "import pandas as pd\nimport sqlite3\ndef create_runs_dataframe():\n    var_0 = sqlite3.connect('runs.db') \n    var_1 = pd.read_sql_query(\"SELECT * from runs\", var_0)\n    conn.close()\n    return var_1", "contrast": "def create_info_df(self):\n        logger.debug(\"running create_info_df\")\n        reader = self.reader()\n        self.info_df = make_df_from_batch(self.name, batch_col=self.batch_col,\n                                          reader=reader)\n        logger.debug(str(self.info_df.head(5)))", "label": 1}
{"index": "gp009390", "code": "def capacity_meyerhof_and_hanna_1978(param_0, param_1, param_2, param_3, param_4=1e6, param_5=0):\n    var_0 = sm.SoilProfile()\n    sp.add_layer(0, param_0)\n    sp.add_layer(param_2, param_1)\n    sp.gwl = param_4\n    return capacity_sp_meyerhof_and_hanna_1978(var_0, param_3)", "contrast": "def meyerhof_hanna_capacity(sl_0, sl_1, h0, fd, wtl, verbose=False):\n    c = sl_1.c_u + ((sl_0.c_u - sl_1.c_u) * h0 / fd.b)\n    phi = sl_1.phi_u + ((sl_0.phi_u - sl_1.phi_u) * h0 / fd.b)\n    gamma_sat = (sl_1.gamma_sat * h0 + sl_0.gamma_sat * (fd.b - h0)) / fd.b\n    gamma_star = gamma_sat / fd.gamma\n    z = fd.depth - wtl\n    q_u1 = (c * fd.N_c + fd.q * fd.N_q + 0.5 * fd.gamma * fd.B * fd.N_gamma * gamma_star * z) * fd.gamma\n    if z < fd.d_f:\n        q_u2 = (c * (fd.N_c + 1) + fd.q * fd.N_q + 0.5 * fd.gamma * fd.B * fd.N_gamma * gamma_star *\n                z * fd.N_qc) * fd.gamma\n    else:\n        q_u2 = ((c * fd.N_c + fd.q * fd.N_q + 0.5 * fd.gamma * fd.B * fd.N_gamma *\n                 gamma_star * z) * fd.gamma) / (1 + fd.N_qc * (z / fd.d_f))\n    q_u = min(q_u1, q_u2)\n    if verbose:\n        print(f\"Meyerhof-Hanna capacity calculation: q_u = {q_u:.2f} kPa\")\n    return q_u", "label": 0}
{"index": "gp075354", "code": "def handle_message_registered(param_0, param_1, param_2):\n        var_0 = None\n        if param_1[\"method\"] == \"EVENT\":\n            logger.debug(\"<%s> <euuid:%s> Event message \"\n                         \"received\" % (param_1[\"cuuid\"], param_1[\"euuid\"]))\n            var_0 = self.event(param_1[\"cuuid\"],\n                                  param_2,\n                                  param_1[\"euuid\"],\n                                  param_1[\"event_data\"],\n                                  param_1[\"timestamp\"],\n                                  param_1[\"priority\"])\n        elif param_1[\"method\"] == \"OK EVENT\":\n            logger.debug(\"<%s> <euuid:%s> Event confirmation message \"\n                         \"received\" % (param_1[\"cuuid\"], param_1[\"euuid\"]))\n            try:\n                del self.event_uuids[param_1[\"euuid\"]]\n            except KeyError:\n                logger.warning(\"<%s> <euuid:%s> Euuid does not exist in event \"\n                               \"buffer. Key was removed before we could process \"\n                               \"it.\" % (param_1[\"cuuid\"], param_1[\"euuid\"]))\n        elif param_1[\"method\"] == \"OK NOTIFY\":\n            logger.debug(\"<%s> <euuid:%s> Ok notify \"\n                         \"received\" % (param_1[\"cuuid\"], param_1[\"euuid\"]))\n            try:\n                del self.event_uuids[param_1[\"euuid\"]]\n            except KeyError:\n                logger.warning(\"<%s> <euuid:%s> Euuid does not exist in event \"\n                               \"buffer. Key was removed before we could process \"\n                               \"it.\" % (param_1[\"cuuid\"], param_1[\"euuid\"]))\n        return var_0", "contrast": "def process_message(msg: str, host: tuple) -> str:\n    return response", "label": 0}
{"index": "gp019432", "code": "def _init_from_npy2d(param_0, param_1, param_2, param_3):\n        if len(mat.shape) != 2:\n            raise ValueError('Input numpy.ndarray must be 2 dimensional')\n        var_0 = np.array(mat.reshape(mat.size), copy=False, dtype=np.float32)\n        var_1 = ctypes.c_void_p()\n        param_2 = param_2 if param_2 is not None else np.nan\n        if param_3 is None:\n            _check_call(_LIB.XGDMatrixCreateFromMat(\n                data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n                c_bst_ulong(mat.shape[0]),\n                c_bst_ulong(mat.shape[1]),\n                ctypes.c_float(param_2),\n                ctypes.byref(var_1)))\n        else:\n            _check_call(_LIB.XGDMatrixCreateFromMat_omp(\n                data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n                c_bst_ulong(mat.shape[0]),\n                c_bst_ulong(mat.shape[1]),\n                ctypes.c_float(param_2),\n                ctypes.byref(var_1),\n                param_3))\n        self.handle = var_1", "contrast": "import numpy as np\ndef initialize_data(mat):\n    if mat.flags['C_CONTIGUOUS'] and mat.dtype == np.float32:\n        return mat\n    else:\n        temp_mat = np.ascontiguousarray(mat, dtype=np.float32)\n        return temp_mat", "label": 0}
{"index": "gp308633", "code": "def tagBlocks(param_0):\n        return [var_0 for var_0 in self.children if isinstance(var_0, AdvancedTag)]", "contrast": "def tagBlocks(self):\n        myBlocks = self.blocks\n        return [block for block in myBlocks if issubclass(block.__class__, AdvancedTag)]", "label": 1}
{"index": "gp155991", "code": "def _detect(param_0):\n        var_0 = []\n        for var_1 in self.slither.contracts_derived:\n            var_2 = self.detect_uninitialized(var_1)\n            for var_3, var_4 in var_2:\n                var_5 = \"{}.{} ({}) is never initialized. It is used in:\\n\"\n                var_5 = info.format(variable.contract.name,\n                                   variable.name,\n                                   variable.source_mapping_str)\n                for var_6 in var_4:\n                    var_5 += \"\\t- {} ({})\\n\".format(f.name, f.source_mapping_str)\n                var_7 = [variable.source_mapping]\n                var_7 += [f.source_mapping for var_6 in var_4]\n                var_8 = self.generate_json_result(var_5)\n                self.add_variable_to_json(var_3, var_8)\n                self.add_functions_to_json(var_4, var_8)\n                results.append(var_8)\n        return var_0", "contrast": "def detect_uninitialized_state_variables(contract):\n    uninitialized_state_variables = {}\n    for contract_name, contract_data in contract.items():\n        for state_variable_name in contract_data['abi']['networks']['debug']['address'].keys():\n            if contract_data['variables'][state_variable_name]['value'] is None:\n                if contract_name in uninitialized_state_variables:\n                    uninitialized_state_variables[contract_name].add(state_variable_name)\n                else:\n                    uninitialized_state_variables[contract_name] = {state_variable_name}\n    return uninitialized_state_variables", "label": 0}
{"index": "gp127592", "code": "def wrap(param_0, param_1):\n        if isinstance(param_1, ApplicationException):\n            return param_1\n        self.with_cause(param_1)\n        return param_0", "contrast": "class ApplicationException(Exception):\n    pass\ndef wrap_with_application_exception(cause):\n    if isinstance(cause, ApplicationException):\n        return cause\n    else:\n        return ApplicationException(str(cause)).with_traceback(cause.__traceback__)", "label": 0}
{"index": "gp276868", "code": "import numpy as np\nimport pandas as pd\ndef extract_features(param_0, param_1, param_2=None, param_3=False):\n    if param_3:\n        var_0 = data_frame.iloc[data_frame.index == param_1, :-1].values\n    else:\n        var_0 = data_frame.loc[param_1].values[:-1]\n    if param_2 is not None:\n        var_0 = np.delete(var_0, np.where(var_0 == param_2))\n    return var_0", "contrast": "def __get_features_for_observation(self, data_frame=None, observation='LA-LL',\n                                       skip_id=None, last_column_is_id=False):\n        try:\n            features = np.array([])\n            if data_frame is None:\n                data_frame = self.data_frame\n            for index, row in data_frame.iterrows():\n                if not skip_id == row['id']:\n                    features_row = np.nan_to_num(row[row.keys().str.contains(observation)].values)\n                    features_row = np.append(features_row, row['id'])\n                    features = np.vstack([features, features_row]) if features.size else features_row\n            if last_column_is_id:\n                if np.ndim(features) > 1:\n                    to_return = features[:,:-1]\n                else:\n                    to_return = features[:-1]\n            else:\n                to_return = features\n            return to_return, data_frame['id'].values\n        except:\n            logging.error(\" observation not found in data frame\")", "label": 1}
{"index": "gp329327", "code": "import numpy as np\ndef get_dtype_from_jsonschema(param_0):\n    var_0 = {\n        'integer': np.int64,\n        'number': np.float64,\n        'string': np.object,\n        'boolean': np.bool_,\n        'array': np.ndarray,\n        'object': np.object\n    }\n    return mapping.get(typespec.get('type', 'object'), np.object)", "contrast": "def __get_dtype(typespec):\n    if 'type' in typespec:\n        return __TYPE_MAP__.get(typespec['type'], np.object_)\n    elif 'enum' in typespec:\n        return np.object_\n    elif 'oneOf' in typespec:\n        types = [__get_dtype(v) for v in typespec['oneOf']]\n        if all([t == types[0] for t in types]):\n            return types[0]\n    return np.object_", "label": 1}
{"index": "gp238555", "code": "import requests\ndef execute_request(param_0):\n    var_0 = requests.get(param_0)\n    return response.content", "contrast": "def request(self,\n                method,\n                url,\n                params=None,\n                data=None,\n                files=None,\n                json=None,\n                timeout=5,\n                headers=None,\n                skip_auth=False):\n        request_url = self.base_url + url\n        floyd_logger.debug(\"Starting request to url: %s with params: %s, data: %s\", request_url, params, data)\n        request_headers = {'x-floydhub-cli-version': get_cli_version()}\n        if self.auth_header:\n            request_headers[\"Authorization\"] = self.auth_header\n        if headers:\n            request_headers.update(headers)\n        try:\n            response = requests.request(method,\n                                        request_url,\n                                        params=params,\n                                        data=data,\n                                        json=json,\n                                        headers=request_headers,\n                                        files=files,\n                                        timeout=timeout)\n        except requests.exceptions.ConnectionError as exception:\n            floyd_logger.debug(\"Exception: %s\", exception, exc_info=True)\n            sys.exit(\"Cannot connect to the Floyd server. Check your internet connection.\")\n        except requests.exceptions.Timeout as exception:\n            floyd_logger.debug(\"Exception: %s\", exception, exc_info=True)\n            sys.exit(\"Connection to FloydHub server timed out. Please retry or check your internet connection.\")\n        floyd_logger.debug(\"Response Content: %s, Headers: %s\" % (response.content, response.headers))\n        self.check_response_status(response)\n        return response", "label": 1}
{"index": "gp128169", "code": "def merge_true_table():\n    var_0 = pd.ExcelWriter(\"True Table.xlsx\")\n    for var_1 in Path(__file__).parent.select_by_ext(\".csv\"):\n        var_2 = pd.read_csv(p.abspath, index_col=0)\n        df.to_excel(var_0, p.fname, index=True)\n    writer.save()", "contrast": "import pandas as pd\ndef merge_true_tables_to_excel(all_true_tables, output_file_name):\n    writer = pd.ExcelWriter(output_file_name, engine='xlsxwriter')\n    for table_name, table_data in all_true_tables.items():\n        df = pd.DataFrame(table_data)\n        df.to_excel(writer, sheet_name=table_name, index=False)\n    writer.save()", "label": 0}
{"index": "gp036238", "code": "def link_mountpoint(param_0, param_1):\n        var_0 = salt.utils.path.join(repo.linkdir, repo._mountpoint)\n        var_1 = salt.utils.path.join(repo.cachedir, repo.root()).rstrip(os.sep)\n        var_2 = False\n        var_3 = False\n        try:\n            with repo.gen_lock(lock_type='mountpoint', timeout=10):\n                var_4 = list(os.walk(repo.linkdir, followlinks=False))\n                if var_4 != repo.linkdir_walk:\n                    log.debug(\n                        'Results of walking %s differ from expected results',\n                        repo.linkdir\n                    )\n                    log.debug('Walk results: %s', var_4)\n                    log.debug('Expected results: %s', repo.linkdir_walk)\n                    var_2 = True\n                else:\n                    if not all(not salt.utils.path.islink(var_5[0])\n                               and os.path.isdir(var_5[0])\n                               for var_5 in var_4[:-1]):\n                        log.debug(\n                            'Linkdir parents of %s are not all directories',\n                            var_0\n                        )\n                        var_2 = True\n                    elif not salt.utils.path.islink(var_0):\n                        var_2 = True\n                    else:\n                        try:\n                            var_6 = salt.utils.path.readlink(var_0)\n                        except Exception:\n                            log.debug(\n                                'Failed to read destination of %s', var_0\n                            )\n                            var_2 = True\n                        else:\n                            if var_6 != var_1:\n                                log.debug(\n                                    'Destination of %s (%s) does not match '\n                                    'the expected value (%s)',\n                                    var_0, var_6, var_1\n                                )\n                                try:\n                                    if salt.utils.platform.is_windows()                                            and not ldest.startswith('\\\\\\\\')                                            and os.path.isdir(var_6):\n                                        shutil.rmtree(var_0)\n                                    else:\n                                        os.remove(var_0)\n                                except Exception as exc:\n                                    log.exception(\n                                        'Failed to remove existing git_pillar '\n                                        'mountpoint link %s: %s',\n                                        var_0, exc.__str__()\n                                    )\n                                var_2 = False\n                                var_3 = True\n                if var_2:\n                    var_3 = True\n                    try:\n                        shutil.rmtree(repo.linkdir)\n                    except OSError:\n                        pass\n                    try:\n                        var_7 = os.path.dirname(var_0)\n                        os.makedirs(var_7)\n                        log.debug('Successfully made linkdir parent %s', var_7)\n                    except OSError as exc:\n                        log.error(\n                            'Failed to os.makedirs() linkdir parent %s: %s',\n                            var_7, exc.__str__()\n                        )\n                        return False\n                if var_3:\n                    try:\n                        os.symlink(var_1, var_0)\n                        log.debug(\n                            'Successfully linked %s to cachedir %s',\n                            var_0, var_1\n                        )\n                        return True\n                    except OSError as exc:\n                        log.error(\n                            'Failed to create symlink to %s at path %s: %s',\n                            var_1, var_0, exc.__str__()\n                        )\n                        return False\n        except GitLockError:\n            log.error(\n                'Timed out setting mountpoint lock for %s remote \\'%s\\'. If '\n                'this error persists, it may be because an earlier %s '\n                'checkout was interrupted. The lock can be cleared by running '\n                '\\'salt-run cache.clear_git_lock %s type=mountpoint\\', or by '\n                'manually removing %s.',\n                self.role, repo.id, self.role, self.role,\n                repo._get_lock_file(lock_type='mountpoint')\n            )\n            return False\n        return True", "contrast": "def ensure_mountpoint(mountpoint_path: str, correct_location: str, correct_path: str) -> bool:\n    if os.path.exists(mountpoint_path):\n        if os.path.realpath(mountpoint_path) == correct_path and os.path.realpath(os.path.join(mountpoint_path, '..')) == correct_location:\n            return True\n        else:\n            return False\n    else:\n        return False", "label": 0}
{"index": "gp220451", "code": "def sort_dataframe_by_column(param_0, param_1):\n    return df.sort_values(by=param_1)", "contrast": "def arrange(*args):\n  names = [column._name for column in args]\n  def f(df):\n    sortby_df = df >> mutate(*args)\n    index = sortby_df.sort_values([str(arg) for arg in args]).index\n    return df.loc[index]\n  return f", "label": 1}
{"index": "gp002689", "code": "def constant(param_0: int, param_1: complex, param_2: str = None) -> SamplePulse:\n    return _sampled_constant_pulse(param_0, param_1, param_2=param_2)", "contrast": "from qiskit.pulse import Waveform\nfrom qiskit.pulse import pulse_lib\ndef generate_sample_pulse(duration, amp, name):\n    sample_pulse = pulse_lib.SamplePulse(list(repeat(amp, duration)), name)\n    return sample_pulse.waveform()", "label": 0}
{"index": "gp005405", "code": "def build(param_0, param_1=None, param_2=False):\n        if param_1 is None:\n            param_1 = []\n        if self.max is not None:\n            if param_2:\n                var_0 = [self.values[0]]\n            else:\n                var_0 = [self.values[0]] * rand.randint(1, self.max+1)\n        else:\n            var_0 = self.values\n        var_1 = []\n        for var_2 in var_0:\n            try:\n                var_3 = utils.val(var_2, param_1, param_2=param_2)\n                joins.append(var_3)\n            except errors.OptGram as e:\n                continue\n        return self.sep.join(var_1)", "contrast": "from marshmallow import fields, pre_load\nclass Join(fields.List):\n    def __init__(self, cls_or_instance, **kwargs):\n        super().__init__(cls_or_instance, **kwargs)\n        self.data_key = 'Join'\n    @pre_load(pass_many=True)\n    def process_data(self, data, many):\n        if isinstance(data, list):\n            return data\n        return [data]", "label": 0}
{"index": "gp022164", "code": "def _multiple_callbacks(param_0, *param_1, **param_2):\n    if isinstance(param_0, list):\n        for var_0 in param_0:\n            var_0(*param_1, **param_2)\n        return\n    if param_0:\n        param_0(*param_1, **param_2)", "contrast": "def send_to_callbacks(*args, **kwargs):\n    if callbacks is None:\n        return\n    elif callable(callbacks):\n        return callbacks(*args, **kwargs)\n    else:\n        results = []\n        for callback in callbacks:\n            result = callback(*args, **kwargs)\n            results.append(result)\n        return results", "label": 0}
{"index": "gp018921", "code": "def compounding(param_0, param_1, param_2):\n    def clip(param_0):\n        return max(param_0, param_1) if (param_0 > param_1) else min(param_0, param_1)\n    var_0 = float(param_0)\n    while True:\n        yield clip(var_0)\n        var_0 *= param_2", "contrast": "def compounding(initial, rate, compound_rate):\n    value = initial\n    yield initial\n    while True:\n        value *= compound_rate\n        yield value", "label": 0}
{"index": "gp236507", "code": "def get_device_by_name(param_0):\n    from soco import discover\n    var_0 = discover()\n    for var_1 in var_0:\n        if device.player_name == param_0:\n            return var_1\n    return None", "contrast": "def by_name(name):\n    devices = discover(all_households=True)\n    for device in (devices or []):\n        if device.player_name == name:\n            return device\n    return None", "label": 1}
{"index": "gp171113", "code": "def find_outer_edges(param_0, param_1):\n    var_0 = []\n    for var_1 in param_1:\n        for var_2 in range(3):\n            var_3 = [var_1[var_2], var_1[(var_2+1)%3]]\n            if var_3 not in var_0 and var_3[::-1] not in var_0:\n                if tri.neighbors[var_1[var_2]] == -1 or tri.neighbors[var_1[(var_2+1)%3]] == -1:\n                    edges.append(var_3)\n                elif not tri.is_encroached(var_1, [var_1[var_2], var_1[(var_2+1)%3]]):\n                    edges.append(var_3)\n    return var_0", "contrast": "def find_local_boundary(tri, triangles):\n    edges = []\n    for triangle in triangles:\n        for i in range(3):\n            pt1 = tri.simplices[triangle][i]\n            pt2 = tri.simplices[triangle][(i + 1) % 3]\n            if (pt1, pt2) in edges:\n                edges.remove((pt1, pt2))\n            elif (pt2, pt1) in edges:\n                edges.remove((pt2, pt1))\n            else:\n                edges.append((pt1, pt2))\n    return edges", "label": 1}
{"index": "gp336386", "code": "import runtimepath\ndef get_vim_runtimepath():\n    return runtimepath.RuntimePath('vim')", "contrast": "def runtimepath(self):\n        if self._runtimepath is None:\n            self._runtimepath = runtimepath.RuntimePath(self)\n        return self._runtimepath", "label": 1}
{"index": "gp003380", "code": "def user(param_0, param_1):\n        var_0 = None\n        if param_1 in self._users:\n            return self._users[param_1]\n        var_1 = urijoin(self.base_url, 'users', param_1)\n        logging.info(\"Getting info for %s\" % (var_1))\n        var_2 = self.fetch(var_1)\n        var_0 = r.text\n        self._users[param_1] = var_0\n        return var_0", "contrast": "def update_user_cache(user_info, user_cache):\n    user_cache.update(user_info)", "label": 0}
{"index": "gp155457", "code": "def store_atlas_zonefile_data(zonefile_data, zonefile_dir, fsync=True):\n    if not os.path.exists(zonefile_dir):\n        os.makedirs(zonefile_dir, 0700 )\n    zonefile_hash = get_zonefile_data_hash( zonefile_data )\n    zonefile_path = atlas_zonefile_path( zonefile_dir, zonefile_hash )\n    zonefile_dir_path = os.path.dirname(zonefile_path)\n    if os.path.exists(zonefile_path):\n        return True\n    if not os.path.exists(zonefile_dir_path):\n        os.makedirs(zonefile_dir_path)\n    try:\n        with open( zonefile_path, \"wb\" ) as f:\n            f.write(zonefile_data)\n            f.flush()\n            if fsync:\n                os.fsync(f.fileno())\n    except Exception, e:\n        log.exception(e)\n        return False\n    return True", "contrast": "def store_zonefile(zonefile_data):\n    authenticated = True \n    if authenticated:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp087432", "code": "def _decode_sense_packet(param_0, param_1, param_2):\n        var_0 = self._sense_packet_to_data(param_2)\n        var_1 = 4\n        var_2 = 0\n        var_3 = len(var_0) - var_1 - 6\n        var_4 = int(var_3 / 2)\n        var_5 = []\n        for var_2 in range(var_4):\n            var_6 = var_2 * 2 + var_1\n            temp.append(self._decode_temp(var_0[var_6], var_0[var_6 + 1]))\n        self._debug(PROP_LOGLEVEL_DEBUG, \"T: \" + str(var_5))\n        for var_7 in self._sense_sensor:\n            if (sensor.sensor_type == PROP_SENSOR_TEMPERATURE):\n                sensor.value = var_5[sensor.index]\n            elif (sensor.sensor_type == PROP_SENSOR_RAW):\n                sensor.value = param_2\n        self._debug(PROP_LOGLEVEL_DEBUG, str(param_0))", "contrast": "def decode_sense_packet(sense_packet):\n    sensors = []\n    for i in range(len(sense_packet)):\n        sensors.append(int(sense_packet[i:i+1]))\n    return sensors", "label": 0}
{"index": "gp333143", "code": "def has_dict(param_0):\n    if isinstance(param_0, dict): \n        if any(isinstance(var_0, dict) for var_0 in data.values()): \n            return True\n    return False", "contrast": "def _check_for_inception(self, root_dict):\n    for key in root_dict:\n      if isinstance(root_dict[key], dict):\n          root_dict[key] = ResponseObject(root_dict[key])\n    return root_dict", "label": 1}
{"index": "gp217293", "code": "def append_transitions(param_0: Tuple, param_1=None) -> Operation:\n    pass ", "contrast": "def append(self, transitions, rows=None):\n    rows = tf.range(self._capacity) if rows is None else rows\n    assert rows.shape.ndims == 1\n    assert_capacity = tf.assert_less(\n        rows, self._capacity,\n        message='capacity exceeded')\n    with tf.control_dependencies([assert_capacity]):\n      assert_max_length = tf.assert_less(\n          tf.gather(self._length, rows), self._max_length,\n          message='max length exceeded')\n    with tf.control_dependencies([assert_max_length]):\n      timestep = tf.gather(self._length, rows)\n      indices = tf.stack([rows, timestep], 1)\n      append_ops = tools.nested.map(\n          lambda var, val: tf.scatter_nd_update(var, indices, val),\n          self._buffers, transitions, flatten=True)\n    with tf.control_dependencies(append_ops):\n      episode_mask = tf.reduce_sum(tf.one_hot(\n          rows, self._capacity, dtype=tf.int32), 0)\n      return self._length.assign_add(episode_mask)", "label": 1}
{"index": "gp031149", "code": "def is_copy_only_path(param_0, param_1):\n    try:\n        for var_0 in param_1['cookiecutter']['_copy_without_render']:\n            if fnmatch.fnmatch(param_0, var_0):\n                return True\n    except KeyError:\n        return False\n    return False", "contrast": "import fnmatch\ndef should_only_copy(path, context):\n    for pattern in context.get('copy_only', []):\n        if fnmatch.fnmatchcase(path, pattern):\n            return True\n    return False", "label": 0}
{"index": "gp061111", "code": "def variant_matches_reference_sequence(param_0, param_1, param_2):\n    if param_2 == \"-\":\n        param_1 = reverse_complement_dna(param_1)\n    return param_1 == variant.ref", "contrast": "def verify_reference_nucleotides(expected_ref, observed_ref):\n    return expected_ref == observed_ref", "label": 0}
{"index": "gp125099", "code": "def check_table_exists(param_0, param_1):\n    var_0 = dbcon.cursor()\n    dbcur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='%s';\" % param_1)\n    var_1 = dbcur.fetchone()\n    dbcur.close()\n    if var_1 is None:\n        return False\n    else:\n        try:\n            return var_1[0] == param_1\n        except IndexError as e:\n            return check_table_exists(param_0, param_1)", "contrast": "def table_exists(dbcon, tablename):\n    cur = dbcon.cursor()\n    cur.execute(\"\"\"SELECT COUNT(*)\n                   FROM information_schema.tables\n                   WHERE table_name = '{0}'\"\"\".format(tablename.replace('\\'', '\\'\\'')))\n    if cur.fetchone()[0] == 1:\n        return True\n    return False", "label": 0}
{"index": "gp131925", "code": "def initialize_request(param_0, param_1, *param_2, **param_3):\n        var_0 = self.get_parser_context(param_1)\n        return Request(\n            param_1,\n            parsers=self.get_parsers(),\n            authenticators=self.get_authenticators(),\n            negotiator=self.get_content_negotiator(),\n            var_0=var_0\n        )", "contrast": "def get_initial_request():\n    return {}", "label": 0}
{"index": "gp009585", "code": "def _quote_to_py_ast(param_0: GeneratorContext, param_1: Quote) -> GeneratedPyAST:\n    assert node.op == NodeOp.QUOTE\n    return _const_node_to_py_ast(param_0, node.expr)", "contrast": "import ast\ndef quote_node(value):\n    return ast.Expr(ast.Constant(value=value))", "label": 0}
{"index": "gp069398", "code": "def normalizeXpath(param_0):\n    var_0 = []\n    for var_1 in range(0, len(param_0)):\n        if var_1 > 0 and len(param_0[var_1-1]) == 0:\n            new_xpath.append(\"/\"+param_0[var_1])\n        elif len(param_0[var_1]) > 0:\n            new_xpath.append(param_0[var_1])\n    return var_0", "contrast": "def normalize_xpath(xpath: List[str]) -> List[str]:\n    refined_xpath = []\n    for element in xpath:\n        if element.startswith(\"[\"):\n            refined_xpath[-1] = refined_xpath[-1] + element\n        else:\n            refined_xpath.append(element)\n    return refined_xpath", "label": 0}
{"index": "gp007406", "code": "def mean_rate(param_0):\n        if self.counter.value == 0:\n            return 0.0\n        else:\n            var_0 = time() - self.start_time\n            return self.counter.value / var_0", "contrast": "import time\nclass MeanRate:\n    def __init__(self):\n        self.start_time = time.time()\n        self.count = 0\n    def mark_event(self):\n        self.count += 1\n    def get_mean_rate(self):\n        current_time = time.time()\n        elapsed_time = current_time - self.start_time\n        return self.count / elapsed_time if elapsed_time > 0 else 0", "label": 0}
{"index": "gp042047", "code": "def make_tar(param_0, param_1, param_2=[], param_3=True):\n    def select(param_0):\n        var_0 = realpath(param_0)\n        for var_1 in param_2:\n            if p.endswith('/'):\n                var_1 = var_1[:-1]\n            if rfn.startswith(var_1):\n                return False\n        if var_0 in python_files:\n            return False\n        return not is_blacklist(param_0)\n    var_0 = []\n    for var_1 in param_1:\n        var_1 = realpath(var_1)\n        compile_dir(var_1, param_3=param_3)\n        var_0 += [(var_2, relpath(realpath(var_2), var_1)) for var_2 in listfiles(var_1)\n                  if select(var_2)]\n    var_3 = tarfile.open(param_0, 'w:gz', format=tarfile.USTAR_FORMAT)\n    var_4 = []\n    for var_5, var_6 in var_0:\n        var_7 = dirname(var_6)\n        if var_7 not in var_4:\n            var_8 = ''\n            for var_9 in split(var_7):\n                var_8 = join(var_8, var_9)\n                if d.startswith('/'):\n                    var_8 = var_8[1:]\n                if var_8 == '' or var_8 in var_4:\n                    continue\n                dirs.append(var_8)\n                var_10 = tarfile.TarInfo(var_8)\n                tinfo.type = tarfile.DIRTYPE\n                tf.addfile(var_10)\n        tf.add(var_5, var_6)\n    tf.close()", "contrast": "import zipfile\nimport os\ndef make_zip_file(fn, source_dis):\n    with zipfile.ZipFile(fn, 'w') as myzip:\n        for root, dirs, files in os.walk(source_dis):\n            for file in files:\n                myzip.write(os.path.join(root, file), file)\n    return fn", "label": 0}
{"index": "gp272854", "code": "def parse_line(param_0: str):\n    return msg", "contrast": "def parse(self, line):\n        if not line:\n            raise KatcpSyntaxError(\"Empty message received.\")\n        type_char = line[0]\n        if type_char not in self.TYPE_SYMBOL_LOOKUP:\n            raise KatcpSyntaxError(\"Bad type character %r.\" % (type_char,))\n        mtype = self.TYPE_SYMBOL_LOOKUP[type_char]\n        parts = self.WHITESPACE_RE.split(line)\n        if not parts[-1]:\n            del parts[-1]\n        name = parts[0][1:]\n        arguments = [self._parse_arg(x) for x in parts[1:]]\n        match = self.NAME_RE.match(name)\n        if match:\n            name = match.group('name')\n            mid = match.group('id')\n        else:\n            raise KatcpSyntaxError(\"Bad message name (and possibly id) %r.\" %\n                                   (name,))\n        return Message(mtype, name, arguments, mid)", "label": 1}
{"index": "gp208935", "code": "def check_config(param_0):\n    var_0 = ['param1', 'param2', 'param3'] \n    for var_1 in var_0:\n        if var_1 not in param_0:\n            raise ValueError('Parameter {} is missing from the config file'.format(var_1))\n    return param_0", "contrast": "def check_config(config, data):\n    if 'tolerance' not in config.keys() or not config['tolerance']:\n        config['tolerance'] = 1E-5\n    if not config.get('maximum_iterations', None):\n        config['maximum_iterations'] = 1000\n    mmin_obs = np.min(data['magnitude'])\n    if config.get('input_mmin', 0) < mmin_obs:\n        config['input_mmin'] = mmin_obs\n    if fabs(config['b-value']) < 1E-7:\n        config['b-value'] = 1E-7\n    return config", "label": 1}
{"index": "gp093408", "code": "def _build_tag_families(param_0,\n                        param_1,\n                        param_2,\n                        param_3,\n                        param_4=lambda _: None):\n    var_0 = defaultdict(set)\n    var_1 = defaultdict(int)\n    for var_2 in param_0:\n        (var_3, var_4) =  paired_align.umt\n        for var_5 in param_1:\n            if paired_align.umt == var_5:\n                var_0[var_5].add(var_2)\n                break\n            elif var_3 == var_5[0] or var_4 == var_5[1]:\n                var_0[var_5].add(var_2)\n                var_1[var_5] += 1\n                break\n            elif (_hamming_dist(var_3, var_5[0]) <= param_2)                or (_hamming_dist(var_4, var_5[1]) <= param_2):\n                var_0[var_5].add(var_2)\n                var_1[var_5] += 1\n                break\n    var_6 = []\n    for var_7 in sorted(var_0):\n        var_8 = TagFamily(var_7,\n                               var_0[var_7],\n                               var_1[var_7],\n                               param_3,\n                               param_4)\n        tag_families.append(var_8)\n    return var_6", "contrast": "def partition_aligns_to_families(aligns, tags):\n    families = {}\n    for align in aligns:\n        best_tag = None\n        best_score = -1\n        for tag in tags:\n            score = get_alignment_score(align, tag)\n            if score > best_score:\n                best_score = score\n                best_tag = tag\n        if best_tag is not None:\n            if best_tag not in families:\n                families[best_tag] = []\n            families[best_tag].append(align)\n    return families\ndef get_alignment_score(align, tag):\n    score = 0\n    for read in align:\n        if read == tag:\n            score += 1\n    return score / len(align) if align else 0.0", "label": 0}
{"index": "gp014057", "code": "def not_in(param_0, param_1):\n        var_0 = self.df.index - table.df.index\n        return Table(df=self.df[var_0], name=self.name)", "contrast": "class Table:\n    def __init__(self, table_data):\n        self.table_data = table_data\n    def not_in(self, other_table):\n        not_in_table = [[nucleus for nucleus in row if nucleus not in other_table.table_data] for row in self.table_data]\n        return Table(not_in_table)", "label": 0}
{"index": "gp037767", "code": "def _fulfills_version_spec(param_0, param_1, param_2,\n                           param_3=False):\n    var_0 = __salt__.get('pkg.version_cmp')\n    if salt.utils.platform.is_freebsd():\n        if isinstance(param_0, dict) and 'version' in param_0:\n            param_0 = param_0['version']\n    for var_1 in param_0:\n        if (param_1 == '==' and fnmatch.fnmatch(var_1, param_2))                or salt.utils.versions.compare(ver1=var_1,\n                                               param_1=param_1,\n                                               ver2=param_2,\n                                               var_0=var_0,\n                                               param_3=param_3):\n            return True\n    return False", "contrast": "import pkg_resources\ndef check_version(specified_version):\n    installed_versions = [pkg.version for pkg in pkg_resources.working_set]\n    if specified_version in installed_versions:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp227753", "code": "def generate_paragraph(param_0, param_1):\n    var_0 = 'Here is an image: <img src=\"' + param_1 + '\">\\n'\n    var_0 += 'The updated relationshiplist is: ' + str(param_0)\n    return var_0", "contrast": "def picture(\n        relationshiplist, picname, picdescription, pixelwidth=None,\n        pixelheight=None, nochangeaspect=True, nochangearrowheads=True,\n        imagefiledict=None):\n    if imagefiledict is None:\n        warn(\n            'Using picture() without imagefiledict parameter will be depreca'\n            'ted in the future.', PendingDeprecationWarning\n        )\n    picid = '2'\n    picpath = abspath(picname)\n    if imagefiledict is not None:\n        if picpath not in imagefiledict:\n            picrelid = 'rId' + str(len(relationshiplist) + 1)\n            imagefiledict[picpath] = picrelid\n            relationshiplist.append([\n                'http: \n                'ionships/image',\n                'media/%s_%s' % (picrelid, basename(picpath))\n            ])\n        else:\n            picrelid = imagefiledict[picpath]\n    else:\n        picrelid = 'rId' + str(len(relationshiplist) + 1)\n        relationshiplist.append([\n            'http://schemas.openxmlformats.org/officeDocument/2006/relations'\n            'hips/image', 'media/' + picname\n        ])\n        media_dir = join(template_dir, 'word', 'media')\n        if not os.path.isdir(media_dir):\n            os.mkdir(media_dir)\n        shutil.copyfile(picname, join(media_dir, picname))\n    image = Image.open(picpath)\n    try:\n        exif = image._getexif()\n        exif = {} if exif is None else exif\n    except:\n        exif = {}\n    imageExif = {}\n    for tag, value in exif.items():\n        imageExif[TAGS.get(tag, tag)] = value\n    imageOrientation = imageExif.get('Orientation', 1)\n    imageAngle = {\n        1: 0, 2: 0, 3: 180, 4: 0, 5: 90, 6: 90, 7: 270, 8: 270\n    }[imageOrientation]\n    imageFlipH = 'true' if imageOrientation in (2, 5, 7) else 'false'\n    imageFlipV = 'true' if imageOrientation == 4 else 'false'\n    if not pixelwidth or not pixelheight:\n        pixelwidth, pixelheight = image.size[0:2]\n    if imageOrientation in (5, 6, 7, 8):\n        pixelwidth, pixelheight = pixelheight, pixelwidth\n    emuperpixel = 12700\n    width = str(pixelwidth * emuperpixel)\n    height = str(pixelheight * emuperpixel)\n    blipfill = makeelement('blipFill', nsprefix='pic')\n    blipfill.append(makeelement('blip', nsprefix='a', attrnsprefix='r',\n                    attributes={'embed': picrelid}))\n    stretch = makeelement('stretch', nsprefix='a')\n    stretch.append(makeelement('fillRect', nsprefix='a'))\n    blipfill.append(makeelement('srcRect', nsprefix='a'))\n    blipfill.append(stretch)\n    nvpicpr = makeelement('nvPicPr', nsprefix='pic')\n    cnvpr = makeelement(\n        'cNvPr', nsprefix='pic',\n        attributes={'id': '0', 'name': 'Picture 1', 'descr': picdescription}\n    )\n    nvpicpr.append(cnvpr)\n    cnvpicpr = makeelement('cNvPicPr', nsprefix='pic')\n    cnvpicpr.append(makeelement(\n        'picLocks', nsprefix='a',\n        attributes={'noChangeAspect': str(int(nochangeaspect)),\n                    'noChangeArrowheads': str(int(nochangearrowheads))}))\n    nvpicpr.append(cnvpicpr)\n    sppr = makeelement('spPr', nsprefix='pic', attributes={'bwMode': 'auto'})\n    xfrm = makeelement(\n        'xfrm', nsprefix='a', attributes={\n            'rot': str(imageAngle * 60000), 'flipH': imageFlipH,\n            'flipV': imageFlipV\n        }\n    )\n    xfrm.append(\n        makeelement('off', nsprefix='a', attributes={'x': '0', 'y': '0'})\n    )\n    xfrm.append(\n        makeelement(\n            'ext', nsprefix='a', attributes={'cx': width, 'cy': height}\n        )\n    )\n    prstgeom = makeelement(\n        'prstGeom', nsprefix='a', attributes={'prst': 'rect'}\n    )\n    prstgeom.append(makeelement('avLst', nsprefix='a'))\n    sppr.append(xfrm)\n    sppr.append(prstgeom)\n    pic = makeelement('pic', nsprefix='pic')\n    pic.append(nvpicpr)\n    pic.append(blipfill)\n    pic.append(sppr)\n    graphicdata = makeelement(\n        'graphicData', nsprefix='a',\n        attributes={'uri': ('http://schemas.openxmlformats.org/drawingml/200'\n                            '6/picture')})\n    graphicdata.append(pic)\n    graphic = makeelement('graphic', nsprefix='a')\n    graphic.append(graphicdata)\n    framelocks = makeelement('graphicFrameLocks', nsprefix='a',\n                             attributes={'noChangeAspect': '1'})\n    framepr = makeelement('cNvGraphicFramePr', nsprefix='wp')\n    framepr.append(framelocks)\n    docpr = makeelement('docPr', nsprefix='wp',\n                        attributes={'id': picid, 'name': 'Picture 1',\n                                    'descr': picdescription})\n    effectextent = makeelement('effectExtent', nsprefix='wp',\n                               attributes={'l': '25400', 't': '0', 'r': '0',\n                                           'b': '0'})\n    extent = makeelement('extent', nsprefix='wp',\n                         attributes={'cx': width, 'cy': height})\n    inline = makeelement('inline', attributes={'distT': \"0\", 'distB': \"0\",\n                                               'distL': \"0\", 'distR': \"0\"},\n                         nsprefix='wp')\n    inline.append(extent)\n    inline.append(effectextent)\n    inline.append(docpr)\n    inline.append(framepr)\n    inline.append(graphic)\n    drawing = makeelement('drawing')\n    drawing.append(inline)\n    run = makeelement('r')\n    run.append(drawing)\n    paragraph = makeelement('p')\n    paragraph.append(run)\n    if imagefiledict is not None:\n        return relationshiplist, paragraph, imagefiledict\n    else:\n        return relationshiplist, paragraph", "label": 1}
{"index": "gp303843", "code": "def remove_duplicates(param_0):\n    return list(dict.fromkeys(param_0))", "contrast": "def unique(iterable):\n    seen = set()\n    return [x for x in iterable if x not in seen and not seen.add(x)]", "label": 1}
{"index": "gp305231", "code": "class Color:\n    def __init__(param_0, param_1, param_2, param_3, param_4):\n        self.r = param_1\n        self.g = param_2\n        self.b = param_3\n        self.a = param_4\n    @classmethod\n    def from_rgb(param_0, param_1, param_2, param_3, param_4=1.0):\n        return param_0(param_1, param_2, param_3, param_4)\n    def blend(param_0, param_1):\n        return Color((1 - other.a) * self.r + other.a * other.r,\n                     (1 - other.a) * self.g + other.a * other.g,\n                     (1 - other.a) * self.b + other.a * other.b,\n                     (1 - other.a) * self.a + other.a * other.a)", "contrast": "def blend(self, other, percent=0.5):\n    dest = 1.0 - percent\n    rgb = tuple(((u * percent) + (v * dest) for u, v in zip(self.__rgb, other.__rgb)))\n    a = (self.__a * percent) + (other.__a * dest)\n    return Color(rgb, 'rgb', a, self.__wref)", "label": 1}
{"index": "gp282521", "code": "def check_networks_or_ports_exist(param_0):\n    return True  ", "contrast": "def tenant_provisioned(tenant_id):\n    session = db.get_reader_session()\n    with session.begin():\n        res = any(\n            session.query(m).filter(m.tenant_id == tenant_id).count()\n            for m in [models_v2.Network, models_v2.Port]\n        )\n    return res", "label": 1}
{"index": "gp236983", "code": "import os\ndef get_static_library_directories():\n    var_0 = ['.a']\n    var_1 = []\n    for var_2, var_3, var_4 in os.walk('.'):\n        for var_5 in var_4:\n            if file.endswith(tuple(var_0)):\n                directories.append(var_2)\n                break\n    return var_1", "contrast": "def library_directories(self):\n        libs = self.find_products('library')\n        if len(libs) > 0:\n            return [os.path.join(self.output_folder)]\n        return []", "label": 1}
{"index": "gp167625", "code": "def delegate(param_0, param_1, *param_2, **param_3):\n        return self.subexecutor.spawn(param_1, *param_2, **param_3)", "contrast": "import gevent\ndef get_operation_as_future(operation):\n    return gevent.spawn(operation)", "label": 0}
{"index": "gp036353", "code": "def delete_task(param_0, param_1='\\\\'):\n    if param_0 not in list_tasks(param_1):\n        return '{0} not found in {1}'.format(param_0, param_1)\n    with salt.utils.winapi.Com():\n        var_0 = win32com.client.Dispatch(\"Schedule.Service\")\n    task_service.Connect()\n    var_1 = task_service.GetFolder(param_1)\n    task_folder.DeleteTask(param_0, 0)\n    if param_0 not in list_tasks(param_1):\n        return True\n    else:\n        return False", "contrast": "import os\ndef delete_task(name, location='\\\\'):\n    path = os.path.join(location, name)\n    if os.path.exists(path):\n        os.remove(path)\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp257449", "code": "class CachedData:\n    def __init__(param_0, param_1):\n        self._data = {}\n        for var_0, var_1 in raw_values.items():\n            setattr(param_0, var_0, var_1)\n            self._data[var_0] = var_1\n    def as_dict(param_0):\n        return self._data", "contrast": "def _data(self):\n        d = {}\n        for k, v in self._row_values.items():\n            attrs = k.rsplit('__', 1)\n            if len(attrs) == 2:\n                fk, fn = attrs\n                if fk not in d:\n                    d[fk] = {}\n                d[fk][fn] = v\n            else:\n                d[k] = v\n        return d", "label": 1}
{"index": "gp247578", "code": "import dask\nfrom dask.distributed import Client\ndef run_workflow_in_parallel_threads(param_0, param_1):\n    var_0 = Client(n_workers=param_1)\n    var_1 = dask.compute(param_0)\n    client.close()\n    return var_1", "contrast": "def run_parallel(workflow, n_threads):\n    scheduler = Scheduler()\n    threaded_worker = Queue() >> thread_pool(\n        *repeat(worker, n_threads))\n    return scheduler.run(threaded_worker, get_workflow(workflow))", "label": 1}
{"index": "gp205973", "code": "import pandas as pd\ndef write_hdf5_series(param_0, param_1, param_2, **param_3):\n    with pd.HDFStore(param_0) as var_0:\n        store.put(param_1, param_2, format='table', data_columns=True, **param_3)", "contrast": "def write_hdf5_series(series, output, path=None, attrs=None, **kwargs):\n    if attrs is None:\n        attrs = format_index_array_attrs(series)\n    return write_hdf5_array(series, output, path=path, attrs=attrs, **kwargs)", "label": 1}
{"index": "gp219346", "code": "def get_single_value_from_select_query(param_0, param_1):\n    return session.execute(param_1).scalar()", "contrast": "async def scalar(self, query, as_tuple=False):\n        query = self._swap_database(query)\n        return (await scalar(query, as_tuple=as_tuple))", "label": 1}
{"index": "gp031474", "code": "def _closest_ref_length(param_0, param_1):\n    var_0 = (len(var_1) for var_1 in param_0)\n    var_2 = min(var_0,\n                          key=lambda ref_length: (abs(ref_length - param_1), ref_length))\n    return var_2", "contrast": "def find_closest_ref_len(references, trans_length):\n    closest_ref_len = None\n    min_diff = float('inf')\n    for ref in references:\n        ref_len = len(ref)\n        diff = abs(ref_len - trans_length)\n        if diff < min_diff:\n            min_diff = diff\n            closest_ref_len = ref_len\n    return closest_ref_len", "label": 0}
{"index": "gp109419", "code": "def _hex_to_rgb(param_0):\n    if len(param_0) != 6:\n        raise ValueError(param_0 + \" is not a string of length 6, cannot convert to rgb.\")\n    return tuple(map(ord, hex_code.decode(\"hex\")))", "contrast": "def _hex_to_rgb(hex_string):\n    r = int(hex_string[0:2], 16)\n    g = int(hex_string[2:4], 16)\n    b = int(hex_string[4:6], 16)\n    return (r, g, b)", "label": 0}
{"index": "gp083332", "code": "def GetCompressedFilesInDir(param_0, param_1, param_2, param_3 = ['.rar',]):\n  goodlogging.Log.Info(\"EXTRACT\", \"Parsing file directory: {0}\".format(param_0))\n  if os.path.isdir(param_0) is True:\n    for var_0 in glob.glob(os.path.join(param_0, '*')):\n      if os.path.splitext(var_0)[1] in param_3:\n        fileList.append(var_0)", "contrast": "import os\ndef get_supported_files(fileDir, fileList, ignoreDirList=None, supportedFormatList=['.rar']):\n    if ignoreDirList is None:\n        ignoreDirList = []\n    for path, dirs, files in os.walk(fileDir):\n        dirs[:] = [d for d in dirs if d not in ignoreDirList]\n        for name in files:\n            if name.endswith(tuple(supportedFormatList)):\n                fileList.append(os.path.join(path, name))", "label": 0}
{"index": "gp243569", "code": "def create_raw_data(param_0: str, param_1: any) -> str:\n    return \"Success: Data written to DB\"", "contrast": "def create_raw(self, key, value):\n        data = None\n        if key is not None and value is not None:\n            data = self.db.create(key.strip(), value)\n        else:\n            self.tcex.log.warning(u'The key or value field was None.')\n        return data", "label": 1}
{"index": "gp218181", "code": "def set_diagnostics_radiation(param_0, param_1):\n    var_0 = param_0 * 1.5\n    var_1 = param_1 * 2.5\n    return None", "contrast": "def do_diagnostics(self):\n        self.OLR = self.subprocess['LW'].flux_to_space\n        self.LW_down_sfc = self.subprocess['LW'].flux_to_sfc\n        self.LW_up_sfc = self.subprocess['LW'].flux_from_sfc\n        self.LW_absorbed_sfc = self.LW_down_sfc - self.LW_up_sfc\n        self.LW_absorbed_atm = self.subprocess['LW'].absorbed\n        self.LW_emission = self.subprocess['LW'].emission\n        self.ASR = (self.subprocess['SW'].flux_from_space -\n                    self.subprocess['SW'].flux_to_space)\n        self.SW_absorbed_atm = self.subprocess['SW'].absorbed\n        self.SW_down_sfc = self.subprocess['SW'].flux_to_sfc\n        self.SW_up_sfc = self.subprocess['SW'].flux_from_sfc\n        self.SW_absorbed_sfc = self.SW_down_sfc - self.SW_up_sfc\n        self.SW_up_TOA = self.subprocess['SW'].flux_to_space\n        self.SW_down_TOA = self.subprocess['SW'].flux_from_space\n        self.planetary_albedo = (self.subprocess['SW'].flux_to_space /\n                                 self.subprocess['SW'].flux_from_space)", "label": 1}
{"index": "gp316904", "code": "def distance_between_centroids(param_0, param_1, param_2):\n    var_0 = item_a.geom.centroid\n    var_1 = item_b.geom.centroid\n    var_2 = centroid_a.distance(var_1)\n    var_3 = min(var_2/param_2, 1.0)\n    return var_3", "contrast": "def start_centroid_distance(item_a, item_b, max_value):\n    start_a = item_a.center_of_mass(item_a.times[0])\n    start_b = item_b.center_of_mass(item_b.times[0])\n    start_distance = np.sqrt((start_a[0] - start_b[0]) ** 2 + (start_a[1] - start_b[1]) ** 2)\n    return np.minimum(start_distance, max_value) / float(max_value)", "label": 1}
{"index": "gp227446", "code": "import numpy as np\nimport matplotlib.pyplot as plt\ndef create_histogram(param_0, param_1, param_2):\n    var_0 = np.random.normal(param_1, param_2, param_0)\n    var_1, var_2, var_3 = plt.hist(var_0, var_2=50, density=True, alpha=0.7)\n    return var_1, var_2, var_3", "contrast": "def normal_h1(size: int = 10000, mean: float = 0, sigma: float = 1) -> Histogram1D:\n    data = np.random.normal(mean, sigma, (size,))\n    return h1(data, name=\"normal\", axis_name=\"x\", title=\"1D normal distribution\")", "label": 1}
{"index": "gp317373", "code": "def add_key_value_pair(param_0, param_1, param_2):\n    if param_1 in dictionary.keys():\n        if param_0[param_1] == param_2:\n            param_0[param_1] = param_2\n    else:\n        param_0[param_1] = param_2\n    return param_0", "contrast": "def add(self, key, item):\n        mmkeys = self.mmkeys\n        if mmkeys is not None and not (key in self.data):\n            lenkey = len(key)\n            start = min(self.minkeylength,lenkey)\n            mmkeysGet = mmkeys.setdefault\n            for i in range(start,lenkey+1):\n                mmkeysGet(key[0:i],[]).append(key)\n        self.data[key] = item", "label": 1}
{"index": "gp038814", "code": "def _get_http_proxy_url():\n    var_0 = ''\n    var_1 = __salt__['config.option']('proxy_host')\n    var_2 = __salt__['config.option']('proxy_port')\n    var_3 = __salt__['config.option']('proxy_username')\n    var_4 = __salt__['config.option']('proxy_password')\n    if var_1 and var_2:\n        if var_3 and var_4:\n            var_0 = 'http://{0}:{1}@{2}:{3}'.format(\n                var_3,\n                var_4,\n                var_1,\n                var_2\n            )\n        else:\n            var_0 = 'http://{0}:{1}'.format(\n                var_1,\n                var_2\n            )\n    return var_0", "contrast": "def get_http_proxy_url(proxy_username, proxy_password, proxy_host, proxy_port):\n    if proxy_username and proxy_password and proxy_host and proxy_port:\n        return f\"http://{proxy_username}:{proxy_password}@{proxy_host}:{proxy_port}\"\n    else:\n        return \"\"", "label": 0}
{"index": "gp077722", "code": "def get_user_by_token(param_0, param_1):\n        if not self.jwt_loader_implementation:\n            return self.default_token_user_loader(param_1)\n        try:\n            var_0 = import_string(self.jwt_loader_implementation)\n        except ImportError:\n            var_1 = 'Failed to import custom JWT user loader implementation. '\n            var_1 += 'Check that configured module exists [{}]'\n            raise x.ConfigurationException(\n                msg.format(self.jwt_loader_implementation)\n            )\n        return var_0(param_1)", "contrast": "from typing import Optional\nfrom boiler.user.models import User\nfrom boilerplate import app\ndef get_user_by_token(token: str) -> Optional[User]:\n    user_loader = app.config.get(\"CUSTOM_USER_LOADER\", None)\n    if user_loader is not None:\n        return user_loader(token)\n    else:\n        user_id = token.get(\"user_id\", None)\n        if user_id is not None:\n            return User.query.filter_by(id=user_id).first()\n        else:\n            return None", "label": 0}
{"index": "gp301718", "code": "def calculate_median(param_0):\n    var_0 = sorted(param_0)\n    var_1 = len(param_0)\n    var_2 = var_1 // 2\n    return (var_0[var_2] + var_0[-var_2-1]) / 2 if var_1 % 2 == 0 else var_0[var_2]", "contrast": "def median(lst):\n    sortedLst = sorted(lst)\n    lstLen = len(lst)\n    index = (lstLen - 1)  \n    if (lstLen % 2):\n        return sortedLst[index]\n    else:\n        return (sortedLst[index] + sortedLst[index + 1])/2.0", "label": 1}
{"index": "gp064801", "code": "def cd_ctx(param_0):\n    var_0 = os.path.abspath(os.curdir)\n    if os.path.isdir(param_0):\n        os.chdir(param_0)\n    yield\n    os.chdir(var_0)", "contrast": "import os\nclass ChangeDir:\n    def __init__(self, directory):\n        self.old_dir = os.getcwd()\n        self.new_dir = directory\n    def __enter__(self):\n        os.chdir(self.new_dir)\n    def __exit__(self, exc_type, exc_value, traceback):\n        os.chdir(self.old_dir)", "label": 0}
{"index": "gp135431", "code": "def keep_on_one_line():\n    class CondensedStream:\n        def __init__(param_0):\n            self.sys_stdout = sys.stdout\n        def write(param_0, param_1):\n            with swap_streams(self.sys_stdout):\n                param_1 = string.replace('\\n', ' ')\n                param_1 = truncate_to_fit_terminal(param_1)\n                if string.strip():\n                    update(param_1)\n        def flush(param_0):\n            with swap_streams(self.sys_stdout):\n                flush()\n    with swap_streams(CondensedStream()):\n        yield", "contrast": "import sys\ndef oneline_output(func):\n    def wrapper(*args, **kwargs):\n        sys.stdout.write(\"\\x1b[2K\\r\")   \n        sys.stdout.flush()\n        with func(*args, **kwargs) as output:\n            for line in output.splitlines():\n                print(line, end=\"\\r\")\n    return wrapper", "label": 0}
{"index": "gp166702", "code": "def I_minus_R(param_0,param_1):\n        if self.__I_R is not None and param_1 == self.__I_R_sv:\n            return self.__I_R\n        else:\n            if param_1 > self.jco.ncol:\n                return self.parcov.zero\n            else:\n                var_0 = self.xtqx.v[:, param_1:]\n                self.__I_R = var_0 * v2.T\n                self.__I_R_sv = param_1\n                return self.__I_R", "contrast": "def get_I_R_at_singular(singular_value):\n    return I - R(singular_value)", "label": 0}
{"index": "gp172802", "code": "from ibis.expr import joins\ndef perform_join(param_0, param_1, param_2, param_3 = 'inner'):\n    return left.join(param_1, param_2, param_3=param_3)", "contrast": "def join(left, right, predicates=(), how='inner'):\n    klass = _join_classes[how.lower()]\n    if isinstance(predicates, Expr):\n        predicates = _L.flatten_predicate(predicates)\n    op = klass(left, right, predicates)\n    return op.to_expr()", "label": 1}
{"index": "gp313614", "code": "import urllib\ndef get_page_content(param_0):\n    with urllib.request.urlopen(param_0) as var_0:\n        var_1 = response.read().decode('utf-8')\n    return var_1", "contrast": "def get_content(self):\n        url = self.build_url()\n        try:\n            self.content_page = requests.get(url)\n            if not(self.content_page.status_code == requests.codes.ok):\n                self.content_page.raise_for_status()\n        except requests.exceptions.RequestException as ex:\n            logging.info('A requests exception has ocurred: ' + str(ex))\n            logging.error(traceback.format_exc())\n            sys.exit(0)", "label": 1}
{"index": "gp071138", "code": "def lookups(param_0, param_1, param_2):\n        var_0 = []\n        for var_1 in models.Model.objects.values('information__type').distinct():\n            var_2 = var_1['information__type']\n            if var_2 is not None:\n                var_2 = ContentType.objects.get(pk=var_2)\n                output.append([ct.pk, ct.app_label + '.' + ct.model])\n        return var_0", "contrast": "def get_options_list():\n    options = [\n        (1, \"Option 1\"),\n        (2, \"Option 2\"),\n        (3, \"Option 3\"),\n    ]\n    return options", "label": 0}
{"index": "gp133006", "code": "def clean(param_0, param_1):\n        var_0 = super(RichTextField, param_0).clean(param_1)\n        if var_0 in fields.EMPTY_VALUES:\n            if self.instance:\n                var_1 = self.get_instance_id(self.instance)\n                if not var_1:\n                    SourceText.objects.filter(content_type=self.ct, object_id=var_1, field=self.field_name, processor=self.processor).delete()\n                else:\n                    SourceText.objects.filter(content_type=self.ct, object_id=var_1, field=self.field_name).delete()\n            self.validate_rendered('')\n            return ''\n        var_2 = smart_unicode(param_1)\n        if self.instance:\n            var_1 = self.get_instance_id(self.instance)\n            try:\n                if not var_1:\n                    var_3 = SourceText(content_type=self.ct, object_id=var_1, field=self.field_name, processor=self.processor)\n                else:\n                    var_3 = SourceText.objects.get(content_type=self.ct, object_id=var_1, field=self.field_name)\n                assert src_text.processor == self.processor\n            except SourceText.DoesNotExist:\n                var_3 = SourceText(content_type=self.ct, object_id=var_1, field=self.field_name, processor=self.processor)\n            src_text.content = var_2\n            try:\n                var_4 = src_text.render()\n            except ProcessorError, e:\n                raise ValidationError(self.error_messages['syntax_error'])\n        else:\n            self.instance = var_3 = SourceText(\n                content_type=self.ct,\n                field=self.field_name,\n                content=var_2,\n                processor=self.processor\n            )\n            try:\n                var_4 = src_text.render()\n            except Exception, err:\n                raise ValidationError(self.error_messages['syntax_error'])\n        self.validate_rendered(var_4)\n        if not hasattr(self.model, RICH_FIELDS_SET):\n            setattr(self.model, RICH_FIELDS_SET, set())\n        getattr(self.model, RICH_FIELDS_SET).add(self.field_name)\n        signals.post_save.connect(receiver=self.post_save_listener, sender=self.model)\n        var_4 = UnicodeWrapper(var_4)\n        setattr(var_4, self.src_text_attr, var_3)\n        return var_4", "contrast": "from django.core.exceptions import ValidationError\nfrom your_app.models import SourceText\ndef clean_field_and_store(field_value):\n    try:\n        cleaned_value = my_cleaning_function(field_value)\n        source_text = SourceText.objects.create(original_value=field_value)\n        return cleaned_value\n    except Exception as e:\n        raise ValidationError(str(e))", "label": 0}
{"index": "gp252317", "code": "def get_file_contents(param_0):\n    with open(param_0, 'r') as var_0:\n        var_1 = f.read().splitlines()\n    return var_1", "contrast": "def file_to_list(path):\n    if not os.path.exists(path):\n        ui.error(c.MESSAGES[\"path_missing\"], path)\n        sys.exit(1)\n    with codecs.open(path, \"r\", \"UTF-8\") as contents:\n        lines = contents.read().splitlines()\n    return lines", "label": 1}
{"index": "gp215981", "code": "import dis\ndef simple_opcode_disassembly(param_0):\n    dis.dis(compile(param_0, '', 'exec'))", "contrast": "def main():\n    test_targets = (    \n        [ARCH_I386, MACH_I386_I386_INTEL_SYNTAX, ENDIAN_MONO, \"\\x55\\x89\\xe5\\xE8\\xB8\\xFF\\xFF\\xFF\", 0x1000],\n        [ARCH_I386, MACH_X86_64_INTEL_SYNTAX, ENDIAN_MONO, \"\\x55\\x48\\x89\\xe5\\xE8\\xA3\\xFF\\xFF\\xFF\", 0x1000],\n        [ARCH_ARM, MACH_ARM_2, ENDIAN_LITTLE, \"\\x04\\xe0\\x2d\\xe5\\xED\\xFF\\xFF\\xEB\", 0x1000],\n        [ARCH_MIPS, MACH_MIPSISA32, ENDIAN_BIG, \"\\x0C\\x10\\x00\\x97\\x00\\x00\\x00\\x00\", 0x1000],\n        [ARCH_POWERPC, MACH_PPC, ENDIAN_BIG, \"\\x94\\x21\\xFF\\xE8\\x7C\\x08\\x02\\xA6\", 0x1000],\n        )\n    for target_arch, target_mach, target_endian, binary, address in test_targets:\n        opcodes = Opcodes(target_arch, target_mach, target_endian)\n        print \"\\n[+] Architecture %s - Machine %d\" %            (opcodes.architecture_name, opcodes.machine)\n        print \"[+] Disassembly:\"\n        for vma, size, disasm in opcodes.disassemble(binary, address):\n            print \"0x%X (size=%d)\\t %s\" % (vma, size, disasm)", "label": 1}
{"index": "gp274542", "code": "def remove_access_list_items(param_0, *param_1):\n    for var_0 in param_1:\n        load_balancer.access_list.remove(var_0)", "contrast": "def delete_access_list_items(self, loadbalancer, item_ids):\n        if not isinstance(item_ids, (list, tuple)):\n            item_ids = [item_ids]\n        valid_ids = [itm[\"id\"] for itm in self.get_access_list(loadbalancer)]\n        bad_ids = [str(itm) for itm in item_ids if itm not in valid_ids]\n        if bad_ids:\n            raise exc.AccessListIDNotFound(\"The following ID(s) are not valid \"\n                    \"Access List items: %s\" % \", \".join(bad_ids))\n        items = \"&\".join([\"id=%s\" % item_id for item_id in item_ids])\n        uri = \"/loadbalancers/%s/accesslist?%s\" % (\n                utils.get_id(loadbalancer), items)\n        resp, body = self.api.method_delete(uri)\n        return body", "label": 1}
{"index": "gp100691", "code": "def add_path(param_0, param_1):\n        if not os.path.exists(param_1):\n            raise RuntimeError('Path does not exists: %s.' % param_1)\n        self.paths.append(param_1)", "contrast": "import json\ndef load_translations(path):\n    with open(path, 'r') as f:\n        return json.load(f)", "label": 0}
{"index": "gp183572", "code": "import subprocess\ndef git_command_generator(param_0):\n  var_0 = subprocess.Popen([\"git\", param_0], stdout=subprocess.PIPE)\n  while True:\n    var_1 = process.stdout.readline()\n    if var_1 == '' and process.poll() is not None:\n        break\n    if var_1:\n        yield output.strip()", "contrast": "def git_lines(*args, git=maybeloggit, **kwargs):\n    err = io.StringIO()\n    try:\n        for line in git('--no-pager', _err=err, *args, _decode_errors='replace', _iter=True, _bg_exc=False, **kwargs):\n            yield line[:-1]  \n    except sh.ErrorReturnCode as e:\n        status('exit_code=%s' % e.exit_code)\n    errlines = err.getvalue().splitlines()\n    if len(errlines) < 3:\n        for line in errlines:\n            status(line)\n    else:\n        vd().push(TextSheet('git ' + ' '.join(args), errlines))", "label": 1}
{"index": "gp282306", "code": "import discord\nfrom ui_embed import UI\ndef create_error_embed(param_0: discord.Channel, param_1: int) -> UI:\n    var_0 = UI(param_0)\n    ui.set_title('Error')\n    ui.set_description(f'Maximum warnings now set to {param_1}')\n    ui.set_color(0xff0000)\n    return var_0", "contrast": "def warning_max_changed(channel, max_warnings):\n    gui = ui_embed.UI(\n        channel,\n        \"Maximum Warnings Changed\",\n        \"Users must now have {} warnings to be banned \"\n        \"(this won't ban existing users with warnings)\".format(max_warnings),\n        modulename=modulename\n    )\n    return gui", "label": 1}
{"index": "gp318185", "code": "def filter_resources(param_0, param_1):\n    var_0 = []\n    for var_1, var_2 in enumerate(param_0):\n        var_3 = [var_4 for var_4 in var_2 if param_1[var_1](var_4)]\n        filtered_resources.append(var_3)\n    return var_0", "contrast": "def filter(self, request, queryset, view):\n        summary_queryset = queryset\n        filtered_querysets = []\n        for queryset in summary_queryset.querysets:\n            filter_class = self._get_filter(queryset)\n            queryset = filter_class(request.query_params, queryset=queryset).qs\n            filtered_querysets.append(queryset)\n        summary_queryset.querysets = filtered_querysets\n        return summary_queryset", "label": 1}
{"index": "gp067611", "code": "def check_cousins(param_0, param_1, param_2):\n        self.logger.debug(\"Checking if {0} and {1} are cousins\".format(\n            param_1, param_2\n        ))\n        pass", "contrast": "def are_cousins(individual_1_id: str, individual_2_id: str) -> bool:\n    grandparents_1 = set([parent for parent in get_parents(individual_1_id) if parent])\n    grandparents_2 = set([parent for parent in get_parents(individual_2_id) if parent])\n    cousins = []\n    for grandparent in grandparents_1:\n        if grandparent in grandparents_2:\n            cousins.append(grandparent)\n    if len(cousins) > 0:\n        return True\n    else:\n        return False\ndef get_parents(individual_id: str) -> list:\n    pass  ", "label": 0}
{"index": "gp029517", "code": "def run_selection(param_0):\r\n        var_0 = self.get_current_editor().get_selection_as_executable_code()\r\n        if var_0:\r\n            self.exec_in_extconsole.emit(text.rstrip(), self.focus_to_editor)\r\n            return\r\n        var_1 = self.get_current_editor()\r\n        var_2 = editor.get_current_line()\r\n        var_0 = line.lstrip()\r\n        if var_0:\r\n            self.exec_in_extconsole.emit(var_0, self.focus_to_editor)\r\n        if editor.is_cursor_on_last_line() and var_0:\r\n            editor.append(editor.get_line_separator())\r\n        editor.move_cursor_to_next('line', 'down')", "contrast": "import sublime\nimport sublime_plugin\nclass RunTextCommand(sublime_plugin.TextCommand):\n  def run(self, edit):\n    sels = self.view.sel()\n    if not sels:\n      return\n    for sel in sels:\n      if sel.empty():\n        line = self.view.line(sel)\n        if line.empty():\n          self.view.run_command(\"insert\", {\"characters\": \"\\n\"})\n          self.view.show(self.view.size())\n          return\n        sel = line\n      code = self.view.substr(sel)\n      self.view.window().run_command(\"exec\", {\"cmd\": [code], \"shell\": True})", "label": 0}
{"index": "gp324358", "code": "def get_direct_members():\n    var_0 = self.get_member_names()\n    var_1 = []\n    for var_2 in var_0:\n        members.append(self.get_member(var_2))\n    return var_1", "contrast": "def get_member_list(self):\n        if not self.is_collection:\n            raise NotImplementedError\n        memberList = []\n        for name in self.get_member_names():\n            member = self.get_member(name)\n            assert member is not None\n            memberList.append(member)\n        return memberList", "label": 1}
{"index": "gp239263", "code": "async def add_alternative_data_id(param_0):\n    if param_0 > 0 and param_0 < 256:\n        return param_0\n    else:\n        return None", "contrast": "async def add_alternative(self, alt, timeout=OTGW_DEFAULT_TIMEOUT):\n        cmd = OTGW_CMD_ADD_ALT\n        alt = int(alt)\n        if alt < 1 or alt > 255:\n            return None\n        ret = await self._wait_for_cmd(cmd, alt, timeout)\n        if ret is not None:\n            return int(ret)", "label": 1}
{"index": "gp032347", "code": "def nodes(**param_0):\n    var_0 = _setup_conn(**param_0)\n    try:\n        var_1 = kubernetes.client.CoreV1Api()\n        var_2 = api_instance.list_node()\n        return [var_3['metadata']['name'] for var_3 in api_response.to_dict().get('items')]\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception('Exception when calling CoreV1Api->list_node')\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**var_0)", "contrast": "import subprocess\ndef kubernetes_nodes(kubeconfig=None, context=None):\n    command = 'kubectl get nodes -o jsonpath=\"{.items[*].metadata.name}\"'\n    if context:\n        command += f' --context={context}'\n    if kubeconfig:\n        command += f' --kubeconfig={kubeconfig}'\n    result = subprocess.run(command, stdout=subprocess.PIPE, shell=True)\n    return result.stdout.decode().strip().split()", "label": 0}
{"index": "gp100782", "code": "def has_comic(param_0):\n    var_0 = [\n        (\"Creators/%s\" % param_0).lower(),\n        (\"GoComics/%s\" % param_0).lower(),\n    ]\n    for var_1 in get_scraperclasses():\n        var_2 = scraperclass.getName().lower()\n        if var_2 in var_0:\n            return True\n    return False", "contrast": "def check_comic_name(name, existing_names):\n    if name in existing_names:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp103180", "code": "def DosDateTimeToTimeTuple(param_0):\n    var_0 = param_0 >> 16\n    var_1 = param_0 & 0xffff\n    var_2 = var_0 & 0x1f\n    var_3 = (var_0 >> 5) & 0xf\n    var_4 = 1980 + (var_0 >> 9)\n    var_5 = 2 * (var_1 & 0x1f)\n    var_6 = (var_1 >> 5) & 0x3f\n    var_7 = var_1 >> 11\n    return time.localtime(\n        time.mktime((var_4, var_3, var_2, var_7, var_6, var_5, 0, 1, -1)))", "contrast": "import datetime\ndef msdos_datetime_to_time_tuple(msdos_datetime):\n    dos_date = (msdos_datetime >> 16) & 0xFFFF\n    dos_time = msdos_datetime & 0xFFFF\n    year = ((dos_date >> 9) & 0x7F) + 1980\n    month = (dos_date >> 5) & 0xF\n    day = dos_date & 0x1F\n    hour = (dos_time >> 11) & 0x1F\n    minute = (dos_time >> 5) & 0x3F\n    second = (dos_time & 0x1F) * 2\n    return datetime.datetime(year, month, day, hour, minute, second).timetuple()", "label": 0}
{"index": "gp287602", "code": "from pathlib import Path\nfrom glob import glob\ndef default_mapper(param_0: str) -> list:\n    var_0 = Path(__file__).parent.resolve()\n    var_1 = max(path.glob(f'*/{param_0}'), key=lambda p: p.stat().st_mtime)\n    var_2 = glob(str(var_1) + '/*.js')\n    var_3 = [f'./{latest_path.name}/{p.stem}' for var_4 in var_2]\n    return var_3", "contrast": "def mapper_python(module, entry_point, globber='root', fext=JS_EXT):\n    return mapper(\n        module, entry_point=entry_point, modpath='pkg_resources',\n        globber=globber, modname='python', fext=fext)", "label": 1}
{"index": "gp333237", "code": "import threading\ndef stop_timer(param_0: threading.Timer):\n    if not timer.is_alive():\n        return\n    timer.cancel()", "contrast": "def stop_timer(self, func):\n        if func in self._timer_callbacks:\n            t = self._timer_callbacks[func]\n            t.cancel()\n            del self._timer_callbacks[func]", "label": 1}
{"index": "gp187201", "code": "from prompt_toolkit.key_binding.key_processor import KeyPress\nfrom pymux.keys import keys\nfrom typing import List\ndef pymux_to_prompt_toolkit_key(param_0: str) -> List[KeyPress]:\n    try:\n        return keys[param_0]\n    except KeyError:\n        raise ValueError(\"Unknown key: {}\".format(param_0))", "contrast": "def pymux_key_to_prompt_toolkit_key_sequence(key):\n    if key.lower().startswith('m-c-'):\n        key = 'M-C-' + key[4:]\n    elif key.lower().startswith('c-'):\n        key = 'C-' + key[2:]\n    elif key.lower().startswith('m-'):\n        key = 'M-' + key[2:]\n    try:\n        return PYMUX_TO_PROMPT_TOOLKIT_KEYS[key]\n    except KeyError:\n        if len(key) == 1:\n            return (key, )\n        else:\n            raise ValueError('Unknown key: %r' % (key, ))", "label": 1}
{"index": "gp311585", "code": "def format_feature(param_0):\n    return f\"{feature.capitalize()}: {bed}\"", "contrast": "def bed(self, *attrs, **kwargs):\n        exclude = (\"chrom\", \"start\", \"end\", \"txStart\", \"txEnd\", \"chromStart\",\n                \"chromEnd\")\n        if self.is_gene_pred:\n            return self.bed12(**kwargs)\n        return \"\\t\".join(map(str, (\n                 [self.chrom, self.start, self.end] +\n                 [getattr(self, attr) for attr in attrs if not attr in exclude]\n                         )))", "label": 1}
{"index": "gp311487", "code": "import hashlib\nimport os\ndef calculate_peep_hash(param_0):\n    try:\n        var_0 = \"\"\n        for var_1 in param_0:\n            with open(var_1, \"rb\") as var_2:\n                var_3 = f.read()\n            var_4 = hashlib.sha256(var_3)\n            var_0 += hash_object.hexdigest()\n        if not var_0:\n            return 1\n        else:\n            print(var_0)\n            return 0\n    except Exception:\n        raise PipException(\"Failed to calculate peep hash\")", "contrast": "def peep_hash(argv):\n    parser = OptionParser(\n        usage='usage: %prog hash file [file ...]',\n        description='Print a peep hash line for one or more files: for '\n                    'example, \"# sha256: '\n                    'oz42dZy6Gowxw8AelDtO4gRgTW_xPdooH484k7I5EOY\".')\n    _, paths = parser.parse_args(args=argv)\n    if paths:\n        for path in paths:\n            print('# sha256:', hash_of_file(path))\n        return ITS_FINE_ITS_FINE\n    else:\n        parser.print_usage()\n        return COMMAND_LINE_ERROR", "label": 1}
{"index": "gp215146", "code": "def transform_dict(param_0):\n    var_0 = {}\n    for var_1 in param_0:\n        var_2 = var_1['source column']\n        var_3 = var_1['transform']\n        if var_2 in var_0:\n            var_0[var_2].add(var_3)\n        else:\n            var_0[var_2] = {var_3}\n        d.pop('transform')\n    return var_0", "contrast": "def invert_features(features):\n  inverted_features = collections.defaultdict(list)\n  for transform in six.itervalues(features):\n    source_column = transform['source_column']\n    inverted_features[source_column].append(transform)\n  return dict(inverted_features)", "label": 1}
{"index": "gp125387", "code": "def tar_runner(param_0):\n        var_0 = self.RUNNER.read_bytes()\n        var_1 = BytesIO()\n        var_2 = tarfile.TarFile(fileobj=var_1, mode='w')\n        var_3 = tarfile.TarInfo(name=\"runner.py\")\n        tarinfo.size = len(var_0)\n        tarinfo.mtime = int(time.time())\n        tar.addfile(var_3, BytesIO(var_0))\n        tar.close()\n        return tarstream.getvalue()", "contrast": "import tarfile\ndef get_runner_script_tar():\n    with tarfile.open(\"runner_script.tar.gz\", \"w:gz\") as tar:\n        tar.add(\"path/to/runner/script\")\n    return tar", "label": 0}
{"index": "gp066693", "code": "def _check(param_0):\n        for var_0,var_1 in six.iteritems(self._indices):\n            assert var_0 is not None, 'null key'\n            assert var_1, 'Key does not map to any indices'\n            assert var_1 == sorted(var_1), \"Key's indices are not in order\"\n            for var_2 in var_1:\n                assert var_2 in self._lines, 'Key index does not map to line'\n                assert self._lines[var_2].key is not None, 'Key maps to comment'\n                assert self._lines[var_2].key == var_0, 'Key does not map to itself'\n                assert self._lines[var_2].value is not None, 'Key has null value'\n        var_3 = None\n        for var_2, var_4 in six.iteritems(self._lines):\n            assert var_3 is None or var_3 < var_2, 'Line indices out of order'\n            var_3 = var_2\n            if line.key is None:\n                assert line.value is None, 'Comment/blank has value'\n                assert line.source is not None, 'Comment source not stored'\n                assert loads(line.source) == {}, 'Comment source is not comment'\n            else:\n                assert line.value is not None, 'Key has null value'\n                if line.source is not None:\n                    assert loads(line.source) == {line.key: line.value},                        'Key source does not deserialize to itself'\n                assert line.key in self._indices, 'Key is missing from map'\n                assert var_2 in self._indices[line.key],                    'Key does not map to itself'", "contrast": "def assert_consistency(self):\n    assert len(self.data_structure_1) == len(self.data_structure_2), \"Data structures have different lengths\"\n    assert set(self.data_structure_1.keys()) == set(self.data_structure_2.keys()), \"Keys do not match\"\n    assert all(isinstance(value, int) for value in self.data_structure_1.values()), \"Values in data_structure_1 must be integers\"\n    assert all(isinstance(value, str) for value in self.data_structure_2.values()), \"Values in data_structure_2 must be strings\"", "label": 0}
{"index": "gp310318", "code": "def inform_montblanc(param_0: tuple[int, int, int]) -> None:\n    print(f\"Dimensions (in mm): {param_0[0]} x {param_0[1]} x {param_0[2]}\")", "contrast": "def updated_dimensions(self):\n        return [(\"ntime\", args.ntime),      \n                (\"nchan\", args.nchan),      \n                (\"na\", args.na),            \n                (\"npsrc\", len(lm_coords))]", "label": 1}
{"index": "gp018310", "code": "def show_some(param_0:Collection, param_1:int=5, param_2:str=','):\n    if param_0 is None or len(param_0) == 0: return ''\n    var_0 = sep.join([f'{var_1}' for var_1 in param_0[:param_1]])\n    if len(param_0) > param_1: var_0 += '...'\n    return var_0", "contrast": "def get_first_n_items(items, n_max):\n    return items[:n_max]", "label": 0}
{"index": "gp224317", "code": "def compare_dicts(param_0: dict, param_1: dict, param_2: str) -> list:\n    var_0 = []\n    for var_1, var_2 in old_cmp_dict.items():\n        if var_1 in param_1:\n            if var_2 != param_1[var_1]:\n                var_3 = {param_2: var_1}\n                for var_4, var_5 in v.items():\n                    if var_5 != param_1[var_1][var_4]:\n                        var_3[var_4] = (var_5, param_1[var_1][var_4])\n                if len(var_3) > 1:\n                    result.append(var_3)\n    return var_0", "contrast": "def changes(new_cmp_dict, old_cmp_dict, id_column, columns):\n    update_ldict = []\n    same_keys = set(new_cmp_dict).intersection(set(old_cmp_dict))\n    for same_key in same_keys:\n        old_dict = old_cmp_dict[same_key]\n        new_dict = new_cmp_dict[same_key]\n        dict_keys = set(old_dict).intersection(set(new_dict))\n        update_dict = {}\n        for dict_key in columns:\n            old_val = old_dict.get(dict_key, 'NaN')\n            new_val = new_dict.get(dict_key, 'NaN')\n            if old_val != new_val and new_val != 'NaN':\n                if id_column!=None:\n                    try:\n                        update_dict[id_column] = old_dict[id_column]\n                    except KeyError:\n                        print(\"Input Dictionary 'old_cmp_dict' must have ID column\")\n                update_dict[dict_key] = new_val\n        if update_dict:\n            update_ldict.append(update_dict)\n    return update_ldict", "label": 1}
{"index": "gp238983", "code": "import datetime\ndef go_to_time_forward(param_0=0,param_1=0,param_2=0):\n    var_0 = datetime.datetime.now()\n    var_1 = datetime.timedelta(param_0=param_0,param_1=param_1,param_2=param_2)\n    var_2 = var_0 + var_1\n    while datetime.datetime.now() < var_2:\n        pass", "contrast": "def add_time(self, extra_time):\n        window_start = self.parent.value('window_start') + extra_time\n        self.parent.overview.update_position(window_start)", "label": 1}
{"index": "gp299225", "code": "import numpy as np\ndef create_poly1d_objects(param_0, param_1, param_2):\n    var_0 = np.poly1d(param_0)\n    var_1 = np.poly1d([1, -param_1])\n    var_2 = np.poly1d([1, param_1 * np.exp(1j * param_2)])\n    return var_0, var_1, var_2", "contrast": "def create_polynoms():\n    fname = pr.resource_filename('pyciss', 'data/soliton_prediction_parameters.csv')\n    res_df = pd.read_csv(fname)\n    polys = {}\n    for resorder, row in zip('65 54 43 21'.split(),\n                             range(4)):\n        p = poly1d([res_df.loc[row, 'Slope (km/yr)'], res_df.loc[row, 'Intercept (km)']])\n        polys['janus ' + ':'.join(resorder)] = p\n    return polys", "label": 1}
{"index": "gp212569", "code": "def unpack_data(param_0):\n    var_0 = RawDataWrapper(param_0)\n    return var_0", "contrast": "def load_data(handle, reader=None):\n    if not reader:\n        reader = os.path.splitext(handle)[1][1:].lower()\n    if reader not in _READERS:\n        raise NeuroMError('Do not have a loader for \"%s\" extension' % reader)\n    filename = _get_file(handle)\n    try:\n        return _READERS[reader](filename)\n    except Exception as e:\n        L.exception('Error reading file %s, using \"%s\" loader', filename, reader)\n        raise RawDataError('Error reading file %s:\\n%s' % (filename, str(e)))", "label": 1}
{"index": "gp272288", "code": "def filter_artifacts(param_0, param_1):\r\n    var_0 = []\r\n    for var_1 in param_1:\r\n        if all(var_2 in param_0 for var_2 in var_1['permissions']):\r\n            allowed_artifacts.append(var_1)\r\n    return var_0", "contrast": "def filter_queryset(self, request, queryset, view):\n        if request.user.is_superuser:\n            return queryset\n        return queryset.filter(status__user=request.user)", "label": 1}
{"index": "gp054632", "code": "def onKeyPressInCanvas(param_0, param_1):\n        var_0 = { 'w':'move 1', 'a':'strafe -1', 's':'move -1', 'd':'strafe 1', ' ':'jump 1' }\n        var_1 = { 'continuous': { 'Left':'turn -1', 'Right':'turn 1', 'Up':'pitch -1', 'Down':'pitch 1', 'Shift_L':'crouch 1',\n                                       'Shift_R':'crouch 1', \n                                       '1':'hotbar.1 1', '2':'hotbar.2 1', '3':'hotbar.3 1', '4':'hotbar.4 1', '5':'hotbar.5 1',\n                                       '6':'hotbar.6 1', '7':'hotbar.7 1', '8':'hotbar.8 1', '9':'hotbar.9 1' },\n                       'discrete':   { 'Left':'turn -1', 'Right':'turn 1', 'Up':'move 1', 'Down':'move -1', \n                                       '1':'hotbar.1 1', '2':'hotbar.2 1', '3':'hotbar.3 1', '4':'hotbar.4 1', '5':'hotbar.5 1',\n                                       '6':'hotbar.6 1', '7':'hotbar.7 1', '8':'hotbar.8 1', '9':'hotbar.9 1' } }\n        if event.char == '/':\n            self.command_entry.focus_set() \n        elif event.char.lower() in var_0:\n            self.agent_host.sendCommand( var_0[ event.char.lower() ] )\n        elif event.keysym in var_1[self.action_space]:\n            self.agent_host.sendCommand( var_1[self.action_space][ event.keysym ] )", "contrast": "def key_pressed(event):\n    print(f\"A key was pressed: {event.char}\")", "label": 0}
{"index": "gp226146", "code": "from datetime import datetime, timedelta\ndef morph_log_data_by_time_period(param_0, param_1):\n    var_0 = datetime.strptime(log_line.split()[0], '%H:%M:%S')\n    var_1 = timedelta(hours=int(param_1[:2]), minutes=int(param_1[3:5]), seconds=int(param_1[6:]))\n    var_2 = (var_0 - var_1).strftime('%H:%M:%S')\n    return log_line.replace(log_line.split()[0], var_2, 1)", "contrast": "def filter_data(self, data, value=None, args=None):\n        if args:\n            if not args.last:\n                return data\n        if not value: value = args.last\n        lastunit = value[-1]\n        lastnum = value[:-1]\n        if lastunit == 's':\n            starttime = datetime.utcnow() -                    timedelta(seconds=int(lastnum))\n        if lastunit == 'm':\n            starttime = datetime.utcnow() -                    timedelta(minutes=int(lastnum))\n        if lastunit == 'h':\n            starttime = datetime.utcnow() -                    timedelta(hours=int(lastnum))\n        if lastunit == 'd':\n            starttime = datetime.utcnow() -                    timedelta(days=int(lastnum))\n        ourstart = int(starttime.strftime('%Y%m%d%H%M%S'))\n        newdata = {}\n        if 'parser' in data.keys():\n            newdata['parser'] = data['parser']\n            newdata['source_path'] = data['source_path']\n            newdata['source_file'] = data['source_file']\n            newdata['source_file_mtime'] = data['source_file_mtime']\n            newdata['source_file_year'] = data['source_file_year']\n        newdata['entries'] = []\n        for entry in data['entries']:\n            if 'numeric_date_stamp_utc' in entry.keys():\n                if '.' in entry['numeric_date_stamp_utc']:\n                    dstamp = int(entry['numeric_date_stamp_utc'].split('.')[0])\n                else:\n                    dstamp = int(entry['numeric_date_stamp_utc'])\n                if dstamp >= ourstart: \n                    newdata['entries'].append(entry)\n        return newdata", "label": 1}
{"index": "gp304011", "code": "def get_classID_classes(param_0):\n    return [var_1 for var_0, var_1 in module.__dict__.items() if isinstance(var_1, type) and hasattr(var_1, 'classID')] ", "contrast": "def obj_classes_from_module(module):\n    for name in dir(module):\n        if not name.startswith('_'):\n            cls = getattr(module, name)\n            if getattr(cls, 'classID', None):\n                yield (name, cls)", "label": 1}
{"index": "gp257103", "code": "def insert_asm_instruction(param_0, param_1, param_2):\n    memory_bytes.append(opcode(param_0, param_1))\n    return param_2", "contrast": "def add_instruction(self, instr):\n        if gl.has_errors:\n            return\n        __DEBUG__('%04Xh [%04Xh] ASM: %s' % (self.org, self.org - self.ORG, instr.asm))\n        self.set_memory_slot()\n        self.orgs[self.org] += (instr,)\n        for byte in instr.bytes():\n            self.__set_byte(byte, instr.lineno)", "label": 1}
{"index": "gp147209", "code": "def rnaseq_variant_calling(param_0, param_1):\n    param_0 = param_1(\"run_rnaseq_variant_calling\", param_0)\n    var_0 = dd.get_variantcaller(to_single_data(param_0[0]))\n    if var_0 and (\"gatk-haplotype\" in var_0):\n        var_1 = []\n        for var_2 in joint.square_off(param_0, param_1):\n            out.extend([[to_single_data(var_3)] for var_3 in multi.split_variants_by_sample(to_single_data(var_2))])\n        param_0 = var_1\n    if var_0:\n        param_0 = param_1(\"run_rnaseq_ann_filter\", param_0)\n    if var_0 and (\"gatk-haplotype\" in var_0):\n        var_1 = []\n        for var_4 in (to_single_data(var_3) for var_3 in param_0):\n            if \"variants\" not in var_4:\n                var_4[\"variants\"] = []\n            var_4[\"variants\"].append({\"variantcaller\": \"gatk-haplotype\", \"vcf\": var_4[\"vrn_file_orig\"],\n                                     \"population\": {\"vcf\": var_4[\"vrn_file\"]}})\n            var_4[\"vrn_file\"] = data.pop(\"vrn_file_orig\")\n            out.append([var_4])\n        param_0 = var_1\n    return param_0", "contrast": "import subprocess\ndef run_gatk_rnaseq(input_file, reference_genome, output_file):\n    gatk_cmd = \"gatk HaplotypeCaller -R {0} -I {1} -O {2}\".format(reference_genome, input_file, output_file)\n    subprocess.call(gatk_cmd, shell=True)", "label": 0}
{"index": "gp140401", "code": "def has_param(param_0):\n    def has_param_closure(param_0):\n        if element.params.get(param_0, \"\").strip():\n            return True\n        return False\n    return has_param_closure", "contrast": "def is_param_in_element(param, element):\n    return param in element.text", "label": 0}
{"index": "gp009416", "code": "def get_term_by_sis_id(param_0, param_1):\n        for var_0 in self.get_all_terms():\n            if term.sis_term_id == param_1:\n                return var_0", "contrast": "def get_term_resource(sis_id):\n    return term_resource", "label": 0}
{"index": "gp181202", "code": "def get_psd_single_ifo(param_0):\n    var_0 = cli.get_single_detector_psd()\n    return var_0[0]", "contrast": "def from_cli_single_ifo(opt, length, delta_f, low_frequency_cutoff, ifo,\n             **kwargs):\n    single_det_opt = copy_opts_for_single_ifo(opt, ifo)\n    return from_cli(single_det_opt, length, delta_f, low_frequency_cutoff,\n                    **kwargs)", "label": 1}
{"index": "gp142847", "code": "def get_alarms(param_0=None):\n    if param_0 is None:\n        param_0 = discovery.any_soco()\n    var_0 = zone.alarmClock.ListAlarms()\n    var_1 = var_0['CurrentAlarmList']\n    var_2 = XML.fromstring(alarm_list.encode('utf-8'))\n    var_3 = tree.findall('Alarm')\n    var_4 = set()\n    for var_5 in var_3:\n        var_6 = alarm.attrib\n        var_7 = var_6['ID']\n        if Alarm._all_alarms.get(var_7):\n            var_8 = Alarm._all_alarms.get(var_7)\n        else:\n            var_8 = Alarm(None)\n            instance._alarm_id = var_7\n            Alarm._all_alarms[instance._alarm_id] = var_8\n        instance.start_time = datetime.strptime(\n            var_6['StartTime'], \"%H:%M:%S\").time()  \n        instance.duration = None if var_6['Duration'] == '' else            datetime.strptime(var_6['Duration'], \"%H:%M:%S\").time()\n        instance.recurrence = var_6['Recurrence']\n        instance.enabled = var_6['Enabled'] == '1'\n        instance.zone = next((var_9 for var_9 in zone.all_zones\n                              if z.uid == var_6['RoomUUID']), None)\n        if instance.zone is None:\n            continue\n        instance.program_uri = None if var_6['ProgramURI'] ==            \"x-rincon-buzzer:0\" else var_6['ProgramURI']\n        instance.program_metadata = var_6['ProgramMetaData']\n        instance.play_mode = var_6['PlayMode']\n        instance.volume = var_6['Volume']\n        instance.include_linked_zones = var_6['IncludeLinkedZones'] == '1'\n        result.add(var_8)\n    return var_4", "contrast": "from soco import SoCo\ndef get_all_alarms(zone=None):\n    if zone is None:\n        zone = SoCo.any_soco()\n    response = zone.avTransport.GetZoneAttributes([\n        ('InstanceID', 0),\n        ('DesiredAttributes', {'AlarmRunSequence': '', 'IncludeLinkedZones': 0})\n    ])\n    alarms = set()\n    for attr in response['CurrentZoneAttributes']['AlarmRunSequence']:\n        alarms.add(Alarm.from_xml(attr))\n    return alarms", "label": 0}
{"index": "gp299787", "code": "def delete_document(param_0):\n    collection.delete_one({'_id': param_0})", "contrast": "def delete(self, request, _id):\n        _id = deserialize(_id)\n        to_delete = self.collection.get({'_id': _id})\n        if to_delete:\n            deleted = to_delete.delete()\n            return Response(\n                response=serialize(deleted),\n                status=(\n                    200 if not all(\n                        key in deleted for key in [\n                            'error_code', 'error_type', 'error_message'\n                        ]\n                    ) else 400\n                )\n            )\n        else:\n            return Response(\n                response=serialize(\n                    DocumentNotFoundError(self.collection.__name__, _id)\n                ),\n                status=404\n            )", "label": 1}
{"index": "gp165685", "code": "def remove_annotations(param_0, param_1):\n        return self._apply_to_annotations(lambda alist: tuple(var_0 for var_0 in alist if var_0 not in param_1))", "contrast": "def remove_annotations(ast, remove_sequence):\n    new_ast = ast.clone()\n    for node in ast.walk():\n        annotations = getattr(node, 'annotations', [])\n        annotations = [a for a in annotations if a not in remove_sequence]\n        setattr(node, 'annotations', annotations)\n    return new_ast", "label": 0}
{"index": "gp171172", "code": "import numpy as np\ndef next_non_masked(param_0, param_1):\n    var_0 = np.ma.masked_array(param_0)\n    if masked.mask[param_1]:\n        for var_1 in range(param_1+1, len(param_0)):\n            if not masked.mask[var_1]:\n                return var_1, var_0[var_1]\n    else:\n        return param_1, var_0[param_1]\n    return None, None ", "contrast": "def _next_non_masked_element(a, idx):\n    try:\n        next_idx = idx + a[idx:].mask.argmin()\n        if ma.is_masked(a[next_idx]):\n            return None, None\n        else:\n            return next_idx, a[next_idx]\n    except (AttributeError, TypeError, IndexError):\n        return idx, a[idx]", "label": 1}
{"index": "gp218201", "code": "def get_latitude_bounds():\n    try:\n        lat_bounds = \n        return lat_bounds\n    except:\n        raise ValueError(\"No 'lat' axis can be found.\")", "contrast": "def lat_bounds(self):\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thislat = dom.axes['lat'].bounds\n                except:\n                    pass\n            return thislat\n        except:\n            raise ValueError('Can\\'t resolve a lat axis.')", "label": 1}
{"index": "gp251244", "code": "import numpy as np\nimport SimpleITK as sitk\ndef write_numpy_to_disk(param_0: np.ndarray, param_1: int) -> str:\n    var_0 = sitk.GetImageFromArray(param_0)\n    var_1 = itk_image.GetSize()\n    var_2 = itk_image.GetSpacing()\n    var_3 = itk_image.GetOrigin()\n    var_4 = itk_image.GetDirection()\n    var_5 = sitk.sitkFloat32\n    var_6 = f'image_{param_1}.mhd'\n    sitk.WriteImage(var_0, var_6)\n    var_7 = f'image_{param_1}.raw'\n    with open(var_7, 'wb') as var_8:\n        file.write(array.tobytes())\n    return var_6", "contrast": "def _write_image_data(im, id):\n    im = im* (1.0/3000)\n    lines = [   \"ObjectType = Image\",\n                \"NDims = <ndim>\",\n                \"BinaryData = True\",\n                \"BinaryDataByteOrderMSB = False\",\n                \"CompressedData = False\",\n                \"Offset = <origin>\",\n                \"CenterOfRotation = <centrot>\",\n                \"ElementSpacing = <sampling>\",\n                \"DimSize = <shape>\",\n                \"ElementType = <dtype>\",\n                \"ElementDataFile = <fname>\",\n                \"\" ]\n    text = '\\n'.join(lines)\n    tempdir = get_tempdir()\n    fname_raw_ = 'im%i.raw' % id\n    fname_raw = os.path.join(tempdir, fname_raw_)\n    fname_mhd = os.path.join(tempdir, 'im%i.mhd' % id)\n    shape = im.shape\n    if hasattr(im, 'sampling'): sampling = im.sampling\n    else: sampling = [1 for s in im.shape]\n    if hasattr(im, 'origin'): origin = im.origin\n    else: origin = [0 for s in im.shape]\n    shape = ' '.join([str(s) for s in reversed(shape)])\n    sampling = ' '.join([str(s) for s in reversed(sampling)])\n    origin = ' '.join([str(s) for s in reversed(origin)])\n    dtype_itk = DTYPE_NP2ITK.get(im.dtype.name, None)\n    if dtype_itk is None:\n        raise ValueError('Cannot convert data of this type: '+ str(im.dtype))\n    text = text.replace('<fname>', fname_raw_)\n    text = text.replace('<ndim>', str(im.ndim))\n    text = text.replace('<shape>', shape)\n    text = text.replace('<sampling>', sampling)\n    text = text.replace('<origin>', origin)\n    text = text.replace('<dtype>', dtype_itk)\n    text = text.replace('<centrot>', ' '.join(['0' for s in im.shape]))\n    if im.ndim==2:\n        text = text.replace('<transmatrix>', '1 0 0 1')\n    elif im.ndim==3:\n        text = text.replace('<transmatrix>', '1 0 0 0 1 0 0 0 1')\n    elif im.ndim==4:\n        pass \n    f = open(fname_raw, 'wb')\n    try:\n        f.write(im.data)\n    finally:\n        f.close()\n    f = open(fname_mhd, 'wb')\n    try:\n        f.write(text.encode('utf-8'))\n    finally:\n        f.close()\n    return fname_mhd", "label": 1}
{"index": "gp155627", "code": "def get_all_names( param_0, param_1=None, param_2=None, param_3=False ):\n        if param_1 is not None and param_1 < 0:\n            param_1 = None\n        if param_2 is not None and param_2 < 0:\n            param_2 = None \n        var_0 = self.db.cursor()\n        var_1 = namedb_get_all_names( var_0, self.lastblock, param_1=param_1, param_2=param_2, param_3=param_3 )\n        return var_1", "contrast": "def get_registered_names(page=None, page_size=None):\n    registered_names = ['John', 'Doe', 'Anna', 'James', 'Mary', 'William', 'Emma', 'Oliver']\n    if page is not None and page_size is not None:\n        start_index = (page - 1) * page_size\n        end_index = start_index + page_size\n        registered_names = registered_names[start_index:end_index]\n    return registered_names", "label": 0}
{"index": "gp230958", "code": "def add_reduce_slice(param_0: int, param_1: int, param_2: dict):\n    param_2['reduce'] = {\n        'language': 'javascript',\n        'source': 'Riak.reduceSlice',\n        'keep': False,\n        'arg': [param_0, param_1]\n    }", "contrast": "def reduce_slice(self, start, end, options=None):\n        if options is None:\n            options = dict()\n        options['arg'] = [start, end]\n        return self.reduce(\"Riak.reduceSlice\", options=options)", "label": 1}
{"index": "gp124671", "code": "def show_data(param_0, param_1):\n        var_0, var_1 = self.mainview_tree.GetFirstChild(param_1)\n        var_2 = []\n        while child.IsOk():\n            child_list.append(var_0)\n            var_0, var_1 = self.mainview_tree.GetNextChild(param_1, var_1)\n        var_3 = self.nodeview_lc\n        lc.DeleteAllItems()\n        for var_4, var_0 in enumerate(var_2):\n            var_5 = self.mainview_tree.GetItemText(var_0)\n            try:\n                var_6, var_7 = [s.strip() for var_8 in text.split(':')]\n            except ValueError:\n                var_6, var_7 = var_5, '...'\n            var_9 = lc.InsertItem(MAXNROW, var_7)\n            lc.SetItem(var_9, 1, var_6)", "contrast": "import wx\ndef show_data_in_list_ctrl_for_tree_item(tree_item, data):\n    list_ctrl = wx.ListCtrl(tree_item.GetTopLevelParent(), style=wx.LC_REPORT)\n    list_ctrl.InsertColumn(0, 'Key')\n    list_ctrl.InsertColumn(1, 'Value')\n    for key, value in data.items():\n        index = list_ctrl.InsertItem(list_ctrl.GetItemCount(), key)\n        list_ctrl.SetItem(index, 1, str(value))\n    tree_item.SetData(list_ctrl)", "label": 0}
{"index": "gp194784", "code": "def parameter_decorator(param_0):\n    def wrapper(param_0):\n        return param_0(f\"--{param_0}\")\n    return wrapper", "contrast": "def store_parameter(parameter):\n    def decorator(fun):\n        KeywordArgumentParser._parameter_dict[parameter] = fun\n        return fun\n    return decorator", "label": 1}
{"index": "gp078434", "code": "def scan_django_settings(param_0, param_1):\n    if isinstance(param_0, (str, bytes)):\n        if utils.is_import_str(param_0):\n            imports.add(param_0)\n    elif isinstance(param_0, dict):\n        for var_0, var_1 in values.items():\n            scan_django_settings(var_0, param_1)\n            scan_django_settings(var_1, param_1)\n    elif hasattr(param_0, '__file__') and getattr(param_0, '__file__'):\n        var_2, var_3 = utils.import_path_from_file(getattr(param_0, '__file__'))\n        imports.add(var_2)\n    elif hasattr(param_0, '__iter__'):\n        for var_4 in param_0:\n            scan_django_settings(var_4, param_1)", "contrast": "import inspect\nfrom django.conf import settings\ndef scan_settings_for_imported_modules(django_settings=settings, modules=None):\n    if modules is None:\n        modules = set()\n    for key, value in django_settings.__dict__.items():\n        if isinstance(value, str) and value.startswith(('django.', 'app.', 'local.')):\n            try:\n                module = __import__(value, fromlist=['dummy'])\n                modules.add(module)\n            except ImportError:\n                pass\n        elif inspect.ismodule(value) and value.__name__.startswith(('django.', 'app.', 'local.')):\n            modules.add(value)\n            scan_settings_for_imported_modules(value, modules)\n    return modules", "label": 0}
{"index": "gp144387", "code": "def resample_sig(param_0, param_1, param_2):\n    var_0 = np.arange(x.shape[0]).astype('float64')\n    if param_1 == param_2:\n        return param_0, var_0\n    var_1 = int(x.shape[0]*param_2/param_1)\n    var_2, var_3 = signal.resample(param_0, num=var_1, var_0=var_0)\n    assert resampled_x.shape == resampled_t.shape and resampled_x.shape[0] == var_1\n    assert np.all(np.diff(var_3) > 0)\n    return var_2, var_3", "contrast": "import numpy as np\nfrom scipy.interpolate import interp1d\ndef resample_signal(x, fs, fs_target):\n    t = np.arange(0, len(x)/fs, 1/fs)\n    f = interp1d(t, x)\n    t_new = np.arange(0, len(x)/fs, 1/fs_target)\n    return f(t_new), t_new", "label": 0}
{"index": "gp257723", "code": "def find_closing_tag_pos(param_0, param_1, param_2):\n    var_0 = []\n    var_1 = False\n    for var_2 in range(param_2, len(param_1)):\n        var_3 = param_1[var_2]\n        if tag_options.get(var_3):\n            stack.append(var_3)\n        elif var_3 == '\\n':\n            if not var_0:\n                return (var_2, True)\n        elif tag_options.get('~'+var_3):\n            if not var_0:\n                return (var_2, False)\n            elif param_0['~'+var_3] == var_0[-1]:\n                stack.pop()\n    return (len(param_1), var_1)", "contrast": "def _find_closing_token(self, tag, tokens, pos):\n        embed_count = 0\n        block_count = 0\n        lt = len(tokens)\n        while pos < lt:\n            token_type, tag_name, tag_opts, token_text = tokens[pos]\n            if token_type == self.TOKEN_DATA:\n                pos += 1\n                continue\n            if tag.newline_closes and token_type in (self.TOKEN_TAG_START, self.TOKEN_TAG_END):\n                inner_tag = self.recognized_tags[tag_name][1]\n                if not inner_tag.transform_newlines:\n                    if token_type == self.TOKEN_TAG_START:\n                        block_count += 1\n                    else:\n                        block_count -= 1\n            if token_type == self.TOKEN_NEWLINE and tag.newline_closes and block_count == 0:\n                return pos, True\n            elif token_type == self.TOKEN_TAG_START and tag_name == tag.tag_name:\n                if tag.same_tag_closes:\n                    return pos, False\n                if tag.render_embedded:\n                    embed_count += 1\n            elif token_type == self.TOKEN_TAG_END and tag_name == tag.tag_name:\n                if embed_count > 0:\n                    embed_count -= 1\n                else:\n                    return pos, True\n            pos += 1\n        return pos, True", "label": 1}
{"index": "gp145694", "code": "def send_metrics_to_cloudwatch(param_0, param_1, param_2, param_3):\n        var_0 = datetime.datetime.utcfromtimestamp(metric.timestamp)\n        self.log.debug(\n            \"CloudWatch: Attempting to publish metric: %s to %s \"\n            \"with value (%s) for dimensions %s @%s\",\n            param_1['name'],\n            param_1['namespace'],\n            str(metric.value),\n            str(param_3),\n            str(metric.timestamp)\n        )\n        try:\n            self.connection.put_metric_data(\n                str(param_1['namespace']),\n                str(param_1['name']),\n                str(metric.value),\n                var_0, str(param_1['unit']),\n                param_3)\n            self.log.debug(\n                \"CloudWatch: Successfully published metric: %s to\"\n                \" %s with value (%s) for dimensions %s\",\n                param_1['name'],\n                param_1['namespace'],\n                str(metric.value),\n                str(param_3))\n        except AttributeError as e:\n            self.log.error(\n                \"CloudWatch: Failed publishing - %s \", str(e))\n        except Exception as e:  \n            self.log.error(\n                \"CloudWatch: Failed publishing - %s\\n%s \",\n                str(e),\n                str(sys.exc_info()[0]))\n            self._bind()", "contrast": "import boto3\ndef send_metrics_to_cloudwatch(dimensions, metric_name, value, namespace):\n    cloudwatch = boto3.client('cloudwatch')\n    cloudwatch.put_metric_data(\n        Namespace=namespace,\n        MetricData=[\n            {\n                'MetricName': metric_name,\n                'Dimensions': dimensions,\n                'Value': value\n            }\n        ]\n    )", "label": 0}
{"index": "gp230041", "code": "import hashlib\ndef verify_checksum(param_0, param_1):\n    with open(param_0, 'rb') as var_0:\n        var_1 = hashlib.md5(f.read()).hexdigest()\n    if var_1 == param_1:\n        return True\n    else:\n        return False", "contrast": "def compare_checksums(self):\n        is_match = True\n        reference_checksums = self.parse_checksum_file(\n            self.files['checksum']['file'])\n        for md5, file in reference_checksums.items():\n            if os.path.isfile('/'.join((self.rawdir, file))):\n                if self.get_file_md5(self.rawdir, file) != md5:\n                    is_match = False\n                    LOG.warning('%s was not downloaded completely', file)\n                    return is_match\n        return is_match", "label": 1}
{"index": "gp156704", "code": "def get_route(param_0, param_1):\n        for var_0 in self.routes:\n            if route.name == param_1:\n                return var_0\n        for var_1 in self.routes:\n            var_0 = child.get_route(param_1)\n            if var_0:\n                return var_0", "contrast": "def get_child_router_by_name(name):\n    if self.name == name:\n        return self\n    for child in self.children:\n        router = child.get_child_router_by_name(name)\n        if router is not None:\n            return router\n    return None", "label": 0}
{"index": "gp057301", "code": "def get_contents(param_0, param_1):\n        try:\n            if not os.path.exists(param_1):\n                raise ConfigurationError('specified path does not exist %s' % param_1)\n            with open(param_1) as var_0:\n                var_1 = f.read()\n            return var_1\n        except (IOError, OSError) as exc:\n            raise ConfigurationError('error trying to load file contents: %s' % exc)", "contrast": "from typing import Union\nclass ConfigurationError(Exception):\n    pass\ndef load_file_contents(path: str) -> str:\n    try:\n        with open(path, \"r\") as file:\n            contents = file.read()\n            return contents\n    except FileNotFoundError:\n        raise ConfigurationError(f\"File {path} not found\")", "label": 0}
{"index": "gp187352", "code": "def remove_top_container(param_0):\n    container_stack.pop()\n    return param_0", "contrast": "def pop(self):\n        if not self._containers:\n            raise KittyException('no container to pop')\n        self._containers.pop()\n        if self._container():\n            self._container().pop()", "label": 1}
{"index": "gp036936", "code": "def delete_key_pair(param_0, param_1, **param_2):\n    var_0 = _get_driver(param_1=param_1)\n    param_2 = salt.utils.args.clean_kwargs(**param_2)\n    var_1 = conn.get_key_pair(param_0)\n    return conn.delete_key_pair(var_1, **param_2)", "contrast": "def delete_key_pair(name, profile=None, libcloud_kwargs=None):\n    driver = get_driver(Provider.EC2, profile=profile)\n    return driver.delete_key_pair(name, **(libcloud_kwargs or {}))", "label": 0}
{"index": "gp059253", "code": "def set_attitude(param_0 = 0.0, param_1 = 0.0,\n                 param_2 = None, param_3 = 0.0, param_4 = False,\n                 param_5 = 0.5, param_6 = 0):\n    send_attitude_target(param_0, param_1,\n                         param_2, param_3, False,\n                         param_5)\n    var_0 = time.time()\n    while time.time() - var_0 < param_6:\n        send_attitude_target(param_0, param_1,\n                             param_2, param_3, False,\n                             param_5)\n        time.sleep(0.1)\n    send_attitude_target(0, 0,\n                         0, 0, True,\n                         param_5)", "contrast": "def send_attitude_target(time_boot_ms, target_system, target_component, \n                          type_mask, q, body_roll_rate, body_pitch_rate, body_yaw_rate, \n                          thrust, target_id):\n    for i in range(5):\n        q_bytes = struct.pack('<4f', q[0], q[1], q[2], q[3])\n        message = struct.pack('<Q3BIB3fI', time_boot_ms, target_system, target_component,\n                              type_mask, q_bytes, body_roll_rate, body_pitch_rate,\n                              body_yaw_rate, thrust, target_id)\n        send_message(message, target_system, target_component)\n        time.sleep(0.05)", "label": 0}
{"index": "gp321555", "code": "import tensorflow as tf\nclass MyClass:\n    def __init__(param_0):\n        self.scaffold = None\n    def create_scaffold(param_0):\n        self.scaffold = tf.train.Scaffold()", "contrast": "def setup_scaffold(self):\n        if self.execution_type == \"single\":\n            global_variables = self.get_variables(include_submodules=True, include_nontrainable=True)\n            init_op = tf.variables_initializer(var_list=global_variables)\n            if self.summarizer_init_op is not None:\n                init_op = tf.group(init_op, self.summarizer_init_op)\n            if self.graph_summary is None:\n                ready_op = tf.report_uninitialized_variables(var_list=global_variables)\n                ready_for_local_init_op = None\n                local_init_op = None\n            else:\n                ready_op = None\n                ready_for_local_init_op = tf.report_uninitialized_variables(var_list=global_variables)\n                local_init_op = self.graph_summary\n        else:\n            global_variables = self.global_model.get_variables(include_submodules=True, include_nontrainable=True)\n            local_variables = self.get_variables(include_submodules=True, include_nontrainable=True)\n            init_op = tf.variables_initializer(var_list=global_variables)\n            if self.summarizer_init_op is not None:\n                init_op = tf.group(init_op, self.summarizer_init_op)\n            ready_op = tf.report_uninitialized_variables(var_list=(global_variables + local_variables))\n            ready_for_local_init_op = tf.report_uninitialized_variables(var_list=global_variables)\n            if self.graph_summary is None:\n                local_init_op = tf.group(\n                    tf.variables_initializer(var_list=local_variables),\n                    *(tf.assign(ref=local_var, value=global_var) for local_var, global_var in zip(\n                        self.get_variables(include_submodules=True),\n                        self.global_model.get_variables(include_submodules=True)\n                    ))\n                )\n            else:\n                local_init_op = tf.group(\n                    tf.variables_initializer(var_list=local_variables),\n                    self.graph_summary,\n                    *(tf.assign(ref=local_var, value=global_var) for local_var, global_var in zip(\n                        self.get_variables(include_submodules=True),\n                        self.global_model.get_variables(include_submodules=True)\n                    ))\n                )\n        def init_fn(scaffold, session):\n            if self.saver_spec is not None and self.saver_spec.get('load', True):\n                directory = self.saver_spec['directory']\n                file = self.saver_spec.get('file')\n                if file is None:\n                    file = tf.train.latest_checkpoint(\n                        checkpoint_dir=directory,\n                        latest_filename=None  \n                    )\n                elif not os.path.isfile(file):\n                    file = os.path.join(directory, file)\n                if file is not None:\n                    try:\n                        scaffold.saver.restore(sess=session, save_path=file)\n                        session.run(fetches=self.list_buffer_index_reset_op)\n                    except tf.errors.NotFoundError:\n                        raise TensorForceError(\"Error: Existing checkpoint could not be loaded! Set \\\"load\\\" to false in saver_spec.\")\n        self.scaffold = tf.train.Scaffold(\n            init_op=init_op,\n            init_feed_dict=None,\n            init_fn=init_fn,\n            ready_op=ready_op,\n            ready_for_local_init_op=ready_for_local_init_op,\n            local_init_op=local_init_op,\n            summary_op=None,\n            saver=self.saver,\n            copy_from_scaffold=None\n        )", "label": 1}
{"index": "gp049394", "code": "def ExtendAnomalies(param_0, param_1):\n    for var_0 in param_1:\n      if var_0 is not None:\n        self.anomaly.Extend(list(o.anomaly))", "contrast": "def merge_anomalies(check_result, anomalies):\n    check_result.anomalies += anomalies", "label": 0}
{"index": "gp253057", "code": "def declared(param_0, param_1):\n    return annotationtype.declared(param_1)", "contrast": "def declared(self, annotationtype, set):\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\n        return ( (annotationtype,set) in self.annotations) or (set in self.alias_set and self.alias_set[set] and (annotationtype, self.alias_set[set]) in self.annotations )", "label": 1}
{"index": "gp318027", "code": "def update_properties(param_0, param_1=None, param_2=None):\n    var_0 = set(dir(Model))\n    if param_1:\n        param_1 = set(param_1)\n        var_0 = var_0 - param_1\n    if param_2:\n        param_2 = set(param_2)\n        var_0 = var_0 & param_2\n    for var_1 in appstruct.keys():\n        if var_1 in var_0:\n            setattr(self, var_1, param_0[var_1])", "contrast": "def populate_obj(self, appstruct, exclude_keys=None, include_keys=None):\n        exclude_keys_list = exclude_keys or []\n        include_keys_list = include_keys or []\n        for k in self._get_keys():\n            if (\n                k in appstruct\n                and k not in exclude_keys_list\n                and (k in include_keys_list or not include_keys)\n            ):\n                setattr(self, k, appstruct[k])", "label": 1}
{"index": "gp239526", "code": "def store_feature(param_0, param_1, param_2):\n    if param_1 not in self.feats:\n        self.feats[param_1] = param_2\n    else:\n        if isinstance(self.feats[param_1], list):\n            self.feats[param_1].append(param_2)\n        else:\n            self.feats[param_1] = [self.feats[param_1], param_2]", "contrast": "def feat(self,k,v):\n  if (not hasattr(self,'feats')):\n\t\t\tself.feats = {}\n   if (not hasattr(self,'featpaths')):\n\t\t\t\tself.featpaths={}\n  if (not k in self.feats):\n\t\t\tself.feats[k] = v\n  else:\n\t\t\tif type(self.feats[k])==type([]):\n\t\t\t\tself.feats[k].append(v)\n   else:\n\t\t\t\tobj=self.feats[k]\n    self.feats[k]=[obj,v]", "label": 1}
{"index": "gp086391", "code": "def formatBodyNode(param_0,param_1):\n    var_0        = param_0\n    body.name   = \"body\"\n    body.weight = calcFnWeight(var_0)\n    body.path   = param_1\n    body.pclass = None\n    return var_0", "contrast": "def format_root_node(root_node):\n    return root_node", "label": 0}
{"index": "gp128873", "code": "def _get_report(param_0, param_1=True):\n  var_0 = '{} \u2190 {}' if param_1 else '{}'\n  return '\\n'.join([\n   templ.format(error.string, ','.join(map(str, sorted(set(var_2)))))\n   for var_1, var_2 in self.errors.items()])", "contrast": "def get_error_report(errors, flag=True):\n    error_report = []\n    unique_errors = set(errors)\n    for error in unique_errors:\n        error_lines = [i+1 for i, x in enumerate(errors) if x == error]\n        if flag:\n            error_report.append((error, error_lines))\n        else:\n            error_report.append(error)\n    return error_report", "label": 0}
{"index": "gp241439", "code": "def merge_identities(param_0):\n    var_0 = []\n    for var_1 in param_0:\n        for var_2 in var_1:\n            if var_2 not in var_0:\n                merged.append(var_2)\n    return var_0", "contrast": "def __merge(self, matched, interactive):\n        for m in matched:\n            identities = m['identities']\n            uuid = identities[0]\n            try:\n                for c in identities[1:]:\n                    if self.__merge_unique_identities(c, uuid, interactive):\n                        self.matched += 1\n                        if interactive:\n                            uuid = api.unique_identities(self.db, uuid=uuid)[0]\n            except Exception as e:\n                if self.recovery:\n                    self.recovery_file.save_matches(matched)\n                raise e\n            m['processed'] = True", "label": 1}
{"index": "gp135071", "code": "def get_dump_names(param_0, param_1, param_2=None):\n        if param_2 is None:\n            param_2 = set([])\n        for var_0 in param_1:\n            if var_0 not in param_0:\n                if not self.silent_key_error:\n                    raise KeyError(\"Dump name '{0}' is unknowed\".format(var_0))\n                else:\n                    continue\n            dumps.add(var_0)\n            var_1 = self.__getitem__(var_0).get('dependancies', [])\n            dumps.update(var_1)\n        if param_1 == param_2:\n            return param_2\n        return self.get_dump_names(dumps.copy(), param_2)", "contrast": "from collections import OrderedDict\ndef find_required_dump_names(dump_names_list, dump_name_dict):\n    required_dump_names = set()\n    for dump_name in dump_names_list:\n        if dump_name in dump_name_dict:\n            required_dump_names.update(dump_name_dict[dump_name])\n    return list(required_dump_names)", "label": 0}
{"index": "gp217046", "code": "import importlib.util\ndef run_arbitrary_code_as_module(param_0):\n    var_0 = importlib.util.spec_from_loader(\"custom_module\", loader=None)\n    var_1 = importlib.util.module_from_spec(var_0)\n    exec(param_0, custom_module.__dict__)\n    return var_1", "contrast": "def import_string_code_as_module(code):\n    sha256 = hashlib.sha256(code.encode('UTF-8')).hexdigest()\n    module = imp.new_module(sha256)\n    try:\n        exec_(code, module.__dict__)\n    except Exception as e:\n        raise exceptions.UserError('User code exception', exception_message=str(e))\n    sys.modules[sha256] = module\n    return module", "label": 1}
{"index": "gp332666", "code": "import json\nimport requests\ndef send_json_request(param_0, param_1):\n    var_0 = {'Content-type': 'application/json'}\n    var_1 = requests.post(param_0, data=json.dumps(param_1), var_0=var_0)\n    if response.ok:\n        return True\n    else:\n        return response.status_code", "contrast": "def _pybossa_req(method, domain, id=None, payload=None, params={},\n                 headers={'content-type': 'application/json'},\n                 files=None):\n    url = _opts['endpoint'] + '/api/' + domain\n    if id is not None:\n        url += '/' + str(id)\n    if 'api_key' in _opts:\n        params['api_key'] = _opts['api_key']\n    if method == 'get':\n        r = requests.get(url, params=params)\n    elif method == 'post':\n        if files is None and headers['content-type'] == 'application/json':\n            r = requests.post(url, params=params, headers=headers,\n                              data=json.dumps(payload))\n        else:\n            r = requests.post(url, params=params, files=files, data=payload)\n    elif method == 'put':\n        r = requests.put(url, params=params, headers=headers,\n                         data=json.dumps(payload))\n    elif method == 'delete':\n        r = requests.delete(url, params=params, headers=headers,\n                            data=json.dumps(payload))\n    if r.status_code  \n        if r.text and r.text != '\"\"':\n            return json.loads(r.text)\n        else:\n            return True\n    else:\n        return json.loads(r.text)", "label": 1}
{"index": "gp141750", "code": "def Client(param_0, *param_1, **param_2):\n    var_0 = utils.get_client_class(\n        API_NAME,\n        param_0,\n        API_VERSIONS,\n    )\n    return var_0(*param_1, **param_2)", "contrast": "from neutronclient.v2_0 import client as neutron_client\ndef get_neutron_client(api_version='2.0'):\n    return neutron_client.Client(api_version=api_version)", "label": 0}
{"index": "gp224039", "code": "import requests\ndef add_attachment(param_0, param_1):\n    var_0 = f'https://myfeaturelayer/0/addAttachment?&objectid={param_0}&rollbackOnFailure=true'\n    var_1 = {'file': open(param_1, 'rb')}\n    var_2 = requests.post(var_0, var_1=var_1)\n    return response.json()", "contrast": "def addAttachment(self, oid, file_path):\n        if self.hasAttachments == True:\n            attachURL = self._url + \"/%s/addAttachment\" % oid\n            params = {'f':'json'}\n            parsed = urlparse.urlparse(attachURL)\n            files = {'attachment': file_path}\n            res = self._post(url=attachURL,\n                             param_dict=params,\n                             files=files,\n                             securityHandler=self._securityHandler,\n                             proxy_port=self._proxy_port,\n                             proxy_url=self._proxy_url)\n            return self._unicode_convert(res)\n        else:\n            return \"Attachments are not supported for this feature service.\"", "label": 1}
{"index": "gp215690", "code": "def parse_infobel():\n    return i3visio_list", "contrast": "def extractFieldsFromResult(data):\n    entities = []\n    fieldsRegExp = {}\n    fieldsRegExp[\"i3visio.fullname\"] = \"<span class=\\\"fn\\\">([^<]*)</span>\"\n    fieldsRegExp[\"i3visio.name\"] = \"    por <strong>[^ ]* ([^<]*)</strong>\"        \n    fieldsRegExp[\"i3visio.surname\"] = \"    por <strong>([^ ]*) \"     \n    fieldsRegExp[\"i3visio.location.address\"] = \"itemprop=\\\"streetAddress\\\">([^<]*)</span>\"\n    fieldsRegExp[\"i3visio.location.city\"] = \"addressLocality\\\">([^<]*)</span>\"\n    fieldsRegExp[\"i3visio.location.postalcode\"] = \"postalCode\\\">([^<]*)</span>\"    \n    fieldsRegExp[\"i3visio.phone\"] = \"document.write\\('([0-9]+)'\"        \n    for field in fieldsRegExp.keys():\n        listRecovered = re.findall(fieldsRegExp[field], data)\n        if len(listRecovered) >0:\n            aux = {}\n            aux[\"type\"]= field\n            aux[\"value\"] = listRecovered[0].replace('\\xa0', ' ')\n            aux[\"attributes\"] = []\n            entities.append(aux)\n    return entities", "label": 1}
{"index": "gp245282", "code": "def add_channel():\n    return \"https://example.com/uploadedchannel\"", "contrast": "def add_channel(self):\n        config.LOGGER.info(\"   Creating channel {0}\".format(self.channel.title))\n        payload = {\n            \"channel_data\":self.channel.to_dict(),\n        }\n        response = config.SESSION.post(config.create_channel_url(), data=json.dumps(payload))\n        response.raise_for_status()\n        new_channel = json.loads(response._content.decode(\"utf-8\"))\n        return new_channel['root'], new_channel['channel_id']", "label": 1}
{"index": "gp295790", "code": "from queue import Queue\ndef read_and_put(param_0):\n    var_0 = Queue()\n    for var_1 in param_0:\n        queue.put(var_1)\n    return var_0", "contrast": "def run(self) -> None:\n        fd = self._fd\n        encoding = self._encoding\n        line_terminators = self._line_terminators\n        queue = self._queue\n        buf = \"\"\n        while True:\n            try:\n                c = fd.read(1).decode(encoding)\n            except UnicodeDecodeError as e:\n                log.warning(\"Decoding error from {!r}: {!r}\", self._cmdargs, e)\n                if self._suppress_decoding_errors:\n                    continue\n                else:\n                    raise\n            if not c:\n                return\n            buf += c\n            for t in line_terminators:\n                try:\n                    t_idx = buf.index(t) + len(t)  \n                    fragment = buf[:t_idx]\n                    buf = buf[t_idx:]\n                    queue.put(fragment)\n                except ValueError:\n                    pass", "label": 1}
{"index": "gp088117", "code": "def feature_extraction(param_0, param_1):\n        assert type(param_1) is list\n        var_0 = []\n        for var_1 in param_1:\n            var_2 = var_1(param_0)\n            assert len(var_2) == algorithm.get_dimension(),                \"Expected %i features from algorithm %s, got %i features\" %                (algorithm.get_dimension(), str(var_1), len(var_2))\n            var_0 += var_2\n        return var_0", "contrast": "def get_features():\n    features = ['feature1', 'feature2', 'feature3']\n    return features", "label": 0}
{"index": "gp287423", "code": "from dataclasses import dataclass\n@dataclass\nclass CharSetCodeField:\n    name: str\ndef create_char_set_code_field(param_0: str) -> CharSetCodeField:\n    return CharSetCodeField(param_0)", "contrast": "def char_code(columns, name=None):\n    if name is None:\n        name = 'Char Code Field (' + str(columns) + ' columns)'\n    if columns <= 0:\n        raise BaseException()\n    char_sets = None\n    for char_set in _tables.get_data('character_set'):\n        regex = '[ ]{' + str(15 - len(char_set)) + '}' + char_set\n        if char_sets is None:\n            char_sets = regex\n        else:\n            char_sets += '|' + regex\n    _character_sets = pp.Regex(char_sets)\n    _unicode_1_16b = pp.Regex('U\\+0[0-8,A-F]{3}[ ]{' + str(columns - 6) + '}')\n    _unicode_2_21b = pp.Regex('U\\+0[0-8,A-F]{4}[ ]{' + str(columns - 7) + '}')\n    char_code_field = (_character_sets | _unicode_1_16b | _unicode_2_21b)\n    char_code_field = char_code_field.setParseAction(lambda s: s[0].strip())\n    char_code_field.setName(name)\n    return char_code_field", "label": 1}
{"index": "gp313450", "code": "def copy_collection(param_0, param_1, param_2=None):\n    if param_2:\n        redis_conn.rename(param_1, param_2)\n        param_1 = param_2\n    redis_conn.duplicate(param_1)\n    return param_0", "contrast": "def copy(self, key=None):\n        other = self.__class__(\n            self.__iter__(),\n            self.maxlen,\n            redis=self.redis,\n            key=key,\n            writeback=self.writeback,\n        )\n        return other", "label": 1}
{"index": "gp258756", "code": "def setup_visualization():\n    plt.rcParams['figure.figsize'] = (10, 6)\n    plt.rcParams['font.size'] = 12\n    plt.rcParams['axes.labelsize'] = 14\n    plt.rcParams['axes.titlesize'] = 16\n    plt.rcParams['xtick.labelsize'] = 12\n    plt.rcParams['ytick.labelsize'] = 12\n    plt.rcParams['legend.fontsize'] = 12", "contrast": "def standard_settings(self):\n        cmd.set('bg_rgb', [1.0, 1.0, 1.0])  \n        cmd.set('depth_cue', 0)  \n        cmd.set('cartoon_side_chain_helper', 1)  \n        cmd.set('cartoon_fancy_helices', 1)  \n        cmd.set('transparency_mode', 1)  \n        cmd.set('dash_radius', 0.05)\n        self.set_custom_colorset()", "label": 1}
{"index": "gp306823", "code": "from pathlib import Path\ndef fix_path(param_0: str) -> Path:\n    var_0 = Path(param_0)\n    if path_obj.is_absolute() or path_obj.drive:\n        return var_0\n    else:\n        return Path(\".\" + str(param_0))", "contrast": "def stringify_with_dot_if_path(x):\n    if isinstance(x, PurePath):\n        return os.path.join('.', str(x))\n    return x", "label": 1}
{"index": "gp154181", "code": "def global_data_dir():\n    var_0 = False\n    if VIRTUALENV_VAR in os.environ:\n        var_0 = os.environ[VIRTUALENV_VAR]\n    elif CONDA_VAR in os.environ:\n        var_0 = sys.prefix\n    elif which('conda'):\n        var_0 = conda_prefix()\n    if var_0:\n        return make_path_posix(os.path.join(var_0, 'share', 'intake'))\n    else:\n        return appdirs.site_data_dir(appname='intake', appauthor='intake')", "contrast": "import os\ndef get_intake_catalog_dir():\n    return os.environ.get('INTAKE_CATALOG_DIR', '')", "label": 0}
{"index": "gp300250", "code": "def extract_file_info(param_0):\n    try:\n        var_0 = param_0[\"file_obj\"]\n    except KeyError:\n        var_0 = None\n    try:\n        var_1 = param_0[\"filename\"]\n    except KeyError:\n        var_1 = None\n    try:\n        var_2 = param_0[\"content_type\"]\n    except KeyError:\n        var_2 = None\n    return var_0, var_1, var_2", "contrast": "def fileinfo(fileobj, filename=None, content_type=None, existing=None):\n        return _FileInfo(fileobj, filename, content_type).get_info(existing)", "label": 1}
{"index": "gp003571", "code": "def items(param_0):\n        return ((var_0, var_1) for var_0, var_1 in zip(self.keys(), self.values()))", "contrast": "def get_items() -> Iterable[(Descriptor, value)]:\n    pass", "label": 0}
{"index": "gp284036", "code": "def add_command_with_delay(param_0, param_1, param_2):\n    var_0 = sequence.copy()\n    new_sequence.append((param_0, param_1))\n    return var_0", "contrast": "def append (self, cmd, delay=0.000, attrs=None):\n    self.lines.append( SeqCmd(cmd, delay, attrs) )", "label": 1}
{"index": "gp279138", "code": "from datetime import datetime\nimport pytz\ndef set_audit_timezone(param_0: str, param_1: dict) -> dict:\n    var_0 = param_1['audit_time']\n    var_1 = datetime.strptime(var_0, '%Y-%m-%d %H:%M:%S.%f')\n    if param_0 in pytz.all_timezones:\n        var_2 = pytz.timezone(param_0)\n    else:\n        try:\n            var_2 = pytz.timezone('Etc/GMT+' + str(int(param_0)))\n        except ValueError:\n            raise ValueError('Invalid timezone format')\n    var_1 = timezone.localize(var_1)\n    param_1['audit_time'] = audit_time.strftime('%Y-%m-%d %H:%M:%S.%f %Z%z')\n    return param_1", "contrast": "def timezone(self, tz):\n        self.data['resolving'].update(\n            timezone=tz,\n            time_show_zone=True)", "label": 1}
{"index": "gp097571", "code": "def get_pr_info(param_0, param_1, param_2):\n    var_0 = requester.get(\n        'https://api.github.com/repos/%s/pulls/%s' % (param_1, param_2))\n    return PRInfo(resp.json())", "contrast": "def get_pull_request_info(pull_request):\n    pr_info = PRInfo(\n        pr_num=pull_request.number,\n        title=pull_request.title,\n        author_login=pull_request.user.login,\n        created_at=pull_request.created_at,\n        updated_at=pull_request.updated_at,\n        merged_at=pull_request.merged_at,\n        base_branch=pull_request.base.ref,\n        head_branch=pull_request.head.ref,\n        state=pull_request.state,\n        additions=pull_request.additions,\n        deletions=pull_request.deletions,\n        changed_files=pull_request.changed_files\n    )\n    return pr_info", "label": 0}
{"index": "gp027662", "code": "def get_runtime_config():\n    from .._connect import main as _glconnect\n    var_0 = _glconnect.get_unity()\n    return unity.list_globals(True)", "contrast": "import turicreate as tc\ndef get_runtime_config_options():\n    return tc.config.get_runtime_config_options()", "label": 0}
{"index": "gp153835", "code": "def _fetch_features(param_0):\n        if self.feature_offset is None:\n            return\n        var_0 = '{}{}/{}?'.format(self.base_url, ServiceType.WFS.value, self.instance_id)\n        var_1 = {'SERVICE': ServiceType.WFS.value,\n                  'REQUEST': 'GetFeature',\n                  'TYPENAMES': DataSource.get_wfs_typename(self.data_source),\n                  'BBOX': str(self.bbox.reverse()) if self.bbox.crs is CRS.WGS84 else str(self.bbox),\n                  'OUTPUTFORMAT': MimeType.get_string(MimeType.JSON),\n                  'SRSNAME': CRS.ogc_string(self.bbox.crs),\n                  'TIME': '{}/{}'.format(self.time_interval[0], self.time_interval[1]),\n                  'MAXCC': 100.0 * self.maxcc,\n                  'MAXFEATURES': SHConfig().max_wfs_records_per_query,\n                  'FEATURE_OFFSET': self.feature_offset}\n        var_2 = var_0 + urlencode(var_1)\n        LOGGER.debug(\"URL=%s\", var_2)\n        var_3 = get_json(var_2)\n        var_4 = self.data_source.is_sentinel1()\n        for var_5 in var_3[\"features\"]:\n            if not var_4 or self._sentinel1_product_check(var_5['properties']['id'], self.data_source):\n                self.tile_list.append(var_5)\n        if len(var_3[\"features\"]) < SHConfig().max_wfs_records_per_query:\n            self.feature_offset = None\n        else:\n            self.feature_offset += SHConfig().max_wfs_records_per_query", "contrast": "def collect_data_from_wfs_service():\n    return {}", "label": 0}
{"index": "gp298365", "code": "def compute_activations(param_0, param_1, param_2):\n    var_0 = np.dot(param_0, param_1) + param_2\n    var_1 = np.maximum(var_0, 0)\n    return var_1", "contrast": "def _compute_hidden_activations(self, X):\n        self._compute_input_activations(X)\n        acts = self.input_activations_\n        if (callable(self.activation_func)):\n            args_dict = self.activation_args if (self.activation_args) else {}\n            X_new = self.activation_func(acts, **args_dict)\n        else:\n            func_name = self.activation_func\n            func = self._internal_activation_funcs[func_name]\n            X_new = func(acts, **self._extra_args)\n        return X_new", "label": 1}
{"index": "gp330911", "code": "def add_namespace_to_graph(param_0, param_1):\n    graph.add_namespace(param_0)\n    return param_1", "contrast": "def add_namespace_to_graph(self, graph: BELGraph) -> Namespace:\n        namespace = self.upload_bel_namespace()\n        graph.namespace_url[namespace.keyword] = namespace.url\n        self._add_annotation_to_graph(graph)\n        return namespace", "label": 1}
{"index": "gp333914", "code": "import xml.etree.ElementTree as ET\ndef read_block_as_xml(param_0):\n    var_0 = ET.fromstring(param_0)\n    return var_0", "contrast": "def _readxml(self):\n        block = re.sub(r'<(/?)s>', r'&lt;\\1s&gt;', self._readblock())\n        try:\n            xml = XML(block)\n        except ParseError:\n            xml = None\n        return xml", "label": 1}
{"index": "gp162611", "code": "def launch_help(param_0, param_1, param_2):\n        var_0 = config[\"help_window_position\"]\n        var_1 = config[\"help_window_size\"]\n        self.help_window = wx.Frame(self.main_window, -1,\n                                    param_1, var_0, var_1)\n        self.help_htmlwindow = wx.html.HtmlWindow(self.help_window, -1,\n                                                  (0, 0), var_1)\n        self.help_window.Bind(wx.EVT_MOVE, self.OnHelpMove)\n        self.help_window.Bind(wx.EVT_SIZE, self.OnHelpSize)\n        self.help_htmlwindow.Bind(wx.EVT_RIGHT_DOWN, self.OnHelpBack)\n        self.help_htmlwindow.Bind(wx.html.EVT_HTML_LINK_CLICKED,\n                                  lambda e: self.open_external_links(e))\n        self.help_htmlwindow.Bind(wx.EVT_MOUSEWHEEL,\n                                  lambda e: self.zoom_html(e))\n        var_2 = os.getcwd()\n        os.chdir(get_help_path())\n        try:\n            if os.path.isfile(param_2):\n                self.help_htmlwindow.LoadFile(param_2)\n            else:\n                self.help_htmlwindow.LoadPage(param_2)\n        except IOError:\n            self.help_htmlwindow.LoadPage(param_2)\n        self.help_window.Show()\n        os.chdir(var_2)", "contrast": "import webbrowser\ndef launch_help(filename):\n    webbrowser.open(filename, new=2)", "label": 0}
{"index": "gp069561", "code": "def interleave_skip(param_0, param_1=None):\n    var_0 = map(iter, param_0)\n    while var_0:\n        for var_1, var_2 in enumerate(var_0):\n            try:\n                yield next(var_2)\n            except StopIteration:\n                del var_0[var_1]", "contrast": "from itertools import chain, izip_longest\ndef interleave_skip(iterables, obj=None):\n    return chain.from_iterable(izip_longest(*iterables, fillvalue=obj))", "label": 0}
{"index": "gp209720", "code": "def create_filename(param_0: str, param_1: str, param_2: int) -> str:\n    return f\"{param_0}_{param_2}dpi.{param_1}\"", "contrast": "def build_filename(filename, filetype='png', resolution=300):\n    filevals = os.path.splitext(filename)\n    if filevals[1]:\n        filetype = filevals[1][1:]\n    if not filetype:\n        filetype = 'png'\n    filename = filevals[0] + '.' + filetype\n    if not resolution:\n        resolution = 300\n    return filename, filetype, resolution", "label": 1}
{"index": "gp179681", "code": "def show_view(param_0):\n    from com.dtmilano.android.viewclient import View\n    import time\n    var_0 = view.getClass()\n    var_1 = view.getUniqueId()\n    var_2 = view.getContentDescription()\n    var_3 = view.getText()\n    var_4 = view.getId()\n    if var_3:\n        var_5 = f\"{var_0} | id: {var_4} | text: '{var_3}' | unique_id: {var_1}\"\n    else:\n        var_5 = f\"{var_0} | id: {var_4} | unique_id: {var_1}\"\n    print(var_5)\n    var_6 = f\"view_screenshot_{var_1}.png\"\n    view.writeImageToFile(var_6, \"PNG\")\n    if var_2:\n        return f\"{var_5} | content description: '{var_2}'\"\n    else:\n        return var_5", "contrast": "def traverseShowClassIdTextContentDescriptionAndScreenshot(view):\n        return ViewClient.traverseShowClassIdAndText(view, View.getContentDescription, 'NAF', extraAction=ViewClient.writeViewImageToFileInDir)", "label": 1}
{"index": "gp177970", "code": "def create_new_token_instance(param_0):\n    return client.tokens.create(param_0=param_0)", "contrast": "def create(self, ttl=values.unset):\n        data = values.of({'Ttl': ttl, })\n        payload = self._version.create(\n            'POST',\n            self._uri,\n            data=data,\n        )\n        return TokenInstance(self._version, payload, account_sid=self._solution['account_sid'], )", "label": 1}
{"index": "gp310941", "code": "import ipaddress\ndef cidr2block(param_0: str) -> tuple:\n    try:\n        var_0 = ipaddress.ip_network(param_0)\n        return str(network.network_address), str(network.broadcast_address)\n    except ValueError:\n        return None", "contrast": "def cidr2block(cidr):\n    if not validate_cidr(cidr):\n        return None\n    ip, prefix = cidr.split('/')\n    prefix = int(prefix)\n    ip = ip2long(ip)\n    shift = 128 - prefix\n    block_start = ip >> shift << shift\n    mask = (1 << shift) - 1\n    block_end = block_start | mask\n    return (long2ip(block_start), long2ip(block_end))", "label": 1}
{"index": "gp130970", "code": "def register_command(param_0, param_1, param_2):\n        if param_1 in self.commands:\n            raise RuntimeError('%s is already defined' % param_1)\n        self.commands[param_1] = param_2", "contrast": "command_registry = {}\ndef register_command(name, command):\n    if name in command_registry:\n        raise RuntimeError(\"A command with this name has already been registered.\")\n    command_registry[name] = command", "label": 0}
{"index": "gp248806", "code": "from typing import Dict, Union\nimport numpy as np\ndef check_output_types(param_0: callable) -> bool:\n    var_0 = 0  \n    var_1 = 0.0  \n    var_2 = param_0(var_0, var_1)\n    if isinstance(var_2, dict):\n        for var_3 in var_2:\n            if not isinstance(var_3, (int, tuple)) or not isinstance(var_2[var_3], float):\n                return False\n        return True\n    elif isinstance(var_2, tuple) and len(var_2) == 2:\n        if isinstance(var_2[0], np.ndarray) and isinstance(var_2[1], np.ndarray):\n            if var_2[0].dtype == np.int and var_2[1].dtype == np.float64:\n                return True\n    return False", "contrast": "def checkTransitionType(self,state):\n        test = self.transition(state)\n        assert isinstance(test,(dict,tuple)), \"Transition function does not return a dict or tuple\"\n        if isinstance(test,dict):\n            assert all(isinstance(states, (int,tuple)) for states in test.keys()), \"Transition function returns a dict, but states are not represented as tuples or integers\"\n            assert all(isinstance(rates, float) for rates in test.values()), \"Transition function returns a dict, but the rates should be floats.\"\n            usesNumpy=False\n        if isinstance(test,tuple):\n            assert len(test)==2, \"The transition function should return two variables: states and rates.\"\n            states,rates = test\n            assert isinstance(states, np.ndarray) and states.ndim==2 and issubclass(states.dtype.type, np.integer), \"The states returned by the transition function need to be an integer 2d numpy array: %r\" %states\n            assert isinstance(rates, np.ndarray) and rates.ndim==1, \"The rates returned by the transition function need to be a 1d numpy array: %r\" % rates\n            usesNumpy = True\n        return usesNumpy", "label": 1}
{"index": "gp123391", "code": "def attrs(param_0):\n        var_0 = dict(self.__dict__) \n        del var_0[\"_matches\"] \n        if self.type != c.COMPUTER: \n            del var_0[\"difficulty\"]\n        return var_0", "contrast": "def get_player_attributes(player):\n    return {\n        'name': player.name,\n        'age': player.age,\n        'height': player.height,\n        'weight': player.weight,\n        'team': player.team,\n        'position': player.position,\n        'number': player.number,\n        'stats': player.stats,\n    }", "label": 0}
{"index": "gp018331", "code": "def link_markdown_cells(param_0, param_1):\n    for var_0, var_1 in enumerate(param_0):\n        if var_1['cell_type'] == 'markdown':\n            var_1['source'] = link_docstring(param_1, var_1['source'])", "contrast": "def create_doc_links(md_text):\n    import re\n    links_pattern = r'`([a-zA-Z0-9._/\\-]+)`'\n    md_text = re.sub(links_pattern, r'[`\\1`](https://docs.example.com/\\1)', md_text)\n    return md_text", "label": 0}
{"index": "gp261355", "code": "def subset_train_data(param_0, param_1, param_2=True):\n    var_0, var_1 = param_0[0], param_0[1]\n    if isinstance(param_0[0], list) or isinstance(param_0[0], np.ndarray):\n        var_2 = var_0[param_1]\n        var_3 = var_1[param_1]\n    elif isinstance(param_0[0], tuple) or isinstance(param_0[0], np.ndarray):\n        var_2 = var_0[param_1, :]\n        var_3 = var_1[param_1]\n    elif isinstance(param_0[0], dict):\n        var_2 = {var_4: var_0[var_4][param_1] for var_4 in x.keys()}\n        var_3 = var_1[param_1]\n    if param_2:\n        return (var_2, var_3) + param_0[2:]\n    else:\n        return var_2, var_3", "contrast": "def subset(train, idx, keep_other=True):\n    test_len(train)\n    y = train[1][idx]\n    if isinstance(train[0], (list, tuple)):\n        x = [x[idx] for x in train[0]]\n    elif isinstance(train[0], dict):\n        x = {k: v[idx] for k, v in train[0].items()}\n    elif isinstance(train[0], np.ndarray):\n        x = train[0][idx]\n    else:\n        raise ValueError(\"Input can only be of type: list, dict or np.ndarray\")\n    if keep_other:\n        return (x, y) + train[2:]\n    else:\n        return (x, y)", "label": 1}
{"index": "gp233779", "code": "def set_condition(param_0: str):\n    var_0 = param_0\n    return var_0", "contrast": "def set_condition(self, value):\n        if value is None or not isinstance(value, str):\n            raise TypeError(\"Condition is required and must be set to a String\")\n        else:\n            self.__condition = value", "label": 1}
{"index": "gp083206", "code": "async def create_connection(\n    param_0,\n    param_1,\n    *,\n    param_2=None,\n    param_3=True,\n    param_4=None,\n    **param_5,\n):\n    param_2 = param_2 or asyncio.get_event_loop()\n    param_3 = True if param_1 == 443 else param_3\n    var_0 = HTTP2ClientConnection(param_0, param_2=param_2, param_3=param_3)\n    if not isinstance(param_4, SSLContext):\n        param_4 = default_ssl_context()\n    await loop.create_connection(\n        lambda: var_0,\n        param_0=param_0,\n        param_1=param_1,\n        ssl=param_4,\n    )\n    return var_0", "contrast": "import http2\nfrom http.client import HTTPConnection\ndef open_http2_connection(host, port):\n    conn = HTTPConnection(host, port)\n    conn.sock = http2.HTTP2Connection(host, port)\n    return conn", "label": 0}
{"index": "gp032593", "code": "def get_linode(param_0=None, param_1=None):\n    if param_1 == 'action':\n        raise SaltCloudSystemExit(\n            'The get_linode function must be called with -f or --function.'\n        )\n    if param_0 is None:\n        param_0 = {}\n    var_0 = kwargs.get('name', None)\n    var_1 = kwargs.get('linode_id', None)\n    if var_0 is None and var_1 is None:\n        raise SaltCloudSystemExit(\n            'The get_linode function requires either a \\'name\\' or a \\'linode_id\\'.'\n        )\n    if var_1 is None:\n        var_1 = get_linode_id_from_name(var_0)\n    var_2 = _query('linode', 'list', args={'LinodeID': var_1})\n    return var_2['DATA'][0]", "contrast": "def get_linode(name=None, linode_id=None):\n    import requests\n    base_url = 'https://api.linode.com/v4'\n    headers = {\n        'Authorization': 'Bearer <api_token>',\n        'Content-Type': 'application/json'\n    }\n    if name is not None:\n        url = base_url + '/linode/instances?label={}'.format(name)\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            linode_id = response.json()['data'][0]['id']\n        else:\n            print('Error: {}'.format(response.json()['errors'][0]['reason']))\n            return None\n    url = base_url + '/linode/instances/{}'.format(linode_id)\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print('Error: {}'.format(response.json()['errors'][0]['reason']))\n        return None", "label": 0}
{"index": "gp017027", "code": "def sortlevel(param_0, param_1=None, param_2=True, param_3=None):\n        return self.sort_values(return_indexer=True, param_2=param_2)", "contrast": "def sort_index(ascending=True, level=None, sort_remaining=True):\n    return Index(sorted(self.values, reverse=(not ascending)))", "label": 0}
{"index": "gp208157", "code": "import sentry_sdk\ndef send_message_to_sentry(param_0):\n    sentry_sdk.capture_message(param_0)", "contrast": "def send_message(message, params, site, logger):\n    client.capture(\n        'Message',\n        message=message,\n        params=tuple(params),\n        data={\n            'site': site,\n            'logger': logger,\n        },\n    )", "label": 1}
{"index": "gp199602", "code": "def add_rule(param_0: str, param_1: str) -> dict:\n    var_0 = {\n        \"status\": \"success\",\n        \"message\": f\"Rule '{param_0}' with tag '{param_1}' added successfully\"\n    }\n    return var_0", "contrast": "def add_rule(self, value, tag):\n        resp = requests.post(url=self.REQUEST_URL.format(**self._params),\n                             json={'rule': {'value': value, 'tag': tag}})\n        return resp.json()", "label": 1}
{"index": "gp262310", "code": "def describe_region(param_0):\n    import pycountry\n    var_0 = language_tag.split('-')[1].upper()\n    try:\n        var_1 = pycountry.countries.get(alpha_2=var_0).name\n    except:\n        var_1 = ''\n    return var_1", "contrast": "def region_name(self, language=DEFAULT_LANGUAGE, min_score: int=75) -> str:\n        return self._get_name('region', language, min_score)", "label": 1}
{"index": "gp134494", "code": "def stream_array(param_0, param_1):\n        def chunkify(param_0):\n            log.debug('Data Stream STARTED')\n            yield '['.encode()\n            try:\n                yield jsonify(next(param_0)).encode()\n            except StopIteration:\n                pass\n            while True:\n                try:\n                    var_0 = next(param_0)\n                except StopIteration:\n                    yield ']'.encode()\n                    break\n                else:\n                    yield ',\\n'.encode()\n                    yield jsonify(var_0).encode()\n            log.debug('Data Stream ENDED')\n        return chunkify(param_1)", "contrast": "import json\ndef stream_to_json_array(stream):\n    array = []\n    for data in stream:\n        json_data = json.loads(data)\n        array.append(json_data)\n    return array", "label": 0}
{"index": "gp005039", "code": "def write(param_0, param_1, param_2=enums.KMIPVersion.KMIP_1_0):\n        var_0 = utils.BytearrayStream()\n        if self._unique_identifier is not None:\n            self._unique_identifier.write(\n                var_0,\n                param_2=param_2\n            )\n        else:\n            raise ValueError(\n                \"The Rekey response payload is missing the unique identifier.\"\n            )\n        if self._template_attribute is not None:\n            self._template_attribute.write(\n                var_0,\n                param_2=param_2\n            )\n        self.length = local_stream.length()\n        super(RekeyResponsePayload, param_0).write(\n            param_1,\n            param_2=param_2\n        )\n        output_stream.write(local_stream.buffer)", "contrast": "def encode_rekey_request_payload(output_stream, kmip_version=KMIPVersion.KMIP_1_0):\n    if not output_stream:\n        raise ValueError(\"Invalid output stream\")", "label": 0}
{"index": "gp107461", "code": "def backup_db(param_0=None, param_1=5):\n    assert \"mysql_user\" in env, \"Missing mysqL_user in env\"\n    assert \"mysql_password\" in env, \"Missing mysql_password in env\"\n    assert \"mysql_host\" in env, \"Missing mysql_host in env\"\n    assert \"mysql_db\" in env, \"Missing mysql_db in env\"\n    if not param_0:\n        param_0 = paths.get_current_release_name()\n    var_0 = param_1+1\n    if not param_0:\n        return\n    env.run(\"mkdir -p %s\" % paths.get_backup_path(\"mysql\"))\n    var_1 = \"mysql/%s.sql.gz\" % param_0\n    var_2 = paths.get_backup_path(var_1)\n    env.run(\"mysqldump -u %s -p%s -h %s %s | gzip -c > %s\" %\n            (env.mysql_user, env.mysql_password, env.mysql_host, env.mysql_db,\n             var_2))\n    env.run(\"ls -dt %s/* | tail -n +%s | xargs rm -rf\" % (\n        paths.get_backup_path(\"mysql\"),\n        var_0)\n    )", "contrast": "def backup_database_release(database, release):\n    backup_filename = f\"{database}_{release}.bak\"\n    print(f\"Successfully backed up {database} database and associated it with release {release}.\")", "label": 0}
{"index": "gp150686", "code": "def _find_inaccessible_hdd_files(param_0):\n        var_0 = []\n        try:\n            var_1 = yield from self.execute(\"list\", [\"hdds\"])\n        except VirtualBoxError:\n            return var_0\n        var_2 = False\n        for var_3 in var_1:\n            try:\n                var_4, var_5 = prop.split(':', 1)\n            except ValueError:\n                continue\n            if name.strip() == \"State\" and value.strip() == \"inaccessible\":\n                var_2 = True\n            if var_2 and name.strip() == \"Location\":\n                hdds.append(value.strip())\n                var_2 = False\n        return reversed(var_0)", "contrast": "import os\ndef find_inaccessible_files(dir_path):\n    inaccessible_files = []\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                with open(file_path) as f:\n                    pass\n            except (IOError, OSError):\n                inaccessible_files.append(file_path)\n    return inaccessible_files", "label": 0}
{"index": "gp196620", "code": "import pulseaudio\ndef retrieve_sink_and_request_update():\n    with pulseaudio.Pulse('mypulseapp') as var_0:\n        var_1 = pulse.default_sink\n        pulse.request_update(var_1)", "contrast": "def server_info_cb(self, context, server_info_p, userdata):\n        server_info = server_info_p.contents\n        self.request_update(context)", "label": 1}
{"index": "gp280184", "code": "import mapnik\ndef renderMapnik(param_0):\n    var_0 = mapnik.Renderer(\"image.png\")\n    var_1 = mapnik.Style()\n    var_2 = mapnik.Layer(\"grid\")\n    var_3 = mapnik.LineSymbolizer()\n    lineSymbolizer.stroke = mapnik.Stroke(mapnik.Color(\"#000000\"), grid.strokewidth)\n    var_4 = mapnik.Rule()\n    mapStyleRule.symbols.append(var_3)\n    mapStylesheet.rules.append(var_4)\n    mapLayer.datasource = mapnik.GeoJSON(file=grid.geojsonfile)\n    mapLayer.styles.append(\"grid\")\n    var_5 = mapnik.Map(grid.imgwidth, grid.imgheight)\n    mapnikMap.append_style(\"grid\", var_1)\n    mapnikMap.layers.append(var_2)\n    mapnikMap.zoom_all()\n    mapnik.render_to_file(var_5, \"image.png\", \"png\")", "contrast": "def render_grid(self, bbox, grid_fields, layer, width=None, height=None):\n        width = width or self.tilesize\n        height = height or self.tilesize\n        self._prepare_rendering(bbox, width=width, height=height)\n        grid = mapnik.Grid(width, height)\n        mapnik.render_layer(self._mapnik, grid, layer=layer, fields=grid_fields)\n        grid = grid.encode()\n        return json.dumps(grid)", "label": 1}
{"index": "gp084854", "code": "def inter_spin_hamiltonian(param_0, param_1, param_2):\n        param_2 *= param_1\n        var_0  = (param_1 - 2*param_2)/2.*self.oper['sumSz2']\n        var_0 += param_2*self.oper['sumSz-sp2']\n        var_0 -= param_2/2.*self.oper['sumSz-or2']\n        var_0 -= param_2*self.oper['Sfliphop']\n        return var_0", "contrast": "def interaction_hamiltonian(coulomb_interaction, hund_coupling):\n    return coulomb_interaction * hund_coupling", "label": 0}
{"index": "gp102035", "code": "def get_details(param_0, param_1, param_2=False):\n        var_0 = RestJob(\n            process_name=node.process_name,\n            timeperiod=node.timeperiod,\n            time_qualifier=node.time_qualifier,\n            number_of_children=len(node.children),\n            number_of_failures='NA' if not node.job_record else node.job_record.number_of_failures,\n            state='NA' if not node.job_record else node.job_record.state,\n            event_log=[] if not node.job_record else node.job_record.event_log)\n        if param_2:\n            return var_0\n        else:\n            return rest_job.document", "contrast": "def get_rest_job_or_document(as_model: bool, id: str):\n    if as_model:\n        return RestJob(id=id, ...other_params)\n    else:\n        return document", "label": 0}
{"index": "gp190053", "code": "def bsh_formula(param_0: int, param_1: int, param_2: int) -> str:\n    return f\"BSH R{param_0}, R{param_1}, #{param_2}\"", "contrast": "def _translate_bsh(self, oprnd1, oprnd2, oprnd3):\n        assert oprnd1.size and oprnd2.size and oprnd3.size\n        assert oprnd1.size == oprnd2.size\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op2_var = self._translate_src_oprnd(oprnd2)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n        if oprnd3.size > oprnd1.size:\n            op1_var_zx = smtfunction.zero_extend(op1_var, oprnd3.size)\n            op2_var_zx = smtfunction.zero_extend(op2_var, oprnd3.size)\n            op2_var_neg_sx = smtfunction.sign_extend(-op2_var, oprnd3.size)\n            shr = smtfunction.extract(op1_var_zx >> op2_var_neg_sx, 0, op3_var.size)\n            shl = smtfunction.extract(op1_var_zx << op2_var_zx, 0, op3_var.size)\n        elif oprnd3.size < oprnd1.size:\n            shr = smtfunction.extract(op1_var >> -op2_var, 0, op3_var.size)\n            shl = smtfunction.extract(op1_var << op2_var, 0, op3_var.size)\n        else:\n            shr = op1_var >> -op2_var\n            shl = op1_var << op2_var\n        result = smtfunction.ite(oprnd3.size, op2_var >= 0, shl, shr)\n        return [op3_var == result] + op3_var_constrs", "label": 1}
{"index": "gp016722", "code": "def last(param_0, param_1=False):\n    var_0 = SparkContext._active_spark_context\n    var_1 = sc._jvm.functions.last(_to_java_column(param_0), param_1)\n    return Column(var_1)", "contrast": "from typing import List, Any\ndef last_value(group: List[Any], ignore_nulls: bool = False) -> Any:\n    values = [val for val in group if val is not None]\n    if values:\n        return values[-1]\n    elif ignore_nulls:\n        return None\n    else:\n        return group[-1]", "label": 0}
{"index": "gp131640", "code": "def get_arg_value_as_type(param_0, param_1, param_2=None, param_3=False):\n        var_0 = self.get_query_argument(param_1, param_2)\n        if isinstance(var_0, int):\n            return var_0\n        if val.lower() in ['true', 'yes']:\n            return True\n        if val.lower() in ['false', 'no']:\n            return False\n        return var_0", "contrast": "def convert_truthy(val):\n    if val.lower() in ['true', 'yes']:\n        return True\n    elif val.lower() in ['false', 'no']:\n        return False\n    else:\n        return val", "label": 0}
{"index": "gp196149", "code": "import os\ndef joinpath(param_0, param_1):\n    if os.path.isabs(param_1):\n        return param_1\n    return os.path.join(param_0 or '', param_1)", "contrast": "def _badpath(path, base):\n    return not _resolved(os.path.join(base, path)).startswith(base)", "label": 1}
{"index": "gp145237", "code": "def _format_map_output(param_0,\n                                  param_1, param_2,\n                                  param_3, param_4,\n                                  param_5,\n                                  param_6, param_7\n                                  ):\n        param_1 = parse_result_format(param_1)\n        var_0 = {\n            'success': param_2\n        }\n        if param_1['result_format'] == 'BOOLEAN_ONLY':\n            return var_0\n        var_1 = param_3 - param_4\n        if param_3 > 0:\n            var_2 = param_5 / param_3\n            var_3 = var_1 / param_3\n            if param_4 > 0:\n                var_4 = param_5 / param_4\n            else:\n                var_4 = None\n        else:\n            var_3 = None\n            var_2 = None\n            var_4 = None\n        var_0['result'] = {\n            'element_count': param_3,\n            'missing_count': var_1,\n            'missing_percent': var_3,\n            'unexpected_count': param_5,\n            'unexpected_percent': var_2,\n            'unexpected_percent_nonmissing': var_4,\n            'partial_unexpected_list': param_6[:param_1['partial_unexpected_count']]\n        }\n        if param_1['result_format'] == 'BASIC':\n            return var_0\n        if 0 < result_format.get('partial_unexpected_count'):\n            try:\n                var_5 = [\n                {'value': var_6, 'count': var_7}\n                for var_6, var_7\n                in sorted(\n                    Counter(param_6).most_common(param_1['partial_unexpected_count']),\n                    var_6=lambda x: (-x[1], x[0]))\n                ]\n            except TypeError:\n                var_5 = [\n                    'partial_exception_counts requires a hashable type']\n            finally:\n                var_0['result'].update(\n                    {\n                        'partial_unexpected_index_list': param_7[:param_1[\n                            'partial_unexpected_count']] if param_7 is not None else None,\n                        'partial_unexpected_counts': var_5\n                    }\n                )\n        if param_1['result_format'] == 'SUMMARY':\n            return var_0\n        var_0['result'].update(\n            {\n                'unexpected_list': param_6,\n                'unexpected_index_list': param_7\n            }\n        )\n        if param_1['result_format'] == 'COMPLETE':\n            return var_0\n        raise ValueError(\"Unknown result_format %s.\" %\n                         (param_1['result_format'],))", "contrast": "from typing import Dict, Any\nfrom great_expectations.execution_engine import ExecutionEngine\ndef map_expectation_over_column(\n    execution_engine: ExecutionEngine,\n    func: Any,\n    column: str,\n    result_format: str,\n    *args,\n    **kwargs,\n) -> Dict[str, Any]:\n    expectation_args = {\n        \"column\": column,\n        \"result_format\": result_format,\n    }\n    return func(execution_engine, *args, **expectation_args, **kwargs)", "label": 0}
{"index": "gp279206", "code": "from smc.elements.servers import ManagementServer\ndef get_contact_addresses():\n    var_0 = ManagementServer.objects.first()\n    return mgt_server.contact_addresses", "contrast": "def contact_addresses(self):\n        return MultiContactAddress(\n            href=self.get_relation('contact_addresses'),\n            type=self.typeof,\n            name=self.name)", "label": 1}
{"index": "gp179533", "code": "def register_artifact_definition(param_0):\n    if artifact_definition.name.lower() in artifact_definitions:\n        raise KeyError(\"Artifact definition for '{}' is already set.\".format(artifact_definition.name.lower()))\n    artifact_definitions[artifact_definition.name.lower()] = param_0", "contrast": "def RegisterDefinition(self, artifact_definition):\n    artifact_definition_name = artifact_definition.name.lower()\n    if artifact_definition_name in self._artifact_definitions:\n      raise KeyError(\n          'Artifact definition already set for name: {0:s}.'.format(\n              artifact_definition.name))\n    self._artifact_definitions[artifact_definition_name] = artifact_definition\n    self._defined_artifact_names.add(artifact_definition.name)\n    for source in artifact_definition.sources:\n      if source.type_indicator == definitions.TYPE_INDICATOR_ARTIFACT_GROUP:\n        self._artifact_name_references.update(source.names)", "label": 1}
{"index": "gp273319", "code": "def dispatch_put_to_server(param_0: str, param_1: str, param_2) -> None:\n    pass ", "contrast": "def send_put(self, mri, attribute_name, value):\n        path = attribute_name + \".value\"\n        typ, value = convert_to_type_tuple_value(serialize_object(value))\n        if isinstance(typ, tuple):\n            _, typeid, fields = typ\n            value = Value(Type(fields, typeid), value)\n        try:\n            self._ctxt.put(mri, {path: value}, path)\n        except RemoteError:\n            if attribute_name == \"exports\":\n                self._queues[mri].get(timeout=DEFAULT_TIMEOUT)\n            else:\n                raise", "label": 1}
{"index": "gp220263", "code": "def get_post_form_kwargs(**param_0):\n    return param_0", "contrast": "def get_post_form_kwargs(self):\n        kwargs = {\n            'user': self.request.user,\n            'forum': self.get_forum(),\n            'topic': self.get_topic(),\n        }\n        post = self.get_post()\n        if post:\n            kwargs.update({'instance': post})\n        if self.request.method in ('POST', 'PUT'):\n            kwargs.update({\n                'data': self.request.POST,\n                'files': self.request.FILES,\n            })\n        return kwargs", "label": 1}
{"index": "gp274295", "code": "import subprocess\ndef is_root_user_enabled():\n    var_0 = subprocess.run(['sudo', '-n', 'true'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    return result.returncode == 0", "contrast": "def root_user_status(self):\n        uri = \"/instances/%s/root\" % self.id\n        resp, body = self.manager.api.method_get(uri)\n        return body[\"rootEnabled\"]", "label": 1}
{"index": "gp295961", "code": "from typing import Optional\nfrom sqlalchemy import Table, MetaData, Column, sql\nfrom sqlalchemy.engine.base import Engine\nfrom sqlalchemy.engine.reflection import Inspector\nclass SqlaColumnInspectionInfo:\n    def __init__(param_0, param_1: str, param_2: str, param_3: bool):\n        self.name = param_1\n        self.type = param_2\n        self.nullable = param_3\ndef get_column_info(param_0: Engine, param_1: str, param_2: str) -> Optional[SqlaColumnInspectionInfo]:\n    var_0 = MetaData()\n    metadata.bind = param_0\n    var_1 = Table(param_1, var_0, autoload=True, autoload_with=param_0)\n    var_2 = Inspector.from_engine(param_0)\n    if param_2 in inspector.get_columns(param_1):\n        var_3 = Column(param_2, type_=sql.NullType)\n        var_4 = table.c[param_2]\n        var_5 = column_info.nullable\n        var_6 = column_info.type.__repr__().split(\".\")[-1].split(\"(\")[0]\n        return SqlaColumnInspectionInfo(param_2, var_6, var_5)\n    else:\n        return None", "contrast": "def get_column_info(engine: Engine, tablename: str,\n                    columnname: str) -> Optional[SqlaColumnInspectionInfo]:\n    for info in gen_columns_info(engine, tablename):\n        if info.name == columnname:\n            return info\n    return None", "label": 1}
{"index": "gp022920", "code": "def get_short_help_str(param_0, param_1=45):\n        return self.short_help or self.help and make_default_short_help(self.help, param_1) or ''", "contrast": "def get_short_help(help_string: str) -> str:\n    short_help = ''\n    words = help_string.split()\n    for word in words:\n        if len(short_help + word) > 20:\n            short_help += '...'\n            break\n        short_help += word + ' '\n    return short_help.strip()", "label": 0}
{"index": "gp302476", "code": "from functools import wraps\nfrom pyramid.httpexceptions import HTTPConflict, HTTPInternalServerError\ndef prevent_uri_resource_in_use(param_0):\n    @wraps(param_0)\n    def wrapper(param_0, *param_1, **param_2):\n        var_0 = request.registry\n        var_1 = registry.queryUtility(IReferencer)\n        try:\n            if referencer.is_uri_resource_in_use():\n                raise HTTPConflict()\n            else:\n                return param_0(param_0, *param_1, **param_2)\n        except Exception:\n            raise HTTPInternalServerError()\n    return wrapper", "contrast": "def protected_operation_with_request(fn):\n    @functools.wraps(fn)\n    def wrapped(request, *args, **kwargs):\n        response = _advice(request)\n        if response is not None:\n            return response\n        else:\n            return fn(request, *args, **kwargs)\n    return wrapped", "label": 1}
{"index": "gp324848", "code": "def gas_viscosity(param_0, param_1, param_2):\n    var_0 = 0.0 \n    return var_0", "contrast": "def calculate_P(self, T, P, method):\n        if method == COOLPROP:\n            mu = PropsSI('V', 'T', T, 'P', P, self.CASRN)\n        elif method in self.tabular_data:\n            mu = self.interpolate_P(T, P, method)\n        return mu", "label": 1}
{"index": "gp151773", "code": "def setup_logging(param_0, param_1=False, param_2=None):\n    if param_2:\n        logging.basicConfig(filename=param_2, filemode='w',\n                            param_0=logging.DEBUG)\n    var_0 = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    var_1 = ColoredFormatter(\"%(levelname)s: %(message)s\", param_1)\n    ch.setFormatter(var_1)\n    var_2 = ('__main__', 'fusesoc',)\n    for var_3 in var_2:\n        var_4 = logging.getLogger(var_3)\n        logger.addHandler(var_0)\n        logger.setLevel(param_0)\n    var_5 = []\n    for var_3 in var_5:\n        var_4 = logging.getLogger(var_3)\n        logger.addHandler(var_0)\n        logger.setLevel(logging.WARNING)\n    logger.debug('Setup logging at level {}.'.format(param_0))", "contrast": "import logging\ndef setup_logging(log_file):\n    logging.basicConfig(filename=log_file,\n                        level=logging.INFO,\n                        format='%(asctime)s [%(levelname)s] %(message)s',\n                        datefmt='%Y-%m-%d %H:%M:%S')", "label": 0}
{"index": "gp112753", "code": "def _open(param_0):\n        try:\n            self.db = connect(\n                host=self.settings.host,\n                port=self.settings.port,\n                user=coalesce(self.settings.username, self.settings.user),\n                passwd=coalesce(self.settings.password, self.settings.passwd),\n                db=coalesce(self.settings.schema, self.settings.db),\n                read_timeout=coalesce(self.settings.read_timeout, (EXECUTE_TIMEOUT / 1000) - 10 if EXECUTE_TIMEOUT else None, 5*60),\n                charset=u\"utf8\",\n                use_unicode=True,\n                ssl=coalesce(self.settings.ssl, None),\n                cursorclass=cursors.SSCursor\n            )\n        except Exception as e:\n            if self.settings.host.find(\"://\") == -1:\n                Log.error(\n                    u\"Failure to connect to {{host}}:{{port}}\",\n                    host=self.settings.host,\n                    port=self.settings.port,\n                    cause=e\n                )\n            else:\n                Log.error(u\"Failure to connect.  PROTOCOL PREFIX IS PROBABLY BAD\", e)\n        self.cursor = None\n        self.partial_rollback = False\n        self.transaction_level = 0\n        self.backlog = []  \n        if self.readonly:\n            self.begin()", "contrast": "def do_not_use():\n    raise Exception(\"DO NOT USE THIS UNLESS YOU close() FIRST\")", "label": 0}
{"index": "gp192065", "code": "def evaluate_bezier_curve(param_0, param_1):\n    var_0, var_1, var_2, var_3 = param_0\n    var_4 = (1-param_1)**3*var_0[0] + 3*param_1*(1-param_1)**2*var_1[0] + 3*param_1**2*(1-param_1)*var_2[0] + param_1**3*var_3[0]\n    var_5 = (1-param_1)**3*var_0[1] + 3*param_1*(1-param_1)**2*var_1[1] + 3*param_1**2*(1-param_1)*var_2[1] + param_1**3*var_3[1]\n    return (var_4, var_5)", "contrast": "def point(self, t):\n        distance = self.end - self.start\n        return self.start + distance*t", "label": 1}
{"index": "gp070347", "code": "def capability(self, cap_name):\n  if cap_name in self.__class_capabilities__:\n\t\t\tfunction_name = self.__class_capabilities__[cap_name]\n   return getattr(self, function_name)", "contrast": "def get_capability(cap_name):\n    return getattr(self, cap_name, None)", "label": 0}
{"index": "gp130308", "code": "def create_logger(param_0, param_1={}):\n        var_0 = logging.getLogger(\"SmartFileSorter\")\n        logger.level = logging.INFO\n        if '--debug' in param_1 and param_1['--debug'] is True:\n            logger.setLevel(logging.DEBUG)\n        var_1 = logging.Formatter('%(asctime)s %(name)s %(levelname)s %(message)s',\n                                               '%Y-%m-%d %H:%M:%S')\n        var_2 = logging.Formatter('%(message)s')\n        var_3 = logging.StreamHandler(stream=sys.stdout)\n        stdout_stream.setFormatter(var_2)\n        logger.addHandler(var_3)\n        if '--log' in param_1 and param_1['--log'] is not None:\n            var_4 = open(param_1['--log'], 'w')\n            var_5 = logging.StreamHandler(stream=var_4)\n            logfile_stream.setFormatter(var_1)\n            logger.addHandler(var_5)\n        if '--dry-run' in param_1 and param_1['--dry-run'] is True:\n            logger.info('Running with --dry-run parameter. Actions will not be performed.')\n        self.logger = var_0", "contrast": "import logging\ndef configure_logger(args):\n    logger = logging.getLogger()\n    if args.debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)", "label": 0}
{"index": "gp266480", "code": "def guess_column_ends(param_0):\n    var_0 = abs(np.diff(param_0))\n    var_1 = 1.5 * np.median(var_0)\n    var_2, var_3 = find_peaks(var_0, height=var_1)\n    return var_2", "contrast": "def _get_column_ends(self):\n        ends = collections.Counter()\n        for line in self.text.splitlines():\n            for matchobj in re.finditer('\\s{2,}', line.lstrip()):\n                ends[matchobj.end()] += 1\n        return ends", "label": 1}
{"index": "gp190709", "code": "def print_errors(param_0, param_1):\n    for var_0, (var_1, var_2) in enumerate(zip(param_0, param_1)):\n        print(f\"Iteration {var_0}: Test Error: {var_1}, Noise Error: {var_2}\")", "contrast": "def learningCurve(expPath, suite):\n  print(\"\\nLEARNING CURVE ================\",expPath,\"=====================\")\n  try:\n    headers=[\"testerror\",\"totalCorrect\",\"elapsedTime\",\"entropy\"]\n    result = suite.get_value(expPath, 0, headers, \"all\")\n    info = []\n    for i,v in enumerate(zip(result[\"testerror\"],result[\"totalCorrect\"],\n                             result[\"elapsedTime\"],result[\"entropy\"])):\n      info.append([i, v[0], v[1], int(v[2]), v[3]])\n    headers.insert(0,\"iteration\")\n    print(tabulate(info, headers=headers, tablefmt=\"grid\"))\n  except:\n    print(\"Couldn't load experiment\",expPath)", "label": 1}
{"index": "gp158396", "code": "def dict_to_attributes_code(param_0):\n    var_0 = []\n    for var_1, var_2 in dict_.iteritems():\n        if isinstance(var_2, dict):\n            var_3 = dict_to_attributes_code(var_2)\n            var_4 = txt.split('\\n')\n            for var_5 in var_4:\n                if not line.startswith(' '):\n                    var_5 = \"%s.%s\" % (var_1, var_5)\n                lines.append(var_5)\n        else:\n            var_6 = pformat(var_2)\n            if '\\n' in var_6:\n                lines.append(\"%s = \\\\\" % var_1)\n                var_6 = indent(var_6)\n                lines.extend(value_txt.split('\\n'))\n            else:\n                var_5 = \"%s = %s\" % (var_1, var_6)\n                lines.append(var_5)\n    return '\\n'.join(var_0)", "contrast": "def dict_to_attributes_code(d):\n    code = \"\"\n    for key in d:\n        if isinstance(d[key], dict):\n            for sub_key in d[key]:\n                code += key + \".\" + sub_key + \" = \" + str(d[key][sub_key]) + \"\\n\"\n        else:\n            code += key + \" = \" + str(d[key]) + \"\\n\"\n    return code", "label": 0}
{"index": "gp218455", "code": "def create_soql_tree(param_0):\n    var_0 = {}\n    var_1 = []\n    var_2 = []\n    var_3 = soql_query.split()\n    for var_4, var_5 in enumerate(var_3):\n        if var_5 == \"SELECT\":\n            var_1 = var_3[var_4+1].split(\",\")\n        elif var_5 == \"FROM\":\n            var_6 = var_3[var_4+1]\n            if \"(\" in var_6:\n                var_7 = object_name.split(\"(\")[1].split(\")\")[0]\n                var_6 = object_name.split(\"(\")[0]\n                var_0[var_7] = {}\n                var_0[var_7][var_6] = {}\n            else:\n                var_0[var_6] = {}\n        elif var_5 == \"WHERE\":\n            var_2 = var_3[var_4+1:]\n    for var_8 in var_1:\n        if \".\" in var_8:\n            var_7 = field.split(\".\")[0]\n            var_9 = field.split(\".\")[1]\n            var_0[var_7][var_6][var_9] = {}\n    for var_8 in var_2:\n        if \".\" in var_8:\n            var_7 = field.split(\".\")[0]\n            var_9 = field.split(\".\")[1]\n            var_0[var_7][var_6][var_9] = {}\n    return var_0", "contrast": "def _from_sql(self, soql):\n        assert not self.soql, \"Don't use _from_sql method directly\"\n        self.soql = soql\n        soql, self.subqueries = split_subquery(soql)\n        match_parse = re.match(r'SELECT (.*) FROM (\\w+)\\b(.*)$', soql, re.I)\n        if not match_parse:\n            raise ProgrammingError('Invalid SQL: %s' % self.soql)\n        fields_sql, self.root_table, self.extra_soql = match_parse.groups()\n        fields = [x.strip() for x in fields_sql.split(',')]\n        self.is_aggregation = bool(pattern_groupby.search(self.extra_soql) or\n                                   pattern_aggregation.search(fields[0]))\n        self.is_plain_count = fields[0].upper() == 'COUNT()'\n        consumed_subqueries = 0\n        expr_alias_counter = 0\n        if not self.is_plain_count:\n            for field in fields:\n                if self.is_aggregation:\n                    match = re.search(r'\\b\\w+$', field)\n                    if match:\n                        alias = match.group()\n                        assert alias not in RESERVED_WORDS, \"invalid alias name\"\n                        if match.start() > 0 and field[match.start() - 1] == ' ':\n                            field = field[match.start() - 1]\n                    else:\n                        alias = 'expr{}'.format(expr_alias_counter)\n                        expr_alias_counter += 1\n                    assert '&' not in field, \"Subquery not expected as field in aggregation query\"\n                elif '&' in field:\n                    assert field == '(&)'  \n                    subquery = QQuery(self.subqueries[consumed_subqueries][0])\n                    consumed_subqueries += 1\n                    self.has_child_rel_field = True\n                    field = subquery\n                    alias = subquery.root_table\n                else:\n                    alias = field\n                    if '.' in alias:\n                        if alias.split('.', 1)[0].lower() == self.root_table.lower():\n                            alias = alias.split('.', 1)[1]\n                        if '.' in alias:\n                            subroots = self.subroots\n                            root_crumbs = alias.lower().split('.')[:-1]\n                            for scrumb in root_crumbs:\n                                subroots.setdefault(scrumb, {})\n                                subroots = subroots[scrumb]\n                self.aliases.append(alias)\n                self.fields.append(field)", "label": 1}
{"index": "gp024331", "code": "def _get_history_daily_window(param_0,\n                                  param_1,\n                                  param_2,\n                                  param_3,\n                                  param_4,\n                                  param_5):\n        var_0 = self.trading_calendar.minute_to_session_label(param_2)\n        var_1 = self._get_days_for_window(var_0, param_3)\n        if len(param_1) == 0:\n            return pd.DataFrame(None,\n                                index=var_1,\n                                columns=None)\n        var_2 = self._get_history_daily_window_data(\n            param_1, var_1, param_2, param_4, param_5\n        )\n        return pd.DataFrame(\n            var_2,\n            index=var_1,\n            columns=param_1\n        )", "contrast": "import pandas as pd\ndef get_history_bars(sids):\n    return pd.DataFrame() ", "label": 0}
{"index": "gp152565", "code": "def to_df(param_0, param_1: str = 'MEMORY', **param_2) -> 'pd.DataFrame':\n        var_0 = self._is_valid()\n        if var_0:\n            print(var_0['LOG'])\n            return None\n        else:\n            return self.sas.sasdata2dataframe(self.table, self.libref, self.dsopts, param_1, **param_2)", "contrast": "import pandas as pd\nimport saspy\ndef export_sas_to_pandas(dataset, method=\"MEMORY\", **kwargs):\n    if method == \"MEMORY\":\n        sas_dataset = saspy.SASdata(dataset)\n        return sas_dataset.to_df()\n    elif method == \"CSV\":\n        csv_file = dataset + \".csv\"\n        sas_dataset = saspy.SASdata(dataset)\n        sas_dataset.to_csv(csv_file)\n        return pd.read_csv(csv_file, **kwargs)\n    else:\n        raise ValueError(\"Invalid method. Choose either MEMORY or CSV.\")", "label": 0}
{"index": "gp071882", "code": "def calculate_normalized_ratios(param_0, param_1, param_2):\n    var_0 = []\n    for var_1 in param_0:\n        quant.update({var_2: str(var_1[var_2] / param_1[var_2])\n                      if var_1[var_2] != 'NA' else 'NA' for var_2 in param_2})\n        outratios.append(var_1)\n    return var_0", "contrast": "from typing import List\nimport numpy as np\ndef calculate_ratios(intensities: List[List[float]], min_intensity: float) -> np.ndarray:\n    intensities = np.array(intensities)\n    intensities[intensities < min_intensity] = np.nan\n    med_intensities = np.nanmedian(intensities, axis=1)\n    ratios = np.divide(intensities.T, med_intensities).T\n    return ratios", "label": 0}
{"index": "gp280112", "code": "def get_cluster_agents():\n    var_0 = []\n    return var_0", "contrast": "def get_agents():\n    agent_list = []\n    agents = __get_all_agents()\n    for agent in agents:\n        agent_list.append(agent[\"hostname\"])\n    return agent_list", "label": 1}
{"index": "gp070999", "code": "def enable(self, folder):\n\t\thost_name = self.base.options.ip\n  email = self.base.options.user\n  isHost(host_name)\n  self.__userExists(email)\n  data = self.getHostData(host_name)\n  website_dir = data['website_dir']\n  full_path = '%s/%s' % (website_dir, folder)\n  full_path_git = '%s/.git' % full_path\n  repository = '%s/%s/%s.git' % (self.data[email]['dir'], host_name, folder)\n  real_repository = '%s/%s.git' % (self.base.git['repositories'], host_name)\n  user_hook = '%s/hooks/post-receive' % repository\n  repo_hooks = '%s/.git/hooks' % website_dir\n  origin = md5(email)\n  if not fileExists(full_path):\n\t\t\terror_message('Folder %s not exists!' % full_path)\n  if not host_name in self.data[email]['projects']:\n\t\t\tself.data[email]['projects'][host_name] = []\n  if folder in self.data[email]['projects'][host_name]:\n\t\t\terror_message('Repository already exists for this user!')\n  self.data[email]['projects'][host_name].append(folder)\n  self.__makeDir(repository)\n  os.system('cd %s && git init --bare 1> /dev/null' % repository)\n  putFile(\n   '%s/.gitignore' % full_path,\n   getTemplate('git-jail-gitignore-default')\n  )\n  os.system(\n   'cd %(full_path)s && git init 1> /dev/null && '\n   'git remote add %(origin)s %(repository)s && '\n   'git add . && '\n   'git commit -am \"Initial commit\" 1> /dev/null && '\n   'git push %(origin)s master 1> /dev/null' % locals()\n  )\n  os.system('chown git:git -R %s' % full_path)\n  os.system('chown git:git -R %s' % repository)\n  os.system('chown git:git -R %s' % real_repository)\n  os.system('chown git:git -R %s/.git' % website_dir)\n  key = hash('%(email)s-%(host_name)s-%(folder)s' % locals())\n  templates = {\n   user_hook: 'git-jail-post-receive-user-repo',\n   '%s/post-commit' % repo_hooks: 'git-jail-post-commit-repo',\n   '%s/post-receive' % repo_hooks: 'git-jail-post-receive-repo',\n   '%s/hooks/post-receive' % real_repository: 'git-jail-post-receive-real-repo',\n  }\n  putFile(\n   '%s/hooks/post-receive.db' % real_repository,\n   '%(website_dir)s;%(full_path)s;.;%(key)s;%(origin)s;' % locals(),\n   'a'\n  )\n  putFile(\n   '%s.db' % user_hook,\n   '%(full_path)s;%(website_dir)s;./%(folder)s/*;%(key)s;%(origin)s;' % locals(),\n   'a'\n  )\n  putFile(\n   '%s/post-receive.db' % repo_hooks,\n   '%(full_path)s;%(real_repository)s;%(key)s;%(origin)s;' % locals(),\n   'a'\n  )\n  for f,t in templates.items():\n\t\t\tputFile(f, getTemplate(t) % locals())\n   os.system('chmod +x %s' % f)\n  info_message('Successful!')", "contrast": "import os\ndef remove_and_add_files(website_dir, folder):\n    os.system('cd %s; git rm --cached %s; git add ./%s/*' % (website_dir, folder, folder))", "label": 0}
{"index": "gp050815", "code": "def style_factory(param_0, param_1):\n        try:\n            var_0 = get_style_by_name(param_1)\n        except ClassNotFound:\n            var_0 = get_style_by_name('vim')\n        var_1 = {}\n        styles.update(style.styles)\n        styles.update(default_style_extensions)\n        var_2 = Token\n        styles.update({\n            t.Menu.Completions.Completion.Current: 'bg:#00aaaa #000000',\n            t.Menu.Completions.Completion: 'bg:#008888 #ffffff',\n            t.Menu.Completions.Meta.Current: 'bg:#00aaaa #000000',\n            t.Menu.Completions.Meta: 'bg:#00aaaa #ffffff',\n            t.Scrollbar.Button: 'bg:#003333',\n            t.Scrollbar: 'bg:#00aaaa',\n            t.Toolbar: 'bg:#222222 #cccccc',\n            t.Toolbar.Off: 'bg:#222222 #696969',\n            t.Toolbar.On: 'bg:#222222 #ffffff',\n            t.Toolbar.Search: 'noinherit bold',\n            t.Toolbar.Search.Text: 'nobold',\n            t.Toolbar.System: 'noinherit bold',\n            t.Toolbar.Arg: 'noinherit bold',\n            t.Toolbar.Arg.Text: 'nobold'\n        })\n        return style_from_dict(var_1)", "contrast": "import pygments.styles as styles\ndef get_pygments_style(style_name):\n    try:\n        return styles.get_style_by_name(style_name)\n    except Exception:\n        return styles.get_style_by_name('vim')", "label": 0}
{"index": "gp299482", "code": "def match_results(param_0, param_1, param_2):\n    var_0 = []\n    for var_1 in param_0:\n        var_2 = model.id\n        var_3 = []\n        for var_4 in param_1:\n            if getattr(var_4, param_2+'_id') == var_2:\n                related_results.append(var_4)\n        setattr(var_1, param_2, var_3)\n        matched_results.extend(var_3)\n    var_5 = set(param_1) - set(var_0)\n    return var_5", "contrast": "def match_one(self, models, results, relation):\n        return self._match_one_or_many(models, results, relation, 'one')", "label": 1}
{"index": "gp232809", "code": "import pandas_datareader.data as web\ndef get_series_info(param_0):\n    var_0 = web.DataReader(param_0, 'fred').describe().iloc[:,0]\n    return var_0", "contrast": "def get_series_info(self, series_id):\n        url = \"%s/series?series_id=%s\" % (self.root_url, series_id)\n        root = self.__fetch_data(url)\n        if root is None or not len(root):\n            raise ValueError('No info exists for series id: ' + series_id)\n        info = pd.Series(root.getchildren()[0].attrib)\n        return info", "label": 1}
{"index": "gp105210", "code": "def usearch_fasta_sort_from_filepath(\n        param_0,\n        param_1=None,\n        param_2=\"sortlen.log\",\n        param_3=False,\n        param_4=False,\n        param_5=False,\n        param_6=None):\n    if not param_1:\n        var_0, param_1 = mkstemp(prefix='usearch_fasta_sort',\n                                     suffix='.fasta')\n    var_1 = join(param_6, param_2)\n    var_2 = {}\n    var_3 = Usearch(var_2, WorkingDir=param_6, param_3=param_3)\n    var_4 = {'--mergesort': param_0,\n            '--output': param_1,\n            }\n    if not param_5:\n        var_4['--log'] = var_1\n    var_5 = var_3(var_4)\n    return var_5, param_1", "contrast": "def generate_sorted_fasta(fasta_filepath, output_filepath, log_name, HALT_EXEC=False, save_intermediate_files=False):\n    import subprocess\n    command = f'usearch --mergesort {fasta_filepath} --output {output_filepath} --log {log_name}'\n    if HALT_EXEC:\n        command += ' --halt_exit'\n    if save_intermediate_files:\n        command += ' --save_intermediate'\n    subprocess.run(command, shell=True)", "label": 0}
{"index": "gp259274", "code": "def close_broker_clients(param_0):\n    for var_0 in param_0:\n        client.close()", "contrast": "def _close_brokerclients(self, clients):\n        def _log_close_failure(failure, brokerclient):\n            log.debug(\n                'BrokerClient: %s close result: %s: %s', brokerclient,\n                failure.type.__name__, failure.getErrorMessage())\n        def _clean_close_dlist(result, close_dlist):\n            if close_dlist == self.close_dlist:\n                self.close_dlist = None\n        if not self.close_dlist:\n            dList = []\n        else:\n            log.debug(\"%r: _close_brokerclients has nested deferredlist: %r\",\n                      self, self.close_dlist)\n            dList = [self.close_dlist]\n        for brokerClient in clients:\n            log.debug(\"Calling close on: %r\", brokerClient)\n            d = brokerClient.close().addErrback(_log_close_failure, brokerClient)\n            dList.append(d)\n        self.close_dlist = DeferredList(dList)\n        self.close_dlist.addBoth(_clean_close_dlist, self.close_dlist)", "label": 1}
{"index": "gp099196", "code": "def compare_parts(param_0, param_1):\n    for var_0, var_1 in enumerate(param_0):\n        if var_1 != param_1[var_0]:\n            return 0\n    if len(param_1) > len(param_0):\n        return ISDIR\n    else:\n        return ISFILE", "contrast": "def check_lists(list1, list2):\n    if not list2.startswith(list1):\n        return 0", "label": 0}
{"index": "gp257344", "code": "def _add_window_title(param_0):\n    if not param_0:\n        return False\n    else:\n        return True", "contrast": "def _set_pyqtgraph_title(layout):\n    if 'title_size' in pytplot.tplot_opt_glob:\n        size = pytplot.tplot_opt_glob['title_size']\n    if 'title_text' in pytplot.tplot_opt_glob:\n        if pytplot.tplot_opt_glob['title_text'] != '':\n            layout.addItem(LabelItem(pytplot.tplot_opt_glob['title_text'], size=size, color='k'), row=0, col=0)\n            return True\n    return False", "label": 1}
{"index": "gp174411", "code": "from typing import List, Dict\nimport mxnet as mx\ndef get_optimizer_config(param_0: argparse.Namespace, param_1: List[int], param_2: Dict[str, str]) -> Dict[str, any]:\n    var_0 = args.num_buckets * args.batch_size_update / args.batch_size\n    var_1 = mx.lr_scheduler.FactorScheduler(step=var_0, factor=args.learning_rate_decay, stop_factor_lr=args.stop_factor_lr)\n    var_2 = {\n        'wd': args.weight_decay,\n        'learning_rate': args.learning_rate,\n        'lr_scheduler': var_1\n    }\n    if args.clip_gradient:\n        var_2['clip_gradient'] = args.clip_gradient\n    var_3 = mx.kvstore.create(args.kvstore) if args.kvstore else None\n    var_4 = args.optimizer.lower()\n    if var_4 == 'adam':\n        var_2['beta1'] = args.adam_beta1\n    elif var_4 == 'nag':\n        var_2['momentum'] = args.momentum\n    optimizer_params.update({var_5: float(var_6) for var_5, var_6 in args.optimizer_params.items()})\n    if args.hybridize:\n        var_2['multi_precision'] = True\n    if args.update_begin_of_epoch:\n        var_2[\"rescale_grad\"] = 1.0 / float(args.batch_size_update)\n    if args.profile_gradient:\n        var_2['profile'] = True\n    if args.bucketing is not None:\n        var_7 = L2Regularization(params=args.weight_decay)\n        var_2[BOPT_DEFAULTS] = BucketingOptions(default_bucket_key=args.bucketing,\n                                                           regularization=var_7,\n                                                           gradient_compression_params=args.gradient_compression_params)\n    return {'type': var_4, 'params': var_2, 'kvstore': var_3}", "contrast": "def create_optimizer_config(args: argparse.Namespace, source_vocab_sizes: List[int],\n                            extra_initializers: List[Tuple[str, mx.initializer.Initializer]] = None) -> OptimizerConfig:\n    optimizer_params = {'wd': args.weight_decay,\n                        \"learning_rate\": args.initial_learning_rate}\n    gradient_clipping_threshold = none_if_negative(args.gradient_clipping_threshold)\n    if gradient_clipping_threshold is None:\n        logger.info(\"Gradient clipping threshold set to negative value. Will not perform gradient clipping.\")\n        gradient_clipping_type = C.GRADIENT_CLIPPING_TYPE_NONE\n    else:\n        gradient_clipping_type = args.gradient_clipping_type\n    effective_batch_size = args.batch_size * args.update_interval\n    if gradient_clipping_threshold is not None and gradient_clipping_type == C.GRADIENT_CLIPPING_TYPE_ABS:\n        optimizer_params[\"clip_gradient\"] = gradient_clipping_threshold\n    if args.momentum is not None:\n        optimizer_params[\"momentum\"] = args.momentum\n    if args.loss_normalization_type == C.LOSS_NORM_VALID:\n        optimizer_params[\"rescale_grad\"] = 1.0 / args.update_interval\n    elif args.loss_normalization_type == C.LOSS_NORM_BATCH:\n        optimizer_params[\"rescale_grad\"] = 1.0 / effective_batch_size\n    if args.optimizer_params:\n        optimizer_params.update(args.optimizer_params)\n    weight_init = initializer.get_initializer(default_init_type=args.weight_init,\n                                              default_init_scale=args.weight_init_scale,\n                                              default_init_xavier_rand_type=args.weight_init_xavier_rand_type,\n                                              default_init_xavier_factor_type=args.weight_init_xavier_factor_type,\n                                              embed_init_type=args.embed_weight_init,\n                                              embed_init_sigma=source_vocab_sizes[0] ** -0.5,\n                                              rnn_init_type=args.rnn_h2h_init,\n                                              extra_initializers=extra_initializers)\n    lr_sched = lr_scheduler.get_lr_scheduler(args.learning_rate_scheduler_type,\n                                             args.checkpoint_interval,\n                                             none_if_negative(args.learning_rate_half_life),\n                                             args.learning_rate_reduce_factor,\n                                             args.learning_rate_reduce_num_not_improved,\n                                             args.learning_rate_schedule,\n                                             args.learning_rate_warmup)\n    config = OptimizerConfig(name=args.optimizer,\n                             params=optimizer_params,\n                             kvstore=args.kvstore,\n                             initializer=weight_init,\n                             gradient_clipping_type=gradient_clipping_type,\n                             gradient_clipping_threshold=gradient_clipping_threshold,\n                             update_interval=args.update_interval)\n    config.set_lr_scheduler(lr_sched)\n    logger.info(\"Optimizer: %s\", config)\n    logger.info(\"Gradient Compression: %s\", gradient_compression_params(args))\n    if args.update_interval > 1:\n        logger.info(\"Gradient accumulation over %d batches. Effective batch size: %d\",\n                    args.update_interval, effective_batch_size)\n    return config", "label": 1}
{"index": "gp317606", "code": "def simple_search(param_0):\n    for var_0 in password_list:\n        if all(keyword.lower() in password.lower() for var_1 in param_0):\n            yield var_0", "contrast": "def simple_search(self, *keywords):\n        matches = []\n        keywords = [kw.lower() for kw in keywords]\n        logger.verbose(\n            \"Performing simple search on %s (%s) ..\",\n            pluralize(len(keywords), \"keyword\"),\n            concatenate(map(repr, keywords)),\n        )\n        for entry in self.filtered_entries:\n            normalized = entry.name.lower()\n            if all(kw in normalized for kw in keywords):\n                matches.append(entry)\n        logger.log(\n            logging.INFO if matches else logging.VERBOSE,\n            \"Matched %s using simple search.\",\n            pluralize(len(matches), \"password\"),\n        )\n        return matches", "label": 1}
{"index": "gp137092", "code": "def close(param_0, param_1=True):\n    self.data_access.close(param_1=param_1)\n    return param_0", "contrast": "def close():\n    data_access.close()", "label": 0}
{"index": "gp121902", "code": "async def get_scene(param_0, param_1, param_2=True) -> Scene:\n        if not param_2:\n            await self.get_scenes()\n        for var_0 in self.scenes:\n            if _scene.id == param_1:\n                return var_0\n        raise ResourceNotFoundException(\"Scene not found scene_id: {}\".format(param_1))", "contrast": "def get_scene_resource():\n    raise ResourceNotFoundException(\"No scene found\")\n    raise PvApiError(\"Something is wrong with the hub\")", "label": 0}
{"index": "gp231438", "code": "def check_checksum(param_0, param_1, param_2):\n    if param_0 in param_2:\n        return param_1 == param_2[param_0]\n    else:\n        return False", "contrast": "def _item_exists_in_bucket(self, bucket, key, checksums):\n        try:\n            obj = self.target_s3.meta.client.head_object(Bucket=bucket, Key=key)\n            if obj and obj.containsKey('Metadata'):\n                if obj['Metadata'] == checksums:\n                    return True\n        except ClientError:\n            return False", "label": 1}
{"index": "gp030896", "code": "def display(param_0=None, param_1=None):\n  _display(param_0=param_0, param_1=param_1, print_message=True, display_handle=None)", "contrast": "import webbrowser\nimport requests\ndef display_tensorboard(port=None, height=None):\n    if port is None:\n        url = 'http://localhost:' + requests.get(\"http://localhost:6006/data/plugin/scalars/tags\").url.split(\":\")[-1]\n    else:\n        url = 'http://localhost:' + str(port)\n    if height is None:\n        height = 800\n    webbrowser.open_new_tab(url + '#height=' + str(height) + ',waterfall')", "label": 0}
{"index": "gp047083", "code": "def metrics(param_0, param_1):\n        return [\n            MetricStub(\n                ensure_unicode(stub.name),\n                stub.type,\n                stub.value,\n                normalize_tags(stub.tags),\n                ensure_unicode(stub.hostname),\n            )\n            for var_0 in self._metrics.get(to_string(param_1), [])\n        ]", "contrast": "def get_metrics_by_name(name:str, metrics:dict) -> dict:\n    return metrics.get(name, {})", "label": 0}
{"index": "gp201765", "code": "class DateTimeValues(object):\n    def __init__(param_0, param_1=None):\n        self._modification_time = param_1\n    @property\n    def modification_time(param_0):\n        return self._modification_time", "contrast": "def modification_time(self):\n    if self._stat_info is None:\n      return None\n    timestamp = int(self._stat_info.st_mtime)\n    return dfdatetime_posix_time.PosixTime(timestamp=timestamp)", "label": 1}
{"index": "gp077408", "code": "def update_unit(param_0, param_1, param_2):\n        return self._create_put_request(resource=UNITS, billomat_id=param_1, send_data=param_2)", "contrast": "def update_unit(unit_id, unit_dict):\n    units[unit_id].update(unit_dict)\n    return units[unit_id]", "label": 0}
{"index": "gp271235", "code": "from typing import List\nfrom sevenbridges.models import ExportBulkRecord\nfrom sevenbridges.errors import SbgError\ndef create_bulk_exports(param_0: List[str], param_1: bool, param_2) -> List[ExportBulkRecord]:\n    try:\n        var_0 = api.exports.bulk_create(param_0, param_1=param_1)\n        return var_0\n    except SbgError as error:\n        raise ValueError(f'Error creating bulk exports: {error}')", "contrast": "def bulk_submit(cls, exports, copy_only=False, api=None):\n        if not exports:\n            raise SbgError('Exports are required')\n        api = api or cls._API\n        items = []\n        for export in exports:\n            file_ = Transform.to_file(export.get('file'))\n            volume = Transform.to_volume(export.get('volume'))\n            location = Transform.to_location(export.get('location'))\n            properties = export.get('properties', {})\n            overwrite = export.get('overwrite', False)\n            item = {\n                'source': {\n                    'file': file_\n                },\n                'destination': {\n                    'volume': volume,\n                    'location': location\n                },\n                'properties': properties,\n                'overwrite': overwrite\n            }\n            items.append(item)\n        data = {'items': items}\n        params = {'copy_only': copy_only}\n        response = api.post(\n            url=cls._URL['bulk_create'], params=params, data=data\n        )\n        return ExportBulkRecord.parse_records(response=response, api=api)", "label": 1}
{"index": "gp072428", "code": "def parse_lines(param_0, param_1):\n    while 1:\n        var_0 = fileinp.readline()\n        if not var_0:\n            break\n        elif not logentry.rstrip():\n            continue  \n        var_1 = False\n        for var_2 in param_0:\n            if lp.grok(var_0):\n                var_1 = True\n        if not var_1:\n            var_3 = logging.getLogger('logparser')\n            logger.warning(\n                'Could not parse line >>>%s<<<', logentry.rstrip())\n            print('Could not parse line >>>%s<<<' % logentry.rstrip())", "contrast": "import fileinput\ndef parse_lines_to_log_parsers(log_parsers):\n    for line in fileinput.input():\n        for parser in log_parsers:\n            parser.parse(line)", "label": 0}
{"index": "gp113389", "code": "def enumerate_zones(param_0):\n        var_0 = []\n        for var_1 in range(1, 8):\n            for var_2 in range(1, 17):\n                var_3 = ZoneID(var_2, var_1)\n                try:\n                    var_4 = yield from self.get_zone_variable(var_3, 'name')\n                    if var_4:\n                        zones.append((var_3, var_4))\n                except CommandException:\n                    break\n        return var_0", "contrast": "def get_zones():\n    zones = [(1, \"Zone A\"), (2, \"Zone B\"), (3, \"Zone C\"), (4, \"Zone D\")]\n    return zones", "label": 0}
{"index": "gp170453", "code": "def export_mesh_to_obj(param_0):\n    var_0 = mesh.export(file_type=\"obj\")\n    return var_0", "contrast": "def export_wavefront(mesh,\n                     include_normals=True,\n                     include_texture=True):\n    face_formats = {('v',): '{}',\n                    ('v', 'vn'): '{}//{}',\n                    ('v', 'vt'): '{}/{}',\n                    ('v', 'vn', 'vt'): '{}/{}/{}'}\n    face_type = ['v']\n    export = 'v '\n    export += util.array_to_string(mesh.vertices,\n                                   col_delim=' ',\n                                   row_delim='\\nv ',\n                                   digits=8) + '\\n'\n    if include_normals and 'vertex_normals' in mesh._cache:\n        face_type.append('vn')\n        export += 'vn '\n        export += util.array_to_string(mesh.vertex_normals,\n                                       col_delim=' ',\n                                       row_delim='\\nvn ',\n                                       digits=8) + '\\n'\n    if (include_texture and\n        'vertex_texture' in mesh.metadata and\n            len(mesh.metadata['vertex_texture']) == len(mesh.vertices)):\n        face_type.append('vt')\n        export += 'vt '\n        export += util.array_to_string(mesh.metadata['vertex_texture'],\n                                       col_delim=' ',\n                                       row_delim='\\nvt ',\n                                       digits=8) + '\\n'\n    face_format = face_formats[tuple(face_type)]\n    faces = 'f ' + util.array_to_string(mesh.faces + 1,\n                                        col_delim=' ',\n                                        row_delim='\\nf ',\n                                        value_format=face_format)\n    export += faces\n    return export", "label": 1}
{"index": "gp278877", "code": "def render_template(param_0: str, param_1: str) -> str:\n    return rendered_template", "contrast": "def render_local_template(service_name, environment, repo_root, template_file):\n    cmd = 'cd {} && ef-cf {} {} --devel --verbose'.format(repo_root, template_file, environment)\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if p.returncode != 0:\n        stderr = indentify('\\n{}'.format(stderr))\n        stdout = indentify('\\n{}'.format(stdout))\n        raise Exception('Service: `{}`, Env: `{}`, Msg: `{}{}`'\n                        .format(service_name, environment, stderr, stdout))\n    logger.debug('Rendered template for `%s` in `%s`', template_file, environment)\n    r = re.match(r\".*(^{.*^})$\", stdout, re.MULTILINE | re.DOTALL)\n    return jsonify(json.loads(r.group(1)))", "label": 1}
{"index": "gp046551", "code": "def _update_container_metrics(param_0, param_1, param_2, param_3):\n        var_0 = list(instance.get('tags', []))  \n        if len(subcontainer.get('aliases', [])) >= 1:\n            var_1 = param_2['aliases'][0]\n        else:\n            self.log.debug(\"Subcontainer doesn't have a name, skipping.\")\n            return\n        tags.append('container_name:%s' % var_1)\n        var_2 = self.kubeutil.image_name_resolver(param_2['spec'].get('image'))\n        if var_2:\n            tags.append('container_image:%s' % var_2)\n            var_3 = container_image.split(\":\")\n            if len(var_3) > 2:\n                var_3 = [':'.join(var_3[:-1]), var_3[-1]]\n            tags.append('image_name:%s' % var_3[0])\n            if len(var_3) == 2:\n                tags.append('image_tag:%s' % var_3[1])\n        try:\n            var_4 = param_2['spec']['labels']\n        except KeyError:\n            self.log.debug(\"Subcontainer, doesn't have any labels\")\n            var_4 = {}\n        if KubeUtil.NAMESPACE_LABEL in var_4 and KubeUtil.POD_NAME_LABEL in var_4:\n            var_0 += self._get_post_1_2_tags(var_4, param_2, param_3)\n        elif KubeUtil.POD_NAME_LABEL in var_4:\n            var_0 += self._get_pre_1_2_tags(var_4, param_2, param_3)\n        else:\n            tags.append(\"pod_name:no_pod\")\n        var_5 = self.kubeutil.are_tags_filtered(var_0)\n        if var_5:\n            self._filtered_containers.add(param_2['id'])\n            return var_0\n        var_6 = param_2['stats'][-1]  \n        self._publish_raw_metrics(NAMESPACE, var_6, var_0)\n        if subcontainer.get(\"spec\", {}).get(\"has_filesystem\") and stats.get('filesystem', []) != []:\n            var_7 = var_6['filesystem'][-1]\n            if var_7['capacity'] > 0:\n                var_8 = float(var_7['usage'])/float(var_7['capacity'])\n                self.publish_gauge(param_0, NAMESPACE + '.filesystem.usage_pct', var_8, var_0)\n            else:\n                self.log.debug(\"Filesystem capacity is 0: cannot report usage metrics.\")\n        if subcontainer.get(\"spec\", {}).get(\"has_network\"):\n            var_9 = var_6['network']\n            self.publish_rate(param_0, NAMESPACE + '.network_errors',\n                              sum(float(var_9[var_10]) for var_10 in NET_ERRORS),\n                              var_0)\n        return var_0", "contrast": "def publish_subcontainer_metrics(subcontainer_name: str, metrics: dict, tags: list = []) -> bool:\n    return True", "label": 0}
{"index": "gp222367", "code": "def generate_changelog_markdown(param_0:str, param_1:dict, param_2:bool) -> str:\n    var_0 = ''\n    if param_2:\n        var_0 += f'# Changelog for version {param_0}\\n\\n'\n    for var_1 in param_1:\n        var_0 += f'## {section.capitalize()}\\n\\n'\n        for var_2 in param_1[var_1]:\n            var_0 += f'{var_2[\"type\"].capitalize()}: {var_2[\"description\"]}\\n'\n            if 'issues' in var_2:\n                var_0 += f'  - Resolves #{\", #\".join(map(str, var_2[\"issues\"]))}\\n'\n            var_0 += '\\n'\n    return var_0", "contrast": "def markdown_changelog(version: str, changelog: dict, header: bool = False) -> str:\n    debug('markdown_changelog(version=\"{}\", header={}, changelog=...)'.format(version, header))\n    output = ''\n    if header:\n        output += '## v{0}\\n'.format(version)\n    for section in CHANGELOG_SECTIONS:\n        if not changelog[section]:\n            continue\n        output += '\\n### {0}\\n'.format(section.capitalize())\n        for item in changelog[section]:\n            output += '* {0} ({1})\\n'.format(item[1], item[0])\n    return output", "label": 1}
{"index": "gp004443", "code": "def _get_obj_count_difference(param_0, param_1):\n    var_0 = _process_in_memory_objects(param_0)\n    var_1 = _process_in_memory_objects(param_1)\n    var_2 = _get_object_count_by_type(var_0)\n    var_3 = _get_object_count_by_type(var_1)\n    return var_2 - var_3", "contrast": "def collection_difference(collection1, collection2):\n    count1 = len(collection1)\n    count2 = len(collection2)\n    count_diff = abs(count1 - count2)\n    return count_diff", "label": 0}
{"index": "gp314321", "code": "def get_hypermap_endpoint(param_0: str, param_1: str, param_2: str) -> str:\n    if param_0 == \"WM\":\n        return param_1\n    else:\n        return param_2", "contrast": "def get_url_endpoint(self):\n        endpoint = self.url\n        if self.type not in ('Hypermap:WorldMap',):\n            endpoint = 'registry/%s/layer/%s/map/wmts/1.0.0/WMTSCapabilities.xml' % (\n                self.catalog.slug,\n                self.id\n            )\n        return endpoint", "label": 1}
{"index": "gp123586", "code": "def rinseElement(param_0, param_1):\n        if '*' in param_1:\n            var_0 = ''.join(ele.split()).split('*')\n            var_1 = var_0[[x.isdigit() for var_2 in var_0].index(True)]\n            var_3 = var_0[[x.isdigit() for var_2 in var_0].index(False)]\n            return dict(zip(('num', 'name'), (int(var_1), var_3)))\n        else:\n            return dict(zip(('num', 'name'), (1, param_1)))", "contrast": "def rinseElement(ele):\n    num, name = ele.split('*')\n    return {'num': num, 'name': name}", "label": 0}
{"index": "gp070173", "code": "def set_errors(param_0, param_1):\n        var_0 = result.get_messages()\n        for var_1 in var_0:\n            if not hasattr(param_0, var_1):\n                continue \n            var_2 = var_0[var_1]\n            if type(var_2) is not list:\n                var_2 = ['<Nested schema result following...>']\n            if var_1 in self.errors:\n                self.errors[var_1].extend(var_2)\n            else:\n                self.errors[var_1] = var_2", "contrast": "def populate_field_errors(field_errors, schema_errors):\n    for error in schema_errors:\n        field_errors[error['field']] = error['message']\n    return field_errors", "label": 0}
{"index": "gp106385", "code": "def create_selfsim(param_0, param_1='rsfx'):\n    var_0 = oracle.n_states - 1\n    var_1 = np.zeros((var_0, var_0))\n    if param_1 == 'com':\n        if not oracle.code:\n            print(\"Codes not generated. Generating codes with encode().\")\n            oracle.encode()\n        var_2 = 0  \n        for var_3, var_4 in oracle.code:  \n            if var_3 == 0:\n                var_5 = 1\n            else:\n                var_5 = var_3\n            var_1[range(var_2, var_2 + var_5), range(var_4 - 1, var_4 - 1 + var_5)] = 1\n            var_1[range(var_4 - 1, var_4 - 1 + var_5), range(var_2, var_2 + var_5)] = 1\n            var_2 = var_2 + var_3\n    elif param_1 == 'sfx':\n        for var_6, var_7 in enumerate(oracle.sfx[1:]):\n            if var_7 != 0:\n                var_1[var_6][var_7 - 1] = 1\n                var_1[var_7 - 1][var_6] = 1\n    elif param_1 == 'rsfx':\n        for var_8 in oracle.latent:\n            var_4 = itertools.product(var_8, repeat=2)\n            for var_9 in var_4:\n                var_1[var_9[0] - 1][var_9[1] - 1] = 1\n    elif param_1 == 'lrs':\n        for var_6, var_3 in enumerate(oracle.lrs[1:]):\n            if var_3 != 0:\n                var_7 = oracle.sfx[var_6 + 1]\n                var_1[range((var_7 - var_3) + 1, var_7 + 1), range(var_6 - var_3 + 1, var_6 + 1)] = 1\n                var_1[range(var_6 - var_3 + 1, var_6 + 1), range((var_7 - var_3) + 1, var_7 + 1)] = 1\n    elif param_1 == 'seg':\n        var_10 = oracle.segment\n        var_2 = 0\n        for var_3, var_4 in var_10:  \n            if var_3 == 0:\n                var_5 = 1\n            else:\n                var_5 = var_3\n            var_1[range(var_2, var_2 + var_5), range(var_4 - 1, var_4 - 1 + var_5)] = 1\n            var_1[range(var_4 - 1, var_4 - 1 + var_5), range(var_2, var_2 + var_5)] = 1\n            var_2 = var_2 + var_3\n    return var_1", "contrast": "import numpy as np\ndef create_self_similarity_matrix(oracle, method):\n    if method == \"comp\":\n        matrix = np.zeros((len(oracle.comp_code), len(oracle.comp_code)))\n        for i in range(len(oracle.comp_code)):\n            for j in range(i, len(oracle.comp_code)):\n                matrix[i, j] = matrix[j, i] = oracle.comp_code[i].similarity(oracle.comp_code[j])\n    elif method == \"sfx\":\n        matrix = np.zeros((len(oracle.sfx_link), len(oracle.sfx_link)))\n        for i in range(len(oracle.sfx_link)):\n            for j in range(i, len(oracle.sfx_link)):\n                matrix[i, j] = matrix[j, i] = oracle.sfx_link[i].similarity(oracle.sfx_link[j])\n    elif method == \"rsfx\":\n        matrix = np.zeros((len(oracle.rsfx_link), len(oracle.rsfx_link)))\n        for i in range(len(oracle.rsfx_link)):\n            for j in range(i, len(oracle.rsfx_link)):\n                matrix[i, j] = matrix[j, i] = oracle.rsfx_link[i].similarity(oracle.rsfx_link[j])\n    elif method == \"lrs\":\n        matrix = np.zeros((len(oracle.lrs_val), len(oracle.lrs_val)))\n        for i in range(len(oracle.lrs_val)):\n            for j in range(i, len(oracle.lrs_val)):\n                matrix[i, j] = matrix[j, i] = oracle.lrs_val[i].similarity(oracle.lrs_val[j])\n    elif method == \"seg\":\n        matrix = np.zeros((len(oracle.patterns), len(oracle.patterns)))\n        for i in range(len(oracle.patterns)):\n            for j in range(i, len(oracle.patterns)):\n                matrix[i, j] = matrix[j, i] = oracle.patterns[i].similarity(oracle.patterns[j])\n    else:\n        raise ValueError(\"Invalid method\")\n    return matrix", "label": 0}
{"index": "gp140830", "code": "def python_2_nonzero_compatible(param_0):\n    if six.PY2:\n        if '__bool__' not in klass.__dict__:\n            raise ValueError(\n                '@python_2_nonzero_compatible cannot be applied to {0} because '\n                'it doesn\\'t define __bool__().'.format(klass.__name__))\n        klass.__nonzero__ = klass.__bool__\n    return param_0", "contrast": "def add_nonzero_method(klass):\n    if not hasattr(klass, '__bool__'):\n        raise TypeError(\"Given class does not define __bool__ method.\")\n    if not hasattr(klass, '__nonzero__'):\n        klass.__nonzero__ = klass.__bool__\n    return klass", "label": 0}
{"index": "gp055437", "code": "def tdot_blas(param_0, param_1=None):\n    if (mat.dtype != 'float64') or (len(mat.shape) != 2):\n        return np.dot(param_0, mat.T)\n    var_0 = mat.shape[0]\n    if param_1 is None:\n        param_1 = np.zeros((var_0, var_0))\n    else:\n        assert(out.dtype == 'float64')\n        assert(out.shape == (var_0, var_0))\n        assert(8 in out.strides)\n        param_1[:] = 0.0\n    param_0 = np.asfortranarray(param_0)\n    param_1 = blas.dsyrk(alpha=1.0, a=param_0, beta=0.0, c=param_1, overwrite_c=1,\n                     trans=0, lower=0)\n    symmetrify(param_1, upper=True)\n    return np.ascontiguousarray(param_1)", "contrast": "import numpy as np\ndef fast_dot(mat):\n    return np.einsum('ij,jk->ik', mat, mat)", "label": 0}
{"index": "gp233975", "code": "def send_data(param_0, param_1):\n    socket.sendall(data.encode())", "contrast": "def send(self, s):\n        self._print_header('======== Sending ({0}) ========'.format(len(s)))\n        self._log_send(s)\n        out = len(s)\n        while s:\n            s = s[self._send(s):]\n        return out", "label": 1}
{"index": "gp125593", "code": "def get_sort_function(param_0):\n    var_0 = tuple((var_1['key'], -1 if var_1['reverse'] else 1) for var_1 in param_0)\n    def sort_function(param_0, param_1):\n        for var_0, var_1 in var_0:\n            var_2 = cmp(getattr(param_0, var_0) if param_0 else param_0, getattr(param_1, var_0) if param_1 else param_1)\n            if var_2 != 0:\n                return var_2 * var_1\n        return 0\n    return sort_function", "contrast": "def make_comparator(keys):\n    def comparator(a, b):\n        for key in keys:\n            attr_a = getattr(a, key['key'])\n            attr_b = getattr(b, key['key'])\n            if attr_a != attr_b:\n                if key.get('reverse', False):\n                    return -1 if attr_a > attr_b else 1\n                else:\n                    return -1 if attr_a < attr_b else 1\n        return 0\n    return comparator", "label": 0}
{"index": "gp331925", "code": "def cast_to_type(param_0, param_1):\n    try:\n        return param_1(param_0)\n    except (ValueError, TypeError):\n        return None", "contrast": "def value(self,ascode=None):\n        if ascode is None:\n            ascode = self.code\n        return self.cast[ascode](self.text)", "label": 1}
{"index": "gp148782", "code": "def GetDateRange(param_0):\n    (var_0, var_1, var_2, var_3) = self.GetDateRangeWithOrigins()\n    return (var_0, var_1)", "contrast": "from datetime import datetime\ndef service_dates(schedule):\n    start_date = datetime.max.date()\n    end_date = datetime.min.date()\n    for start, end in schedule:\n        if start < start_date:\n            start_date = start\n        if end > end_date:\n            end_date = end\n    return (start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\"))", "label": 0}
{"index": "gp314837", "code": "def add_final_message(param_0, param_1, param_2):\n    message_list.append(param_2)\n    if param_0 and profiler.parent:\n        profiler.parent.add_message(param_1)\n    else:\n        message_list.clear()", "contrast": "def finish(self, msg=None):\n        if self._finished or self.disable:\n            return        \n        self._finished = True\n        if msg is not None:\n            self(msg)\n        self._new_msg(\"< Exiting %s, total time: %0.4f ms\", \n                      self._name, (ptime.time() - self._firstTime) * 1000)\n        type(self)._depth -= 1\n        if self._depth < 1:\n            self.flush()", "label": 1}
{"index": "gp006655", "code": "def child_value(param_0, param_1=None):\n        if param_1 is None:\n            var_0 = lib.lsl_child_value(self.e)\n        else:\n            var_0 = lib.lsl_child_value_n(self.e, str.encode(param_1))\n        return res.decode('utf-8')", "contrast": "def get_child_value(node, name=None):\n    if name:\n        return node.findtext(name)\n    else:\n        for child in node:\n            if child.text is not None:\n                return child.text\n    return None", "label": 0}
{"index": "gp021405", "code": "def fetcher_loop_v1(param_0, param_1, param_2=False,\n                    param_3=0, param_4=None):\n    while True:\n        var_0, var_1 = data_queue.get()\n        if var_0 is None:\n            break\n        if param_2:\n            var_1 = _as_in_context(var_1, context.cpu_pinned(param_3))\n        else:\n            var_1 = _as_in_context(var_1, context.cpu())\n        if param_4 is not None:\n            with param_4:\n                param_1[var_0] = var_1\n        else:\n            param_1[var_0] = var_1", "contrast": "def fetcher_loop(queue, reorder_dict):\n    while queue:\n        data = queue.pop(0)\n        key = data['key']\n        value = data['value']\n        if key in reorder_dict:\n            reorder_dict[key].append(value)\n        else:\n            reorder_dict[key] = [value]", "label": 0}
{"index": "gp273081", "code": "def refine_dataset(param_0, **param_1):\n    def sanitize(param_0):\n        return ''.join(var_0 if c.isalnum() or var_0 in '._' else '_' for var_0 in param_0)\n    var_0 = []\n    for var_1 in param_0:\n        var_2 = True\n        for var_3, var_4 in kwargs.items():\n            if callable(var_4):\n                var_5 = sanitize(str(item.get(var_3, '')))\n                if not var_4(var_5):\n                    var_2 = False\n                    break\n            else:\n                var_5 = sanitize(str(item.get(var_3, '')))\n                var_6 = sanitize(str(var_4))\n                if var_5 != var_6:\n                    var_2 = False\n                    break\n        if var_2:\n            result.append(var_1)\n    return var_0", "contrast": "def where(self, **kwargs):\n        clauses = copy(self.clauses)\n        for dimension, condition in kwargs.items():\n            if dimension in self.clauses:\n                raise Exception('There should be only one clause for {}'.format(dimension))\n            if dimension not in self.schema:\n                raise Exception('The dimension {} doesn\\'t exist'.format(dimension))\n            if isfunction(condition) or isinstance(condition, functools.partial):\n                clauses[dimension] = condition\n            else:\n                clauses[dimension] = functools.partial((lambda x, y: x == y), self._sanitize_dimension(str(condition)))\n        return self._copy(clauses=clauses)", "label": 1}
{"index": "gp310254", "code": "def debug_output(param_0: str) -> str:\n    return condition.replace('[\"', '.').replace('\"][', '.').replace('\"]', '').replace('[', '.').replace(']', '')", "contrast": "def printable_name(column, path=None):\n    pieces = [column.name]\n    path = path or path_of(column)\n    for segment in path:\n        if isinstance(segment, str):\n            pieces.append(segment)\n        else:\n            pieces[-1] += \"[{}]\".format(segment)\n    return \".\".join(pieces)", "label": 1}
{"index": "gp158250", "code": "def prepend_string_list(param_0, param_1, param_2, param_3):\n        var_0 = self.get(param_3)\n        var_1 = self.get_string_list(param_1)\n        var_1 = [param_2] + [var_2 for var_2 in var_1 if var_2 != param_2]\n        var_1 = var_1[:var_0]\n        self.beginWriteArray(param_1)\n        for var_3 in range(len(var_1)):\n            self.setArrayIndex(var_3)\n            self.setValue(\"entry\", var_1[var_3])\n        self.endArray()", "contrast": "def prepend_string(string_list: List[str], new_string: str, max_length: int) -> List[str]:\n    if new_string in string_list:\n        string_list.remove(new_string)\n    string_list.insert(0, new_string)\n    return string_list[:max_length]", "label": 0}
{"index": "gp115149", "code": "def set_data(param_0, param_1):\n        if param_1 is None:\n            raise errors.NullArgument('data cannot be None')\n        if not isinstance(param_1, DataInputStream):\n            raise errors.InvalidArgument('data must be instance of DataInputStream')\n        var_0 = JSONClientValidated('repository',\n                                    runtime=self._runtime).raw()\n        var_1 = gridfs.GridFS(var_0)\n        self._my_map['data'] = filesys.put(data._my_data)\n        data._my_data.seek(0)\n        self._my_map['base64'] = base64.b64encode(data._my_data.read())", "contrast": "def set_content_data(self, data):\n    if data is None:\n        raise NullArgument()\n    if not isinstance(data, osid.transport.DataInputStream):\n        raise InvalidArgument('data argument is not an instance of osid.transport.DataInputStream')\n    if Metadata.isReadOnly() is True:\n        raise NoAccess()", "label": 0}
{"index": "gp148070", "code": "def prep_rg_names(param_0, param_1, param_2, param_3):\n    if param_2 and param_3:\n        var_0 = \"%s_%s_%s\" % (param_0[\"lane\"], param_3, param_2)\n    else:\n        var_0 = param_0[\"description\"]\n    return {\"rg\": param_0[\"description\"],\n            \"sample\": param_0[\"description\"],\n            \"lane\": var_0,\n            \"pl\": (tz.get_in([\"algorithm\", \"platform\"], param_0)\n                   or tz.get_in([\"algorithm\", \"platform\"], param_0, \"illumina\")).lower(),\n            \"lb\": tz.get_in([\"metadata\", \"library\"], param_0),\n            \"pu\": tz.get_in([\"metadata\", \"platform_unit\"], param_0) or var_0}", "contrast": "def generate_read_groups(*items):\n    read_groups = []\n    for item in items:\n        if isinstance(item, str):\n            item = [item]\n        for i in range(len(item)):\n            read_groups.extend([f\"{it}_R{j}\" for j in range(1, 3)])\n    return read_groups", "label": 0}
{"index": "gp017627", "code": "def _addsub_int_array(param_0, param_1, param_2):\n        assert not is_period_dtype(param_0)\n        assert param_2 in [operator.add, operator.sub]\n        if self.freq is None:\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n        elif isinstance(self.freq, Tick):\n            var_0 = Timedelta(self.freq)\n            return param_2(param_0, var_0 * param_1)\n        assert not is_timedelta64_dtype(param_0)\n        return param_2(param_0, np.array(param_1) * self.freq)", "contrast": "import operator\nimport numpy as np\nfrom pandas import Index, ExtensionArray\ndef time_shift_array(other: (Index, ExtensionArray, np.ndarray), op: callable):\n    return other.__array_ufunc__(op, asarray=True)", "label": 0}
{"index": "gp249050", "code": "def create_form_base_new(param_0, param_1, param_2=None, param_3=None, param_4=None):\n    if param_2 is None:\n        param_2 = 'form_base_new.html'\n    if param_3 is None:\n        param_3 = reverse_lazy('home')\n    if request.method == 'POST':\n        var_0 = param_1(request.POST)\n        if form.is_valid():\n            form.save()\n            return redirect(param_3)\n    else:\n        var_0 = param_1()\n    var_1 = {\n        'form': var_0,\n    }\n    if param_4 is not None:\n        context.update(param_4)\n    return render(param_0, param_2, var_1)", "contrast": "def create(self):\n        self.add_handlers({'^T': self.quit, '^Q': self.quit})\n        self.services_tft = self.add(npyscreen.TitleFixedText,\n                                     name='No services running.',\n                                     value='')\n        services = Services(self.core, external=self.external)\n        if services:\n            self.services_tft.hidden = True\n            for service in services:\n                value = ''\n                for val in service[1]:\n                    value += val+', '\n                self.add(npyscreen.TitleFixedText,\n                         name=service[0],\n                         value=value[:-2])", "label": 1}
{"index": "gp016700", "code": "def build(param_0):\n        var_0 = self._param_grid.keys()\n        var_1 = self._param_grid.values()\n        def to_key_value_pairs(param_0, param_1):\n            return [(var_0, key.typeConverter(var_1)) for var_0, var_1 in zip(param_0, param_1)]\n        return [dict(to_key_value_pairs(var_0, var_2)) for var_2 in itertools.product(*var_1)]", "contrast": "from itertools import product\ndef build_all_combinations(param_grid):\n    keys, values = zip(*param_grid.items())\n    combinations = product(*values)\n    for combination in combinations:\n        yield dict(zip(keys, combination))", "label": 0}
{"index": "gp141342", "code": "def results(param_0, param_1=100):\n        var_0 = True if self.high_mark is not None else False\n        var_1 = self.high_mark - self.low_mark if var_0 else None\n        var_2 = 0\n        var_3 = self.get_params()\n        var_3[\"offset\"] = self.low_mark\n        var_3[\"limit\"] = param_1\n        while not var_0 and var_1 is None or var_2 < var_1:\n            if var_0 or var_1 is not None:\n                var_4 = var_1 - var_2\n                var_3[\"limit\"] = var_4 if var_4 < param_1 else param_1\n            var_5 = self.resource._meta.api.http_resource(\"GET\", self.resource._meta.resource_name, var_3=var_3)\n            var_6 = self.resource._meta.api.resource_deserialize(r.text)\n            if not var_0:\n                var_1 = var_6[\"meta\"][\"total_count\"]\n            if var_6[\"meta\"][\"total_count\"] < var_1:\n                var_1 = var_6[\"meta\"][\"total_count\"]\n            var_3[\"offset\"] = var_6[\"meta\"][\"offset\"] + var_6[\"meta\"][\"limit\"]\n            for var_7 in var_6[\"objects\"]:\n                var_2 += 1\n                yield var_7", "contrast": "import requests\ndef api_results(url, params):\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    results = response.json()['results']\n    yield results\n    while response.json()['next']:\n        response = requests.get(response.json()['next'])\n        response.raise_for_status()\n        results = response.json()['results']\n        yield results", "label": 0}
{"index": "gp130502", "code": "def put_stream(param_0, param_1, param_2=None, param_3=None):\n        import Queue\n        import time\n        import threading\n        var_0 = metadata.get('md5', None) if param_2 else None\n        var_1 = ('public-read' if metadata.get('public',\n                                             False) else metadata.get('acl',\n                                                                      'public-read')) if param_2 else 'public-read'\n        var_2 = self._prefix(self._rename(param_1))\n        class ThreadUploader(threading.Thread):\n            def __init__(param_0, param_1, param_2):\n                threading.Thread.__init__(param_0)\n                self.n = param_1\n                self.queue = param_2\n            def run(param_0):\n                while True:\n                    var_0, var_1, var_2 = self.queue.get()\n                    if var_0 is None:  \n                        logger.debug(\n                            \"put_stream: Thread {} exiting\".format(\n                                self.n))\n                        self.queue.task_done()\n                        return\n                    logger.debug(\n                        \"put_stream: Thread {}: processing part: {}\".format(\n                            self.n,\n                            var_1))\n                    var_3 = time.time()\n                    try:\n                        mp.upload_part_from_file(var_2, var_1)\n                    finally:\n                        self.queue.task_done()\n                        var_4 = time.time()\n                        logger.debug(\"put_stream: Thread {}, part {}. time = {} rate =  {} b/s\" .format(\n                            self.n, var_1, round(var_4 - var_3, 3), round((float(buf.tell()) / (var_4 - var_3)), 2)))\n        if param_2 is None:\n            param_2 = {}\n        if var_0:\n            param_2['md5'] = var_0  \n        for var_3, var_4 in metadata.items():\n            if not var_4:\n                del param_2[var_3]\n        var_5 = param_0\n        var_6 = 50 * 1024 * 1024  \n        var_7 = 4\n        var_8 = Queue.Queue(maxsize=100)\n        for var_9 in range(var_7):\n            var_10 = ThreadUploader(var_9, var_8)\n            t.setDaemon(True)\n            t.start()\n        class flo:\n            def __init__(param_0, param_1):\n                import io\n                self.mp = this.bucket.initiate_multipart_upload(\n                    var_2,\n                    param_2=param_2)\n                self.part_number = 1\n                self.buffer = io.BytesIO()\n                self.total_size = 0\n                self.rel_path = param_1\n            def _send_buffer(param_0):\n                logger.debug(\n                    \"_send_buffer: sending part {} to thread pool size: {}, total_size = {}\" .format(\n                        self.part_number,\n                        self.buffer.tell(),\n                        self.total_size))\n                self.buffer.seek(0)\n                thread_upload_queue.put(\n                    (self.mp, self.part_number, self.buffer))\n            def write(param_0, param_1):\n                import io\n                self.buffer.write(param_1)  \n                self.total_size += len(param_1)\n                if self.buffer.tell() > var_6:\n                    self._send_buffer()\n                    self.part_number += 1\n                    self.buffer = io.BytesIO()\n            def writelines(param_0, param_1):\n                raise NotImplemented()\n            def close(param_0):\n                if self.buffer.tell() > 0:\n                    self._send_buffer()\n                thread_upload_queue.join()\n                for var_0 in range(var_7):\n                    thread_upload_queue.put(\n                        (None, None, None))  \n                thread_upload_queue.join()\n                if self.total_size > 0:\n                    self.mp.complete_upload()\n                    this.bucket.set_acl(var_1, var_2)\n                this.put_metadata(self.rel_path, param_2)\n            def __enter__(param_0):\n                return param_0\n            def __exit__(param_0, param_1, param_2, param_3):\n                if param_1:\n                    return False\n                self.close()\n        return flo(param_1)", "contrast": "import boto3\ndef get_s3_flo(bucket, key):\n    s3_client = boto3.client('s3')\n    multipart_upload = s3_client.create_multipart_upload(Bucket=bucket, Key=key)\n    upload_id = multipart_upload['UploadId']\n    def write(data):\n        nonlocal upload_id\n        nonlocal s3_client\n        s3_client.upload_part(Body=data, Bucket=bucket, Key=key,\n                              PartNumber=len(parts) + 1, UploadId=upload_id)\n    parts = []\n    return write", "label": 0}
{"index": "gp062250", "code": "def clear_surroundings(param_0):\n        var_0 = self.grd.eight_neighbors(self.current_y, self.current_x)\n        for var_1 in var_0:\n            self.grd.set_tile(var_1[0], var_1[1], ' ')", "contrast": "def clear_surrounding_cells(grid, agent_position):\n    row, col = agent_position\n    for i in range(row-1, row+2):\n        for j in range(col-1, col+2):\n            if i in range(len(grid)) and j in range(len(grid[0])):\n                grid[i][j] = 0", "label": 0}
{"index": "gp266256", "code": "def reset_atom_value(param_0):\n    return param_0", "contrast": "def reset(self, newval):\n        oldval = self._state.get()\n        self._state.set(newval)\n        self.notify_watches(oldval, newval)\n        return newval", "label": 1}
{"index": "gp190504", "code": "def generate_sequences(param_0):\n    if param_0 < switchover_iterations:\n        var_0 = \"Sequence A\"\n        var_1 = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n    else:\n        var_0 = \"Sequence B\"\n        var_1 = [(10, 11, 12), (13, 14, 15), (16, 17, 18)]\n    return var_1, var_0", "contrast": "def getHighOrderSequenceChunk(it, switchover=1000, w=40, n=2048):\n  if it%10==3:\n    s = numpy.random.randint(5)\n    if it <= switchover:\n      if s==0:\n        label=\"XABCDE\"\n      elif s==1:\n        label=\"YCBEAF\"\n      elif s==2:\n        label=\"GHIJKL\"\n      elif s==3:\n        label=\"WABCMN\"\n      else:\n        label=\"ZDBCAE\"\n    else:\n      if s==0:\n        label=\"XCBEAF\"\n      elif s==1:\n        label=\"YABCDE\"\n      elif s==2:\n        label=\"GABCMN\"\n      elif s==3:\n        label=\"WHIJKL\"\n      else:\n        label=\"ZDHICF\"\n    vecs = letterSequence(label)\n  else:\n    vecs= [getRandomVector(w, n)]\n    label=\".\"\n  return vecs,label", "label": 1}
{"index": "gp309503", "code": "def find_link_html(param_0: dict, param_1: bool) -> str:\n    var_0 = \"https://example.com\"\n    var_1 = \"<a href='{}'\".format(var_0)\n    for var_2, var_3 in attrs.items():\n        var_1 += \" {}='{}'\".format(var_2, var_3)\n    var_1 += \">Link Text</a>\"\n    return var_1 if not param_1 else f\"<span>{var_1}</span>\"", "contrast": "def expand_db_attributes(attrs, for_editor):\n        try:\n            editor_attrs    = ''\n            link            = Link.objects.get(id=attrs['id'])\n            if for_editor:\n                editor_attrs = 'data-linktype=\"link\" data-id=\"{0}\" '.format(\n                    link.id\n                )\n            return '<a {0}href=\"{1}\" title=\"{2}\">'.format(\n                editor_attrs,\n                escape(link.get_absolute_url()),\n                link.title\n            )\n        except Link.DoesNotExist:\n            return '<a>'", "label": 1}
{"index": "gp295775", "code": "def serialize(param_0):\n    if isinstance(param_0, (list, tuple)):\n        return [serialize(var_0) for var_0 in param_0]\n    elif isinstance(param_0, dict):\n        return {var_1: serialize(var_2) for var_1, var_2 in obj.items()}\n    elif isinstance(param_0, str):\n        return param_0\n    else:\n        return str(param_0)", "contrast": "def _serialize_iterable(obj):\n    if isinstance(obj, (tuple, set)):\n        obj = list(obj)\n    for item in obj:\n        obj[obj.index(item)] = serialize_obj(item)\n    return obj", "label": 1}
{"index": "gp125092", "code": "def insert_row(param_0, param_1, *param_2):\n    var_0 = dbconn.cursor()\n    cur.execute(\"INSERT INTO '{name}' VALUES{args}\".format(name=param_1, param_2=param_2))\n    dbconn.commit()", "contrast": "def insert_row(dbconn, table_name, *args):\n    sql_query = \"INSERT INTO \" + table_name + \" VALUES \" + str(args)\n    cursor = dbconn.cursor()\n    cursor.execute(sql_query)\n    dbconn.commit() \n    cursor.close()", "label": 0}
{"index": "gp290584", "code": "def get_sync_status(param_0: str) -> str:\n    var_0, var_1, var_2 = package.split('/')\n    return 'synced'  ", "contrast": "def status(ctx, opts, owner_repo_package):\n    owner, repo, slug = owner_repo_package\n    click.echo(\n        \"Getting status of %(package)s in %(owner)s/%(repo)s ... \"\n        % {\n            \"owner\": click.style(owner, bold=True),\n            \"repo\": click.style(repo, bold=True),\n            \"package\": click.style(slug, bold=True),\n        },\n        nl=False,\n    )\n    context_msg = \"Failed to get status of package!\"\n    with handle_api_exceptions(ctx, opts=opts, context_msg=context_msg):\n        with maybe_spinner(opts):\n            res = get_package_status(owner, repo, slug)\n            ok, failed, _, status_str, stage_str, reason = res\n    click.secho(\"OK\", fg=\"green\")\n    if not stage_str:\n        package_status = status_str\n    else:\n        package_status = \"%(status)s / %(stage)s\" % {\n            \"status\": status_str,\n            \"stage\": stage_str,\n        }\n    if ok:\n        status_colour = \"green\"\n    elif failed:\n        status_colour = \"red\"\n    else:\n        status_colour = \"magenta\"\n    click.secho(\n        \"The package status is: %(status)s\"\n        % {\"status\": click.style(package_status, fg=status_colour)}\n    )\n    if reason:\n        click.secho(\n            \"Reason given: %(reason)s\" % {\"reason\": click.style(reason, fg=\"yellow\")},\n            fg=status_colour,\n        )", "label": 1}
{"index": "gp245226", "code": "def send_message():\n    while True:\n        try:\n            break\n        except WebsocketClosedError:\n            run() ", "contrast": "def send(self, data):\n        while not self.stopped():\n            try:\n                self.ws.send(data)\n                return\n            except websocket.WebSocketConnectionClosedException:\n                time.sleep(0.1)", "label": 1}
{"index": "gp046082", "code": "def overpass_request(param_0, param_1=None, param_2=180, param_3=None):\n    var_0 = 'http://overpass-api.de/api/interpreter'\n    var_1 = requests.Request('GET', var_0, params=param_0).prepare().url\n    var_2 = get_from_cache(var_1)\n    if var_2 is not None:\n        return var_2\n    else:\n        if param_1 is None:\n            var_3 = get_pause_duration()\n        log('Pausing {:,.2f} seconds before making API POST request'.format(var_3))\n        time.sleep(var_3)\n        var_4 = time.time()\n        log('Posting to {} with timeout={}, \"{}\"'.format(var_0, param_2, param_0))\n        var_5 = requests.post(var_0, param_0=param_0, param_2=param_2, headers=get_http_headers())\n        var_6 = len(response.content) / 1000.\n        var_7 = re.findall(r'//(?s)(.*?)/', var_0)[0]\n        log('Downloaded {:,.1f}KB from {} in {:,.2f} seconds'.format(var_6, var_7, time.time()-var_4))\n        try:\n            var_8 = response.json()\n            if 'remark' in var_8:\n                log('Server remark: \"{}\"'.format(var_8['remark'], level=lg.WARNING))\n            save_to_cache(var_1, var_8)\n        except Exception:\n            if response.status_code in [429, 504]:\n                if param_3 is None:\n                    param_3 = get_pause_duration()\n                log('Server at {} returned status code {} and no JSON data. Re-trying request in {:.2f} seconds.'.format(var_7,\n                                                                                                                         response.status_code,\n                                                                                                                         param_3),\n                                                                                                                         level=lg.WARNING)\n                time.sleep(param_3)\n                var_8 = overpass_request(param_0=param_0, param_1=param_1, param_2=param_2)\n            else:\n                log('Server at {} returned status code {} and no JSON data'.format(var_7, response.status_code), level=lg.ERROR)\n                raise Exception('Server returned no JSON data.\\n{} {}\\n{}'.format(var_5, response.reason, response.text))\n        return var_8", "contrast": "import requests\nimport time\ndef send_request_to_overpass_api(data, pause_duration=None, timeout=180, error_pause_duration=30):\n    if pause_duration is not None:\n        time.sleep(pause_duration)\n    url = \"http://overpass-api.de/api/interpreter\"\n    try:\n        response = requests.post(url, data=data, timeout=timeout)\n        result = response.json()\n        return result\n    except requests.exceptions.RequestException as e:\n        print(\"Error: \", e)\n        time.sleep(error_pause_duration)\n        send_request_to_overpass_api(data=data, pause_duration=None, timeout=timeout, error_pause_duration=error_pause_duration)", "label": 0}
{"index": "gp321047", "code": "import logging\ndef log_info(param_0, *param_1, **param_2):\n    logging.basicConfig(level=logging.INFO)\n    logging.info(param_0, *param_1, **param_2)", "contrast": "def info(self, msg, *args, **kwargs):\n    self._baseLogger.info(self, self.getExtendedMsg(msg), *args, **kwargs)", "label": 1}
{"index": "gp167895", "code": "def main(param_0=None):\n    if param_0 is None:\n        param_0 = sys.argv\n    var_0 = U.OptionParser(version=\"%prog version: $Id$\",\n                            usage=usage,\n                            description=globals()[\"__doc__\"])\n    var_1 = U.OptionGroup(var_0, \"count-specific options\")\n    parser.add_option(\"--wide-format-cell-counts\", dest=\"wide_format_cell_counts\",\n                      action=\"store_true\",\n                      default=False,\n                      help=(\"output the cell counts in a wide format \"\n                            \"(rows=genes, columns=cells)\"))\n    parser.add_option_group(var_1)\n    (var_2, var_3) = U.Start(var_0, param_0=param_0, add_group_dedup_options=False)\n    options.per_gene = True  \n    U.validateSamOptions(var_2, var_1=False)\n    if options.random_seed:\n        np.random.seed(options.random_seed)\n    if options.stdin != sys.stdin:\n        var_4 = options.stdin.name\n        options.stdin.close()\n    else:\n        raise ValueError(\"Input on standard in not currently supported\")\n    if options.in_sam:\n        var_5 = \"r\"\n    else:\n        var_5 = \"rb\"\n    var_6 = pysam.Samfile(var_4, var_5)\n    var_7 = U.getTempFilename(dir=options.tmpdir)\n    var_8 = U.openFile(var_7, mode=\"w\")\n    var_9, var_10, var_11 = 0, 0, 0\n    var_12 = options.gene_tag\n    var_13 = None\n    if options.chrom:\n        var_14 = infile.fetch(reference=options.chrom)\n    else:\n        if options.gene_transcript_map:\n            var_13 = sam_methods.getMetaContig2contig(\n                var_6, options.gene_transcript_map)\n            var_15 = \"MC\"\n            var_14 = sam_methods.metafetcher(var_6, var_13, var_15)\n            var_12 = var_15\n        else:\n            var_14 = infile.fetch()\n    var_16 = sam_methods.get_bundles(\n        var_2,\n        only_count_reads=True,\n        metacontig_contig=var_13)\n    var_17 = network.UMIClusterer(options.method)\n    for var_18, var_19, var_20 in var_16(var_14):\n        if var_20 == \"single_read\":\n            continue\n        var_21, var_22 = var_19\n        var_23 = bundle.keys()\n        var_24 = {var_25: var_18[var_25][\"count\"] for var_25 in var_23}\n        var_9 += sum(counts.values())\n        while var_9 >= var_11 + 1000000:\n            var_11 += 1000000\n            U.info(\"Parsed %i input reads\" % var_11)\n        var_26 = var_17(\n            var_24,\n            threshold=options.threshold)\n        var_27 = len(var_26)\n        if options.per_cell:\n            tmpfile.write(\"%s\\n\" % \"\\t\".join((var_21, cell.decode(), str(var_27))))\n        else:\n            tmpfile.write(\"%s\\n\" % \"\\t\".join((var_21, str(var_27))))\n        var_10 += var_27\n    tmpfile.close()\n    if options.per_cell:\n        var_28 = {}\n        with U.openFile(var_7, mode=\"r\") as var_29:\n            var_30 = set()\n            var_31 = set()\n            for var_32 in var_29:\n                var_21, var_22, var_27 = line.strip().split(\"\\t\")\n                genes.add(var_21)\n                cells.add(var_22)\n                if var_21 not in var_28:\n                    var_28[var_21] = {}\n                var_28[var_21][var_22] = var_27\n        if options.wide_format_cell_counts:  \n            options.stdout.write(\n                \"%s\\t%s\\n\" % (\"gene\", \"\\t\".join(sorted(var_31))))\n            for var_21 in sorted(var_30):\n                var_24 = []\n                for var_22 in sorted(var_31):\n                    if var_22 in var_28[var_21]:\n                        counts.append(var_28[var_21][var_22])\n                    else:\n                        counts.append(0)\n                options.stdout.write(\n                    \"%s\\t%s\\n\" % (var_21, \"\\t\".join(map(str, var_24))))\n        else:  \n            options.stdout.write(\"%s\\t%s\\t%s\\n\" % (\"gene\", \"cell\", \"count\"))\n            for var_21 in sorted(var_30):\n                for var_22 in sorted(list(var_28[var_21].keys())):\n                    options.stdout.write(\"%s\\t%s\\t%s\\n\" % (\n                        var_21, var_22, var_28[var_21][var_22]))\n    else:\n        options.stdout.write(\"%s\\t%s\\n\" % (\"gene\", \"count\"))\n        with U.openFile(var_7, mode=\"r\") as var_29:\n            for var_32 in var_29:\n                options.stdout.write(var_32)\n    os.unlink(var_7)\n    for var_33 in bundle_iterator.read_events.most_common():\n        U.info(\"%s: %s\" % (var_33[0], var_33[1]))\n    U.info(\"Number of (post deduplication) reads counted: %i\" % var_10)\n    U.Stop()", "contrast": "import argparse\nimport sys\ndef script_main(argv=None):\n    if argv is None:\n        argv = sys.argv[1:]\n    parser = argparse.ArgumentParser(description='Script main')\n    args = parser.parse_args(argv)", "label": 0}
{"index": "gp137732", "code": "def get_local_annotations(\n            param_0, param_1, param_2=None, param_3=None, param_4=lambda *p: True\n    ):\n        var_0 = []\n        param_2 = () if param_2 is None else param_2\n        try:\n            var_1 = get_local_property(\n                param_1, Annotation.__ANNOTATIONS_KEY__, var_0, param_3=param_3\n            )\n            if not var_1:\n                if ismethod(param_1):\n                    var_2 = get_method_function(param_1)\n                    var_1 = get_local_property(\n                        var_2, Annotation.__ANNOTATIONS_KEY__,\n                        var_0, param_3=param_3\n                    )\n                    if not var_1:\n                        var_1 = get_local_property(\n                            var_2, Annotation.__ANNOTATIONS_KEY__,\n                            var_0\n                        )\n                elif isfunction(param_1):\n                    var_1 = get_local_property(\n                        param_1, Annotation.__ANNOTATIONS_KEY__,\n                        var_0\n                    )\n        except TypeError:\n            raise TypeError('target {0} must be hashable'.format(param_1))\n        for var_3 in var_1:\n            var_4 = isinstance(var_3, param_0)\n            var_5 = not isinstance(var_3, param_2)\n            var_6 = param_4(param_1, param_3, var_3)\n            if var_4 and var_5 and var_6:\n                result.append(var_3)\n        return var_0", "contrast": "def get_local_annotations(cls, target, exclude=(type(None),), ctx=None, select=lambda target, ctx, ann: True):\n    annotations = []\n    if target is None:\n        return annotations\n    if ctx is None:\n        ctx = target\n    for name in dir(target):\n        try:\n            value = getattr(target, name)\n            if isinstance(value, cls):\n                for cls_exclude in exclude:\n                    if isinstance(value, cls_exclude):\n                        break\n                else:\n                    if select(target, ctx, value):\n                        annotations.append(value)\n        except AttributeError:\n            pass\n    return annotations", "label": 0}
{"index": "gp093510", "code": "def get_last_live_chat(param_0):\n        var_0 = datetime.now()\n        var_1 = self.get_query_set()\n        var_1 = lcqs.filter(\n            chat_ends_at__lte=var_0,\n            ).order_by('-chat_ends_at')\n        for var_2 in var_1:\n            if itm.chat_ends_at + timedelta(days=3) > var_0:\n                return var_2\n        return None", "contrast": "import datetime\ndef get_recent_live_chat():\n    recent_chats = LiveChat.objects.filter(end_date__gte=datetime.datetime.today()-datetime.timedelta(days=3))\n    if recent_chats.exists():\n        return recent_chats.latest('end_date')\n    else:\n        return None", "label": 0}
{"index": "gp230468", "code": "def dissociate_values_or_ranges(param_0):\n        var_0 = []\n        for var_1 in vlan_id_range.split(\",\"):\n            if \"-\" in var_1:\n                var_2, var_3 = value_or_range.split(\"-\")\n                vlans.extend(range(int(var_2), int(var_3)+1))\n            else:\n                vlans.append(int(var_1))\n        return var_0\nenet = []\nenet = dissociate_values_or_ranges('1-2,5')\nprint(enet) ", "contrast": "def dissociate_values_or_ranges(self, vlan_id_range):\n        values_or_ranges = vlan_id_range.split(',')\n        vlan_ids = []\n        if len(values_or_ranges) == 1 and '-' not in values_or_ranges[0]:\n            vlan_ids = list(range(1, int(values_or_ranges[0]) + 1))\n        else:\n            for value_or_range in values_or_ranges:\n                value_or_range.strip()\n                if '-' not in value_or_range:\n                    vlan_ids.append(int(value_or_range))\n                else:\n                    start, end = value_or_range.split('-')\n                    range_ids = range(int(start), int(end) + 1)\n                    vlan_ids.extend(range_ids)\n        return vlan_ids", "label": 1}
{"index": "gp010285", "code": "def argmax(param_0, param_1):\n        var_0 = np.absolute(param_1).argmax()\n        return np.unravel_index(var_0, C.shape)", "contrast": "import numpy as np\ndef get_ArgMax(C):\n    indices = np.unravel_index(C.argmax(), C.shape)\n    return indices", "label": 0}
{"index": "gp280105", "code": "def authenticate(param_0):\n    if param_0 == 'VALID_TOKEN':\n        return 'ACS_TOKEN'\n    else:\n        return None", "contrast": "def authenticate_oauth(oauth_token):\n    url = _gen_url('acs/api/v1/auth/login')\n    creds = {\n        'token': oauth_token\n    }\n    response = dcos.http.request('post', url, json=creds)\n    if response.status_code == 200:\n        return response.json()['token']\n    else:\n        return None", "label": 1}
{"index": "gp233221", "code": "from Bio import Entrez\ndef recover_entrez_data(param_0, param_1, param_2, *param_3, **param_4):\n    var_0 = 0\n    var_1 = []\n    while var_0 < param_1:\n        var_2 = Entrez.efetch(db=param_0['db'], webhistory=param_0['query_key'], query_key=param_0['query_key'], var_0=var_0, retmax=param_2, *param_3, **param_4)\n        var_3 = handle.read()\n        results.append(var_3)\n        var_0 += param_2\n    return var_1", "contrast": "def entrez_batch_webhistory(record, expected, batchsize, *fnargs, **fnkwargs):\n    results = []\n    for start in range(0, expected, batchsize):\n        batch_handle = entrez_retry(\n            Entrez.efetch,\n            retstart=start,\n            retmax=batchsize,\n            webenv=record[\"WebEnv\"],\n            query_key=record[\"QueryKey\"],\n            *fnargs,\n            **fnkwargs)\n        batch_record = Entrez.read(batch_handle, validate=False)\n        results.extend(batch_record)\n    return results", "label": 1}
{"index": "gp067437", "code": "def replace(param_0, param_1, param_2, param_3=None):\n        return self.replace_vobject(param_1, readOne(param_2), param_3)", "contrast": "def update_remind_command(uid, icalendar_file):\n    with open('remind_file', 'r+') as file:\n        remind_commands = file.readlines()\n        for i, remind_command in enumerate(remind_commands):\n            if uid in remind_command:\n                remind_commands[i] = f'REM {uid} MSG {icalendar_file}\\n'\n                break\n        file.seek(0)\n        file.writelines(remind_commands)\n        file.truncate()", "label": 0}
{"index": "gp127610", "code": "def removeChild(param_0, param_1, param_2=True):\n        if _is_iterable(param_1):\n            for var_0 in param_1:\n                self.removeChild(param_1=var_0, param_2=param_2)\n            return\n        if not self.childs:\n            return\n        var_1 = None\n        if param_2:\n            var_1 = child.endtag\n        for var_2 in self.childs:\n            if var_2 != param_1:\n                e.removeChild(param_1, param_2)\n                continue\n            if param_2 and var_1 in self.childs:\n                self.childs.remove(var_1)\n            self.childs.remove(var_2)", "contrast": "def remove_child(child, end_tag_too=True):\n    parent = child.parentNode\n    parent.removeChild(child)\n    if end_tag_too:\n        end_tag = parent.ownerDocument.createTextNode(child.outerHTML.rsplit(child.innerHTML, 1)[-1])\n        parent.insertBefore(end_tag, child.nextSibling)", "label": 0}
{"index": "gp255994", "code": "def generate_dic(param_0, param_1):\n    var_0 = {}\n    for var_1 in param_0:\n        var_2, var_3 = var_1\n        dic.setdefault(var_2, []).append(var_3)\n    for var_2, var_3 in dic.items():\n        yield var_2, sep.join(var_3)", "contrast": "def _parsed_items(items, sep=_SEP, **options):\n    parse = _parse if options.get(\"ac_parse_value\") else anyconfig.utils.noop\n    for key, val in items:\n        yield (key, parse(val, sep))", "label": 1}
{"index": "gp270470", "code": "def submit_completion(param_0, param_1):\n    return (BlockCompletion(), True)", "contrast": "def submit_completion(self, block_key, completion):\n        return BlockCompletion.objects.submit_completion(\n            user=self._user,\n            course_key=self._course_key,\n            block_key=block_key,\n            completion=completion\n        )", "label": 1}
{"index": "gp270592", "code": "import os\ndef get_metadata(param_0):\n    return os.stat(param_0)", "contrast": "def get_file_metadata(self, secure_data_path, version=None):\n        if not version:\n            version = \"CURRENT\"\n        payload = {'versionId': str(version)}\n        secret_resp = head_with_retry(str.join('', [self.cerberus_url, '/v1/secure-file/', secure_data_path]),\n                                    params=payload, headers=self.HEADERS)\n        throw_if_bad_response(secret_resp)\n        return secret_resp.headers", "label": 1}
{"index": "gp090805", "code": "def _parse_message(param_0, param_1):\n        try:\n            var_0, var_1 = data.split(':')\n            self.serial_number, self.value = values.split(',')\n            self.value = int(self.value, 16)\n            var_2 = lambda b: self.value & (1 << (b - 1)) > 0\n            self.battery = var_2(2)\n            self.supervision = var_2(3)\n            self.loop[2] = var_2(5)\n            self.loop[1] = var_2(6)\n            self.loop[3] = var_2(7)\n            self.loop[0] = var_2(8)\n        except ValueError:\n            raise InvalidMessageError('Received invalid message: {0}'.format(param_1))", "contrast": "def parse_message(data):\n    try:\n    except:\n        raise InvalidMessageError('Invalid message data')", "label": 0}
{"index": "gp334067", "code": "def add_weight_to_learner(param_0):\n    def weighted_learner(param_0, param_1, param_2):\n        return param_0(param_0, param_1)\n    return weighted_learner", "contrast": "def WeightedLearner(unweighted_learner):\n    def train(dataset, weights):\n        return unweighted_learner(replicated_dataset(dataset, weights))\n    return train", "label": 1}
{"index": "gp095108", "code": "def read_json_document(param_0):\n    if not title.endswith('.json'):\n        juicer.utils.Log.log_warn(\"File name (%s) does not end with '.json', appending it automatically.\" % param_0)\n        param_0 += '.json'\n    if not os.path.exists(param_0):\n        raise IOError(\"Could not find file: '%s'\" % param_0)\n    var_0 = open(param_0, 'r')\n    var_1 = f.read()\n    f.close()\n    return load_json_str(var_1)", "contrast": "import json\ndef read_json_file(file_path):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    return data", "label": 0}
{"index": "gp008084", "code": "def clean_year_month(param_0, param_1, param_2):\n    var_0 = False\n    var_1 = \"The date given was invalid.\"\n    if param_2 not in xrange(1, 13) and param_2 is not None:\n        param_1 = now.month\n        var_0 = var_1\n    while param_1 > 12:\n        param_1 -= 12\n        param_0 += 1\n    while param_1 < 1:\n        param_1 += 12\n        param_0 -= 1\n    param_0, param_1, var_0 = _check_year(param_0, param_1, var_0, var_1)\n    return param_0, param_1, var_0", "contrast": "import datetime\ndef validate_month(month_orig, year):\n    current_year = datetime.datetime.now().year\n    current_month = datetime.datetime.now().month\n    if abs(current_year - year) > 50:\n        return \"Year out of range.\"\n    if month_orig not in range(1, 13):\n        return \"Invalid month.\"\n    if abs(current_year - year) == 50 and month_orig > current_month:\n        return \"Month out of range.\"\n    if month_orig != month_orig % 12:\n        year += (month_orig-1) // 12\n        month_orig = ((month_orig-1) % 12)+1\n    return month_orig, year", "label": 0}
{"index": "gp047497", "code": "def fill_and_wait_models(param_0=EXAMPLES_PER_GENERATION,\n                         param_1=None,\n                         param_2=8,\n                         param_3=100,\n                         param_4=False):\n    param_1 = param_1 or fsdb.golden_chunk_dir()\n    var_0 = ExampleBuffer(param_0)\n    var_1 = fsdb.get_models()[-param_3:]\n    if not param_4:\n        with timer(\"Rsync\"):\n            smart_rsync(var_1[-1][0] - 6)\n    var_2 = tqdm(map(files_for_model, var_1), total=len(var_1))\n    buf.parallel_fill(list(itertools.chain(*var_2)), param_2=param_2)\n    print(\"Filled buffer, watching for new games\")\n    while fsdb.get_latest_model()[0] == var_1[-1][0]:\n        with timer(\"Rsync\"):\n            smart_rsync(var_1[-1][0] - 2)\n        var_3 = tqdm(map(files_for_model, var_1[-2:]), total=len(var_1))\n        buf.update(list(itertools.chain(*var_3)))\n        time.sleep(60)\n    var_4 = fsdb.get_latest_model()\n    print(\"New model!\", var_4[1], \"!=\", var_1[-1][1])\n    print(var_0)\n    buf.flush(os.path.join(param_1, str(var_4[0] + 1) + '.tfrecord.zz'))", "contrast": "def update_ringbuffer():\n    ringbuffer = initialize_ringbuffer()\n    while True:\n        ringbuffer = rsync_and_update_ringbuffer(ringbuffer)\n        if new_model_detected():\n            dump_ringbuffer_for_training(ringbuffer)\n            break", "label": 0}
{"index": "gp084211", "code": "def create_element_tree(param_0=None, param_1=None, **param_2):\n    if param_0 is None:\n        return ElementTree()\n    var_0 = isinstance(param_0, ElementType)\n    var_1 = param_0 if var_0 else Element(param_0)\n    if param_1 is not None:\n        element.text = param_1\n    element.attrib.update(param_2)\n    return ElementTree(var_1)", "contrast": "from xml.etree import ElementTree as ET\ndef create_element_tree(elem_or_name=None, text=None, **attribute_kwargs):\n    root = None\n    if elem_or_name is None:\n        root = ET.Element('')\n    elif isinstance(elem_or_name, ET.Element):\n        root = elem_or_name\n    else:\n        root = ET.Element(elem_or_name)\n    if text:\n        root.text = text\n    for attribute_name, attribute_value in attribute_kwargs.items():\n        root.set(attribute_name, attribute_value)\n    return ET.ElementTree(root)", "label": 0}
{"index": "gp158334", "code": "def del_hyperedge(param_0, param_1):\n        if (param_1 in self.hyperedges()):\n            for var_0 in self.edge_links[param_1]:\n                self.node_links[var_0].remove(param_1)\n            del(self.edge_links[param_1])\n            self.del_edge_labeling(param_1)\n            self.graph.del_node((param_1,'h'))", "contrast": "def delete_hyperedge(hyperedge):\n    del hyperedge", "label": 0}
{"index": "gp261519", "code": "def count_instances(param_0):\n    var_0 = {}\n    for var_1 in param_0:\n        var_2 = obj.__class__.__name__\n        var_0[var_2] = count.get(var_2, 0) + 1\n    return var_0", "contrast": "def gcstats():\n    all = gc.get_objects()\n    _stats = {}\n    for obj in all:\n        K = type(obj)\n        if K is StatsDelta:\n            continue  \n        elif K is InstanceType:  \n            K = getattr(obj, '__class__', K)\n        K = str(K)\n        try:\n            _stats[K] += 1\n        except KeyError:\n            _stats[K] = 1\n    del all\n    return _stats", "label": 1}
{"index": "gp236307", "code": "def delete_template(param_0):\n    return status_code", "contrast": "def delete_template(self, template_id):\n        url = self.TEMPLATE_DELETE_URL\n        request = self._get_request()\n        response = request.post(url + template_id, get_json=False)\n        return response", "label": 1}
{"index": "gp102898", "code": "def list_build_configurations_for_product_version(param_0, param_1, param_2=200, param_3=0, param_4=\"\", param_5=\"\"):\n    var_0 = list_build_configurations_for_project_raw(param_0, param_1, param_2, param_3, param_4, param_5)\n    if var_0:\n        return utils.format_json_list(var_0)", "contrast": "def get_build_configurations(product_version):\n    build_configs = []\n    return build_configs", "label": 0}
{"index": "gp131053", "code": "def load(param_0, param_1=None, param_2=True):\n        if param_1 is not None:\n            self.cause = param_1\n            if param_2:\n                self.message = error_message(param_0, self.base_message, param_1)\n        self.exc_type, self.exc_value, self.exc_traceback = sys.exc_info()\n        self.traceback_formatted = traceback.format_exc()\n        self.traceback = traceback.extract_tb(self.exc_traceback)", "contrast": "def load_exception_data():\n    return sys.exc_info()", "label": 0}
{"index": "gp227654", "code": "allowed_types = []\ndef add_type(param_0, param_1):\n    allowed_types.append((param_0, param_1))", "contrast": "def addAllowedType(self, assoc_type, session_type=None):\n        if self.allowed_types is None:\n            self.allowed_types = []\n        if session_type is None:\n            available = getSessionTypes(assoc_type)\n            if not available:\n                raise ValueError('No session available for association type %r'\n                                 % (assoc_type,))\n            for session_type in getSessionTypes(assoc_type):\n                self.addAllowedType(assoc_type, session_type)\n        else:\n            checkSessionType(assoc_type, session_type)\n            self.allowed_types.append((assoc_type, session_type))", "label": 1}
{"index": "gp041929", "code": "def remove_rules(param_0, param_1):\n        var_0 = []\n        param_1 = description.lower()\n        for var_1 in range(0, len(self.extract_rules)):\n            if self.extract_rules[var_1]['regex'].search(param_1):\n                rm.append(var_1)\n        for var_1 in var_0:\n            self.extract_rules.pop(var_1)\n        return len(var_0)", "contrast": "def remove_rules(description):\n    count = 0\n    for rule in rules_list:\n        if rule.description == description:\n            rules_list.remove(rule)\n            count += 1\n    return count", "label": 0}
{"index": "gp041953", "code": "def get_my_contacts(param_0):\n        var_0 = self.wapi_functions.getMyContacts()\n        return [Contact(var_1, param_0) for var_1 in var_0]", "contrast": "def fetch_added_contacts() -> list:\n    contacts = [\n        {'name': 'Alice', 'phone': '123456789'},\n        {'name': 'Bob', 'phone': '987654321'},\n        {'name': 'Charlie', 'phone': '456123789'}\n    ]\n    return contacts", "label": 0}
{"index": "gp037643", "code": "def _gather_buffer_space():\n    if HAS_PSUTIL and psutil.version_info >= (0, 6, 0):\n        var_0 = psutil.virtual_memory().total\n    else:\n        import platform\n        import salt.grains.core\n        var_1 = {'kernel': platform.system()}\n        var_2 = salt.grains.core._memdata(var_1)\n        var_0 = var_2['mem_total'] * 1024 * 1024\n    return max([var_0 * 0.05, 10 << 20])", "contrast": "import psutil\ndef calculate_buffer_space():\n    system_cpu_usage = psutil.cpu_percent()\n    system_memory_usage = psutil.virtual_memory().percent\n    total_disk_space = psutil.disk_usage('/').total\n    free_disk_space = psutil.disk_usage('/').free\n    used_disk_space = total_disk_space - free_disk_space\n    buffer_space = used_disk_space * (system_cpu_usage + system_memory_usage)\n    return buffer_space", "label": 0}
{"index": "gp275798", "code": "def compose_src_dict(param_0:str, param_1:bool=False, param_2:bool=False, param_3:int=10) -> dict:\n    var_0 = {\n        'name': param_0,\n        'paramsonly': param_1,\n        'reoptimize': param_2,\n        'npts': param_3\n    }\n    return var_0", "contrast": "def get_src_model(self, name, paramsonly=False, reoptimize=False,\n                      npts=None, **kwargs):\n        self.logger.debug('Generating source dict for ' + name)\n        optimizer = kwargs.get('optimizer', self.config['optimizer'])\n        if npts is None:\n            npts = self.config['gtlike']['llscan_npts']\n        name = self.get_source_name(name)\n        source = self.like[name].src\n        spectrum = source.spectrum()\n        normPar = self.like.normPar(name)\n        src_dict = defaults.make_default_dict(defaults.source_flux_output)\n        src_dict.update({'name': name,\n                         'pivot_energy': 1000.,\n                         'ts': np.nan,\n                         'loglike': np.nan,\n                         'npred': 0.0,\n                         'npred_wt': 0.0,\n                         'loglike_scan': np.nan * np.ones(npts),\n                         'dloglike_scan': np.nan * np.ones(npts),\n                         'eflux_scan': np.nan * np.ones(npts),\n                         'flux_scan': np.nan * np.ones(npts),\n                         'norm_scan': np.nan * np.ones(npts),\n                         })\n        src_dict.update(gtutils.gtlike_spectrum_to_vectors(spectrum))\n        src_dict['spectral_pars'] = gtutils.get_function_pars_dict(spectrum)\n        src_dict['model_counts'] = self.model_counts_spectrum(\n            name, summed=True)\n        src_dict['model_counts_wt'] = self.model_counts_spectrum(\n            name, summed=True, weighted=True)\n        src_dict['npred'] = self.like.NpredValue(str(name))\n        try:\n            src_dict['npred_wt'] = self.like.NpredValue(str(name), True)\n        except (TypeError, NotImplementedError):\n            src_dict['npred_wt'] = src_dict['npred']\n        try:\n            thesrc = self.like[name]\n            src_dict['flux'] = self.like.flux(name, self.energies[0],\n                                              self.energies[-1])\n            src_dict['flux100'] = self.like.flux(name, 100., 10 ** 5.5)\n            src_dict['flux1000'] = self.like.flux(name, 1000., 10 ** 5.5)\n            src_dict['flux10000'] = self.like.flux(name, 10000., 10 ** 5.5)\n            src_dict['eflux'] = self.like.energyFlux(name,\n                                                     self.energies[0],\n                                                     self.energies[-1])\n            src_dict['eflux100'] = self.like.energyFlux(name, 100.,\n                                                        10 ** 5.5)\n            src_dict['eflux1000'] = self.like.energyFlux(name, 1000.,\n                                                         10 ** 5.5)\n            src_dict['eflux10000'] = self.like.energyFlux(name, 10000.,\n                                                          10 ** 5.5)\n            src_dict['dnde'] = self.like[name].spectrum()(\n                pyLike.dArg(src_dict['pivot_energy']))\n            src_dict['dnde100'] = self.like[name].spectrum()(\n                pyLike.dArg(100.))\n            src_dict['dnde1000'] = self.like[name].spectrum()(\n                pyLike.dArg(1000.))\n            src_dict['dnde10000'] = self.like[name].spectrum()(\n                pyLike.dArg(10000.))\n            if normPar.getValue() == 0:\n                normPar.setValue(1.0)\n                dnde_index = -get_spectral_index(self.like[name],\n                                                 src_dict['pivot_energy'])\n                dnde100_index = -get_spectral_index(self.like[name],\n                                                    100.)\n                dnde1000_index = -get_spectral_index(self.like[name],\n                                                     1000.)\n                dnde10000_index = -get_spectral_index(self.like[name],\n                                                      10000.)\n                normPar.setValue(0.0)\n            else:\n                dnde_index = -get_spectral_index(self.like[name],\n                                                 src_dict['pivot_energy'])\n                dnde100_index = -get_spectral_index(self.like[name],\n                                                    100.)\n                dnde1000_index = -get_spectral_index(self.like[name],\n                                                     1000.)\n                dnde10000_index = -get_spectral_index(self.like[name],\n                                                      10000.)\n            src_dict['dnde_index'] = dnde_index\n            src_dict['dnde100_index'] = dnde100_index\n            src_dict['dnde1000_index'] = dnde1000_index\n            src_dict['dnde10000_index'] = dnde10000_index\n        except Exception:\n            self.logger.error('Failed to update source parameters.',\n                              exc_info=True)\n        if not self.get_free_source_params(name) or paramsonly:\n            return src_dict\n        emax = 10 ** 5.5\n        try:\n            src_dict['flux_err'] = self.like.fluxError(name,\n                                                       self.energies[0],\n                                                       self.energies[-1])\n            src_dict['flux100_err'] = self.like.fluxError(name, 100., emax)\n            src_dict['flux1000_err'] = self.like.fluxError(name, 1000., emax)\n            src_dict['flux10000_err'] = self.like.fluxError(name, 10000., emax)\n            src_dict['eflux_err'] =                self.like.energyFluxError(name, self.energies[0],\n                                          self.energies[-1])\n            src_dict['eflux100_err'] = self.like.energyFluxError(name, 100.,\n                                                                 emax)\n            src_dict['eflux1000_err'] = self.like.energyFluxError(name, 1000.,\n                                                                  emax)\n            src_dict['eflux10000_err'] = self.like.energyFluxError(name, 10000.,\n                                                                   emax)\n        except Exception:\n            pass\n        lnlp = self.profile_norm(name, savestate=True,\n                                 reoptimize=reoptimize, npts=npts,\n                                 optimizer=optimizer)\n        src_dict['loglike_scan'] = lnlp['loglike']\n        src_dict['dloglike_scan'] = lnlp['dloglike']\n        src_dict['eflux_scan'] = lnlp['eflux']\n        src_dict['flux_scan'] = lnlp['flux']\n        src_dict['norm_scan'] = lnlp['xvals']\n        src_dict['loglike'] = np.max(lnlp['loglike'])\n        flux_ul_data = utils.get_parameter_limits(\n            lnlp['flux'], lnlp['dloglike'])\n        eflux_ul_data = utils.get_parameter_limits(\n            lnlp['eflux'], lnlp['dloglike'])\n        if normPar.getValue() == 0:\n            normPar.setValue(1.0)\n            flux = self.like.flux(name, self.energies[0], self.energies[-1])\n            flux100 = self.like.flux(name, 100., emax)\n            flux1000 = self.like.flux(name, 1000., emax)\n            flux10000 = self.like.flux(name, 10000., emax)\n            eflux = self.like.energyFlux(name, self.energies[0],\n                                         self.energies[-1])\n            eflux100 = self.like.energyFlux(name, 100., emax)\n            eflux1000 = self.like.energyFlux(name, 1000., emax)\n            eflux10000 = self.like.energyFlux(name, 10000., emax)\n            flux100_ratio = flux100 / flux\n            flux1000_ratio = flux1000 / flux\n            flux10000_ratio = flux10000 / flux\n            eflux100_ratio = eflux100 / eflux\n            eflux1000_ratio = eflux1000 / eflux\n            eflux10000_ratio = eflux10000 / eflux\n            normPar.setValue(0.0)\n        else:\n            flux100_ratio = src_dict['flux100'] / src_dict['flux']\n            flux1000_ratio = src_dict['flux1000'] / src_dict['flux']\n            flux10000_ratio = src_dict['flux10000'] / src_dict['flux']\n            eflux100_ratio = src_dict['eflux100'] / src_dict['eflux']\n            eflux1000_ratio = src_dict['eflux1000'] / src_dict['eflux']\n            eflux10000_ratio = src_dict['eflux10000'] / src_dict['eflux']\n        src_dict['flux_ul95'] = flux_ul_data['ul']\n        src_dict['flux100_ul95'] = flux_ul_data['ul'] * flux100_ratio\n        src_dict['flux1000_ul95'] = flux_ul_data['ul'] * flux1000_ratio\n        src_dict['flux10000_ul95'] = flux_ul_data['ul'] * flux10000_ratio\n        src_dict['eflux_ul95'] = eflux_ul_data['ul']\n        src_dict['eflux100_ul95'] = eflux_ul_data['ul'] * eflux100_ratio\n        src_dict['eflux1000_ul95'] = eflux_ul_data['ul'] * eflux1000_ratio\n        src_dict['eflux10000_ul95'] = eflux_ul_data['ul'] * eflux10000_ratio\n        fd = None\n        try:\n            fd = FluxDensity.FluxDensity(self.like, name)\n            src_dict['covar'] = fd.covar\n        except RuntimeError:\n            pass\n        if fd and len(src_dict['covar']) and src_dict['covar'].ndim >= 1:\n            loge = np.linspace(self.log_energies[0],\n                               self.log_energies[-1], 50)\n            src_dict['model_flux'] = self.bowtie(name, fd=fd, loge=loge)\n            src_dict['dnde100_err'] = fd.error(100.)\n            src_dict['dnde1000_err'] = fd.error(1000.)\n            src_dict['dnde10000_err'] = fd.error(10000.)\n            src_dict['pivot_energy'] = src_dict['model_flux']['pivot_energy']\n            e0 = src_dict['pivot_energy']\n            src_dict['dnde'] = self.like[name].spectrum()(pyLike.dArg(e0))\n            src_dict['dnde_err'] = fd.error(e0)\n        if not reoptimize:\n            src_dict['ts'] = self.like.Ts2(name, reoptimize=reoptimize)\n        else:\n            src_dict['ts'] = -2.0 * lnlp['dloglike'][0]\n        return src_dict", "label": 1}
{"index": "gp075699", "code": "def normalize_genotypes(param_0):\n    param_0 = genotypes.genotypes\n    return (param_0 - np.nanmean(param_0)) / np.nanstd(param_0)", "contrast": "def normalize_genotypes(genotypes):\n    normalized_genotypes = []\n    for genotype in genotypes:\n        normalized_genotype = genotype/sum(genotype)\n        normalized_genotypes.append(normalized_genotype)\n    return numpy.array(normalized_genotypes)", "label": 0}
{"index": "gp230317", "code": "from flask_cors import CORS\ndef enable_cors(param_0, param_1):\n    return CORS(param_0, **param_1)", "contrast": "def add(self,\n            routing_entity,\n            config: _ConfigType = None,\n            webview: bool=False):\n        if webview:\n            warnings.warn('webview argument is deprecated, '\n                          'views are handled authomatically without '\n                          'extra settings',\n                          DeprecationWarning,\n                          stacklevel=2)\n        return self._cors_impl.add(routing_entity, config)", "label": 1}
{"index": "gp138789", "code": "def p_case(param_0,param_1):\n    if len(param_1)==4: param_1[0] = CaseX(param_1[2],None)\n    elif len(param_1)==6: param_1[0] = CaseX(param_1[2],param_1[4])\n    else: raise NotImplementedError('unk_len',len(param_1)) ", "contrast": "def expression(case, whenlist, else_case=None):\n    if else_case is not None:\n        return case.get(whenlist[-1], else_case)\n    else:\n        return case.get(whenlist[-1])", "label": 0}
{"index": "gp152088", "code": "def adb_cmd(param_0, param_1, **param_2):\n        param_2['timeout'] = kwargs.get('timeout', self._adb_shell_timeout)\n        if isinstance(param_1, list) or isinstance(param_1, tuple):\n            return self.adb_device.run_cmd(*list(param_1), **param_2)\n        return self.adb_device.run_cmd(param_1, **param_2)", "contrast": "import subprocess\ndef adb(command):\n    if isinstance(command, str):\n        command = command.split()\n    adb_cmd = ['adb'] + command\n    output = subprocess.check_output(adb_cmd)\n    return output.decode('utf-8')", "label": 0}
{"index": "gp106177", "code": "def mergeidngram(param_0, param_1, param_2=3, param_3=False, param_4=False):\n    var_0 = ['mergeidngram']\n    if param_2:\n        cmd.extend(['-n', param_2])\n    if param_3:\n        cmd.append('-ascii_input')\n    if param_4:\n        cmd.append('-ascii_output')\n    if len(input_file) > 1:\n        raise MergeError(\"mergeidngram needs at least 1 input file\")\n    cmd.extend(param_1)\n    var_0 = [str(var_1) for var_1 in var_0]\n    with open(param_0,'w+') as var_2:\n        with  output_to_debuglogger() as var_3:\n            var_4 = subprocess.call(var_0, stdout=var_2, stderr=var_3)\n    var_5 = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(var_0), var_4))\n    if var_4 != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (var_0[0], var_4))", "contrast": "import struct\ndef merge_id_ngram_files(files, is_ascii=False):\n    ngram_set = set()\n    mode = 'rb' if not is_ascii else 'r'\n    for file in files:\n        with open(file, mode) as f:\n            if not is_ascii:\n                size_bytes = f.read(4)\n                while size_bytes:\n                    size = struct.unpack('i', size_bytes)[0]\n                    ngram = f.read(size).decode('utf-8')\n                    ngram_set.add(ngram)\n                    size_bytes = f.read(4)\n            else:\n                for line in f:\n                    ngram_set.add(line.strip())\n    return ngram_set", "label": 0}
{"index": "gp016388", "code": "def find_gui_and_backend(param_0=None):\n    import matplotlib\n    if param_0 and param_0 != 'auto':\n        var_0 = backends[param_0]\n    else:\n        var_0 = matplotlib.rcParams['backend']\n        param_0 = backend2gui.get(var_0, None)\n    return param_0, var_0", "contrast": "def get_gui_backend(gui: str) -> Tuple[str, str]:\n    backends_dict = {\n        'tk': 'TkAgg',\n        'gtk': 'GTKAgg',\n        'wx': 'WXAgg',\n        'qt': 'Qt4Agg',\n        'qt4': 'Qt4Agg',\n        'inline': 'module://IPython.zmq.pylab.backend_inline'\n    }\n    return gui, backends_dict[gui.lower()]", "label": 0}
{"index": "gp296281", "code": "def rename_and_alter_column(param_0: str, param_1: str, param_2: str, param_3: str):\n    var_0 = f\"ALTER TABLE {param_0} RENAME COLUMN {param_1} TO {param_2};\"\n    var_0 += f\"ALTER TABLE {param_0} ALTER COLUMN {param_2} {param_3};\"\n    execute_query(var_0)", "contrast": "def change_column_if_table_exists(self,\n                                      tablename: str,\n                                      oldfieldname: str,\n                                      newfieldname: str,\n                                      newdef: str) -> Optional[int]:\n        if not self.table_exists(tablename):\n            return None\n        if not self.column_exists(tablename, oldfieldname):\n            return None\n        sql = \"ALTER TABLE {t} CHANGE COLUMN {old} {new} {newdef}\".format(\n            t=tablename,\n            old=oldfieldname,\n            new=newfieldname,\n            newdef=newdef,\n        )\n        log.info(sql)\n        return self.db_exec_literal(sql)", "label": 1}
{"index": "gp252254", "code": "from decimal import Decimal\ndef create_decimal_field(param_0=0, param_1=False, param_2=True, param_3=True, param_4=None):\n    return {'type': Decimal, 'default': param_0, 'required': param_1, 'repr': param_2, 'cmp': param_3, 'key': param_4}", "contrast": "def DecimalField(default=NOTHING, required=True, repr=True, cmp=True,\n                 key=None):\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, Decimal)\n    return attrib(default=default, converter=lambda x: Decimal(x),\n                  validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))", "label": 1}
{"index": "gp221865", "code": "def deserialize(param_0, param_1):\n    if isinstance(param_0, list):\n        return dict(param_0)\n    elif isinstance(param_0, dict):\n        return {param_1(var_0): var_1 for var_0, var_1 in attr.items()}\n    else:\n        raise TypeError(\"Parameter 'attr' must be a dictionary or list of key-value pairs.\")", "contrast": "def deserialize_dict(self, attr, dict_type):\n        if isinstance(attr, list):\n            return {x['key']: self.deserialize_data(x['value'], dict_type) for x in attr}\n        if isinstance(attr, ET.Element):\n            attr = {el.tag: el.text for el in attr}\n        return {k: self.deserialize_data(v, dict_type) for k, v in attr.items()}", "label": 1}
{"index": "gp048979", "code": "def DownloadDir(param_0, param_1, param_2=8192, param_3=True):\n  if not os.path.isdir(param_1):\n    os.makedirs(param_1)\n  var_0 = aff4.FACTORY.Open(param_0)\n  for var_1 in fd.OpenChildren():\n    if param_3:\n      var_2 = utils.JoinPath(param_1, child.urn.Path())\n      var_2 = os.path.dirname(var_2)\n      if not os.path.isdir(var_2):\n        os.makedirs(var_2)\n      var_3 = os.path.join(var_2, child.urn.Basename())\n    else:\n      var_3 = os.path.join(param_1, child.urn.Basename())\n    logging.info(u\"Downloading %s to %s\", child.urn, var_3)\n    with open(var_3, \"wb\") as var_4:\n      try:\n        var_5 = child.Read(param_2)\n        while var_5:\n          out_fd.write(var_5)\n          var_5 = child.Read(param_2)\n      except IOError as e:\n        logging.error(\"Failed to read %s. Err: %s\", child.urn, e)", "contrast": "import os\nimport stat\nfrom typing import List\ndef download_aff4(aff4_path: str, output_dir: str, bufsize: int = 8192, preserve_path: bool = False) -> List[str]:\n    from pyaff4 import data_store\n    from pyaff4 import aff4_image\n    os.makedirs(output_dir, exist_ok=True)\n    aff4_files = []\n    with aff4_image.ImageFactoryOpen(aff4_path) as aff4:\n        for child in aff4:\n            if isinstance(child, aff4_image.AFF4Volume):\n                if not preserve_path:\n                    continue\n                prefix = output_dir if output_dir.endswith(os.sep) else f\"{output_dir}{os.sep}\"\n                path = f\"{prefix}{child.urn.path}\"\n                os.makedirs(path, exist_ok=True)\n                for stream in child:\n                    stream_path = os.path.join(path, os.path.basename(stream.urn.value))\n                    with open(stream_path, \"wb+\") as f:\n                        for chunk in stream.Read(len(stream)):\n                            f.write(chunk)\n                            aff4_files.append(stream_path)                       \n            else:\n                prefix = output_dir if output_dir.endswith(os.sep) else f\"{output_dir}{os.sep}\"\n                file_path = f\"{prefix}{child.urn.path}\"\n                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n                with open(file_path, \"wb+\") as f:\n                    for chunk in child.Read(len(child)):\n                        f.write(chunk)\n                        aff4_files.append(file_path)\n                os.chmod(file_path, stat.S_IRUSR | stat.S_IWUSR)\n    return aff4_files", "label": 0}
{"index": "gp034582", "code": "def getObjectTypeBit(param_0, param_1):\n        if isinstance(param_1, string_types):\n            param_1 = t.upper()\n            try:\n                return self.objectType[param_1]\n            except KeyError:\n                raise CommandExecutionError((\n                    'Invalid object type \"{0}\".  It should be one of the following:  {1}'\n                    ).format(param_1, ', '.join(self.objectType)))\n        else:\n            return param_1", "contrast": "def get_bit_value(string_obj):\n    return ''.join(format(ord(c), '08b') for c in string_obj)", "label": 0}
{"index": "gp266476", "code": "def import_scraper(param_0):\n    from scraper_registry import registry\n    if param_0 not in registry:\n        raise ValueError('Scraper \"{}\" not found in registry.'.format(param_0))\n    var_0 = registry[param_0]\n    var_1 = scraper_module.Scraper()\n    return var_1", "contrast": "def get_scraper(mod_path, scraper_type):\n    try:\n        module = importlib.import_module(mod_path)\n    except ImportError as e:\n        raise ScrapeError(\"could not import %s\" % mod_path, e)\n    ScraperClass = None\n    for k, v in module.__dict__.items():\n        if k.startswith('_'):\n            continue\n        if getattr(v, 'scraper_type', None) == scraper_type:\n            if ScraperClass:\n                raise ScrapeError(\"two %s scrapers found in module %s: %s %s\" %\n                                  (scraper_type, mod_path, ScraperClass, k))\n            ScraperClass = v\n    if not ScraperClass:\n        raise ScrapeError(\"no %s scraper found in module %s\" % (\n            scraper_type, mod_path))\n    return ScraperClass", "label": 1}
{"index": "gp153713", "code": "def newPage(param_0, param_1=-1, param_2=595, param_3=842):\n    doc._newPage(param_1, param_2=param_2, param_3=param_3)\n    return param_0[param_1]", "contrast": "def create_page():\n    return Page()", "label": 0}
{"index": "gp320039", "code": "def parse_complex_list(param_0):\n    var_0 = {}\n    for var_1 in param_0:\n        for var_2, var_3 in complex_struct.items():\n            if var_2 in var_0:\n                var_0[var_2] += str(var_3)\n            else:\n                var_0[var_2] = str(var_3)\n    return var_0", "contrast": "def _parse_complex_list(self, prop):\n        xpath_root = self._get_xroot_for(prop)\n        xpath_map = self._data_structures[prop]\n        return parse_complex_list(self._xml_tree, xpath_root, xpath_map, prop)", "label": 1}
{"index": "gp040768", "code": "def formatResults(param_0, param_1, param_2):\n        var_0 = \"\"\n        if param_1 == 'add':\n            var_0 += \"user(s) added:\\n\"\n            for var_1 in param_2:\n                if isinstance(var_1, str):\n                    var_0 += \"identifier: %s\\n\" % var_1\n                else:\n                    var_0 += \"uid: %d\\n\\n\" % var_1\n        elif param_1 == 'remove':\n            var_0 += \"user(s) removed:\\n\"\n            for var_1 in param_2:\n                if var_1:\n                    var_0 += \"identifier: %s\\n\" % (var_1)\n        elif param_1 == 'update':\n            var_0 += \"user(s) updated:\\n\"\n            for var_1 in param_2:\n                if var_1:\n                    var_0 += \"identifier: %s\\n\" % (var_1)\n        elif param_1 == 'get':\n            var_0 += \"user(s) found:\\n\"\n            for var_1 in param_2:\n                if var_1:\n                    for var_2 in sorted(user.keys()):\n                        if var_2 != 'bb_password':\n                            var_0 += \"%s: %s\\n\" % (var_2, var_1[var_2])\n                    var_0 += \"\\n\"\n                else:\n                    var_0 += \"no match found\\n\"\n        return var_0", "contrast": "def format_db_results(op, results):\n    if op == 'add':\n        return f\"{results[0]} added successfully with ID {results[1]}.\"\n    elif op == 'remove':\n        return f\"{results[0]} with ID {results[1]} removed successfully.\"\n    elif op == 'update':\n        return f\"{results[0]} with ID {results[1]} updated successfully.\"\n    elif op == 'get':\n        if len(results) == 0:\n            return \"No results found.\"\n        else:\n            return \"\\n\".join([f\"{i[0]} - {i[1]}\" for i in results])", "label": 0}
{"index": "gp132108", "code": "def get_box_files(param_0, param_1):\n  var_0 = '/'.join([self.api_uri,\n      self.boxes_suffix,\n      param_1,\n      self.files_suffix\n      ])\n  return self._req('get', var_0)", "contrast": "def get_file_infos(box_key):\n    file_infos = [{'name': 'file1.txt', 'size': '10KB'}, {'name': 'file2.png', 'size': '2MB'}, {'name': 'file3.mp3', 'size': '5MB'}]\n    return 200, file_infos", "label": 0}
{"index": "gp311257", "code": "def apply_config_overrides(param_0, param_1):\n    for var_0, var_1 in overrides.items():\n        var_2 = key.split('.')\n        var_3 = param_0\n        for var_4 in var_2[:-1]:\n            var_3 = getattr(var_3, var_4)\n        setattr(var_3, var_2[-1], var_1)", "contrast": "def handle_overrides(graph, overrides):\n    for key in overrides:\n        levels = key.split('.')\n        part = graph\n        for lvl in levels[:-1]:\n            try:\n                part = part[lvl]\n            except KeyError:\n                raise KeyError(\"'%s' override failed at '%s'\", (key, lvl))\n        try:\n            part[levels[-1]] = overrides[key]\n        except KeyError:\n            raise KeyError(\"'%s' override failed at '%s'\", (key, levels[-1]))", "label": 1}
{"index": "gp025048", "code": "async def on_error(param_0, param_1, *param_2, **param_3):\n        print('Ignoring exception in {}'.format(param_1), file=sys.stderr)\n        traceback.print_exc()", "contrast": "async def default_error_handler(self, event, *args, **kwargs):\n    print(f\"Ignoring exception in {event}\", file=sys.stderr)\n    traceback.print_exc()", "label": 0}
{"index": "gp092755", "code": "def get_blocks(param_0):\n        self.process_triple_quoted_doc_string()\n        var_0 = 0\n        for var_1, var_2 in self._blocks:\n            if var_1 > 0:\n                var_0 += 1\n            yield var_1, var_2\n        if self.expects_body_or_pass and var_0 == 0:\n            yield 1, \"pass\"\n            return", "contrast": "class MyClass:\n    def my_func(self):\n        self.process_triple_quoted_doc_string()\n        pass", "label": 0}
{"index": "gp277188", "code": "def update_dict(param_0, param_1):\n    for var_0, var_1 in d2.items():\n        if var_0 in param_0:\n            if isinstance(var_1, dict) and isinstance(param_0[var_0], dict):\n                update_dict(param_0[var_0], var_1)\n            elif '__delete__' in var_1:\n                del param_0[var_0]\n            else:\n                param_0[var_0] = var_1\n        else:\n            param_0[var_0] = var_1\n    return param_0", "contrast": "def update(d1, d2):\n    NoneType = type(None)\n    if d2.get(\"__delete__\", False):\n        return {}\n    for k, v in d2.items():\n        if isinstance(v, dict):\n            if v.get(\"__delete__\", False):\n                del d1[k]\n            else:\n                d1[k] = update(d1.get(k, {}), v)\n        elif isinstance(v, (tuple, list)) and all(isinstance(li, (NoneType, dict)) for li in v):\n            orig_list = d1.get(k, [])\n            new_list = []\n            pairs = list(zip_longest(orig_list, v, fillvalue=None))\n            for orig_item, new_item in pairs:\n                if orig_item is None:\n                    orig_item = {}  \n                if new_item is None:\n                    new_item = {}\n                if new_item.get(\"__delete__\", False):\n                    d = None  \n                else:\n                    d = update(orig_item, new_item)\n                if d is not None:\n                    new_list.append(d)\n            d1[k] = new_list\n        else:\n            if k in d1 and v == \"__delete__\":\n                del d1[k]\n            else:\n                d1[k] = v\n    return d1", "label": 1}
{"index": "gp172381", "code": "def resolve_forward_references(param_0, param_1=None):\n    if param_1 is None:\n        param_1 = {}\n    for var_0, var_1 in routes.items():\n        var_2 = []\n        for var_3 in var_1:\n            if var_3 in param_0:\n                var_4 = resolve_forward_references({var_3: param_0[var_3]}, param_1)[var_3]\n            else:\n                var_4 = var_3\n            resolved_dependencies.append(var_4)\n        param_1[var_0] = var_2\n    return param_1", "contrast": "def _populate_route_attributes(self):\n        route_schema = self._validate_stone_cfg()\n        self.api.add_route_schema(route_schema)\n        for namespace in self.api.namespaces.values():\n            env = self._get_or_create_env(namespace.name)\n            for route in namespace.routes:\n                self._populate_route_attributes_helper(env, route, route_schema)", "label": 1}
{"index": "gp148687", "code": "def _validate_label(param_0, param_1):\n        var_0 = compile(\"^[a-z]{1}$\")\n        var_1 = compile(\"^[0]{1}$|^[1-9]{1,2}$\")\n        var_2 = compile(\"^[a-zA-Z ]{1,}$\")\n        if not match(var_0, param_1)            and not match(var_1, param_1)                and not match(var_2, param_1):\n                    raise InvalidLabelError(\n                        \"{} is not a valid label\".format(param_1)\n                    )\n        return param_1", "contrast": "def validate_label(label):\n    if not isinstance(label, str):\n        raise TypeError(\"Label must be a string.\")\n    if not label.isalnum():\n        raise ValueError(\"Label must be alphanumeric.\")", "label": 0}
{"index": "gp011125", "code": "def wrapHeart(param_0):\n    var_0 = taservice.MultiService()\n    service.setServiceParent(var_0)\n    maybeAddHeart(var_0)\n    return var_0", "contrast": "from twisted.application.service import MultiService\nfrom twisted.application.internet import TimerService, TCPServer\ndef wrap_service_with_heart(service, interval_seconds):\n    multi_service = MultiService()\n    timer_service = TimerService(interval_seconds, service.checkHeart)\n    tcp_service = TCPServer(0, service, interface=\"127.0.0.1\")\n    multi_service.addService(timer_service)\n    multi_service.addService(tcp_service)\n    return multi_service", "label": 0}
{"index": "gp139574", "code": "def save(param_0, param_1=False, *param_2, **param_3):\n        if param_1 or (self.id is None and self.site_id is None):\n            self.site_id = current_site_id()\n        super(SiteRelated, param_0).save(*param_2, **param_3)", "contrast": "def set_site(record, current_site, update_site=False):\n    if update_site or not record.site:\n        record.site = current_site", "label": 0}
{"index": "gp132995", "code": "def childgroup(param_0, param_1):\n        var_0 = getattr(param_0, \"grid\", None)\n        var_1 = getattr(param_0, \"named_grid\", None)\n        if var_0 is not None:\n            var_2 = self._childgroup(field.children, var_0)\n        elif var_1 is not None:\n            var_2 = self._childgroup_by_name(field.children, var_1)\n        else:\n            raise AttributeError(u\"Missing the grid or named_grid argument\")\n        return var_2", "contrast": "def get_row_fields(field):\n    row_fields = []\n    return row_fields", "label": 0}
{"index": "gp251994", "code": "def parse_team_abbreviation(param_0):\n    var_0 = param_0(\"div.sidearm-team-name>span.longname\")\n    var_1 = team_name_tag.attr(\"class\").split(\"-\")[-1].upper()\n    return var_1", "contrast": "def _parse_team_abbreviation(self, stats):\n        team_tag = stats(PLAYER_SCHEME['team_abbreviation'])\n        team = re.sub(r'.*/cbb/schools/', '', str(team_tag('a')))\n        team = re.sub(r'/.*', '', team)\n        return team", "label": 1}
{"index": "gp293650", "code": "import base64\ndef authorize_to_registry(param_0: str, param_1: str, param_2: str):\n    var_0 = f\"{param_0}:{param_1}\"\n    var_1 = base64.b64encode(auth_string.encode('utf-8')).decode('utf-8')\n    var_2 = {\"Authorization\": f\"Basic {var_1}\"}\n    var_3 = {\"scope\": param_2}\n    return {\"headers\": var_2, \"params\": var_3}", "contrast": "async def login(\n        sess: aiohttp.ClientSession,\n        registry_url: yarl.URL,\n        credentials: dict,\n        scope: str) -> dict:\n    if credentials.get('username') and credentials.get('password'):\n        basic_auth = aiohttp.BasicAuth(\n            credentials['username'], credentials['password'],\n        )\n    else:\n        basic_auth = None\n    realm = registry_url / 'token'  \n    service = 'registry'            \n    async with sess.get(registry_url / 'v2/', auth=basic_auth) as resp:\n        ping_status = resp.status\n        www_auth_header = resp.headers.get('WWW-Authenticate')\n        if www_auth_header:\n            match = re.search(r'realm=\"([^\"]+)\"', www_auth_header)\n            if match:\n                realm = match.group(1)\n            match = re.search(r'service=\"([^\"]+)\"', www_auth_header)\n            if match:\n                service = match.group(1)\n    if ping_status == 200:\n        log.debug('docker-registry: {0} -> basic-auth', registry_url)\n        return {'auth': basic_auth, 'headers': {}}\n    elif ping_status == 404:\n        raise RuntimeError(f'Unsupported docker registry: {registry_url}! '\n                           '(API v2 not implemented)')\n    elif ping_status == 401:\n        params = {\n            'scope': scope,\n            'offline_token': 'true',\n            'client_id': 'docker',\n            'service': service,\n        }\n        async with sess.get(realm, params=params, auth=basic_auth) as resp:\n            log.debug('docker-registry: {0} -> {1}', registry_url, realm)\n            if resp.status == 200:\n                data = json.loads(await resp.read())\n                token = data.get('token', None)\n                return {'auth': None, 'headers': {\n                    'Authorization': f'Bearer {token}'\n                }}\n    raise RuntimeError('authentication for docker registry '\n                       'f{registry_url} failed')", "label": 1}
{"index": "gp198416", "code": "def store_job_info(param_0, param_1):\n    param_1['job_info'] = param_0", "contrast": "def store_job_info_artifact(job, job_info_artifact):\n    job_details = json.loads(job_info_artifact['blob'])['job_details']\n    for job_detail in job_details:\n        job_detail_dict = {\n            'title': job_detail.get('title'),\n            'value': job_detail['value'],\n            'url': job_detail.get('url')\n        }\n        for (k, v) in job_detail_dict.items():\n            max_field_length = JobDetail._meta.get_field(k).max_length\n            if v is not None and len(v) > max_field_length:\n                logger.warning(\"Job detail '%s' for job_guid %s too long, truncating\",\n                               v[:max_field_length], job.guid)\n                job_detail_dict[k] = v[:max_field_length]\n        job_detail_dict['defaults'] = {'url': job_detail_dict['url']}\n        del job_detail_dict['url']\n        JobDetail.objects.update_or_create(\n            job=job,\n            **job_detail_dict)", "label": 1}
{"index": "gp043967", "code": "def get_identifier(param_0, param_1, param_2):\n    if isinstance(param_0, six.string_types):\n        var_0 = module_globals.get(param_0)\n        if var_0 is None:\n            raise ValueError('Unknown {}: {}'.format(param_2, param_0))\n        return var_0\n    elif callable(param_0):\n        return param_0\n    else:\n        raise ValueError('Could not interpret identifier')", "contrast": "def get_callable(identifier, module_globals, module_name):\n    if isinstance(identifier, str):\n        return module_globals[identifier]\n    elif callable(identifier):\n        return identifier\n    else:\n        raise TypeError(f\"{identifier} is not a string or callable\")", "label": 0}
{"index": "gp268721", "code": "from typing import List, Dict\ndef list_users(param_0: int, param_1: str, param_2: str, param_3: Dict[str, str]) -> List[User]:\n    pass", "contrast": "def list_users(self, **kwargs):\n        kwargs = self._verify_sort_options(kwargs)\n        kwargs = self._verify_filters(kwargs, User)\n        api = self._get_api(iam.AccountAdminApi)\n        return PaginatedResponse(api.get_all_users, lwrap_type=User, **kwargs)", "label": 1}
{"index": "gp183947", "code": "def process_sphinx_gallery_config(param_0):\n    var_0 = {}\n    if 'reference_url' in param_0:\n        var_0['reference_url'] = param_0['reference_url']\n    if 'gallery_dirs' in param_0:\n        var_0['gallery_dirs'] = param_0['gallery_dirs']\n    if 'examples_dirs' in param_0:\n        var_0['examples_dirs'] = param_0['examples_dirs']\n    if 'filename_pattern' in param_0:\n        var_0['filename_pattern'] = param_0['filename_pattern']\n    if 'line_numbers' in param_0:\n        var_0['line_numbers'] = param_0['line_numbers']\n    if 'backreferences_dir' in param_0:\n        var_0['backreferences_dir'] = param_0['backreferences_dir']\n    if 'doc_module' in param_0:\n        var_0['doc_module'] = param_0['doc_module']\n    if 'subsection_order' in param_0:\n        var_0['subsection_order'] = param_0['subsection_order']\n    if 'within_subsection_order' in param_0:\n        var_0['within_subsection_order'] = param_0['within_subsection_order']\n    if 'gallery_menu' in param_0:\n        var_0['gallery_menu'] = param_0['gallery_menu']\n    if 'sphinx_gallery_conf' in param_0:\n        var_0['sphinx_gallery_conf'] = param_0['sphinx_gallery_conf']\n    return var_0", "contrast": "def parse_config(app):\n    try:\n        plot_gallery = eval(app.builder.config.plot_gallery)\n    except TypeError:\n        plot_gallery = bool(app.builder.config.plot_gallery)\n    src_dir = app.builder.srcdir\n    abort_on_example_error = app.builder.config.abort_on_example_error\n    lang = app.builder.config.highlight_language\n    gallery_conf = _complete_gallery_conf(\n        app.config.sphinx_gallery_conf, src_dir, plot_gallery,\n        abort_on_example_error, lang, app.builder.name)\n    app.config.sphinx_gallery_conf = gallery_conf\n    app.config.html_static_path.append(glr_path_static())\n    return gallery_conf", "label": 1}
{"index": "gp038670", "code": "def _load_policy_definitions(param_0='c:\\\\Windows\\\\PolicyDefinitions',\n                             param_1='en-US'):\n    var_0 = INSTALL_LANGUAGE\n    var_1 = lxml.etree.Element('policyDefinitions')\n    t_policy_definitions.append(lxml.etree.Element('categories'))\n    t_policy_definitions.append(lxml.etree.Element('policies'))\n    t_policy_definitions.append(lxml.etree.Element('policyNamespaces'))\n    var_2 = lxml.etree.Element(\n            'policyDefinitionResources')\n    var_3 = etree.XPath('/policyDefinitions/policies')\n    var_4 = etree.XPath('/policyDefinitions/categories')\n    var_5 = etree.XPath(\n            '/policyDefinitions/policyNamespaces')\n    var_6 = etree.XPath(\n            '//*[local-name() = \"policyDefinitionResources\"]/*')\n    var_7 = etree.XPath('/policyDefinitionResources')\n    for var_8, var_9, var_10 in salt.utils.path.os_walk(param_0):\n        if var_8 == param_0:\n            for var_11 in var_10:\n                var_12 = os.path.join(var_8, var_11)\n                var_13 = lxml.etree.XMLParser(remove_comments=True)\n                try:\n                    var_14 = lxml.etree.parse(var_12, var_13=var_13)\n                except lxml.etree.XMLSyntaxError:\n                    try:\n                        var_14 = _remove_unicode_encoding(var_12)\n                    except Exception:\n                        log.exception('Handle this explicitly')\n                        log.error('A error was found while processing admx '\n                                  'file %s, all policies from this file will '\n                                  'be unavailable via this module', var_12)\n                        continue\n                var_15 = xmltree.getroot().nsmap\n                var_16 = ''\n                if None in var_15:\n                    var_15['None'] = var_15[None]\n                    namespaces.pop(None)\n                    var_16 = 'None:'\n                var_17 = xmltree.xpath(\n                        '/{0}policyDefinitions/{0}policyNamespaces/{0}target/@prefix'.format(var_16),\n                        var_15=var_15)[0]\n                var_18 = xmltree.xpath(\n                        '/{0}policyDefinitions/{0}policyNamespaces/{0}target/@namespace'.format(var_16),\n                        var_15=var_15)[0]\n                var_19 = xmltree.xpath(\n                        '/{0}policyDefinitions/{0}categories/{0}category'.format(var_16),\n                        var_15=var_15)\n                for var_20 in var_19:\n                    var_21 = var_20\n                    var_21 = _updateNamespace(var_21, var_18)\n                    var_4(var_1)[0].append(var_21)\n                var_22 = xmltree.xpath('/{0}policyDefinitions/{0}policies/{0}policy'.format(var_16),\n                                         var_15=var_15)\n                for var_23 in var_22:\n                    var_24 = var_23\n                    var_24 = _updateNamespace(var_24, var_18)\n                    if 'key' in temp_pol.attrib:\n                        var_24 = _updatePolicyElements(var_24, temp_pol.attrib['key'])\n                    var_3(var_1)[0].append(var_24)\n                var_25 = xmltree.xpath(\n                        '/{0}policyDefinitions/{0}policyNamespaces/{0}*'.format(var_16),\n                        var_15=var_15)\n                for var_26 in var_25:\n                    var_27 = var_26\n                    var_27 = _updateNamespace(var_27, var_18)\n                    var_5(var_1)[0].append(var_27)\n                var_28 = os.path.join(\n                    var_8,\n                    param_1,\n                    os.path.splitext(var_11)[0] + '.adml')\n                if not __salt__['file.file_exists'](var_28):\n                    log.info('An ADML file in the specified ADML language '\n                             '\"%s\" does not exist for the ADMX \"%s\", the '\n                             'the abbreviated language code will be tried.',\n                             param_1, var_11)\n                    var_28 = os.path.join(\n                        var_8,\n                        language.split('-')[0],\n                        os.path.splitext(var_11)[0] + '.adml')\n                    if not __salt__['file.file_exists'](var_28):\n                        log.info('An ADML file in the specified ADML language '\n                                 'code %s does not exist for the ADMX \"%s\", '\n                                 'the fallback language will be tried.',\n                                 param_1[:2], var_11)\n                        var_28 = os.path.join(\n                            var_8,\n                            var_0,\n                            os.path.splitext(var_11)[0] + '.adml')\n                        if not __salt__['file.file_exists'](var_28):\n                            log.info('An ADML file in the specified ADML '\n                                     'fallback language \"%s\" '\n                                     'does not exist for the ADMX \"%s\" '\n                                     'the abbreviated fallback language code '\n                                     'will be tried.',\n                                     var_0, var_11)\n                            var_28 = os.path.join(\n                                var_8,\n                                display_language_fallback.split('-')[0],\n                                os.path.splitext(var_11)[0] + '.adml')\n                            if not __salt__['file.file_exists'](var_28):\n                                var_29 = ('An ADML file in the specified ADML language '\n                                       '\"{0}\" and the fallback language \"{1}\" do not '\n                                       'exist for the ADMX \"{2}\".')\n                                raise SaltInvocationError(msg.format(param_1,\n                                                                     var_0,\n                                                                     var_11))\n                try:\n                    var_14 = lxml.etree.parse(var_28)\n                except lxml.etree.XMLSyntaxError:\n                    try:\n                        var_14 = _remove_unicode_encoding(var_28)\n                    except Exception:\n                        log.exception('Handle this explicitly')\n                        log.error('An error was found while processing '\n                                  'adml file %s, all policy '\n                                  'language data from this file will be '\n                                  'unavailable via this module',\n                                  var_28)\n                        continue\n                if None in var_15:\n                    var_15['None'] = var_15[None]\n                    namespaces.pop(None)\n                var_30 = var_6(var_14)\n                for var_31 in var_30:\n                    var_32 = var_31\n                    var_32 = _updateNamespace(var_32, var_18)\n                    var_7(var_2)[0].append(var_32)\n    __context__['lgpo.policy_definitions'] = var_1\n    __context__['lgpo.policy_resources'] = var_2", "contrast": "import os\nfrom xml.etree.ElementTree import ElementTree, Element, SubElement\ndef process_ADMX_files(policy_def_path):\n    root = Element('policies')\n    for filename in os.listdir(policy_def_path):\n        if filename.endswith('.admx'):\n            filepath = os.path.join(policy_def_path, filename)\n            with open(filepath, 'r') as f:\n                xml = ElementTree(file=f).getroot()\n            for category in xml.findall('category'):\n                root.append(category)\n    return ElementTree(root)", "label": 0}
{"index": "gp207520", "code": "def allow_if_not_authenticated(param_0):\n    if not user.is_authenticated:\n        return True\n    else:\n        return False", "contrast": "def anonymous_required(view, redirect_to=None):\n    if redirect_to is None:\n        redirect_to = settings.LOGIN_REDIRECT_URL\n    @wraps(view)\n    def wrapper(request, *a, **k):\n        if request.user and request.user.is_authenticated():\n            return HttpResponseRedirect(redirect_to)\n        return view(request, *a, **k)\n    return wrapper", "label": 1}
{"index": "gp000891", "code": "def from_config(param_0, param_1):\n    return param_0(**{\n        'initializers': [tf.compat.v2.initializers.deserialize(var_0)\n                         for var_0 in config.get('initializers', [])],\n        'sizes': config.get('sizes', []),\n        'validate_args': config.get('validate_args', False),\n    })", "contrast": "def initialize_from_config(config_dict):\n    initializer_type = config_dict['initializer_type']\n    initializer_params = config_dict['initializer_params']\n    if initializer_type == 'normal':\n        return tf.initializers.RandomNormal(**initializer_params)\n    elif initializer_type == 'uniform':\n        return tf.initializers.RandomUniform(**initializer_params)\n    elif initializer_type == 'truncated_normal':\n        return tf.initializers.TruncatedNormal(**initializer_params)\n    else:\n        raise ValueError('Initializer type not recognized.')", "label": 0}
{"index": "gp213769", "code": "import os\ndef which(param_0):\n    for var_0 in os.environ[\"PATH\"].split(os.pathsep):\n        var_1 = os.path.join(var_0, param_0)\n        if os.path.exists(var_1) and os.access(var_1, os.X_OK):\n            return var_1\n    return None", "contrast": "def which(cmd):\n    def is_exe(fp):\n        return os.path.isfile(fp) and os.access(fp, os.X_OK)\n    fpath, fname = os.path.split(cmd)\n    if fpath:\n        if is_exe(cmd):\n            return cmd\n    else:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            exe_file = os.path.join(path, cmd)\n            if is_exe(exe_file):\n                return exe_file\n    return None", "label": 1}
{"index": "gp268583", "code": "import subprocess\ndef start_ffmpeg_process(param_0):\n    var_0 = ['ffmpeg', '-i', param_0, '-']\n    var_1 = subprocess.Popen(var_0, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    while True:\n        var_2 = process.stdout.readline().decode('utf-8')\n        if var_2 == '' and process.poll() is not None:\n            break\n        if var_2:\n            print(output.strip())\n    var_3 = process.poll()\n    return var_3", "contrast": "async def start_worker(\n        self,\n        cmd: List[str],\n        input_source: str,\n        output: Optional[str] = None,\n        extra_cmd: Optional[str] = None,\n        pattern: Optional[str] = None,\n        reading: str = FFMPEG_STDERR,\n    ) -> None:\n        if self.is_running:\n            _LOGGER.warning(\"Can't start worker. It is allready running!\")\n            return\n        if reading == FFMPEG_STDERR:\n            stdout = False\n            stderr = True\n        else:\n            stdout = True\n            stderr = False\n        await self.open(\n            cmd=cmd,\n            input_source=input_source,\n            output=output,\n            extra_cmd=extra_cmd,\n            stdout_pipe=stdout,\n            stderr_pipe=stderr,\n        )\n        self._input = await self.get_reader(reading)\n        self._read_task = self._loop.create_task(self._process_lines(pattern))\n        self._loop.create_task(self._worker_process())", "label": 1}
{"index": "gp053356", "code": "def get_layer(param_0, param_1: str=None):\n        if param_1 is None:\n            param_1 = self.__last_layer_name\n        return self.__layers[param_1]", "contrast": "def retrieve_layer_by_name(name=None):\n    if name is None:\n        return network.layers[-1].output\n    else:\n        for layer in network.layers:\n            if layer.name == name:\n                return layer.output\n    return None", "label": 0}
{"index": "gp187452", "code": "def make_login_config_consistent(param_0):\n    param_0['username'] = param_0['username'].lower()\n    import hashlib\n    var_0 = 'my-secret-salt'  \n    var_1 = param_0['password']\n    var_2 = hashlib.sha256((var_1 + var_0).encode()).hexdigest()\n    param_0['password'] = var_2\n    return param_0", "contrast": "def sanitize_loginurl (self):\n        url = self[\"loginurl\"]\n        disable = False\n        if not self[\"loginpasswordfield\"]:\n            log.warn(LOG_CHECK,\n            _(\"no CGI password fieldname given for login URL.\"))\n            disable = True\n        if not self[\"loginuserfield\"]:\n            log.warn(LOG_CHECK,\n            _(\"no CGI user fieldname given for login URL.\"))\n            disable = True\n        if self.get_user_password(url) == (None, None):\n            log.warn(LOG_CHECK,\n            _(\"no user/password authentication data found for login URL.\"))\n            disable = True\n        if not url.lower().startswith((\"http:\", \"https:\")):\n            log.warn(LOG_CHECK, _(\"login URL is not a HTTP URL.\"))\n            disable = True\n        urlparts = urlparse.urlsplit(url)\n        if not urlparts[0] or not urlparts[1] or not urlparts[2]:\n            log.warn(LOG_CHECK, _(\"login URL is incomplete.\"))\n            disable = True\n        if disable:\n            log.warn(LOG_CHECK,\n              _(\"disabling login URL %(url)s.\") % {\"url\": url})\n            self[\"loginurl\"] = None", "label": 1}
{"index": "gp008705", "code": "def get_overrides_filename(param_0):\n    var_0 = os.environ.get(param_0)\n    if var_0 is None:\n        var_1 = 'Please set the {} environment variable.'.format(param_0)\n        raise EnvironmentError(var_1)\n    return var_0", "contrast": "import os\ndef get_config_override_file():\n    return os.environ.get('CONFIG_OVERRIDE_FILE', None)", "label": 0}
{"index": "gp031501", "code": "def _truncate_seq_pair(param_0, param_1, param_2, param_3):\n        while True:\n            var_0 = len(param_1) + len(param_2)\n            if var_0 <= param_3:\n                break\n            if len(param_1) > len(param_2):\n                tokens_a.pop()\n            else:\n                tokens_b.pop()", "contrast": "def truncate_to_max_length(sequence_pair, max_length):\n    sequence_pair[0] = sequence_pair[0][:max_length]\n    sequence_pair[1] = sequence_pair[1][:max_length]", "label": 0}
{"index": "gp006531", "code": "def multiple_l2_norm(param_0):\n    var_0 = [T.as_tensor_variable(var_1).flatten() for var_1 in param_0]\n    var_0 = [(var_1 if t.ndim > 0 else t.dimshuffle('x'))\n                 for var_1 in var_0]\n    var_2 = T.join(0, *var_0)\n    return T.sqrt(T.sqr(var_2).sum())", "contrast": "import numpy as np\ndef l2_norm(tensors):\n    tensors = np.asarray(tensors)\n    return np.sqrt(np.sum(np.square(tensors), axis=-1))", "label": 0}
{"index": "gp298500", "code": "def wrap_format_call(param_0, **param_1):\n    return param_0(format(*kwargs.values()), kwargs.keys())", "contrast": "def _format(self, _, result):\n        return self._fmt.format(six.text_type(result))", "label": 1}
{"index": "gp275742", "code": "def extract_paths(param_0):\n    var_0 = []\n    for var_1 in param_0:\n        if isinstance(var_1, list):\n            for var_2 in var_1:\n                files.append(var_2)\n        else:\n            files.append(var_1)\n    return var_0", "contrast": "def get_files(files, extnames=['.root']):\n    files_out = []\n    for f in files:\n        mime = mimetypes.guess_type(f)\n        if os.path.splitext(f)[1] in extnames:\n            files_out += [f]\n        elif mime[0] == 'text/plain':\n            files_out += list(np.loadtxt(f, unpack=True, dtype='str'))\n        else:\n            raise Exception('Unrecognized input type.')\n    return files_out", "label": 1}
{"index": "gp115587", "code": "def get_gradebook_admin_session(param_0, param_1):\n        if not self.supports_gradebook_admin():\n            raise errors.Unimplemented()\n        return sessions.GradebookAdminSession(param_1=param_1, runtime=self._runtime)", "contrast": "def get_gradebook_admin_session(proxy):\n    if proxy is None:\n        raise NullArgument()\n    elif not supports_gradebook_admin():\n        raise Unimplemented()\n    else:\n        try:\n            gradebook_admin_session = osid.grading.GradebookAdminSession(proxy=proxy)\n        except:\n            raise OperationFailed()\n        return gradebook_admin_session", "label": 0}
{"index": "gp219232", "code": "import configparser\ndef write_config_to_ini(param_0, param_1=True):\n    var_0 = configparser.ConfigParser(delimiters=(' = ' if param_1 else '='))\n    config_string.read_dict(param_0)\n    return config_string.write_string()", "contrast": "def write(self, fp, space_around_delimiters=True):\n        if space_around_delimiters:\n            d = \" {0} \".format(self._delimiters[0])\n        else:\n            d = self._delimiters[0]\n        if self._defaults:\n            self._write_section(fp, self.default_section,\n                                    self._defaults.items(), d)\n        for section in self._sections:\n            self._write_section(fp, section,\n                                self._sections[section].items(), d)", "label": 1}
{"index": "gp247950", "code": "def default_conflict_solver(param_0, param_1):\n    if len(param_1) > len(param_0):\n        return param_0\n    return param_1", "contrast": "def _default_conflict_solver(match, conflicting_match):\n    if len(conflicting_match.initiator) < len(match.initiator):\n        return conflicting_match\n    if len(match.initiator) < len(conflicting_match.initiator):\n        return match\n    return None", "label": 1}
{"index": "gp255344", "code": "def circularize(param_0):\n    var_0 = param_0[0]\n    var_1 = param_0[1:]\n    return var_1 + var_0", "contrast": "def circularize(self):\n        if self.top[-1].seq == '-' and self.bottom[0].seq == '-':\n            raise ValueError('Cannot circularize - termini disconnected.')\n        if self.bottom[-1].seq == '-' and self.top[0].seq == '-':\n            raise ValueError('Cannot circularize - termini disconnected.')\n        copy = self.copy()\n        copy.circular = True\n        copy.top.circular = True\n        copy.bottom.circular = True\n        return copy", "label": 1}
{"index": "gp198801", "code": "def check_changes(param_0: str, param_1: str) -> str:\n    if param_0 != param_1:\n        return param_1\n    else:\n        return None", "contrast": "def rename_in_module(occurrences_finder, new_name, resource=None,\n                     pymodule=None, replace_primary=False, region=None,\n                     reads=True, writes=True):\n    if resource is not None:\n        source_code = resource.read()\n    else:\n        source_code = pymodule.source_code\n    change_collector = codeanalyze.ChangeCollector(source_code)\n    for occurrence in occurrences_finder.find_occurrences(resource, pymodule):\n        if replace_primary and occurrence.is_a_fixed_primary():\n            continue\n        if replace_primary:\n            start, end = occurrence.get_primary_range()\n        else:\n            start, end = occurrence.get_word_range()\n        if (not reads and not occurrence.is_written()) or           (not writes and occurrence.is_written()):\n            continue\n        if region is None or region[0] <= start < region[1]:\n            change_collector.add_change(start, end, new_name)\n    return change_collector.get_changed()", "label": 1}
{"index": "gp075148", "code": "def _send_request(param_0, param_1, param_2={}):\n        param_2[\"wsid\"] = params.get(\"wsid\", self.userid)\n        param_2[\"pw\"]   = params.get(\"pw\", self.password)\n        var_0 = os.path.join(self.base_url, param_1)\n        if self.request_type == 'GET':\n            var_1 = requests.get(var_0, param_2=param_2)\n        elif self.request_type == 'POST':\n            var_1 = requests.post(var_0, data=param_2)\n        else:\n            raise ValueError('`resest_type` is invalid!')\n        var_2 = r.status_code\n        if var_2 != 200:\n            raise Exception(\"%s failed with status: %d\"%(param_1, var_2))\n        return var_1", "contrast": "import requests\ndef submit_request(job_type, **params):\n    url = 'http://example.com/submit_job'\n    headers = {'Content-Type': 'application/json'}\n    data = {'job_type': job_type, **params}\n    r = requests.post(url, headers=headers, json=data)\n    return r", "label": 0}
{"index": "gp030152", "code": "def is_cursor_on_first_line(param_0):\r\n        var_0 = self.textCursor()\r\n        cursor.movePosition(QTextCursor.StartOfBlock)\r\n        return cursor.atStart()", "contrast": "def is_cursor_on_first_line(cursor_position):\n    return cursor_position == 0", "label": 0}
{"index": "gp086646", "code": "def prior_sediment_rate(*param_0, **param_1):\n        var_0 = param_1['acc_mean']\n        var_1 = param_1['acc_shape']\n        var_2 = np.linspace(0, 6 * np.max(var_0), 100)\n        var_3 = stats.gamma.pdf(var_2, a=var_1,\n                            scale=1 / (var_1/var_0))\n        return var_3, var_2", "contrast": "import numpy as np\nfrom scipy.stats import norm\ndef get_prior_density(sediment_accumulation_values, prior_mean, prior_stddev):\n    x = np.linspace(0, max(sediment_accumulation_values), 1000)\n    y = norm.pdf(x, loc=prior_mean, scale=prior_stddev)\n    return y, x", "label": 0}
{"index": "gp302523", "code": "import re\ndef split_vv_sequence(param_0):\n    var_0 = r'(?<=[aeiou])(?=[^aeiouy]{0,2}(?<![aeiou]{2})$)'\n    return re.sub(var_0, '.', param_0)", "contrast": "def T2(word, rules):\n    WORD = word\n    offset = 0\n    for vv in vv_sequences(WORD):\n        seq = vv.group(1)\n        if not phon.is_diphthong(seq) and not phon.is_long(seq):\n            i = vv.start(1) + 1 + offset\n            WORD = WORD[:i] + '.' + WORD[i:]\n            offset += 1\n    rules += ' T2' if word != WORD else ''\n    return WORD, rules", "label": 1}
{"index": "gp304114", "code": "def get_favorited_images(param_0):\n    return favorited_images", "contrast": "def get_favorites(self):\n        url = self._imgur._base_url + \"/3/account/{0}/favorites\".format(self.name)\n        resp = self._imgur._send_request(url, needs_auth=True)\n        return [_get_album_or_image(thing, self._imgur) for thing in resp]", "label": 1}
{"index": "gp197070", "code": "def moment_matrix(param_0):\n    var_0, var_1 = input_matrix.shape\n    var_2 = np.zeros((var_1, var_1))\n    for var_3 in range(var_1):\n        if np.array_equal(param_0[:, var_3], np.zeros(var_0)):\n            var_2[var_3, var_3] = 0\n        else:\n            var_2[var_3, var_3] = np.dot(param_0[:, var_3], param_0[:, var_3])\n            for var_4 in range(var_3+1, var_1):\n                if np.array_equal(param_0[:, var_4], np.zeros(var_0)):\n                    var_2[var_3, var_4] = 0\n                    var_2[var_4, var_3] = 0\n                else:\n                    var_2[var_3, var_4] = var_2[var_4, var_3] = np.dot(param_0[:, var_3], param_0[:, var_4])\n    return var_2", "contrast": "def _M2_sparse(Xvar, mask_X, Yvar, mask_Y, weights=None):\n    C = np.zeros((len(mask_X), len(mask_Y)))\n    C[np.ix_(mask_X, mask_Y)] = _M2_dense(Xvar, Yvar, weights=weights)\n    return C", "label": 1}
{"index": "gp086724", "code": "def main(param_0, param_1=None):\n        if param_1 is None:\n            param_1 = sys.argv[1:]\n        try:\n            var_0 = param_0()\n            o.parseOptions(param_1)\n        except usage.UsageError as e:\n            print(o.getSynopsis())\n            print(o.getUsage())\n            print(str(e))\n            return 1\n        except CLIError as ce:\n            print(str(ce))\n            return ce.returnCode\n        return 0", "contrast": "import argparse\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(description='Fill in command-line arguments from argv')\n    parser.add_argument('-f', '--file', help='path to input file')\n    parser.add_argument('-o', '--output', help='path to output file')\n    parser.add_argument('-v', '--verbose', action='store_true', help='verbose mode')\n    return parser.parse_args(argv)", "label": 0}
{"index": "gp297798", "code": "def apply_template_values(param_0, param_1):\n    with open(param_0, 'r') as var_0:\n        var_1 = file.read()\n    for var_2, var_3 in values.items():\n        var_1 = manifest_content.replace(\"{{\"+var_2+\"}}\", var_3)\n    return var_1", "contrast": "def manifest(self, values, *paths, filename: str = None) -> Dict:\n        filename = filename or self.filename(*paths)\n        with open(filename, 'r') as fp:\n            template = Template(fp.read())\n        return yaml.load(template.render(values))", "label": 1}
{"index": "gp053129", "code": "def get_element_spd_dos(param_0, param_1):\n        param_1 = get_el_sp(param_1)\n        var_0 = {}\n        for var_1, var_2 in self.pdos.items():\n            if site.specie == param_1:\n                for var_3, var_4 in atom_dos.items():\n                    var_5 = _get_orb_type_lobster(var_3)\n                    if var_5 not in var_0:\n                        var_0[var_5] = var_4\n                    else:\n                        var_0[var_5] =                            add_densities(var_0[var_5], var_4)\n        return {var_3: Dos(self.efermi, self.energies, var_6)\n                for var_3, var_6 in el_dos.items()}", "contrast": "def get_element_spd_dos(el):\n    spd_densities = {\"S\": [], \"P\": [], \"D\": []}\n    if el not in LobsterCompleteDos.structure.composition.elements:\n        raise ValueError(\"Element not found in structure composition\")\n    for site, site_dos in LobsterCompleteDos.get_site_dos().items():\n        if site.specie == el:\n            el_dos = site_dos.get_element_dos()\n            for orbital, dos in el_dos.items():\n                if \"s\" in orbital.lower():\n                    spd_densities[\"S\"].append(dos)\n                elif \"p\" in orbital.lower():\n                    spd_densities[\"P\"].append(dos)\n                elif \"d\" in orbital.lower():\n                    spd_densities[\"D\"].append(dos)\n    return {el: spd_densities}", "label": 0}
{"index": "gp050517", "code": "def _to_dict(param_0):\n        var_0 = {}\n        if hasattr(param_0, 'text') and self.text is not None:\n            var_0['text'] = self.text\n        if hasattr(param_0, 'created') and self.created is not None:\n            var_0['created'] = datetime_to_string(self.created)\n        if hasattr(param_0, 'updated') and self.updated is not None:\n            var_0['updated'] = datetime_to_string(self.updated)\n        return var_0", "contrast": "def to_json(model):\n    return model.to_dict() ", "label": 0}
{"index": "gp088403", "code": "def get_string(param_0):\n    var_0 = bytearray()\n    for var_1 in param_0:\n        if not var_1:\n            break\n        ba.append(var_1)\n    return bytes(var_0)", "contrast": "def grab_string(stream: bytearray) -> bytes:\n    index = 0\n    for byte in stream:\n        if byte == 0:\n            break\n        index += 1\n    return bytes(stream[:index])", "label": 0}
{"index": "gp058034", "code": "def settings_get(param_0, param_1, param_2):\n        var_0 = self._get_blocks([block_id.hex()])[0]\n        var_1 = BlockHeader()\n        block_header.ParseFromString(block.header)\n        try:\n            var_2 = self._settings_view_factory.create_settings_view(\n                block_header.state_root_hash)\n        except KeyError:\n            LOGGER.error(\n                'Settings from block %s requested, but root hash %s was '\n                'missing. Returning no setting values.',\n                block_id.hex(),\n                block_header.state_root_hash)\n            return []\n        var_3 = []\n        for var_4 in param_2:\n            try:\n                var_5 = settings_view.get_setting(var_4)\n            except KeyError:\n                continue\n            result.append((var_4, var_5))\n        return var_3", "contrast": "def get_key_value_pairs() -> List[Tuple[str, str]]:\n    key_value_pairs = [('key1', 'value1'), ('key2', 'value2'), ('key3', 'value3')]\n    return key_value_pairs", "label": 0}
{"index": "gp250854", "code": "import os\nimport documentcloud\ndef upload_directory(param_0):\n    var_0 = documentcloud.DocumentCloud()\n    var_1 = []\n    for var_2 in os.listdir(param_0):\n        if file_name.endswith('.pdf'):\n            with open(os.path.join(param_0, var_2), 'rb') as var_3:\n                var_4, var_5 = os.path.splitext(var_2)\n                var_6 = client.documents.upload(\n                    var_3, var_4=var_4, source=os.path.abspath(var_2),\n                )\n                documents.append(var_6)\n    return var_1", "contrast": "def upload_directory(\n        self, path, source=None, description=None,\n        related_article=None, published_url=None, access='private',\n        project=None, data=None, secure=False, force_ocr=False\n    ):\n        path_list = []\n        for (dirpath, dirname, filenames) in os.walk(path):\n            path_list.extend([\n                os.path.join(dirpath, i) for i in filenames\n                if i.lower().endswith(\".pdf\")\n            ])\n        obj_list = []\n        for pdf_path in path_list:\n            obj = self.upload(\n                pdf_path, source=source, description=description,\n                related_article=related_article, published_url=published_url,\n                access=access, project=project, data=data, secure=secure,\n                force_ocr=force_ocr\n            )\n            obj_list.append(obj)\n        return obj_list", "label": 1}
{"index": "gp028502", "code": "def get_event_name(param_0, param_1):\n        return '{prefix:.{width}}-{postfix}'.format(prefix=param_0, width=max(0, 63 - len(param_1)), postfix=param_1)[:64]", "contrast": "def get_lambda_event_name(event_name):\n    return event_name.replace(\" \", \"-\").lower()", "label": 0}
{"index": "gp173862", "code": "def parse_dsv_file(param_0, param_1):\n    try:\n        var_0 = \"\\t\"\n        var_1 = []\n        for var_2 in param_1:\n            var_2 = line.strip()\n            var_3 = line.split(var_0)\n            data.append(var_3)\n        return var_1\n    except Exception as exception:\n        parser_mediator.ProduceExtractionError(\n            \"unable to parse DSV file with error: {0!s}\".format(exception))\n        raise errors.UnableToParseFile(\n            \"unable to parse DSV file with error: {0!s}\".format(exception))", "contrast": "def ParseFileObject(self, parser_mediator, file_object):\n    if not self._encoding:\n      self._encoding = parser_mediator.codepage\n    try:\n      if not self._HasExpectedLineLength(file_object):\n        display_name = parser_mediator.GetDisplayName()\n        raise errors.UnableToParseFile((\n            '[{0:s}] Unable to parse DSV file: {1:s} with error: '\n            'unexpected line length.').format(self.NAME, display_name))\n    except UnicodeDecodeError as exception:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile(\n          '[{0:s}] Unable to parse DSV file: {1:s} with error: {2!s}.'.format(\n              self.NAME, display_name, exception))\n    try:\n      line_reader = self._CreateLineReader(file_object)\n      reader = self._CreateDictReader(line_reader)\n      row_offset = line_reader.tell()\n      row = next(reader)\n    except (StopIteration, csv.Error, UnicodeDecodeError) as exception:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile(\n          '[{0:s}] Unable to parse DSV file: {1:s} with error: {2!s}.'.format(\n              self.NAME, display_name, exception))\n    number_of_columns = len(self.COLUMNS)\n    number_of_records = len(row)\n    if number_of_records != number_of_columns:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile((\n          '[{0:s}] Unable to parse DSV file: {1:s}. Wrong number of '\n          'records (expected: {2:d}, got: {3:d})').format(\n              self.NAME, display_name, number_of_columns,\n              number_of_records))\n    for key, value in row.items():\n      if self._MAGIC_TEST_STRING in (key, value):\n        display_name = parser_mediator.GetDisplayName()\n        raise errors.UnableToParseFile((\n            '[{0:s}] Unable to parse DSV file: {1:s}. Signature '\n            'mismatch.').format(self.NAME, display_name))\n    row = self._ConvertRowToUnicode(parser_mediator, row)\n    if not self.VerifyRow(parser_mediator, row):\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile((\n          '[{0:s}] Unable to parse DSV file: {1:s}. Verification '\n          'failed.').format(self.NAME, display_name))\n    self.ParseRow(parser_mediator, row_offset, row)\n    row_offset = line_reader.tell()\n    for row in reader:\n      if parser_mediator.abort:\n        break\n      row = self._ConvertRowToUnicode(parser_mediator, row)\n      self.ParseRow(parser_mediator, row_offset, row)\n      row_offset = line_reader.tell()", "label": 1}
{"index": "gp005584", "code": "def maxs(param_0, param_1):\n        self.x_max, self.y_max, self.z_max = param_1", "contrast": "import numpy as np\ndef set_max_values(x: float, y: float, z: float) -> np.ndarray:\n    max_values = np.array([x, y, z])\n    max_values = np.max(max_values)\n    return max_values", "label": 0}
{"index": "gp336287", "code": "def load_experiment_resource(param_0: str, param_1: str) -> str:\n    import os\n    from pathlib import Path\n    var_0 = Path(param_1) / param_0\n    if os.path.isfile(var_0):\n        with open(var_0, 'r') as var_1:\n            var_2 = f.read()\n            return var_2\n    else:\n        return \"\"", "contrast": "def load_description(name, root=''):\n        desc = ''\n        try:\n            desc = Component.load_resource(name, root=root)\n        except (IOError, ImportError):\n            pass\n        return desc", "label": 1}
{"index": "gp241450", "code": "def check_overlapped_periods(param_0):\n    for var_0 in range(len(param_0)):\n        for var_1 in range(var_0+1, len(param_0)):\n            if param_0[var_0][1] > param_0[var_1][0] and param_0[var_0][0] < param_0[var_1][1]:\n                return True\n    return False", "contrast": "def __validate_enrollment_periods(self, enrollments):\n        for a, b in itertools.combinations(enrollments, 2):\n            max_start = max(a.start, b.start)\n            min_end = min(a.end, b.end)\n            if max_start < min_end:\n                msg = \"invalid GrimoireLab enrollment dates. \"                      \"Organization dates overlap.\"\n                raise InvalidFormatError(cause=msg)\n        return enrollments", "label": 1}
{"index": "gp079187", "code": "def filter_traceback(param_0, param_1, param_2=CURRENT_PACKAGE):\n    if not isinstance(param_1, types.TracebackType):\n        return param_1\n    def in_namespace(param_0):\n        return param_0 and (n.startswith(param_2 + '.') or param_0 == param_2)\n    while param_1 and in_namespace(tb.tb_frame.f_globals['__package__']):\n        param_1 = tb.tb_next\n    var_0 = param_1\n    var_1 = 0\n    while param_1 and not in_namespace(tb.tb_frame.f_globals['__package__']):\n        param_1 = tb.tb_next\n        var_1 += 1\n    return ''.join(traceback.format_exception(error.__class__, param_0, var_0, var_1))", "contrast": "import traceback\ndef filter_stacktrace(parent_trace, variable_name):\n    child_trace = traceback.extract_stack()\n    trace_idx = len(parent_trace) - 1\n    while trace_idx >= 0 and not parent_trace[trace_idx][0].startswith(\"<module>\"):\n        if variable_name in parent_trace[trace_idx][0].f_globals:\n            break\n        trace_idx -= 1\n    parent_trace = parent_trace[trace_idx:]\n    return parent_trace + child_trace", "label": 0}
{"index": "gp258634", "code": "def print_in_color(param_0):\n    if param_0:\n        print(\"\\033[1;31;48mHello, This is printed in color.\")\n    else:\n        print(\"Hello, this is printed in black and white.\")", "contrast": "def set_theme(color=True):\n        if color:\n            Console.theme = Console.theme_color\n        else:\n            Console.theme = Console.theme_bw\n        Console.color = color", "label": 1}
{"index": "gp181879", "code": "import nacl.signing\ndef generate_random_signing_key():\n    return nacl.signing.SigningKey.generate()", "contrast": "def generate(cls):\n        return cls(\n            random(nacl.bindings.crypto_sign_SEEDBYTES),\n            encoder=encoding.RawEncoder,\n        )", "label": 1}
{"index": "gp329042", "code": "import random\ndef resolve_iupac_hetero_codes(param_0):\n    var_0 = {'R': ('A', 'G'), 'Y': ('C', 'T'), 'S': ('G', 'C'),\n                    'W': ('A', 'T'), 'K': ('G', 'T'), 'M': ('A', 'C')}\n    var_1 = ''\n    for var_2 in param_0:\n        if var_2 in var_0:\n            var_1 += random.choice(var_0[var_2])\n        else:\n            var_1 += var_2\n    return var_1", "contrast": "def _resolveambig(subseq):\n    N = []\n    for col in subseq:\n        rand = np.random.binomial(1, 0.5)\n        N.append([_AMBIGS[i][rand] for i in col])\n    return np.array(N)", "label": 1}
{"index": "gp293761", "code": "import re\ndef match_records(param_0, param_1, param_2, param_3):\n    var_0 = re.compile(param_1)\n    var_1 = []\n    for var_2 in param_0:\n        if param_2 in var_2 and param_3 in var_2:\n            if regex.match(var_2[param_2]) is not None:\n                result.append(var_2)\n    return var_1", "contrast": "def filterMapNames(regexText,  records=getIndex(), excludeRegex=False, closestMatch=True):\n    bestScr = 99999 \n    regex = re.compile(regexText, flags=re.IGNORECASE)\n    ret = []\n    if excludeRegex: \n        if regexText and closestMatch: \n            for m in list(records):\n                if re.search(regex, m.name): continue \n                score = len(m.name) \n                if score == bestScr:\n                    bestScr = score\n                    ret.append(m)\n                elif score <  bestScr: \n                    bestScr = score\n                    ret = [m]\n        else: \n            for m in list(records):\n                if re.search(regex, m.name): continue \n                ret.append(m) \n    else: \n        if regexText and closestMatch: \n            for m in records:\n                if not re.search(regex, m.name): continue \n                score = len(m.name) \n                if score == bestScr:\n                    bestScr = score\n                    ret.append(m)\n                elif score <  bestScr: \n                    bestScr = score\n                    ret = [m]\n        else: \n            for m in records:\n                if not re.search(regex, m.name): continue \n                ret.append(m) \n    return ret", "label": 1}
{"index": "gp147644", "code": "def get_parallel_regions_block(param_0):\n    var_0 = [utils.to_single_data(var_1) for var_1 in param_0]\n    var_2 = _get_parallel_regions(var_0[0])\n    var_3 = []\n    var_4 = 10\n    for var_5 in tz.partition_all(var_4, var_2):\n        out.append({\"region_block\": [\"%s:%s-%s\" % (var_6, var_7, var_8) for var_6, var_7, var_8 in var_5]})\n    return var_3", "contrast": "def retrieve_block_group(cwl_target, num_cores):\n    blocks = []\n    num_blocks = num_cores * 2\n    count = 0\n    for i in range(num_blocks):\n        block = []\n        for j in range(count, len(cwl_target), num_blocks):\n            block.append(cwl_target[j])\n        count += 1\n        blocks.append(block)\n    return blocks", "label": 0}
{"index": "gp182990", "code": "import cache\nfrom datetime import datetime, timedelta\nMAX_ATTEMPTS = 5\nLOCKOUT_TIME = 10 \ndef login(param_0, param_1):\n    var_0 = cache.get(param_0, 0)\n    var_0 += 1\n    cache.set(param_0, var_0, LOCKOUT_TIME * 60)\n    if var_0 >= MAX_ATTEMPTS:\n        raise AxesSignalPermissionDenied(\"USER LOCKED OUT\")\n    if param_0 == \"valid_user\" and param_1 == \"valid_password\":\n        return True\n    else:\n        return False", "contrast": "def user_login_failed(\n            self,\n            sender,\n            credentials: dict,\n            request: AxesHttpRequest = None,\n            **kwargs\n    ):  \n        if request is None:\n            log.error('AXES: AxesCacheHandler.user_login_failed does not function without a request.')\n            return\n        username = get_client_username(request, credentials)\n        client_str = get_client_str(username, request.axes_ip_address, request.axes_user_agent, request.axes_path_info)\n        if self.is_whitelisted(request, credentials):\n            log.info('AXES: Login failed from whitelisted client %s.', client_str)\n            return\n        failures_since_start = 1 + self.get_failures(request, credentials)\n        if failures_since_start > 1:\n            log.warning(\n                'AXES: Repeated login failure by %s. Count = %d of %d. Updating existing record in the cache.',\n                client_str,\n                failures_since_start,\n                settings.AXES_FAILURE_LIMIT,\n            )\n        else:\n            log.warning(\n                'AXES: New login failure by %s. Creating new record in the cache.',\n                client_str,\n            )\n        cache_key = get_client_cache_key(request, credentials)\n        self.cache.set(cache_key, failures_since_start, self.cache_timeout)\n        if failures_since_start >= settings.AXES_FAILURE_LIMIT:\n            log.warning('AXES: Locking out %s after repeated login failures.', client_str)\n            user_locked_out.send(\n                'axes',\n                request=request,\n                username=username,\n                ip_address=request.axes_ip_address,\n            )\n            raise AxesSignalPermissionDenied('Locked out due to repeated login failures.')", "label": 1}
{"index": "gp047195", "code": "def fit(param_0, param_1, param_2=None, **param_3):\n        if is_dataframe(param_1):\n            self.X = X.values\n            if self.features_ is None:\n                self.features_ = X.columns\n        else:\n            self.X = param_1\n        self.y = param_2\n        super(MissingDataVisualizer, param_0).fit(param_1, param_2, **param_3)", "contrast": "def fit(self, X, y, **kwargs):\n    return self", "label": 0}
{"index": "gp117028", "code": "def match_deadline(param_0, param_1, param_2, param_3):\n        self._match_minimum_date_time('deadline', param_1, param_3)\n        self._match_maximum_date_time('deadline', param_2, param_3)", "contrast": "def match_assessments(start, end, match):\n    if start is None or end is None:\n        raise NullArgument(\"Start or end is null\")\n    if end < start:\n        raise InvalidArgument(\"End is less than start\")\n    assessments = get_all_assessments() \n    matching_assessments = []\n    for assessment in assessments:\n        if match == True:\n            if assessment.end_time >= start and assessment.end_time <= end:\n                matching_assessments.append(assessment)\n        else:\n            if assessment.end_time < start or assessment.end_time > end:\n                matching_assessments.append(assessment)\n    return matching_assessments", "label": 0}
{"index": "gp210433", "code": "import json\nfrom pypot.primitive.move import Move\ndef load_move_from_json(param_0):\n    var_0 = json.loads(param_0)\n    return Move.from_dict(var_0)", "contrast": "def loads(cls, str):\n        d = json.loads(str)\n        return cls.create(d)", "label": 1}
{"index": "gp170863", "code": "import numpy as np\ndef generate_basis(param_0):\n    param_0 = param_0 / np.linalg.norm(param_0)\n    var_0 = np.array([1, 0, 0])\n    if np.allclose(param_0, var_0):\n        var_0 = np.array([0, 1, 0])\n    var_1 = np.cross(param_0, var_0)\n    var_1 /= np.linalg.norm(var_1)\n    var_0 = np.cross(var_1, param_0)\n    return var_0, var_1, param_0", "contrast": "def generate_basis(z):\n    z = np.array(z, dtype=np.float64, copy=True)\n    if z.shape != (3,):\n        raise ValueError('z must be (3,) float!')\n    z /= np.linalg.norm(z)\n    x = np.array([-z[1], z[0], 0.0])\n    if np.isclose(np.linalg.norm(x), 0.0):\n        x = np.array([1.0, 0.0, 0.0])\n    else:\n        x /= np.linalg.norm(x)\n    y = np.cross(z, x)\n    result = np.array([x, y, z], dtype=np.float64)\n    return result", "label": 1}
{"index": "gp031077", "code": "def text_list_to_colors_simple(param_0):\n    var_0 = list(set(param_0))\n    uNames.sort()\n    var_1 = [ uNames.index(var_2) for var_2 in param_0 ]\n    var_1 = np.array(var_1)\n    var_1 = 255 * (var_1 - textToColor.min()) /                  (textToColor.max() - textToColor.min())\n    var_3 = generateColorMap();\n    var_4 = [var_3[int(var_5)] for var_5 in var_1]\n    return var_4", "contrast": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport hashlib\nimport random\ndef generate_colors(names):\n    processed_names = [''.join([char.lower() for char in name if char.isalpha()]) for name in names]\n    vectorizer = CountVectorizer()\n    bow = vectorizer.fit_transform(processed_names)\n    lda = LatentDirichletAllocation(n_components=3, random_state=42)\n    topic_dist = lda.fit_transform(bow)\n    colors = []\n    for topic_prob in topic_dist:\n        md5 = hashlib.md5(str(topic_prob).encode('utf-8')).hexdigest()\n        color = '#' + md5[:6]\n        colors.append(color)\n    return colors", "label": 0}
{"index": "gp191227", "code": "def move_cursor_to_line(param_0):\n    vim.current.window.cursor = (param_0, 0)", "contrast": "def _go_to_line(editor, line):\n    b = editor.application.current_buffer\n    b.cursor_position = b.document.translate_row_col_to_index(max(0, int(line) - 1), 0)", "label": 1}
{"index": "gp143973", "code": "def ones(param_0, param_1, **param_2):\n        return self._write_op(self._ones_nosync, param_1, **param_2)", "contrast": "import zarr\ndef create_array(**kwargs):\n    return zarr.creation.ones(**kwargs)", "label": 0}
{"index": "gp230838", "code": "def compute_total_remote_file_size(param_0, param_1):\n    var_0 = 0\n    return var_0", "contrast": "def _compute_remote_size(self, options):\n        size = self.local_path.size\n        if (self._ase.mode == blobxfer.models.azure.StorageModes.Page and\n                self.local_path.use_stdin):\n            if options.stdin_as_page_blob_size == 0:\n                allocatesize = _MAX_PAGE_BLOB_SIZE\n                self._needs_resize = True\n            else:\n                allocatesize = options.stdin_as_page_blob_size\n        elif size > 0:\n            if self._ase.is_encrypted:\n                allocatesize = (size  \n                    self._AES_BLOCKSIZE\n            else:\n                allocatesize = size\n        else:\n            allocatesize = 0\n        self._ase.size = allocatesize\n        if blobxfer.util.is_not_empty(self._ase.replica_targets):\n            for rt in self._ase.replica_targets:\n                rt.size = allocatesize\n        if self._verbose:\n            logger.debug('remote size for {} is {} bytes'.format(\n                self._ase.path, self._ase.size))", "label": 1}
{"index": "gp105946", "code": "def loader_for_type(param_0, param_1):\n        for var_0, var_1 in Mimer.TYPES.iteritems():\n            for var_2 in var_1:\n                if ctype.startswith(var_2):\n                    return var_0", "contrast": "def get_deserialize_function(mimetype):\n    deserialize_functions = {\n        'application/json': json.loads,\n        'application/xml': xmltodict.parse,\n        'text/csv': csv.reader\n    }\n    return deserialize_functions.get(mimetype, None)", "label": 0}
{"index": "gp177296", "code": "import email\ndef create_message_structure(param_0, param_1=False):\n    with open(param_0, 'rb') as var_0:\n        var_1 = f.read()\n    return email.message_from_bytes(var_1, param_1=param_1)", "contrast": "def parse(self, fp, headersonly=False):\n        feedparser = FeedParser(self._class, policy=self.policy)\n        if headersonly:\n            feedparser._set_headersonly()\n        while True:\n            data = fp.read(8192)\n            if not data:\n                break\n            feedparser.feed(data)\n        return feedparser.close()", "label": 1}
{"index": "gp069830", "code": "def qteMakeWindowActive(param_0, param_1: QtmacsWindow):\n        if param_1 in self._qteWindowList:\n            windowObj.activateWindow()\n        else:\n            self.qteLogger.warning('Window to activate does not exist')", "contrast": "def activate_window(windowObj):\n    if not isinstance(windowObj, QtmacsWindow):\n        raise QtmacsArgumentError(\"Invalid argument type\")\n    applets = windowObj.applets()\n    if len(applets) > 0:\n        applets[0].setFocus()\n    windowObj.activateWindow()", "label": 0}
{"index": "gp318405", "code": "import zlib\ndef merge_blocks(param_0):\n    var_0 = b\"\".join(param_0)\n    var_0 = zlib.compress(var_0)\n    return list(var_0)", "contrast": "def merge(blocks):\n    current_block = blocks[sorted(blocks.keys())[0]]\n    compressed_data = []\n    eof = False\n    while not eof:\n        data_size_to_append = None\n        next_block = None\n        i = 0\n        while i < len(current_block.data) - 1:\n            current_byte = current_block.data[i]\n            next_byte = current_block.data[i + 1]\n            if current_byte == RLE_BYTE:\n                if next_byte == RLE_BYTE:\n                    i += 2\n                else:\n                    i += 3\n            elif current_byte == SPECIAL_BYTE:\n                if next_byte in SPECIAL_DEFAULTS:\n                    i += 3\n                elif next_byte == SPECIAL_BYTE:\n                    i += 2\n                else:\n                    data_size_to_append = i\n                    if next_byte == EOF_BYTE:\n                        eof = True\n                    else:\n                        next_block = blocks[next_byte]\n                    break\n            else:\n                i += 1\n        assert data_size_to_append is not None, \"Ran off the end of a \"            \"block without encountering a block switch or EOF\"\n        compressed_data.extend(current_block.data[0:data_size_to_append])\n        if not eof:\n            assert next_block is not None, \"Switched blocks, but did \"                \"not provide the next block to switch to\"\n            current_block = next_block\n    return compressed_data", "label": 1}
{"index": "gp283697", "code": "import subprocess\ndef run_pngout(param_0):\n    subprocess.call([\"pngout\", param_0])", "contrast": "def pngout(ext_args):\n    args = _PNGOUT_ARGS + [ext_args.old_filename, ext_args.new_filename]\n    extern.run_ext(args)\n    return _PNG_FORMAT", "label": 1}
{"index": "gp282937", "code": "import matplotlib.pyplot as plt\ndef abundance_plots(param_0, param_1, param_2='All', param_3=False, param_4='Lagrangian'):\n    var_0, var_1 = plt.subplots(2, 2, figsize=(12, 8)) \n    var_1 = ax.ravel()\n    if param_2 == 'All':\n        param_2 = p.abu_fields\n    for var_2, var_3 in enumerate(param_2):\n        if var_2 == 4:\n            break\n        var_1[var_2].plot(p.get(param_4), p.abundances[var_3])\n        var_1[var_2].set_title('{0}'.format(var_3))\n        var_1[var_2].set_xlim(param_1)\n    if param_3:\n        plt.show()", "contrast": "def abu_profiles(p,ifig=1,xlm=xlm,ylm=(-8,0),show=False,abunds='All',xaxis=xaxis_type, figsize1=(8,8)):\n    matplotlib.rc('figure',facecolor='white',figsize=figsize1)\n    f, ([ax1,ax2],[ax3,ax4]) = pl.subplots(2, 2, sharex=False, sharey=True, figsize=figsize1)\n    all_isos=[['h1','he3','he4','li6','c12','c13','n13','n14','n15','o16','o17','o18','f19'],['ne20','ne21','ne22','na22','na23','mg24','mg25','mg26','al26','al27','si28','si29','si30'], ['p31', 's32','s33', 's34','s36','cl35','cl37','ar36', 'ar38','ar40', 'k39', 'k40','k41'],\n['ca40','ca42','ca48','sc45','ti46','ti48','ti50','v50','v51','cr52','cr54','mn55','fe56']]\n    if abunds == 'All':\n        abus=[[],[],[],[]]\n        j=0\n        for i, row in enumerate(all_isos):\n            for iso in row:\n                if iso in p.cols:\n                    abus[i].append(iso)\n                    j+=1  \n        abus1=[]\n        abus2 =[[],[],[],[]]\n        for l in range(len(abus)):\n            for k in range(len(abus[l])):\n                abus1.append(abus[l][k])\n        is_small_isos = False                \n        for i in range(len(abus)):\n            if len(abus[i]) < 5:\n                is_small_isos = True\n                print(\"Missing isotopes from the default list. Distributing the ones you have over the panels.\")\n        if is_small_isos:\n            n=4\n            quo, rem = divmod(len(abus1), n)\n            for i in range(len(abus2)):\n                for k in range(i*quo,(i+1)*quo+rem):\n                    abus2[i].append(abus1[k])\n                    abus = abus2\n    else:\n        abus = abus    \n    ax = [ax1,ax2,ax3,ax4]\n    xxx = p.get('radius') if xaxis is \"Eulerian\" else p.get('mass') \n    mass = p.get('mass')                      \n    radius = p.get('radius')*ast.rsun_cm/1.e8  \n    if xaxis is \"Eulerian\":\n        xxx = radius\n        if xlm[0] == 0 and xlm[1] == 0:\n            indtop = 0\n            indbot = len(mass)-1\n        else: \n            indbot = np.where(radius>=xlm[0])[0][-1]\n            indtop = np.where(radius<xlm[1])[0][0]\n        xll = (radius[indbot],radius[indtop])\n        xxlabel = \"Radius (Mm)\"\n    elif xaxis is \"Lagrangian\": \n        xxx = mass\n        xll = xlm\n        xxlabel = \"$M / \\mathrm{M_{sun}}$\"\n    else:\n        print(\"Error: don't understand xaxis choice, must be Lagrangian or Eulerian\")\n    for i in range(4):\n        for thing in abus[i]:\n            ind = abus[i].index(thing)\n            ax[i].plot(xxx, np.log10(p.get(thing)), ls=u.linestylecb(ind,a,b)[0],            marker=u.linestylecb(ind,a,b)[1], color=u.linestylecb(ind,a,b)[2],            markevery=50,label=thing)\n        ax[i].set_ylim(ylm)\n        ax[i].set_xlim(xll)\n        ax[i].legend(loc=1)\n        ax[i].set_xlabel(xxlabel)\n        if i%2 == 0:\n            ax[i].set_ylabel('log X')\n    title_str = \"Abundance plot: \"+'t ='+str(title_format%p.header_attr['star_age'])              +' dt ='+str(title_format%p.header_attr['time_step'])              +'model number = '+str(int(p.header_attr['model_number']))\n    f.suptitle(title_str, fontsize=12)\n    f.tight_layout()\n    f.subplots_adjust(left=0.1, bottom=0.1, right=0.95, top=0.9, wspace=0, hspace=0.1)\n    f.savefig('abuprof'+str(int(p.header_attr['model_number'])).zfill(6)+'.png')", "label": 1}
{"index": "gp053237", "code": "def _get_formatted_error(param_0, param_1):\n        def bits(param_0):\n            while param_0:\n                var_0 = param_0 & (~param_0+1)\n                yield var_0\n                param_0 ^= var_0\n        var_0 = self.m_objPCANBasic.GetErrorText(param_1, 0)\n        if var_0[0] != PCAN_ERROR_OK:\n            var_1 = []\n            for var_2 in bits(param_1):\n                var_0 = self.m_objPCANBasic.GetErrorText(var_2, 0)\n                if var_0[0] != PCAN_ERROR_OK:\n                    var_3 = \"An error occurred. Error-code's text ({0:X}h) couldn't be retrieved\".format(param_1)\n                else:\n                    var_3 = var_0[1].decode('utf-8', errors='replace')\n                strings.append(var_3)\n            var_4 = '\\n'.join(var_1)\n        else:\n            var_4 = var_0[1].decode('utf-8', errors='replace')\n        return var_4", "contrast": "def get_error_message(err_code):\n    buffer_size = 255\n    buffer = ctypes.create_unicode_buffer(buffer_size)\n    num_chars = ctypes.windll.kernel32.GetErrorTextW(err_code, buffer, buffer_size)\n    if num_chars == 0:\n        return f\"Unable to retrieve error message. Error code: {ctypes.windll.kernel32.GetLastError()}\"\n    else:\n        return buffer.value", "label": 0}
{"index": "gp158505", "code": "def ipc_weights(param_0, param_1):\n    if event.all():\n        return numpy.ones(time.shape[0])\n    var_0, var_1 = kaplan_meier_estimator(~param_0, param_1)\n    var_2 = numpy.searchsorted(var_0, param_1[param_0])\n    var_3 = var_1[var_2]\n    assert (var_3 > 0).all()\n    var_4 = numpy.zeros(time.shape[0])\n    var_4[param_0] = 1.0 / var_3\n    return var_4", "contrast": "def compute_censoring_weights(event, time):\n    from lifelines import KaplanMeierFitter\n    kmf = KaplanMeierFitter()\n    kmf.fit(time, event, label=\"Kaplan Meier Estimate\")\n    probs_to_not_be_observed_before = kmf.predict(time)\n    observation_probs = 1.0/probs_to_not_be_observed_before\n    observation_probs /= len(observation_probs)\n    weights = np.zeros(len(observation_probs))\n    for i in range(len(observation_probs)):\n        if event[i]:\n            weights[i] = observation_probs[i]\n        else:\n            weights[i] = 1.0 - observation_probs[i]\n    return weights", "label": 0}
{"index": "gp065248", "code": "def validate(param_0):\n    @wraps(param_0)\n    def mod_run(param_0, param_1):\n        self.validate_input(param_1)\n        var_0 = param_0(param_0, param_1)\n        self.validate_result(var_0)\n        return var_0\n    return mod_run", "contrast": "from functools import wraps\ndef validate_inputs(inputs):\ndef validate_output(output):\ndef decorate_run_method(func):\n    @wraps(func)\n    def wrapper(inputs):\n        if not validate_inputs(inputs):\n            raise ValueError(\"Invalid inputs\")\n        output = func(inputs)\n        if not validate_output(output):\n            raise ValueError(\"Invalid output\")\n        return output\n    return wrapper", "label": 0}
{"index": "gp160689", "code": "def previous_close(param_0, param_1):\n        var_0 = previous_divider_idx(self.market_closes_nanos, dt.value)\n        return pd.Timestamp(self.market_closes_nanos[var_0], tz=UTC)", "contrast": "import pandas as pd\ndef get_previous_close(dt):\n    prev_close = pd.Timestamp(dt.date() - pd.Timedelta(days=1), tz='UTC').normalize() + pd.Timedelta(hours=23, minutes=59, seconds=59)\n    return prev_close", "label": 0}
{"index": "gp248358", "code": "import RPi.GPIO as GPIO\nimport time\ndef calculate_depth():\n    var_0 = 23\n    var_1 = 24\n    GPIO.setup(var_0, GPIO.OUT)\n    GPIO.setup(var_1, GPIO.IN)\n    GPIO.output(var_0, True)\n    time.sleep(0.00001)\n    GPIO.output(var_0, False)\n    var_2 = time.time()\n    var_3 = time.time()\n    while GPIO.input(var_1) == 0:\n        var_2 = time.time()\n    while GPIO.input(var_1) == 1:\n        var_3 = time.time()\n    var_4 = var_3 - var_2\n    var_5 = (var_4 * 34300) / 2\n    var_6 = 30 - var_5\n    return var_6", "contrast": "def main():\n    trig_pin = 17\n    echo_pin = 27\n    hole_depth = 80  \n    value = sensor.Measurement(trig_pin,\n                               echo_pin\n                               )\n    raw_measurement = value.raw_distance()\n    liquid_depth = value.depth_metric(raw_measurement, hole_depth)\n    print(\"Depth = {} centimeters\".format(liquid_depth))", "label": 1}
{"index": "gp132001", "code": "def filter(param_0, param_1):\n        if isinstance(param_1, str) or isinstance(param_1, unicode):\n            var_0 = param_1\n            param_1 = lambda x: x.get(var_0)\n        return CoursesList(filter(param_1, param_0))", "contrast": "def filter_courses(courses, criteria):\n    if isinstance(criteria, str):\n        return [course for course in courses if course.get(criteria)]\n    elif callable(criteria):\n        return [course for course in courses if criteria(course)]\n    else:\n        raise ValueError(\"Criteria must be a string or a function.\")", "label": 0}
{"index": "gp325342", "code": "def solve_toeplitz(param_0, param_1, param_2, param_3):\n    var_0 = len(param_3)\n    var_1 = var_0 - 1\n    var_2 = [0] * var_0\n    var_3 = [0] * (var_1 + 1)\n    var_4 = [0] * (var_1 + 1)\n    var_5 = [0] * (var_1 + 1)\n    var_3[0] = param_1[0] / param_0\n    var_4[0] = param_3[0] / param_0\n    for var_6 in range(1, var_1 + 1):\n        var_5[var_6] = param_2[var_6 - 1] / var_3[var_6 - 1]\n        var_3[var_6] = (param_1[var_6] - var_5[var_6] * param_1[var_6 - 1]) / (param_0 - var_5[var_6] * param_2[var_6 - 1])\n        var_4[var_6] = (param_3[var_6] - var_5[var_6] * var_4[var_6 - 1]) / (param_0 - var_5[var_6] * param_2[var_6 - 1])\n    var_2[var_1] = var_4[var_1]\n    for var_6 in range(var_1 - 1, -1, -1):\n        var_2[var_6] = var_4[var_6] - var_3[var_6] * var_2[var_6 + 1]\n    return var_2", "contrast": "def TOEPLITZ(T0, TC, TR, Z):\n    assert len(TC)>0\n    assert len(TC)==len(TR)\n    M = len(TC)\n    X = numpy.zeros(M+1,dtype=complex)\n    A = numpy.zeros(M,dtype=complex)\n    B = numpy.zeros(M,dtype=complex)\n    P = T0\n    if P == 0: raise ValueError(\"P must be different from zero\")\n    if P == 0: raise ValueError(\"P must be different from zero\")\n    X[0] = Z[0]/T0 \n    for k in range(0, M):\n        save1 = TC[k]\n        save2 = TR[k]\n        beta = X[0]*TC[k]\n        if k == 0:\n            temp1 = -save1 / P\n            temp2 = -save2 / P\n        else:\n            for j in range(0, k):\n                save1 = save1 + A[j] * TC[k-j-1]\n                save2 = save2 + B[j] * TR[k-j-1]\n                beta = beta + X[j+1] * TC[k-j-1]\n            temp1 = -save1 / P\n            temp2 = -save2/P\n        P = P * (1. - (temp1*temp2))\n        if P <= 0:\n            raise ValueError(\"singular matrix\")\n        A[k] = temp1\n        B[k] = temp2\n        alpha = (Z[k+1]-beta)/P\n        if k == 0: \n            X[k+1] = alpha\n            for j in range(0,k+1):\n                X[j] = X[j] + alpha * B[k-j]\n            continue\n        for j in range(0, k):\n            kj = k-j-1\n            save1 = A[j]\n            A[j] = save1 + temp1 * B[kj] \n            B[kj] = B[kj] + temp2*save1\n        X[k+1] = alpha\n        for j in range(0,k+1):\n            X[j] = X[j] + alpha*B[k-j]\n    return X", "label": 1}
{"index": "gp191042", "code": "def fill_table():\n    var_0 = []\n    table.append({'name': 'John Doe', 'age': 24, 'city': 'Seattle'})\n    table.append({'name': 'Jane Smith', 'age': 32, 'city': 'San Francisco'})\n    table.append({'name': 'Bob Johnson', 'age': 45, 'city': 'New York'})\n    return var_0", "contrast": "def data_filler_simple_registration(self, number_of_rows, db):\n        try:\n            simple_registration = db\n            for i in range(0, number_of_rows):\n                post_simple_reg = {\n                    \"id\": rnd_id_generator(self),\n                    \"email\": self.faker.safe_email(),\n                    \"password\": self.faker.md5(raw_output=False)\n                }\n                simple_registration.save(post_simple_reg)\n            logger.warning(\n                'simple_registration Commits are successful after write job!',\n                extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)", "label": 1}
{"index": "gp035183", "code": "def pruned(param_0, param_1=None, param_2=None):\n    var_0 = {'name': param_0, 'result': None, 'comment': '', 'changes': {}}\n    if __opts__['test']:\n        var_0['result'] = None\n        var_0['comment'] = 'Directory \\'{0}\\' is set to be pruned'.format(\n            param_0)\n        return var_0\n    try:\n        var_1 = __salt__['bower.prune'](dir=param_0, runas=param_1, param_2=param_2)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        var_0['result'] = False\n        var_0['comment'] = 'Error pruning \\'{0}\\': {1}'.format(param_0, err)\n        return var_0\n    var_0['result'] = True\n    if var_1:\n        var_0['comment'] = 'Directory \\'{0}\\' was successfully pruned'.format(param_0)\n        var_0['changes'] = {'old': [], 'new': var_1}\n    else:\n        var_0['comment'] = 'No packages were pruned from directory \\'{0}\\''.format(param_0)\n    return var_0", "contrast": "import os\ndef clean_bower_dir(name, user):\n    os.system(\"sudo -u \" + user + \" bower prune --allow-root\", cwd=name)", "label": 0}
{"index": "gp206781", "code": "import numpy as np\ndef apply_salt_and_pepper_noise(param_0, param_1, param_2=None, param_3=None):\n    if param_2 is None:\n        param_2 = np.min(param_0)\n    if param_3 is None:\n        param_3 = np.max(param_0)\n    var_0 = X.size\n    var_1 = int(var_0 * param_1)\n    var_2 = np.random.choice(var_0, var_1, replace=False)\n    var_3 = np.zeros(var_0)\n    var_3[var_2] = 1\n    var_3 = noisy_pixels.reshape(X.shape)\n    var_4 = X.copy()\n    var_4[var_3 == 1] = np.random.choice([param_2, param_3], size=var_1)\n    return var_4", "contrast": "def salt_and_pepper_noise(X, v):\n    X_noise = X.copy()\n    n_features = X.shape[1]\n    mn = X.min()\n    mx = X.max()\n    for i, sample in enumerate(X):\n        mask = np.random.randint(0, n_features, v)\n        for m in mask:\n            if np.random.random() < 0.5:\n                X_noise[i][m] = mn\n            else:\n                X_noise[i][m] = mx\n    return X_noise", "label": 1}
{"index": "gp003520", "code": "def session_new(param_0, **param_1):\n        var_0 = self._get_path('session_new')\n        var_1 = self._GET(var_0, param_1)\n        self._set_attrs_to_values(var_1)\n        return var_1", "contrast": "import requests\ndef generate_session_id(request_token):\n    url = \"https://api.example.com/generate_session_id\"\n    headers = {\"Authorization\": f\"Bearer {request_token}\"}\n    response = requests.get(url, headers=headers)\n    return response.json()", "label": 0}
{"index": "gp165887", "code": "async def addFeedData(param_0, param_1, param_2, param_3=None):\n        async with await self.snap() as var_0:\n            snap.strict = False\n            return await snap.addFeedData(param_1, param_2, param_3=param_3)", "contrast": "def add_data(name: str, items: list, seqn: tuple) -> int:\n    expected_offset = None\n    return expected_offset", "label": 0}
{"index": "gp310526", "code": "def remove_sister(param_0=None):\n    if not self.up:\n        return None\n    var_0 = self.up.children\n    if not var_0:\n        return None\n    if param_0:\n        if param_0 in var_0:\n            sisters.remove(param_0)\n            return param_0\n        else:\n            return None\n    else:\n        var_1 = var_0[0]\n        sisters.remove(var_1)\n        return var_1", "contrast": "def remove_sister(self, sister=None):\n        sisters = self.get_sisters()\n        if len(sisters) > 0:\n            if sister is None:\n                sister = sisters.pop(0)\n            return self.up.remove_child(sister)", "label": 1}
{"index": "gp276108", "code": "def get_catalog_yaml_file_name(param_0):\n    return param_0 + '_catalog.yaml'", "contrast": "def catalog_split_yaml(self, **kwargs):\n        kwargs_copy = self.base_dict.copy()\n        kwargs_copy.update(**kwargs)\n        self._replace_none(kwargs_copy)        \n        localpath = NameFactory.catalog_split_yaml_format.format(**kwargs_copy)\n        if kwargs.get('fullpath', False):\n            return self.fullpath(localpath=localpath)\n        return localpath", "label": 1}
{"index": "gp038626", "code": "def halted(param_0, param_1=True):\n    var_0 = {'name': param_0,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n    var_1 = __salt__['zoneadm.list'](installed=True)\n    if param_0 in var_1:\n        if var_1[param_0]['state'] != 'running':\n            var_0['result'] = True\n            var_0['comment'] = 'Zone {0} already halted'.format(param_0)\n        else:\n            if not __opts__['test']:\n                var_2 = __salt__['zoneadm.shutdown'](param_0) if param_1 else __salt__['zoneadm.halt'](param_0)\n            if __opts__['test'] or var_2['status']:\n                var_0['result'] = True\n                var_0['changes'][param_0] = 'halted'\n                var_0['comment'] = 'Zone {0} halted'.format(param_0)\n            else:\n                var_0['result'] = False\n                var_0['comment'] = 'Failed to halt {0}'.format(param_0)\n    else:\n        var_0['comment'] = []\n        var_0['comment'].append(\n            'The zone {0} is not in the installed state.'.format(param_0)\n        )\n        for var_3 in var_1:\n            if var_1[var_3]['uuid'] == param_0:\n                var_0['comment'].append(\n                    'The zone {0} has a uuid of {1}, please use the zone name instead!'.format(\n                        var_3,\n                        param_0,\n                    )\n                )\n        var_0['result'] = True\n        var_0['comment'] = \"\\n\".join(var_0['comment'])\n    return var_0", "contrast": "def ensure_zone_halted(name: str, graceful: bool) -> None:\n    if graceful:\n        print(f\"Performing graceful shutdown of zone {name}\")\n    else:\n        print(f\"Halting zone {name} immediately\")", "label": 0}
{"index": "gp283658", "code": "from datetime import datetime\ndef compare_timestamps_file(param_0, param_1):\n    with open(param_0, 'r') as var_0:\n        var_1 = [datetime.strptime(line.strip(), '%Y-%m-%d %H:%M:%S.%f') for var_2 in var_0]\n    param_1 = datetime.strptime(param_1, '%Y-%m-%d %H:%M:%S.%f')\n    timestamps.append(param_1)\n    return max(var_1)", "contrast": "def _max_timestamps(dirname_full, remove, compare_tstamp):\n    tstamp = _get_timestamp_cached(dirname_full, remove)\n    return max_none((tstamp, compare_tstamp))", "label": 1}
{"index": "gp278430", "code": "def custom_destructor(param_0, param_1):\n    del param_0\n    adapter.fragments.remove(param_0)\n    del self", "contrast": "def destroy(self):\n        fragment = self.fragment\n        if fragment:\n            fragment.setFragmentListener(None)\n            if self.adapter is not None:\n                self.adapter.removeFragment(self.fragment)\n            del self.fragment\n        super(AndroidFragment, self).destroy()", "label": 1}
{"index": "gp055387", "code": "def Kdiag(param_0, param_1):\n        var_0 = self.variance_Yt\n        var_1 = self.variance_Yx\n        var_2 = 1./(2*self.lengthscale_Yt)\n        var_3 = 1./(2*self.lengthscale_Yx)\n        var_4 = self.a\n        var_5 = self.b\n        var_6 = self.c\n        var_7 = (2*var_2 )*var_0*var_1\n        var_8 = ( - 2*var_3 )*var_0*var_1\n        var_9 = ( 4*3*var_3**2 )*var_0*var_1\n        var_10 = np.zeros(X.shape[0])\n        var_11 = index_to_slices(param_1[:,-1])\n        for var_12, var_13 in enumerate(var_11):\n            for var_14 in var_13:\n                if var_12==0:\n                    var_10[var_14]+= var_0*var_1\n                elif var_12==1:\n                    var_10[var_14]+= var_5**2*var_7 - 2*var_4*var_6*var_8 + var_4**2*var_9 + var_6**2*var_0*var_1\n                else:\n                    raise ValueError(\"invalid input/output index\")\n        return var_10", "contrast": "import numpy as np\ndef compute_covariance_diagonal(X):\n    cov_matrix = np.cov(X, rowvar=False)\n    cov_diag = np.diag(cov_matrix)\n    return cov_diag", "label": 0}
{"index": "gp209483", "code": "def rupture_to_element(param_0, param_1=None):\n    var_0 = Element(param_1,\n                      name=str(rup.rupid),\n                      seed=str(rup.seed))\n    for var_1, var_2 in rup.events_by_ses.items():\n        var_3 = Element(var_0,\n                          name=str(var_1))\n        for var_4 in var_2:\n            var_5 = Element(var_3,\n                        event.name,\n                        attrs={'N026': str(event.magnitude),\n                               'N027': str(event.latitude),\n                               'N028': str(event.longitude),\n                               'N029': str(event.depth)})\n    return var_0", "contrast": "def rupture_to_element(rup, parent=None):\n    if parent is None:\n        parent = et.Element('root')\n    rup_elem = et.SubElement(parent, rup.typology)\n    elem = et.SubElement(rup_elem, 'stochasticEventSets')\n    n = 0\n    for ses in rup.events_by_ses:\n        eids = rup.events_by_ses[ses]['eid']\n        n += len(eids)\n        ses_elem = et.SubElement(elem, 'SES', id=ses)\n        ses_elem.text = ' '.join(str(eid) for eid in eids)\n    rup_elem.set('id', rup.rupid)\n    rup_elem.set('multiplicity', str(n))\n    sub_elems(rup_elem, rup, 'magnitude',  'strike', 'dip', 'rake')\n    h = rup.hypocenter\n    et.SubElement(rup_elem, 'hypocenter', dict(lon=h.x, lat=h.y, depth=h.z))\n    if rup.is_from_fault_source:\n        mesh_elem = et.SubElement(rup_elem, 'mesh')\n        for i, row in enumerate(rup.lons):\n            for j, col in enumerate(row):\n                node_elem = et.SubElement(mesh_elem, 'node')\n                node_elem.set('row', str(i))\n                node_elem.set('col', str(j))\n                node_elem.set('lon', str(rup.lons[i][j]))\n                node_elem.set('lat', str(rup.lats[i][j]))\n                node_elem.set('depth', str(rup.depths[i][j]))\n        mesh_elem.set('rows', str(i + 1))\n        mesh_elem.set('cols', str(j + 1))\n    elif rup.is_gridded_surface:\n        mesh_elem = et.SubElement(rup_elem, 'mesh')\n        for j, _ in enumerate(rup.lons):\n            node_elem = et.SubElement(mesh_elem, 'node')\n            node_elem.set('row', '0')\n            node_elem.set('col', str(j))\n            node_elem.set('lon', str(rup.lons[j]))\n            node_elem.set('lat', str(rup.lats[j]))\n            node_elem.set('depth', str(rup.depths[j]))\n    else:\n        if rup.is_multi_surface:\n            assert len(rup.lons) % 4 == 0\n            assert len(rup.lons) == len(rup.lats) == len(rup.depths)\n            for offset in range(len(rup.lons) // 4):\n                start = offset * 4\n                end = offset * 4 + 4\n                lons = rup.lons[start:end]  \n                lats = rup.lats[start:end]  \n                depths = rup.depths[start:end]  \n                ps_elem = et.SubElement(\n                    rup_elem, 'planarSurface')\n                top_left, top_right, bottom_left, bottom_right =                    zip(lons, lats, depths)\n                for el_name, corner in (\n                        ('topLeft', top_left),\n                        ('topRight', top_right),\n                        ('bottomLeft', bottom_left),\n                        ('bottomRight', bottom_right)):\n                    corner_elem = et.SubElement(ps_elem, el_name)\n                    corner_elem.set('lon', '%.7f' % corner[0])\n                    corner_elem.set('lat', '%.7f' % corner[1])\n                    corner_elem.set('depth', '%.7f' % corner[2])\n        else:\n            ps_elem = et.SubElement(rup_elem, 'planarSurface')\n            for el_name, corner in (\n                    ('topLeft', rup.top_left_corner),\n                    ('topRight', rup.top_right_corner),\n                    ('bottomLeft', rup.bottom_left_corner),\n                    ('bottomRight', rup.bottom_right_corner)):\n                corner_elem = et.SubElement(ps_elem, el_name)\n                corner_elem.set('lon', '%.7f' % corner[0])\n                corner_elem.set('lat', '%.7f' % corner[1])\n                corner_elem.set('depth', '%.7f' % corner[2])\n    return parent", "label": 1}
{"index": "gp289428", "code": "def merge_device_changes(param_0: dict, param_1: dict) -> dict:\n    var_0 = {}\n    for var_1, var_2 in updated_devices.items():\n        if var_1 not in param_0:\n            var_0[var_1] = var_2\n        else:\n            for var_3, var_4 in device_info.items():\n                if param_0[var_1].get(var_3) != var_4:\n                    if var_1 not in var_0:\n                        var_0[var_1] = {var_3: var_4}\n                    else:\n                        var_0[var_1][var_3] = var_4\n    return var_0", "contrast": "def resolve_updates(orig_list, updated_list):\n        if updated_list is not None and updated_list:\n            if orig_list is None:\n                orig_list = updated_list\n            else:\n                for new_device in updated_list:\n                    was_found = False\n                    for device in orig_list:\n                        if new_device.cid == device.cid:\n                            was_found = True\n                            break\n                    if not was_found:\n                        orig_list.append(new_device)\n                for device in orig_list:\n                    should_remove = True\n                    for new_device in updated_list:\n                        if device.cid == new_device.cid:\n                            should_remove = False\n                            break\n                    if should_remove:\n                        orig_list.remove(device)\n            [device.update() for device in orig_list]\n        return orig_list", "label": 1}
{"index": "gp145387", "code": "def _auth_headers(param_0):\n        if self.has_cookies():\n            return [(\"Cookie\", _make_cookie_header(list(self.get_cookies().items())))]\n        elif self.basic and (self.username and self.password):\n            var_0 = 'Basic %s' % b64encode((\"%s:%s\" % (self.username, self.password)).encode('utf-8')).decode('ascii')\n            return [(\"Authorization\", var_0)]\n        elif self.token is _NoAuthenticationToken:\n            return []\n        else:\n            if self.token.startswith('Splunk '):\n                var_0 = self.token\n            else:\n                var_0 = 'Splunk %s' % self.token\n            return [(\"Authorization\", var_0)]", "contrast": "def get_authentication_headers(context):\n    auth_header_name = \"Authorization\"\n    auth_token = context.get_authentication_token()\n    auth_header_value = f\"Bearer {auth_token}\"\n    headers = [(auth_header_name, auth_header_value)]\n    return headers", "label": 0}
{"index": "gp016874", "code": "def runJob(param_0, param_1, param_2, param_3=None, param_4=False):\n        if param_3 is None:\n            param_3 = range(rdd._jrdd.partitions().size())\n        var_0 = rdd.mapPartitions(param_2)\n        var_1 = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, param_3)\n        return list(_load_from_socket(var_1, mappedRDD._jrdd_deserializer))", "contrast": "def runJob(rdd, partitionFunc, partitions=None, allowLocal=False):\n    if partitions:\n        results = rdd.mapPartitionsWithIndex(lambda i, part: partitionFunc(part) if i in partitions else []).collect()\n    else:\n        results = rdd.mapPartitions(partitionFunc).collect()\n    return [elem for sublist in results for elem in sublist]", "label": 0}
{"index": "gp088537", "code": "def get_monomers(param_0, param_1=True):\n        if param_1 and self.ligands:\n            var_0 = self._monomers + self.ligands._monomers\n        else:\n            var_0 = self._monomers\n        return iter(var_0)", "contrast": "def retrieve_monomers(ampal_obj, ligands=False):\n    monomers = ampal_obj.get_monomers()\n    if ligands:\n        monomers += ampal_obj.get_ligands()\n    return monomers", "label": 0}
{"index": "gp184595", "code": "def send_persisted_messages(param_0):\n    var_0 = retrieve_persisted_messages(param_0)\n    for var_1 in var_0:\n        websocket.send(var_1)\n    clear_persisted_messages(param_0)", "contrast": "def send_persisted_messages(self, websocket):\n        for channel in self._subscription.channels:\n            message = self._connection.get(channel)\n            if message:\n                websocket.send(message)", "label": 1}
{"index": "gp127472", "code": "def classinstances(param_0):\n        var_0 = [var_1 for var_1 in cls.allinstances() if type(var_1) == param_0]\n        return var_0", "contrast": "class JB_Gui:\n    instances = []\n    def __init__(self):\n        self.__class__.instances.append(self)\n    @classmethod\n    def get_all_instances(cls):\n        instance_list = []\n        for instance in cls.instances:\n            if isinstance(instance, cls):\n                instance_list.append(instance)\n        return instance_list", "label": 0}
{"index": "gp138043", "code": "def index_genome_alignment_by_locus(param_0, param_1, param_2=False):\n  var_0 = functools.partial(genome_alignment_iterator,\n                                 reference_species=\"hg19\", index_friendly=True)\n  var_1 = JustInTimeGenomeAlignmentBlock.build_hash\n  var_2 = IndexedFile(param_0, var_0, var_1)\n  idx.write_index(param_1, param_2=param_2)", "contrast": "def build_index(coords, ref_genome):\n    index = {}\n    for i, coord in enumerate(coords):\n        key = ref_genome[coord[0]:coord[1]]\n        if key in index:\n            index[key].append(i)\n        else:\n            index[key] = [i]\n    return index", "label": 0}
{"index": "gp302003", "code": "import time\nimport threading\nimport sqlite3\ndef update_and_commit_with_insert(param_0):\n    var_0 = sqlite3.connect('test.db')\n    var_1 = conn.cursor()\n    c.execute(param_0)\n    conn.commit()\n    time.sleep(1)  \n    var_2 = c.lastrowid\n    c.execute(f\"SELECT * FROM TABLE_NAME WHERE ROW_ID = {var_2}\")\n    var_3 = c.fetchone()\n    conn.close()\n    return var_3", "contrast": "def update(sql, *args, **kwargs):\n        assert \"update\" in sql.lower(), 'This function requires an update statement, provided: {}'.format(sql)\n        cursor = CoyoteDb.execute_and_commit(sql, *args, **kwargs)\n        last_row_id = cursor.lastrowid\n        return last_row_id", "label": 1}
{"index": "gp292293", "code": "import toml\nfrom typing import Optional\ndef load_config(param_0: Optional[str] = None):\n    if param_0 is None:\n        var_0 = {\"key1\": \"value1\", \"key2\": \"value2\"}\n        with open('example_config.toml', 'w') as var_1:\n            toml.dump(var_0, var_1)\n    else:\n        with open(param_0) as var_1:\n            var_2 = toml.load(var_1)\n        return var_2", "contrast": "def get_or_create_config(path, config):\n    if os.path.isfile(path):\n        with open(path) as fh:\n            _LOG.debug(\"loading config from %s\", os.path.abspath(path))\n            config._inflate(toml.load(fh))\n    else:\n        try:\n            os.makedirs(os.path.dirname(path))\n        except OSError:\n            pass\n        with open(path, \"w\") as fh:\n            toml.dump(config._deflate(), fh)", "label": 1}
{"index": "gp275763", "code": "def create_summed_likelihood(param_0):\n    from fermipy.like import LikelihoodModel, SummedLikelihood\n    var_0 = []\n    for var_1 in param_0:\n        likelihood_objects.append(LikelihoodModel(var_1))\n    var_2 = SummedLikelihood()\n    for var_3 in var_0:\n        summed_likelihood.addComponent(var_3)\n    return var_2", "contrast": "def _create_likelihood(self, srcmdl=None):\n        self._like = SummedLikelihood()\n        for c in self.components:\n            c._create_binned_analysis(srcmdl)\n            self._like.addComponent(c.like)\n        self.like.model = self.like.components[0].model\n        self._fitcache = None\n        self._init_roi_model()", "label": 1}
{"index": "gp167186", "code": "def _parentage(param_0, param_1, param_2):\n        if not param_2:\n            return tuple(param_1)\n        if not param_2[0]:\n            return tuple(param_1)\n        var_0, var_1 = param_2[0], param_2[1:]\n        var_2 = param_1[0]\n        var_3 = var_0[0]\n        for var_4 in var_0:\n            if category.idx > leaf_node.idx:\n                break\n            var_3 = var_4\n        var_5 = tuple(param_1) + (var_3,)\n        return self._parentage(var_5, var_1)", "contrast": "def get_ancestors(categories, levels):\n    def find_parent(categories, parent_idx):\n        parent = None\n        for category in categories:\n            if category.idx <= parent_idx:\n                if parent is None or parent.idx < category.idx:\n                    parent = category\n        return parent\n    ancestors = []\n    parent_idx = categories[0].idx\n    for level in levels:\n        parent = find_parent(level, parent_idx)\n        if parent is not None:\n            ancestors.insert(0, parent)\n            parent_idx = parent.idx\n    return tuple(ancestors)", "label": 0}
{"index": "gp026295", "code": "def get_read_options(param_0, param_1):\n    if param_1 is None:\n        if param_0:\n            return datastore_pb2.ReadOptions(\n                read_consistency=datastore_pb2.ReadOptions.EVENTUAL\n            )\n        else:\n            return datastore_pb2.ReadOptions()\n    else:\n        if param_0:\n            raise ValueError(\"eventual must be False when in a transaction\")\n        else:\n            return datastore_pb2.ReadOptions(transaction=param_1)", "contrast": "def validate_read_options(eventual: bool, transaction_id: bytes) -> datastore_pb2.ReadOptions:\n    if eventual and transaction_id is not None:\n        raise ValueError(\"Eventual consistency is not compatible with transaction ID\")\n    read_options = datastore_pb2.ReadOptions()\n    read_options.read_consistency = (datastore_pb2.EVENTUAL if eventual\n                                     else datastore_pb2.STRONG)\n    read_options.transaction.CopyFrom(datastore_pb2.TransactionOptions(\n        read_write=datastore_pb2.TransactionOptions.ReadOnly(\n            previous_transaction=transaction_id)))\n    return read_options", "label": 0}
{"index": "gp297038", "code": "def has_full_name(param_0):\n    var_0 = name.split()\n    if len(var_0) < 2:\n        return False\n    for var_1 in var_0:\n        if len(var_1) == 1 or var_1[-1] == '.':\n            return False\n    return True", "contrast": "def author_name_contains_fullnames(author_name):\n    def _is_initial(name_part):\n        return len(name_part) == 1 or u'.' in name_part\n    parsed_name = ParsedName(author_name)\n    if len(parsed_name) == 1:\n        return False\n    elif any([_is_initial(name_part) for name_part in parsed_name]):\n        return False\n    return True", "label": 1}
{"index": "gp128337", "code": "def netdevs():\n    with open('/proc/net/dev') as var_0:\n        var_1 = f.readlines()\n    var_2={}\n    var_3 = namedtuple('data',['rx','tx'])\n    for var_4 in var_1[2:]:\n        var_4 = line.split(':')\n        if var_4[0].strip() != 'lo':\n            var_2[var_4[0].strip()] = var_3(float(var_4[1].split()[0])/(1024.0*1024.0), \n                                                float(var_4[1].split()[8])/(1024.0*1024.0))\n    return var_2", "contrast": "import psutil\ndef get_network_bytes():\n    net_io_counters = psutil.net_io_counters(pernic=True)\n    network_bytes = {}\n    for nic, addrs in psutil.net_if_addrs().items():\n        if nic in net_io_counters:\n            bytes_sent = net_io_counters[nic].bytes_sent\n            bytes_recv = net_io_counters[nic].bytes_recv\n            network_bytes[nic] = {\"RX_bytes\": bytes_recv, \"TX_bytes\": bytes_sent}\n    return network_bytes", "label": 0}
{"index": "gp002614", "code": "def _validate(param_0, param_1):\n        var_0 = qobj.config.n_qubits\n        var_1 = self.configuration().n_qubits\n        if var_0 > var_1:\n            raise BasicAerError('Number of qubits {} '.format(var_0) +\n                                'is greater than maximum ({}) '.format(var_1) +\n                                'for \"{}\".'.format(self.name()))\n        if hasattr(qobj.config, 'shots') and qobj.config.shots != 1:\n            logger.info('\"%s\" only supports 1 shot. Setting shots=1.',\n                        self.name())\n            qobj.config.shots = 1\n        for var_2 in qobj.experiments:\n            var_3 = experiment.header.name\n            if getattr(experiment.config, 'shots', 1) != 1:\n                logger.info('\"%s\" only supports 1 shot. '\n                            'Setting shots=1 for circuit \"%s\".',\n                            self.name(), var_3)\n                experiment.config.shots = 1\n            for var_4 in experiment.instructions:\n                if operation.name in ['measure', 'reset']:\n                    raise BasicAerError('Unsupported \"%s\" instruction \"%s\" ' +\n                                        'in circuit \"%s\" ', self.name(),\n                                        operation.name, var_3)", "contrast": "def semantic_validations(qobj) -> None:\n    for experiment in qobj.experiments:\n        if experiment.config.shots != 1:\n            raise ValueError(\"Semantic Error: Shots must be set to 1\")\n        for op in experiment.instructions:\n            if op.name == 'measure' and op.op.index != 0:\n                raise ValueError(\"Semantic Error: Measurements cannot be in the middle\")", "label": 0}
{"index": "gp132201", "code": "def get_image_dimension(param_0, param_1):\n        var_0 = (None, None)\n        try:\n            if url.startswith('//'):\n                param_1 = 'http:' + param_1\n            var_1 = requests.get(param_1).content\n            var_2 = Image.open(BytesIO(var_1))\n            var_0 = im.size\n        except Exception:\n            logger.warning(\"Error getting image size {}\".format(param_1), exc_info=True)\n        return var_0", "contrast": "from urllib.request import urlopen\nfrom PIL import ImageFile\ndef get_image_size(url):\n    try:\n        with urlopen(url) as file:\n            parser = ImageFile.Parser()\n            while True:\n                data = file.read(1024)\n                if not data:\n                    break\n                parser.feed(data)\n                if parser.image:\n                    return parser.image.size\n    except:\n        pass\n    return (None, None)", "label": 0}
{"index": "gp321624", "code": "def store_demonstration(param_0, param_1):\n    demo_memory.append(param_0)\n    return param_1", "contrast": "def import_demo_experience(self, states, internals, actions, terminal, reward):\n        fetches = self.import_demo_experience_output\n        feed_dict = self.get_feed_dict(\n            states=states,\n            internals=internals,\n            actions=actions,\n            terminal=terminal,\n            reward=reward\n        )\n        self.monitored_session.run(fetches=fetches, feed_dict=feed_dict)", "label": 1}
{"index": "gp263552", "code": "def serialize_inline_query_result_cached_audio(param_0):\n    var_0 = {\n        'type': 'audio',\n        'audio_file_id': self.audio_file_id,\n        'caption': self.caption,\n        'parse_mode': self.parse_mode,\n        'reply_markup': self.reply_markup,\n    }\n    return var_0", "contrast": "def to_array(self):\n        array = super(InlineQueryResultCachedAudio, self).to_array()\n        array['type'] = u(self.type)  \n        array['id'] = u(self.id)  \n        array['audio_file_id'] = u(self.audio_file_id)  \n        if self.caption is not None:\n            array['caption'] = u(self.caption)  \n        if self.parse_mode is not None:\n            array['parse_mode'] = u(self.parse_mode)  \n        if self.reply_markup is not None:\n            array['reply_markup'] = self.reply_markup.to_array()  \n        if self.input_message_content is not None:\n            array['input_message_content'] = self.input_message_content.to_array()  \n        return array", "label": 1}
{"index": "gp251195", "code": "def _load_object_from_filesystem(param_0):\n    with open(param_0, 'rb') as var_0:\n        var_1 = f.read()\n    return var_1", "contrast": "def load(self):\n        if self.is_persisted:\n            with open(self.file_name, 'rb') as f:\n                self.object_property = pickle.load(f)", "label": 1}
{"index": "gp318976", "code": "def add_tags(param_0: List[str]) -> bool:\n    var_0 = []\n    for var_1 in param_0:\n        if var_1 not in var_0:\n            added_tags.append(var_1)\n        else:\n            return False\n    return True", "contrast": "def _add_tags(self, tags):\n        alltagsadded = True\n        for tag in tags:\n            if not self._add_tag(tag):\n                alltagsadded = False\n        return alltagsadded", "label": 1}
{"index": "gp262734", "code": "import re\ndef gather_team_data(param_0, param_1, param_2, param_3, param_4):\n    var_0 = []\n    var_1 = base_url.format(re.findall(param_2, param_3)[0])\n    var_2 = requests.get(var_1).content\n    var_3 = BeautifulSoup(var_2, 'html.parser')\n    var_4 = team_soup.find_all('div', {'class': 'team-info'})\n    for var_5 in var_4:\n        var_6 = {\n            'team_name': info.find('a', {'class': 'team-name'}).text,\n            'location': info.find('div', {'class': 'team-location'}).text,\n            'record': info.find('div', {'class', 'team-record'}).text,\n            'sport': param_4\n        }\n        team_data.append(var_6)\n    return var_0", "contrast": "def _get_team_info_raw(soup, base_url, team_pattern, team, sport):\n    team_url = None\n    team_name = None\n    for link in soup.find_all('a'):\n        if re.search(team_pattern, link.string):\n            team_name = link.string\n            team_url = base_url.replace('/teams/', link['href'])\n    if team_url is not None and team_name is not None:\n        team_soup = BeautifulSoup(requests.get(team_url).content, 'html.parser')\n        team_info_raw = team_soup.find('div', id='meta').contents[3].get_text().split('\\n')\n        team_info_raw = [x.replace('\\t', '') for x in team_info_raw]\n        team_info_raw = [x.strip() for x in team_info_raw if x != '']\n        team_info_raw[0] = team_name\n        return team_info_raw\n    else:\n        raise errors.TeamNotFoundError(sport, team)", "label": 1}
{"index": "gp152490", "code": "def schoice(param_0, param_1: str, param_2: int = 10) -> str:\n        return ''.join(self.choice(list(param_1))\n                       for var_0 in range(param_2))", "contrast": "def choice_func(seq, end):\n    import random\n    return ''.join(random.choice(seq) for _ in range(end))", "label": 0}
{"index": "gp089380", "code": "def children(param_0) -> NodeList:\n        return NodeList([var_0 for var_0 in self.childNodes\n                         if e.nodeType == Node.ELEMENT_NODE])", "contrast": "def get_child_nodes():\n    child_nodes = [1, 2, 3, 4, 5]\n    return child_nodes", "label": 0}
{"index": "gp094374", "code": "def correlation(param_0, param_1, param_2=None, param_3=[1], param_4=None):\n    var_0 = P.shape[0]\n    var_1 = np.asarray(param_3).max()\n    if var_1 < var_0:\n        return correlation_matvec(param_0, param_1, param_2=param_2, param_3=param_3)\n    else:\n        return correlation_decomp(param_0, param_1, param_2=param_2, param_3=param_3, param_4=param_4)", "contrast": "def time_correlation(P, obs1, obs2=None, times=None, k=None):\n    if obs2 is None:\n        obs2 = obs1\n    if times is None:\n        times = [0]\n    if k is None:\n        k = min(P.shape)\n    eigvals, eigvecs = np.linalg.eig(P)\n    idx = np.argsort(eigvals)[::-1][:k]\n    eigvals, eigvecs = eigvals[idx], eigvecs[:, idx]\n    correlations = np.zeros_like(times, dtype=np.float)\n    for i, tau in enumerate(times):\n        P_tau = eigvecs.dot(np.diag(np.exp(tau * eigvals))).dot(np.linalg.inv(eigvecs))\n        E1_tau = obs1.dot(P_tau)\n        E2_tau = obs2.dot(P_tau)\n        correlations[i] = np.sum(E1_tau * E2_tau)\n    return correlations", "label": 0}
{"index": "gp086226", "code": "def _parse_join(param_0, param_1):\n        var_0 = []\n        tokens.pop(0)  \n        tokens.pop(0)  \n        children.append(self._parse_nested_interval(param_1))\n        while param_1[0] == ',':\n            tokens.pop(0)\n            children.append(self._parse_nested_interval(param_1))\n        tokens.pop(0)  \n        var_1, var_2 = next((child.chromosome, child.strand) for var_3 in var_0)\n        var_4 = min(child.start.position for var_3 in var_0)\n        var_5 = max(child.stop.position for var_3 in var_0)\n        var_6 = NestedGenomicInterval(var_4, var_5, var_1=var_1, var_2=var_2)\n        parent.children = var_0\n        return var_6", "contrast": "def parse_join(join_string):\n    join_args = join_string.strip('join()').split(',')\n    if len(join_args) == 1:\n        return (parse_super_range(join_args[0]), None)\n    else:\n        return (parse_super_range(join_args[0]), parse_super_range(join_args[1]))", "label": 0}
{"index": "gp267776", "code": "def get_input_shard_spec(param_0: int, param_1: int) -> dict:\n    if param_0 < 1 or param_1 < 1:\n        raise ValueError(\"Number of inputs and shards must be positive integers.\")\n    var_0 = param_0 // param_1\n    var_1 = {\n        \"inputs\": []\n    }\n    for var_2 in range(param_1):\n        var_3 = var_2 * var_0\n        var_4 = var_3 + var_0\n        if var_2 == param_1 - 1:  \n            var_4 = param_0\n        var_1[\"inputs\"].append({\n            \"start\": var_3,\n            \"end\": var_4\n        })\n    return var_1", "contrast": "def to_json(self):\n    new_pos = self._blob_reader.tell()\n    if self._has_iterated:\n      new_pos -= 1\n    return {self.BLOB_KEY_PARAM: self._blob_key,\n            self.INITIAL_POSITION_PARAM: new_pos,\n            self.END_POSITION_PARAM: self._end_position}", "label": 1}
{"index": "gp123038", "code": "def subscribe(param_0, param_1, param_2=(tc.VAR_ROAD_ID, tc.VAR_LANEPOSITION), param_3=0, param_4=2**31 - 1):\n        Domain.subscribe(param_0, param_1, param_2, param_3, param_4)", "contrast": "def subscribe(string:str, integers:list[int], start:int, end:int) -> None:\n    pass", "label": 0}
{"index": "gp056954", "code": "def strip_output(param_0):\n    nb.metadata.pop('signature', None)\n    nb.metadata.pop('widgets', None)\n    for var_0 in _cells(param_0):\n        if 'outputs' in var_0:\n            var_0['outputs'] = []\n        if 'prompt_number' in var_0:\n            var_0['prompt_number'] = None\n    return param_0", "contrast": "def strip_outputs(notebook_obj):\n    for cell in notebook_obj['cells']:\n        if 'outputs' in cell:\n            cell['outputs'] = []\n    return notebook_obj", "label": 0}
{"index": "gp068864", "code": "def get_id_head(param_0):\n        var_0 = None\n        for var_1 in param_0:\n            if target_node.is_head():\n                var_0 = target_node.get_id()\n                break\n        return var_0", "contrast": "def get_head_target_id(targets):\n    for target in targets:\n        if target[\"isHead\"]:\n            return target[\"id\"]\n    return None", "label": 0}
{"index": "gp280515", "code": "import numpy as np\nfrom scipy.stats import entropy\ndef jensen_shannon_divergence(param_0, param_1):\n    var_0 = (param_0 + param_1) / 2\n    return (entropy(param_0, var_0) + entropy(param_1, var_0)) / 2", "contrast": "def js_divergence(p, q):\n    m = .5 * (p+q)\n    js_div = .5*kl_divergence(p, m) + .5*kl_divergence(q, m)\n    return js_div", "label": 1}
{"index": "gp145703", "code": "def _publish_replset(param_0, param_1, param_2):\n        var_0 = param_2 + ['replset']\n        self._publish_dict_with_prefix(param_1, var_0)\n        var_1 = len(param_1['members'])\n        var_2 = reduce(lambda value, node: value + node['health'],\n                               param_1['members'], 0)\n        self._publish_dict_with_prefix({\n            'healthy_nodes': var_2,\n            'total_nodes': var_1\n        }, var_0)\n        for var_3 in param_1['members']:\n            var_4 = var_3[self.config['replset_node_name']]\n            var_5 = str(replset_node_name.split('.')[0])\n            self._publish_dict_with_prefix(var_3, var_0 + ['node', var_5])", "contrast": "def publish_replica_set_stats(response):\n    numeric_values = []\n    healthy_nodes = 0\n    total_nodes = 0\n    observed_statuses = {}\n    for key, value in response.items():\n        if isinstance(value, (int, float)):\n            numeric_values.append((key, value))\n        elif key == 'members':\n            total_nodes = value\n            for member in value:\n                if member['health'] == 1:\n                    healthy_nodes += 1\n                observed_statuses[member['_id']] = member['stateStr']\n    print(\"Numeric Values:\")\n    for key, value in numeric_values:\n        print(f\"{key}: {value}\")\n    print(f\"\\nHealthy Nodes: {healthy_nodes}/{total_nodes}\")\n    print(\"\\nObserved Statuses:\")\n    for node_id, status in observed_statuses.items():\n        print(f\"{node_id}: {status}\")", "label": 0}
{"index": "gp299543", "code": "def set_join_clause(param_0, param_1):\n    param_0 += \" JOIN \" + param_1\n    return param_0", "contrast": "def _set_join(self, query=None):\n        if not query:\n            query = self._query\n        foreign_key = '%s.%s' % (self._related.get_table(), self._second_key)\n        query.join(self._parent.get_table(), self.get_qualified_parent_key_name(), '=', foreign_key)", "label": 1}
{"index": "gp260499", "code": "def draw_box(param_0):\n    cb.rect(0, 0, cb.width, cb.height)\n    cb.refresh()", "contrast": "def draw_box(cb, x0, y0, w, h, fg=colors.default_fg, bg=colors.default_bg, h_seps=[], v_seps=[]):\n    w -= 1\n    h -= 1\n    corners = [(x0, y0), (x0 + w, y0), (x0, y0 + h), (x0 + w, y0 + h)]\n    fg = fg()\n    bg = bg()\n    for i, c in enumerate(corners):\n        cb.put(c[0], c[1], BOX_CORNERS[i], fg, bg)\n    for s in h_seps + [0, h]:\n        cb.put(x0 + 1, y0 + s, symbols[\"BOX_HORIZONTAL\"] * (w - 1), fg, bg)\n    for y in range(1, h):\n        for s in v_seps + [0, w]:\n            cb.put(x0 + s, y0 + y, symbols[\"BOX_VERTICAL\"], fg, bg)\n    for s in h_seps:\n        cb.put(x0,     y0 + s, symbols[\"BOX_X_LEFT\"],  fg, bg)\n        cb.put(x0 + w, y0 + s, symbols[\"BOX_X_RIGHT\"], fg, bg)\n    for s in v_seps:\n        cb.put(x0 + s, y0,     symbols[\"BOX_X_TOP\"],    fg, bg)\n        cb.put(x0 + s, y0 + h, symbols[\"BOX_X_BOTTOM\"], fg, bg)", "label": 1}
{"index": "gp327064", "code": "def do_import(param_0):\n    self.create_tables()\n    var_0 = self.gen_rows()\n    for var_1 in var_0:\n        self.connection.execute(f\"INSERT INTO {self.table_name} VALUES ({','.join(['?' for var_2 in range(self.num_fields)])})\", var_1)\n    for var_3 in self.post_import_hooks:\n        var_3(self.connection)\n    for var_4 in self.indexes:\n        self.connection.execute(var_4)", "contrast": "def import_(self, conn):\n        if self.print_progress:\n            print('Beginning', self.__class__.__name__)\n        self._conn = conn\n        self.create_table(conn)\n        if self.mode in ('all', 'import') and self.fname and self.exists() and self.table not in ignore_tables:\n            self.insert_data(conn)\n        if self.mode in ('all', 'index') and hasattr(self, 'index'):\n            self.create_index(conn)\n        if self.mode in ('all', 'import') and hasattr(self, 'post_import'):\n            self.run_post_import(conn)\n        conn.commit()", "label": 1}
{"index": "gp333881", "code": "def undo_scaling(param_0, param_1, param_2=True):\n    var_0 = param_1[0]\n    var_1 = param_1[1]\n    if param_2:\n        param_0 = np.clip(param_0, var_0, var_1)\n    var_2 = (param_0 - var_0) / (var_1 - var_0)\n    return var_2", "contrast": "def inverse_transform(self, X):\n        X = check_array(X, copy=self.copy)\n        X -= self.min_\n        X /= self.scale_\n        return X", "label": 1}
{"index": "gp164809", "code": "def _start_execution_in_container(\n            param_0, param_1, param_2, param_3, param_4, param_5, param_6, param_7, param_8,\n            param_9, param_10,\n            param_11, param_12, param_13, param_14,\n            param_15, param_16):\n        assert self._use_namespaces\n        if param_6 is None:\n            env.update(self._env_override)\n        param_1 = self._build_cmdline(param_1, param_5=param_5)\n        var_0 = 128\n        var_1 = 129\n        var_2, var_3 = os.pipe() \n        var_4, var_5 = os.pipe() \n        var_6 = b'A'\n        var_7 = b'B'\n        if param_6 is None:\n            param_7 = os.path.abspath(param_7 or os.curdir)\n        else:\n            param_6 = os.path.abspath(param_6)\n            param_7 = os.path.abspath(param_7)\n        def grandchild():\n            try:\n                var_0 = container.get_my_pid_from_procfs()\n                container.mount_proc()\n                container.drop_capabilities()\n                container.reset_signal_handling()\n                param_15() \n                os.write(var_5, str(var_0).encode())\n                var_1 = os.read(var_2, 1)\n                assert var_1 == var_7, var_1\n            finally:\n                os.close(var_2)\n                os.close(var_5)\n        def child():\n            try:\n                logging.debug(\"Child: child process of RunExecutor with PID %d started\",\n                              container.get_my_pid_from_procfs())\n                container.block_all_signals()\n                var_0 = {sys.stdin, sys.stdout, sys.stderr,\n                    var_5, var_2, param_2, param_3, param_4} - {None}\n                container.close_open_fds(keep_files=var_0)\n                try:\n                    if self._container_system_config:\n                        libc.sethostname(container.CONTAINER_HOSTNAME)\n                    if not self._allow_network:\n                        container.activate_network_interface(\"lo\")\n                    var_1 = os.read(var_2, len(var_6))\n                    assert var_1 == var_6, var_1\n                    if param_6 is not None:\n                        self._setup_root_filesystem(param_6)\n                    else:\n                        self._setup_container_filesystem(\n                            param_8,\n                            param_12 if param_13 else None,\n                            param_9,\n                            param_10)\n                except EnvironmentError as e:\n                    logging.critical(\"Failed to configure container: %s\", e)\n                    return var_0\n                try:\n                    os.chdir(param_7)\n                except EnvironmentError as e:\n                    logging.critical(\n                        \"Cannot change into working directory inside container: %s\", e)\n                    return var_0\n                try:\n                    var_2 = subprocess.Popen(param_1,\n                                        param_2=param_2,\n                                        param_3=param_3, param_4=param_4,\n                                        param_5=param_5,\n                                        close_fds=False,\n                                        preexec_fn=grandchild)\n                except (EnvironmentError, RuntimeError) as e:\n                    logging.critical(\"Cannot start process: %s\", e)\n                    return var_0\n                var_3 = [libc.CAP_SYS_ADMIN] if param_13 else []\n                container.drop_capabilities(keep=var_3)\n                container.close_open_fds(keep_files={sys.stdout, sys.stderr, var_5, var_2})\n                if _HAS_SIGWAIT:\n                    var_4 = container.wait_for_child_and_forward_all_signals(\n                        grandchild_proc.pid, param_1[0])\n                else:\n                    container.forward_all_signals_async(grandchild_proc.pid, param_1[0])\n                    var_4 = self._wait_for_process(grandchild_proc.pid, param_1[0])\n                logging.debug(\"Child: process %s terminated with exit code %d.\",\n                              param_1[0], var_4[0])\n                if param_13:\n                    libc.umount(temp_dir.encode())\n                os.write(var_5, pickle.dumps(var_4))\n                os.close(var_5)\n                os.read(var_2, 1)\n                os.close(var_2)\n                return 0\n            except EnvironmentError as e:\n                logging.exception(\"Error in child process of RunExecutor\")\n                return var_0\n            except:\n                logging.exception(\"Error in child process of RunExecutor\")\n                return var_1\n        try: \n            try:\n                var_8 = container.execute_in_namespace(child, use_network_ns=not self._allow_network)\n            except OSError as e:\n                raise BenchExecException(\n                    \"Creating namespace for container mode failed: \" + os.strerror(e.errno))\n            logging.debug(\"Parent: child process of RunExecutor with PID %d started.\", var_8)\n            def check_child_exit_code():\n                var_0, var_1 = self._wait_for_process(var_8, param_1[0])\n                var_0 = util.ProcessExitCode.from_raw(var_0)\n                logging.debug(\"Parent: child process of RunExecutor with PID %d terminated with %s.\",\n                              var_8, var_0)\n                if var_0:\n                    if child_exitcode.value:\n                        if child_exitcode.value == var_0:\n                            raise BenchExecException(\"execution in container failed, check log for details\")\n                        elif child_exitcode.value == var_1:\n                            raise BenchExecException(\"unexpected error in container\")\n                        raise OSError(child_exitcode.value, os.strerror(child_exitcode.value))\n                    raise OSError(0, \"Child process of RunExecutor terminated with \" + str(var_0))\n            os.close(var_2)\n            os.close(var_5)\n            container.setup_user_mapping(var_8, uid=self._uid, gid=self._gid)\n            os.write(var_3, var_6) \n            try:\n                var_9 = int(os.read(var_4, 10)) \n            except ValueError:\n                check_child_exit_code()\n                assert False, \"Child process of RunExecutor terminated cleanly but did not send expected data.\"\n            logging.debug(\"Parent: executing %s in grand child with PID %d via child with PID %d.\",\n                          param_1[0], var_9, var_8)\n            cgroups.add_task(var_9)\n            var_10 = param_14()\n            os.write(var_3, var_7)\n            var_11 = os.dup(var_4)\n            var_12 = os.dup(var_3)\n        finally:\n            os.close(var_4)\n            os.close(var_3)\n        def wait_for_grandchild():\n            try:\n                var_0 = os.read(var_11, 1024)\n            except OSError as e:\n                if self.PROCESS_KILLED and e.errno == errno.EINTR:\n                    var_0 = os.read(var_11, 1024)\n                else:\n                    raise e\n            var_1, var_2 = pickle.loads(var_0)\n            var_3 = \"/proc/{}/root\".format(var_8)\n            var_4 = param_16(\n                var_10, util.ProcessExitCode.from_raw(var_1), var_3)\n            if param_13:\n                self._transfer_output_files(\n                    var_3 + param_8,\n                    param_7,\n                    param_12,\n                    param_13)\n            os.close(var_11)\n            os.close(var_12) \n            check_child_exit_code()\n            return var_1, var_2, var_4\n        return var_9, wait_for_grandchild", "contrast": "import os\ndef execute_in_container(command):\n    if os.geteuid() != 0:\n        print(\"This function must be run as root.\")\n        return\n    container_pid = os.fork()\n    if container_pid == 0:\n        os.unshare(os.CLONE_NEWNS)\n        os.unshare(os.CLONE_NEWPID)\n        os.unshare(os.CLONE_NEWUTS)\n        os.unshare(os.CLONE_NEWIPC)\n        with open('/etc/network/interfaces', 'w') as f:\n            f.write('auto lo\\niface lo inet loopback\\n')\n        os.mount('tmpfs', '/tmp', 'tmpfs')\n        os.setgroups([])\n        os.setresgid(65534, 65534, 65534)\n        os.setresuid(65534, 65534, 65534)\n        os.chdir('/tmp')\n        os.chroot('/tmp')\n        os.system(command)\n    else:\n        container_status = os.waitpid(container_pid, 0)[1]\n        if os.WIFSIGNALED(container_status):\n            print(\"Command terminated with signal\", os.WTERMSIG(container_status))\n        else:\n            print(\"Command exited with status\", os.WEXITSTATUS(container_status))", "label": 0}
{"index": "gp049542", "code": "def RetryUpload(param_0, param_1, param_2, param_3):\n    if self.IsErrorRetryable(param_3):\n      var_0 = 0\n      var_1 = config.CONFIG[\"BigQuery.retry_interval\"]\n      while var_0 < config.CONFIG[\"BigQuery.retry_max_attempts\"]:\n        time.sleep(sleep_interval.seconds)\n        logging.info(\"Retrying job_id: %s\", param_2)\n        var_0 += 1\n        try:\n          var_2 = job.execute()\n          return var_2\n        except errors.HttpError as e:\n          if self.IsErrorRetryable(e):\n            var_1 *= config.CONFIG[\"BigQuery.retry_multiplier\"]\n            logging.exception(\"Error with job: %s, will retry in %s\", param_2,\n                              var_1)\n          else:\n            raise BigQueryJobUploadError(\n                \"Can't retry error code %s. Giving up\"\n                \" on job: %s.\" % (e.resp.status, param_2))\n    else:\n      raise BigQueryJobUploadError(\"Can't retry error code %s. Giving up on \"\n                                   \"job: %s.\" % (error.resp.status, param_2))\n    raise BigQueryJobUploadError(\n        \"Giving up on job:%s after %s retries.\" % (param_2, var_0))", "contrast": "import time\nclass BigQueryJobUploadError(Exception):\n    pass\ndef retry_upload_job(job, job_id, error):\n    retry_max_attempts = 3\n    retry_delay_sec = 5\n    for i in range(retry_max_attempts):\n        try:\n            return job.upload_from_filename(job_id)\n        except error as e:\n            time.sleep(retry_delay_sec)\n    raise BigQueryJobUploadError('Failed after {} attempts'.format(retry_max_attempts))", "label": 0}
{"index": "gp145464", "code": "def encrypt(param_0):\n        var_0 = self.parameters.get(\"Plaintext\")\n        if isinstance(var_0, six.text_type):\n            var_0 = value.encode('utf-8')\n        return json.dumps({\"CiphertextBlob\": base64.b64encode(var_0).decode(\"utf-8\"), 'KeyId': 'key_id'})", "contrast": "import base64\ndef encrypt(value):\n    encoded = base64.b64encode(value.encode('utf-8'))\n    return encoded\ndef decrypt(value):\n    decoded = base64.b64decode(value).decode('utf-8')\n    return decoded", "label": 0}
{"index": "gp101395", "code": "def is_image(param_0):\n    return os.path.isfile(param_0) and filename.lower().endswith(ImageExts)", "contrast": "def is_image(filename):\n    image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.bmp']\n    return any(filename.lower().endswith(img_ext) for img_ext in image_extensions)", "label": 0}
{"index": "gp017350", "code": "def _get_list_axis(param_0, param_1, param_2=None):\n        if param_2 is None:\n            param_2 = self.axis or 0\n        try:\n            return self.obj._take(param_1, param_2=param_2)\n        except IndexError:\n            raise IndexError(\"positional indexers are out-of-bounds\")", "contrast": "def get_series_values(key, axis=0):\n    return s.iloc[key]", "label": 0}
{"index": "gp197719", "code": "def write_association_to_file(param_0, param_1):\n    with open(param_1, 'a') as var_0:\n        file.write(param_0 + '\\n')", "contrast": "def write_assoc(self, assoc):\n        if assoc.get(\"header\", False):\n            return\n        subj = assoc['subject']\n        db, db_object_id = self._split_prefix(subj)\n        rel = assoc['relation']\n        qualifier = rel['id']\n        if assoc['negated']:\n            qualifier = 'NOT|' + qualifier\n        goid = assoc['object']['id']\n        ev = assoc['evidence']\n        evidence = self.ecomap.coderef_to_ecoclass(ev['type'])\n        withfrom = \"|\".join(ev['with_support_from'])\n        reference = \"|\".join(ev['has_supporting_reference'])\n        date = assoc['date']\n        assigned_by = assoc['provided_by']\n        annotation_properties = '' \n        interacting_taxon_id = assoc['interacting_taxon']\n        vals = [db,\n                db_object_id,\n                qualifier,\n                goid,\n                reference,\n                evidence,\n                withfrom,\n                interacting_taxon_id, \n                date,\n                assigned_by,\n                self._extension_expression(assoc['object_extensions']),\n                annotation_properties]\n        self._write_row(vals)", "label": 1}
{"index": "gp140637", "code": "def _file_prefix(\n            param_0):\n        self.log.info('starting the ``_file_prefix`` method')\n        if self.ra:\n            var_0 = datetime.now()\n            var_1 = now.strftime(\"%Y%m%dt%H%M%S%f_tns_conesearch_\")\n        elif self.name:\n            var_1 = self.name + \"_tns_conesearch_\"\n        elif self.internal_name:\n            var_1 = self.internal_name + \"_tns_conesearch_\"\n        elif self.discInLastDays:\n            var_2 = str(self.discInLastDays)\n            var_0 = datetime.now()\n            var_1 = now.strftime(\n                var_2 + \"d_since_%Y%m%d_tns_conesearch_\")\n        self.log.info('completed the ``_file_prefix`` method')\n        return var_1", "contrast": "def get_file_prefix(search_type):\n    if search_type == 'linear':\n        prefix = 'linear_search_results_'\n    elif search_type == 'binary':\n        prefix = 'binary_search_results_'\n    else:\n        prefix = 'search_results_'\n    return prefix", "label": 0}
{"index": "gp043990", "code": "def swap_dims(param_0, param_1):\n        var_0 = self._to_temp_dataset().swap_dims(param_1)\n        return self._from_temp_dataset(var_0)", "contrast": "def swap_dims(dims_dict):\n    return Dataset(data_vars=None, coords=None, attrs=None, compat=None, name=None, indexes=None, explicit_coords=None, encoding=None).rename(dims_dict)", "label": 0}
{"index": "gp326369", "code": "def intermediate_to_config(param_0):\n    self.config = {'nodes': {}, 'links': {}}\n    for var_0 in self.intermediate_data['nodes']:\n        self.config['nodes'][var_0['name']] = {'location': var_0['location']}\n    for var_1 in self.intermediate_data['links']:\n        self.config['links'][(var_1['src'], var_1['dst'])] = {'cost': var_1['cost']}", "contrast": "def to_netjson(self, remove_block=True):\n        result = OrderedDict()\n        intermediate_data = list(self.intermediate_data[self.intermediate_key])\n        for index, block in enumerate(intermediate_data):\n            if self.should_skip_block(block):\n                continue\n            if remove_block:\n                self.intermediate_data[self.intermediate_key].remove(block)\n            result = self.to_netjson_loop(block, result, index + 1)\n        return result", "label": 1}
{"index": "gp277663", "code": "import subprocess\ndef read_sphinx_config(param_0: str, param_1: str) -> dict:\n    try:\n        var_0 = subprocess.run(['sphinx-build', '-b', 'dummy', '-D', f'version={param_1}', param_0, '-q'],\n                                       capture_output=True, text=True, check=True)\n        var_1 = sphinx_config.stdout.split('\\n')\n        var_2 = {}\n        for var_3 in var_1:\n            if var_3:\n                var_4, var_5 = pair.split('=')\n                var_2[var_4] = var_5\n        return var_2\n    except subprocess.CalledProcessError:\n        raise HandledError('sphinx-build failed')", "contrast": "def read_config(source, current_name):\n    log = logging.getLogger(__name__)\n    queue = multiprocessing.Queue()\n    config = Config.from_context()\n    with TempDir() as temp_dir:\n        argv = ('sphinx-build', source, temp_dir)\n        log.debug('Running sphinx-build for config values with args: %s', str(argv))\n        child = multiprocessing.Process(target=_read_config, args=(argv, config, current_name, queue))\n        child.start()\n        child.join()  \n        if child.exitcode != 0:\n            log.error('sphinx-build failed for branch/tag while reading config: %s', current_name)\n            raise HandledError\n    config = queue.get()\n    return config", "label": 1}
{"index": "gp084180", "code": "def replace_state_by_id(param_0, param_1, param_2, **param_3):\n        param_3['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._replace_state_by_id_with_http_info(param_1, param_2, **param_3)\n        else:\n            (var_0) = cls._replace_state_by_id_with_http_info(param_1, param_2, **param_3)\n            return var_0", "contrast": "def replace_state_by_id(async: bool, state_id: str, state: State) -> Union[State, Thread]:\n    pass ", "label": 0}
{"index": "gp330721", "code": "def exception_tween_factory(param_0, param_1):\n    def exception_tween(param_0):\n        if not request.environ.get('ALLOW_OWS_SERVICE'):\n            raise Exception(\"Access to OWS service is not allowed\")\n        var_0 = param_0(param_0)\n        return var_0\n    return exception_tween", "contrast": "def ows_security_tween_factory(handler, registry):\n    security = owssecurity_factory(registry)\n    def ows_security_tween(request):\n        try:\n            security.check_request(request)\n            return handler(request)\n        except OWSException as err:\n            logger.exception(\"security check failed.\")\n            return err\n        except Exception as err:\n            logger.exception(\"unknown error\")\n            return OWSNoApplicableCode(\"{}\".format(err))\n    return ows_security_tween", "label": 1}
{"index": "gp055532", "code": "def optimize_auto(param_0,param_1=10000,param_2=True):\n        self.Z.fix(warning=False)\n        self.kern.fix(warning=False)\n        self.kern_row.fix(warning=False)\n        self.Zr.fix(warning=False)\n        self.Xr.fix(warning=False)\n        self.optimize(param_1=int(0.1*param_1),messages=param_2)\n        self.unfix()\n        self.optimize(param_1=param_1,messages=param_2)", "contrast": "def optimize_params(max_iters, verbose):\n    for i in range(max_iters):\n        if verbose:\n            print(f\"Iteration {i+1} of {max_iters}\")", "label": 0}
{"index": "gp272263", "code": "def validate_schema(param_0, param_1):\n    try:\n        validate(param_0, param_1)\n    except ValidationError:\n        raise ValidationError(\"Validation failed\")", "contrast": "def validate(schema, value, noun='value'):\n    errors = schema.errors(value)\n    if errors:\n        error_details = ''\n        for error in errors:\n            if error.pointer:\n                error_details += '  - %s: %s\\n' % (error.pointer, error.message)\n            else:\n                error_details += '  - %s\\n' % error.message\n        raise ValidationError('Invalid %s:\\n%s' % (noun, error_details))", "label": 1}
{"index": "gp279743", "code": "def get_deserializer(param_0):\n    if param_0 == 'json':\n        return json.loads\n    elif param_0 == 'yaml':\n        return yaml.safe_load\n    elif param_0 == 'pickle':\n        return pickle.load\n    else:\n        raise ValueError(f'Invalid format: {param_0}')", "contrast": "def get_deserializer(serializer_format):\n    if serializer_format == Format.JSON:\n        return _deserialize_json\n    if serializer_format == Format.PICKLE:\n        return _deserialize_pickle", "label": 1}
{"index": "gp177579", "code": "import azure.mgmt.resource.resources as resources\nfrom azure.common.credentials import ServicePrincipalCredentials\nfrom azure.mgmt.resource.exceptions import ResourceNotFound\ndef try_delete_resource_group(param_0, param_1, param_2):\n    var_0 = resources.ResourceManagementClient(param_2, param_0)\n    try:\n        resource_client.resource_groups.delete(param_1)\n        print(\"Resource group '{}' has been deleted.\".format(param_1))\n    except ResourceNotFound:\n        print(\"Resource group '{}' was not found.\".format(param_1))", "contrast": "def _checkIfClusterExists(self):\n        ansibleArgs = {\n            'resgrp': self.clusterName,\n            'region': self._zone\n        }\n        try:\n            self.callPlaybook(self.playbook['check-cluster'], ansibleArgs, wait=True)\n        except RuntimeError:\n            logger.info(\"The cluster could not be created. Try deleting the cluster if it already exits.\")\n            raise", "label": 1}
{"index": "gp253932", "code": "def write_to_socket(param_0: str) -> None:\n    socket.sendall(frame_data.encode())", "contrast": "def write_to_socket(self, frame_data):\n        self._wr_lock.acquire()\n        try:\n            total_bytes_written = 0\n            bytes_to_send = len(frame_data)\n            while total_bytes_written < bytes_to_send:\n                try:\n                    if not self.socket:\n                        raise socket.error('connection/socket error')\n                    bytes_written = (\n                        self.socket.send(frame_data[total_bytes_written:])\n                    )\n                    if bytes_written == 0:\n                        raise socket.error('connection/socket error')\n                    total_bytes_written += bytes_written\n                except socket.timeout:\n                    pass\n                except socket.error as why:\n                    if why.args[0] in (EWOULDBLOCK, EAGAIN):\n                        continue\n                    self._exceptions.append(AMQPConnectionError(why))\n                    return\n        finally:\n            self._wr_lock.release()", "label": 1}
{"index": "gp284380", "code": "from random import randint\nfrom datetime import datetime, timedelta\ndef generate_data(param_0=30):\n    var_0 = datetime.now()\n    var_1 = [{'x': (var_0 - timedelta(param_0=var_2)).timestamp(), 'y': randint(50, 200)} for var_2 in range(param_0)]\n    var_3 = []\n    var_4 = 0\n    for var_5 in var_1:\n        var_4 += var_5['y']\n        total.append(var_4)\n    return var_1, var_3", "contrast": "def newlogins(sessions):\n    if not sessions:\n        return [], []\n    users = {}\n    dates = {}\n    for session in sessions:\n        user = session.user\n        date = session.started_at.strftime(\"%Y/%m/%d\")\n        if user not in users:\n            users[user] = date\n            if date not in dates:\n                dates[date] = [user]\n            else:\n                dates[date].append(user)\n    data = []\n    total = []\n    previous = 0\n    for date in sorted(dates.keys()):\n        date_epoch = unix_time_millis(datetime.strptime(date, \"%Y/%m/%d\"))\n        data.append({\"x\": date_epoch, \"y\": len(dates[date])})\n        previous += len(dates[date])\n        total.append({\"x\": date_epoch, \"y\": previous})\n    return data, total", "label": 1}
{"index": "gp091009", "code": "def encode(param_0, param_1):\r\n    if param_1 == 'utf-8-bom':\r\n        return BOM_UTF8 + text.encode(\"utf-8\"), 'utf-8-bom'\r\n    var_0 = get_coding(param_0)\r\n    if var_0:\r\n        try:\r\n            return text.encode(var_0), var_0\r\n        except (UnicodeError, LookupError):\r\n            raise RuntimeError(\"Incorrect encoding (%s)\" % var_0)\r\n    if (param_1 and orig_coding.endswith('-default') or\r\n       orig_coding.endswith('-guessed')):\r\n        var_0 = orig_coding.replace(\"-default\", \"\")\r\n        var_0 = orig_coding.replace(\"-guessed\", \"\")\r\n        try:\r\n            return text.encode(var_0), var_0\r\n        except (UnicodeError, LookupError):\r\n            pass\r\n    try:\r\n        return text.encode('ascii'), 'ascii'\r\n    except UnicodeError:\r\n        pass\r\n    return text.encode('utf-8'), 'utf-8'", "contrast": "def encode_text(text: str, orig_coding: str) -> Tuple[str, str]:\n    encoded = text.encode(orig_coding)\n    return encoded.decode(), orig_coding", "label": 0}
{"index": "gp163071", "code": "def _sorted_keys(param_0, param_1, param_2, param_3=False):\n        var_0 = lambda t: t[::-1]\n        if param_3:\n            var_1 = lambda t: t[::-1] > param_2[::-1]\n        else:\n            var_1 = lambda t: t[::-1] < param_2[::-1]\n        var_2 = sorted(param_1, key=var_0, param_3=param_3)\n        var_3 = sum(1 for var_4 in ifilter(var_1, var_2))\n        var_2 = var_2[var_3:] + var_2[:var_3]\n        for var_5 in var_2:\n            yield var_5", "contrast": "def sorted_key_generator(keys, startkey, reverse=False):\n    sorted_keys = sorted(keys, reverse=reverse)\n    index = sorted_keys.index(startkey)\n    for key in sorted_keys[index:]:\n        yield key", "label": 0}
{"index": "gp301171", "code": "def empty_label(param_0):\n    return {\n        'start': 0,\n        'end': param_0,\n        'value': None\n    }", "contrast": "def empty(self, duration):\n        ann = super(DynamicLabelTransformer, self).empty(duration)\n        ann.append(time=0, duration=duration, value=None)\n        return ann", "label": 1}
{"index": "gp122590", "code": "def cybox_defined_object_in_fact_term_handler(param_0, param_1, param_2, param_3, param_4):\n        param_4['fact_term_name'] = self.RE_DEFINED_OBJECT.sub('Properties', param_2['term'])\n        return True", "contrast": "def update_xml_properties(xml_file):\n    try:\n        import xml.etree.ElementTree as ET\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n        nodes = root.findall(\".//Defined_Object\")\n        for node in nodes:\n            node.tag = 'Properties'\n        tree.write(xml_file)\n    except:\n        print('Error updating the XML file.')", "label": 0}
{"index": "gp221833", "code": "import requests\ndef send_request(param_0, **param_1):\n    if 'session' in param_1:\n        var_0 = param_1['session']\n        kwargs.pop('session')\n    else:\n        var_0 = requests.Session()\n    var_1 = {'cookies', 'verify', 'timeout', 'allow_redirects', 'proxies', 'cert'}\n    param_1 = {var_2:var_3 for var_2,var_3 in kwargs.items() if var_2 in var_1}\n    return session.request(method=request.method, url=request.url, **param_1)", "contrast": "def send(self, request, **kwargs):\n        requests_kwargs = self._configure_send(request, **kwargs)\n        return super(RequestsHTTPSender, self).send(request, **requests_kwargs)", "label": 1}
{"index": "gp084623", "code": "def get_stripe_gateway_by_id(param_0, param_1, **param_2):\n        param_2['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._get_stripe_gateway_by_id_with_http_info(param_1, **param_2)\n        else:\n            (var_0) = cls._get_stripe_gateway_by_id_with_http_info(param_1, **param_2)\n            return var_0", "contrast": "def get_stripe_gateway_by_id(stripe_gateway_id: str, async: bool = False) -> 'StripeGateway':\n    url = '/stripeGateways/{stripeGatewayId}'\n    url = url.replace('{format}', 'json')\n    url = url.replace('{' + 'stripeGatewayId' + '}', str(stripe_gateway_id))\n    queryParams = {}\n    headerParams = {}\n    formParams = {}\n    return self.api_client.call_api(\n            url, 'GET',\n            headerParams=headerParams,\n            queryParams=queryParams,\n            postBody=postBody,\n            async=async)['response']", "label": 0}
{"index": "gp219713", "code": "def parse_unlock_result(param_0):\n    if result.startswith(\"OK\"):\n        return True\n    else:\n        raise pyhsm.exception.YHSM_CommandFailed(\"YubiHSM failed to unlock key storage\")", "contrast": "def parse_result(self, data):\n        fmt = \"B\"\n        self.status, = struct.unpack(fmt, data)\n        if self.status == pyhsm.defines.YSM_STATUS_OK:\n            return True\n        else:\n            raise pyhsm.exception.YHSM_CommandFailed(pyhsm.defines.cmd2str(self.command), self.status)", "label": 1}
{"index": "gp053211", "code": "def lifter(param_0, param_1=22):\n    if param_1 > 0:\n        var_0,var_1 = numpy.shape(param_0)\n        var_2 = numpy.arange(var_1)\n        var_3 = 1 + (param_1/2.)*numpy.sin(numpy.pi*var_2/param_1)\n        return var_3*param_0\n    else:\n        return param_0", "contrast": "import numpy as np\ndef apply_lifter(cepstra, L=22):\n    if L <= 0:\n        return cepstra\n    nframes, ncoeff = np.shape(cepstra)\n    lift = 1 + (L / 2.) * np.sin(np.pi * np.arange(ncoeff) / L)\n    return lift * cepstra", "label": 0}
{"index": "gp307444", "code": "def call_from_base_type(param_0):\n    if isinstance(param_0, _BaseValue):\n        return value._from_base_type()\n    else:\n        return param_0", "contrast": "def _opt_call_from_base_type(self, value):\n    if isinstance(value, _BaseValue):\n      value = self._call_from_base_type(value.b_val)\n    return value", "label": 1}
{"index": "gp129217", "code": "def make_date(param_0, param_1=parse_date):\n    if not param_0:\n        return datetime.date(1970, 1, 1)\n    if isinstance(param_0, basestring):\n        param_0 = param_1(param_0)\n    try:\n        param_0 = dt.timetuple()[:3]\n    except:\n        param_0 = tuple(param_0)[:3]\n    return datetime.date(*param_0)", "contrast": "import datetime\nimport numpy as np\ndef make_date(dt):\n    if dt is None or dt == '':\n        return datetime.date(1970, 1, 1)\n    if isinstance(dt, str):\n        dt = datetime.datetime.strptime(dt, '%I:%M %p').time()\n    elif isinstance(dt, (datetime.datetime, np.datetime64)):\n        dt = dt.time()        \n    elif isinstance(dt, datetime.time):\n        pass\n    else:\n        raise ValueError('Unsupported date/time format: {}'.format(type(dt)))\n    return dt", "label": 0}
{"index": "gp190666", "code": "import torch.nn as nn\ndef create_network():\n    return nn.Sequential(\n        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.Flatten(),\n        nn.Linear(in_features=32*8*8, out_features=10)\n    )", "contrast": "def createDenseCNNModel(self):\n    model = nn.Sequential(\n      nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels[0],\n                kernel_size=self.kernel_size[0], stride=self.stride[0],\n                padding=self.padding[0]),\n      nn.MaxPool2d(kernel_size=2),\n      nn.ReLU(),\n      nn.Conv2d(in_channels=self.out_channels[0], out_channels=self.out_channels[1],\n                kernel_size=self.kernel_size[1], stride=self.stride[1],\n                padding=self.padding[1]),\n      nn.MaxPool2d(kernel_size=2),\n      nn.ReLU(),\n      Flatten(),\n      nn.Linear(self.cnn_output_len[1], self.n),\n      nn.ReLU(),\n      nn.Linear(self.n, self.output_size),\n      nn.LogSoftmax(dim=1)\n    )\n    model.to(self.device)\n    if torch.cuda.device_count() > 1:\n      model = torch.nn.DataParallel(model)\n    return model", "label": 1}
{"index": "gp243334", "code": "def unlock_next_button():\n    clear_aggregation_layers()\n    enable_next_button()\n@QtCore.pyqtSlot()\ndef on_radiobutton_activated():\n    unlock_next_button()", "contrast": "def on_rbAggLayerNoAggregation_toggled(self):\n        self.parent.aggregation_layer = None\n        self.parent.pbnNext.setEnabled(True)", "label": 1}
{"index": "gp148272", "code": "def get_document(param_0, param_1, param_2):\n    import dialogflow_v2beta1 as dialogflow\n    var_0 = dialogflow.DocumentsClient()\n    var_1 = client.document_path(param_0, param_1,\n                                         param_2)\n    var_2 = client.get_document(var_1)\n    print('Got Document:')\n    print(' - Display Name: {}'.format(response.display_name))\n    print(' - Knowledge ID: {}'.format(response.name))\n    print(' - MIME Type: {}'.format(response.mime_type))\n    print(' - Knowledge Types:')\n    for var_3 in response.knowledge_types:\n        print('    - {}'.format(KNOWLEDGE_TYPES[var_3]))\n    print(' - Source: {}\\n'.format(response.content_uri))", "contrast": "from google.cloud import dialogflow_v2beta1 as dialogflow\ndef get_document(project_id, knowledge_base_id, document_id):\n    client = dialogflow.DocumentsClient()\n    document_path = client.document_path(\n        project_id, knowledge_base_id, document_id\n    )\n    document = client.get_document(request={\"name\": document_path})\n    return document", "label": 0}
{"index": "gp151380", "code": "def get_preconfigured_ssl_connection(\n            param_0,\n            param_1: Optional[OpenSslVersionEnum] = None,\n            param_2: Optional[str] = None,\n            param_3: Optional[bool] = None,\n    ) -> SslConnection:\n        if param_1 is not None:\n            var_0 = param_1\n            var_1 = None\n        else:\n            var_0 = self.highest_ssl_version_supported\n            var_1 = self.openssl_cipher_string_supported\n        if param_3 is not None:\n            var_1 = None\n        if self.client_auth_credentials is not None:\n            var_2 = False\n        else:\n            var_2 = True\n            if self.client_auth_requirement == ClientAuthenticationServerConfigurationEnum.REQUIRED:\n                var_2 = False\n        var_3 = SslConnectionConfigurator.get_connection(\n            ssl_version=var_0,\n            server_info=param_0,\n            var_1=var_1,\n            param_2=param_2,\n            param_3=param_3,\n            var_2=var_2,\n        )\n        return var_3", "contrast": "import ssl\ndef get_ssl_connection(server_name):\n    context = ssl.create_default_context()\n    context.check_hostname = False\n    context.verify_mode = ssl.CERT_NONE\n    connection = context.wrap_socket(socket.socket(), server_hostname=server_name)\n    return connection", "label": 0}
{"index": "gp107129", "code": "def delete_attachment(param_0, param_1, param_2):\n        return super(Conversations, param_0).delete(\n            param_1,\n            param_2,\n            endpoint_override='/attachments/%s.json' % attachment.id,\n            out_type=Attachment,\n        )", "contrast": "def delete_attachment(session, attachment):\n    url = f\"https://docsapi.helpscout.net/v1/attachments/{attachment.id}\"\n    response = session.delete(url)\n    response.raise_for_status()", "label": 0}
{"index": "gp009364", "code": "def resize_height(param_0, param_1, param_2=Image.LANCZOS):\n    try:\n        var_0 = param_1[1]\n    except:\n        var_0 = param_1\n    var_1 = image.format\n    var_2 = image.copy()\n    var_3 = img.size\n    if var_3[1] == var_0:\n        return param_0\n    var_4 = int(math.ceil((var_0 / var_3[1]) * var_3[0]))\n    img.thumbnail((var_4, var_0), param_2)\n    img.format = var_1\n    return var_2", "contrast": "from PIL import Image\ndef resize_image(image, size):\n    return image.resize(size)", "label": 0}
{"index": "gp286397", "code": "def finish_and_upload_report(param_0):\n    stats.update()\n    if configuration.uploading_enabled:\n        stats.upload_report()\n        stats.save_reports()\n    elif not configuration.uploading_enabled and not configuration.uploading_disabled:\n        if user_wants_uploading():\n            configuration.uploading_enabled = True\n            finish_and_upload_report(param_0)\n        else:\n            stats.save_report()\n    else:\n        stats.save_report()", "contrast": "def submit(self, info, *flags):\n        if not self.recording:\n            return\n        env_val = os.environ.get(self.env_var, '').lower()\n        if env_val not in (None, '', '1', 'on', 'enabled', 'yes', 'true'):\n            self.status = Stats.DISABLED_ENV\n            self.notes = None\n            return\n        if self.notes is None:\n            raise ValueError(\"This report has already been submitted\")\n        all_info, self.notes = self.notes, None\n        all_info.extend(self._to_notes(info))\n        for flag in flags:\n            flag(self, all_info)\n        now = time.time()\n        secs = int(now)\n        msecs = int((now - secs) * 1000)\n        all_info.insert(0, ('date', '%d.%d' % (secs, msecs)))\n        if self.user_id:\n            all_info.insert(1, ('user', self.user_id))\n        logger.debug(\"Generated report:\\n%r\", (all_info,))\n        def generator():\n            for key, value in all_info:\n                yield _encode(key) + b':' + _encode(value) + b'\\n'\n        filename = 'report_%d_%d.txt' % (secs, msecs)\n        if not self.sending:\n            fullname = os.path.join(self.location, filename)\n            with open(fullname, 'wb') as fp:\n                for l in generator():\n                    fp.write(l)\n            sys.stderr.write(self.prompt.prompt)\n            return\n        old_reports = [f for f in os.listdir(self.location)\n                       if f.startswith('report_')]\n        old_reports.sort()\n        old_reports = old_reports[:4]  \n        for old_filename in old_reports:\n            fullname = os.path.join(self.location, old_filename)\n            try:\n                with open(fullname, 'rb') as fp:\n                    r = requests.post(self.drop_point, data=fp.read(),\n                                      timeout=1, verify=self.ssl_verify)\n                    r.raise_for_status()\n            except Exception as e:\n                logger.warning(\"Couldn't upload %s: %s\", old_filename, str(e))\n                break\n            else:\n                logger.info(\"Submitted report %s\", old_filename)\n                os.remove(fullname)\n        try:\n            r = requests.post(self.drop_point, data=b''.join(generator()),\n                              timeout=1, verify=self.ssl_verify)\n        except requests.RequestException as e:\n            logger.warning(\"Couldn't upload report: %s\", str(e))\n            fullname = os.path.join(self.location, filename)\n            with open(fullname, 'wb') as fp:\n                for l in generator():\n                    fp.write(l)\n        else:\n            try:\n                r.raise_for_status()\n                logger.info(\"Submitted report\")\n            except requests.RequestException as e:\n                logger.warning(\"Server rejected report: %s\", str(e))", "label": 1}
{"index": "gp151423", "code": "def login(self, username='0000', userid=0, password=None):\n  if password and len(password) > 20:\n\t\t\tself.logger.error('password longer than 20 characters received')\n   raise Exception('password longer than 20 characters, login failed')\n  self.send(C1218LogonRequest(username, userid))\n  data = self.recv()\n  if data != b'\\x00':\n\t\t\tself.logger.warning('login failed, username and user id rejected')\n   return False\n  if password is not None:\n\t\t\tself.send(C1218SecurityRequest(password))\n   data = self.recv()\n   if data != b'\\x00':\n\t\t\t\tself.logger.warning('login failed, password rejected')\n    return False\n  self.logged_in = True\n  return True", "contrast": "def login_to_device(username: str, userid: int, password: str) -> bool:\n    if len(username) > 10 or len(password) > 20 or not (0 <= userid <= 65535):\n        return False\n    return True", "label": 0}
{"index": "gp043452", "code": "def remove_keywords_from_dict(param_0, param_1):\n        for var_0, var_1 in keyword_dict.items():\n            if not isinstance(var_1, list):\n                raise AttributeError(\"Value of key {} should be a list\".format(var_0))\n            for var_2 in var_1:\n                self.remove_keyword(var_2)", "contrast": "def remove_keywords_from_dict(keyword_dict):\n    for key, value in keyword_dict.items():\n        if not isinstance(value, list):\n            raise AttributeError(f\"Value for '{key}' in keyword_dict should be a list.\")\n        keyword_dict[key] = [v for v in value if v != key]\n    return keyword_dict", "label": 0}
{"index": "gp160864", "code": "def astensor(param_0: TensorLike) -> BKTensor:\n    param_0 = np.asarray(param_0, dtype=CTYPE)\n    return param_0", "contrast": "import tensorflow as tf\nimport numpy as np\ndef convert_numpy_to_tensor(arr: np.ndarray) -> tf.Tensor:\n    tensor = tf.convert_to_tensor(arr)\n    return tensor", "label": 0}
{"index": "gp190828", "code": "def compute_cnn_max_pool_width(param_0, param_1, param_2, param_3=3, param_4=2):\n    var_0 = param_0\n    for var_1 in range(param_1):\n        var_0 = var_0 - param_3 + 1\n    for var_1 in range(param_2):\n        var_0 = (var_0 - param_4) // param_4 + 1\n    return var_0", "contrast": "def computeMaxPool(input_width):\n  wout = math.floor((input_width + 2 * PADDING - KERNEL_SIZE) / STRIDE + 1)\n  return int(math.floor(wout / 2.0))", "label": 1}
{"index": "gp027100", "code": "def batch_augment(param_0, param_1, param_2='/CPU:0'):\n  with tf.device(param_2):\n    return tf.map_fn(param_1, param_0)", "contrast": "import torch\ndef apply_augmentation(x, func, device):\n    x = x.to(device)\n    augmented_x = []\n    for image in x:\n        augmented_x.append(func(image))\n    return torch.stack(augmented_x)", "label": 0}
{"index": "gp118944", "code": "def file_add(param_0, param_1, param_2):\n        var_0 = self._params.get('log', self._discard)\n        if not isinstance(param_2, list):\n            param_2 = [param_2]\n        for var_1 in param_2:\n            if var_1 not in self._file_event_map:\n                self._watch_files.add(var_1)\n                self._file_event_map[var_1] = {}\n            self._file_event_map[var_1][ev.get_key()] = param_1\n        log.debug(\"Added event key %r, action %r to path%s: %s\", ev.get_key(), ev._handler_name, ses(len(param_2)), param_2)", "contrast": "def register_event(path: str, task: object, action: str) -> None:\n    if not hasattr(register_event, 'registry'):\n        register_event.registry = {}\n    registry = register_event.registry\n    path_registry = registry.setdefault(path, {})\n    task_action = path_registry.get(task)\n    if task_action is None:\n        path_registry[task] = action\n    elif isinstance(task_action, list):\n        task_action.append(action)\n    else:\n        path_registry[task] = [task_action, action]", "label": 0}
{"index": "gp079810", "code": "def _update_status(param_0, param_1=None):\n        var_0 = '%s.%s' % (self.cluster_id, param_1) if param_1 else str(self.cluster_id)\n        var_1 = ['-format', '\"%d\"', 'JobStatus']\n        var_2 = 'condor_q {0} {1} && condor_history {0} {1}'.format(var_0, ' '.join(var_1))\n        var_3 = [var_2]\n        var_4, var_5 = self._execute(var_3, shell=True, run_in_job_dir=False)\n        if var_5:\n            log.error('Error while updating status for job %s: %s', var_0, var_5)\n            raise HTCondorError(var_5)\n        if not var_4:\n            log.error('Error while updating status for job %s: Job not found.', var_0)\n            raise HTCondorError('Job not found.')\n        var_4 = out.replace('\\\"', '').split('\\n')\n        var_6 = 0\n        for var_7 in var_4:\n            try:\n                var_6 = int(status_code_str.strip())\n            except:\n                pass\n        log.info('Job %s status: %d', var_0, var_6)\n        var_8 = CONDOR_JOB_STATUSES[var_6]\n        return var_8", "contrast": "def get_workflow_status() -> str:\n    pass", "label": 0}
{"index": "gp231093", "code": "def send_blocks(param_0, param_1, param_2, param_3):\n    if param_3 == socket.SOCK_STREAM:\n        var_0 = 0\n        while len(param_0) > 0:\n            var_1 = param_0[:param_1]\n            var_0 += param_2(var_1)\n            param_0 = param_0[param_1:]\n        return var_0\n    elif param_3 == socket.SOCK_DGRAM:\n        var_0 = 0\n        for var_2 in range(0, len(param_0), param_1):\n            var_1 = param_0[var_2:var_2+param_1]\n            var_0 += param_2(var_1)\n        return var_0\n    else:\n        raise ValueError(\"Unknown socket type\")", "contrast": "def write_to_stream(stream, response, block_len=1024):\n        try:\n            i_start = 0\n            i_end = block_len\n            while True:\n                if i_end > len(response):\n                    stream(response[i_start:])\n                    break\n                stream(response[i_start:i_end])\n                i_start = i_end\n                i_end += block_len\n        except (socket.timeout, socket.error) as exception:\n            logger.debug('Error sending response to the client: %s', exception)", "label": 1}
{"index": "gp011513", "code": "def open_in_browser(param_0, param_1=None):\n    import os\n    import webbrowser\n    import tempfile\n    if not isinstance(param_0, etree._ElementTree):\n        param_0 = etree.ElementTree(param_0)\n    var_0, var_1 = tempfile.mkstemp(suffix='.html')\n    var_2 = os.fdopen(var_0, 'wb')\n    try:\n        doc.write(var_2, method=\"html\", param_1=param_1 or doc.docinfo.encoding or \"UTF-8\")\n    finally:\n        f.close()\n    var_3 = 'file://' + fn.replace(os.path.sep, '/')\n    print(var_3)\n    webbrowser.open(var_3)", "contrast": "import webbrowser\nimport tempfile\ndef open_html_in_browser(html_content):\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n        filename = f.name\n        f.write(html_content)\n    webbrowser.open('file://' + filename)", "label": 0}
{"index": "gp033771", "code": "def list_datacenters_via_proxy(param_0=None, param_1=None):\n    if not param_0:\n        var_0 = salt.utils.vmware.get_datacenters(param_1,\n                                                    get_all_datacenters=True)\n    else:\n        var_0 = salt.utils.vmware.get_datacenters(param_1,\n                                                    param_0)\n    return [{'name': salt.utils.vmware.get_managed_object_name(var_1)}\n            for var_1 in var_0]", "contrast": "def list_datacenters_via_proxy(prox, datacenter_names=None, service_instance=None):\n    try:\n        si = __get_service_instance_via_proxy(prox, service_instance)\n        content = si.RetrieveContent()\n        datacenters = __get_datacenters(content, datacenter_names)\n        ret_list = []\n        for dc in datacenters:\n            dc_name = dc.name\n            dc_moref = dc._moId\n            dc_dict = {'name': dc_name,\n                       'moref': dc_moref}\n            ret_list.append(dc_dict)\n        return ret_list\n    except Exception as exc:\n        return {'error': \"{0}\".format(exc)}", "label": 0}
{"index": "gp288483", "code": "from typing import Any, Callable\ndef method_caller(param_0: str) -> Callable[..., Any]:\n    def call_method(param_0: Any, *param_1: Any, **param_2: Any) -> Any:\n        return getattr(param_0, param_0)(*param_1, **param_2)\n    return call_method", "contrast": "def method_caller(method_name, *args, **kwargs):\n def call_method(target):\n\t\tfunc = getattr(target, method_name)\n  return func(*args, **kwargs)\n return call_method", "label": 1}
{"index": "gp086590", "code": "def getImportFromObjects(param_0):\n    var_0 = [x.asname for var_1 in node.names if x.asname]\n    var_2 = [x.name for var_1 in node.names if not x.asname]\n    return var_0+var_2", "contrast": "import_ast = {\n    'node1': {\n        'imports': ['module1', 'module2'],\n        'objects': {\n            'item1': 'module1',\n            'item2': 'module2'\n        }\n    },\n    'node2': {\n        'imports': ['module3'],\n        'objects': {\n            'item3': 'module3'\n        }\n    }\n}\ndef get_imported_objects(node, import_ast):\n    imported_objects = []\n    for imp in import_ast[node]['imports']:\n        for obj in import_ast[node]['objects']:\n            if import_ast[node]['objects'][obj] == imp:\n                imported_objects.append(obj)\n    return imported_objects", "label": 0}
{"index": "gp032211", "code": "def write_cron_file(param_0, param_1):\n    if _check_instance_uid_match(param_0) or __grains__.get('os_family') in ('Solaris', 'AIX'):\n        return __salt__['cmd.retcode'](_get_cron_cmdstr(param_1),\n                                       runas=param_0,\n                                       python_shell=False) == 0\n    else:\n        return __salt__['cmd.retcode'](_get_cron_cmdstr(param_1, param_0),\n                                       python_shell=False) == 0", "contrast": "import os\ndef write_cron_file(user, file_path):\n    try:\n        os.system(f\"crontab -u {user} {file_path}\")\n        return True\n    except Exception:\n        return False", "label": 0}
{"index": "gp205565", "code": "def equivalent_contigs(param_0):\n    var_0 = []\n    for var_1 in contig_dict.values():\n        var_2 = True\n        for var_3 in var_0:\n            if var_3 == var_1:\n                var_2 = False\n                break\n        if var_2:\n            equivalent_contig_sets.append(var_1)\n    return var_0", "contrast": "def _get_identical_contigs(self, hits_dict):\n        equivalent_contigs = []\n        for qry_name, containing in hits_dict.items():\n            equivalent = set()\n            for containing_name in containing:\n                if containing_name in hits_dict and qry_name in hits_dict[containing_name]:\n                    equivalent.add(containing_name)\n                    equivalent.add(qry_name)\n            if len(equivalent):\n                equivalent_contigs.append(equivalent)\n                equivalent_contigs = self._collapse_list_of_sets(equivalent_contigs)\n        return equivalent_contigs", "label": 1}
{"index": "gp317998", "code": "from gevent import monkey\nfrom socketio.server import SocketIOServer\ndef start_socketio_server():\n    monkey.patch_all()  \n    var_0 = SocketIOServer(('0.0.0.0', 8080), app)\n    server.serve_forever()", "contrast": "def serve_forever(django=False):\n    logger = getLogger(\"irc.dispatch\")\n    logger.setLevel(settings.LOG_LEVEL)\n    logger.addHandler(StreamHandler())\n    app = IRCApplication(django)\n    server = SocketIOServer((settings.HTTP_HOST, settings.HTTP_PORT), app)\n    print \"%s [Bot: %s] listening on %s:%s\" % (\n        settings.GNOTTY_VERSION_STRING,\n        app.bot.__class__.__name__,\n        settings.HTTP_HOST,\n        settings.HTTP_PORT,\n    )\n    server.serve_forever()", "label": 1}
{"index": "gp004002", "code": "def option_attrname(param_0, param_1, param_2=None):\n        if param_2 is None:\n            param_2 = self.get_option_def(param_1)\n        return optdict.get(\"dest\", opt.replace(\"-\", \"_\"))", "contrast": "def get_config_attribute(config, opt):\n    return config[opt]", "label": 0}
{"index": "gp109911", "code": "def wordvalue(param_0):\n    var_0 = 0\n    for var_1 in enumerate(param_0):\n        var_0 += letternum(param_0[var_1[0]])\n    return var_0", "contrast": "def get_alphabetical_value(word):\n    return sum(ord(letter.lower()) - 96 for letter in word)", "label": 0}
{"index": "gp139883", "code": "def authorize(param_0, param_1=None, param_2=None, param_3=None):\n        var_0 = None\n        if param_2 and param_3:\n            var_1 = ExistAuthBasic(param_2, param_3)\n            auth.authorize()\n            if auth.token:\n                var_0 = auth.token['access_token']\n        elif param_1:\n            var_0 = param_1\n        else:\n            var_1 = ExistAuth(self.client_id, self.client_secret, 'code', self.redirect_uri)\n            auth.browser_authorize()\n            if auth.token:\n                var_0 = auth.token['access_token']\n        if var_0:\n            self.write_config(var_0)\n        else:\n            print('ERROR: We were unable to authorize to use the Exist API.')", "contrast": "import cherrypy\ndef authorize_user():\n    class AuthServer(object):\n        @cherrypy.expose\n        def authorize(self):\n            raise cherrypy.HTTPRedirect(\"https://example.com/login\")\n        @cherrypy.expose\n        def callback(self, code=None):\n            with open(\"config.ini\", \"w\") as config:\n                config.write(f\"access_token={access_token}\\n\")\n                config.write(f\"refresh_token={refresh_token}\\n\")\n                config.write(f\"expires_at={expires_at}\\n\")\n            raise cherrypy.HTTPRedirect(\"https://example.com/success\")\n    cherrypy.quickstart(AuthServer())", "label": 0}
{"index": "gp279201", "code": "def add_category_tags(param_0):\n    for var_0 in param_0:\n        try:\n            var_1 = smc.elements.other.Category(var_0)\n        except:\n            print(\"Category tag not found, creating new tag\")\n            var_2 = smc.elements.other.Category.create(var_0)\n    return None", "contrast": "def add_category(self, category):\n        assert isinstance(category, list), 'Category input was expecting list.'\n        from smc.elements.other import Category\n        for tag in category:\n            category = Category(tag)\n            try:\n                category.add_element(self.href)\n            except ElementNotFound:\n                Category.create(name=tag)\n                category.add_element(self.href)", "label": 1}
{"index": "gp242177", "code": "from werkzeug.routing import BuildError\ndef inject_x_forwarded_port(param_0, param_1):\n    try:\n        var_0 = int(request_headers.get('X-Forwarded-Port', ''))\n    except ValueError:\n        raise BuildError('Invalid X-Forwarded-Port header value')\n    if var_0:\n        var_1 = request_headers.get('X-Forwarded-Server', '')\n        var_2 = request_headers.get('X-Forwarded-Script-Name', '')\n        var_3 = request_headers.get('X-Forwarded-Path', '')\n        var_4 = request_headers.get('X-Forwarded-QueryString', '')\n        url_adapter.server_name = var_1\n        url_adapter.script_name = var_2\n        url_adapter.path_info = var_3\n        url_adapter.query_string = var_4\n        url_adapter.url_scheme = 'https' if var_0 == 443 else 'http'\n        url_adapter.server_port = var_0", "contrast": "def use_forwarded_port(graph):\n    context = _request_ctx_stack.top\n    if _request_ctx_stack is None:\n        return None\n    forwarded_host = graph.config.port_forwarding.get(\"host\")\n    forwarded_port = request.headers.get(\"X-Forwarded-Port\")\n    if not forwarded_port and not forwarded_host:\n        return None\n    if \":\" in context.url_adapter.server_name:\n        server_host, server_port = context.url_adapter.server_name.split(\":\", 1)\n    else:\n        server_host = context.url_adapter.server_name\n        server_port = 443 if context.url_adapter.url_scheme == \"https\" else 80\n    if forwarded_host:\n        server_name = forwarded_host\n    elif server_port:\n        server_name = \"{}:{}\".format(server_host, forwarded_port)\n    else:\n        server_name = \"{}:{}\".format(server_host, server_port)\n    context.url_adapter.server_name = server_name\n    return server_name", "label": 1}
{"index": "gp271895", "code": "import openpyxl\ndef read_xlsx_to_list(param_0):\n    var_0 = openpyxl.load_workbook(param_0)\n    var_1 = wb.active\n    var_2 = []\n    var_3 = [cell.value for var_4 in var_1[1]]\n    for var_5 in ws.iter_rows(min_row=2, values_only=True):\n        data_list.append({var_3[var_6]: var_5[var_6] for var_6 in range(len(var_3))})\n    return var_2", "contrast": "def _read_xlsx_table(path):\n    workbook = pyxl.load_workbook(path)\n    worksheet = workbook.active\n    table = helpers.sheet_to_table(worksheet)\n    return table", "label": 1}
{"index": "gp095374", "code": "def po_to_unicode(param_0):\n    var_0 = po_obj.__str__()\n    if type(var_0) != types.UnicodeType:\n        var_0 = po_text.decode('utf-8')\n    return var_0", "contrast": "import polib\ndef po_to_unicode(po_obj):\n    unicode_string = ''\n    if isinstance(po_obj, polib.POFile):\n        for entry in po_obj:\n            if entry.msgid_plural:\n                unicode_string += '{}\\nmsgid {}\\nmsgid_plural {}\\nmsgstr[0] {}\\n\\n'.format(\n                    entry.comment, entry.msgid, entry.msgid_plural, entry.msgstr[0])\n                for index, string in enumerate(entry.msgstr[1:], start=1):\n                    unicode_string += 'msgstr[{}] {}\\n'.format(index, string)\n            else:\n                unicode_string += '{}\\nmsgid {}\\nmsgstr {}\\n\\n'.format(\n                    entry.comment, entry.msgid, entry.msgstr)\n    elif isinstance(po_obj, polib.POEntry):\n        if po_obj.msgid_plural:\n            unicode_string += 'msgid {}\\nmsgid_plural {}\\nmsgstr[0] {}\\n\\n'.format(\n                po_obj.msgid, po_obj.msgid_plural, po_obj.msgstr[0])\n            for index, string in enumerate(po_obj.msgstr[1:], start=1):\n                unicode_string += 'msgstr[{}] {}\\n'.format(index, string)\n        else:\n            unicode_string += 'msgid {}\\nmsgstr {}\\n\\n'.format(po_obj.msgid, po_obj.msgstr)\n    else:\n        raise ValueError('Invalid po_obj type. Must be polib.PoFile or polib.PoEntry.')\n    return unicode_string", "label": 0}
{"index": "gp166645", "code": "def get_dataframe():\n        var_0 = []\n        [names.extend(line.split()) for var_1 in CONTROL_VARIABLE_LINES]\n        var_2 = []\n        [defaults.extend(line.split()) for var_1 in CONTROL_DEFAULT_LINES]\n        var_3, var_4,var_5,var_6 = [],[],[],[]\n        for var_7,var_8 in zip(var_0,var_2):\n            if '[' in var_7 or ']' in var_7:\n                required.append(False)\n            else:\n                required.append(True)\n            var_9,var_10,var_11 = ControlData._parse_value(var_8)\n            types.append(var_10)\n            formats.append(var_11)\n            cast_defaults.append(var_9)\n        return pandas.DataFrame({\"name\":var_0,\"type\":var_3,\n                                     \"value\":var_5,\"required\":var_4,\n                                    \"format\":var_6})", "contrast": "import pandas as pd\ndef get_control_section_dataframe():\n    df = pd.DataFrame({\n        'Control Section': [1, 2, 3, 4, 5],\n        'Parameter 1': [10, 20, 30, 40, 50],\n        'Parameter 2': [100, 200, 300, 400, 500],\n        'Parameter 3': [1000, 2000, 3000, 4000, 5000]\n    })\n    return df", "label": 0}
{"index": "gp182794", "code": "def find_host(param_0, param_1, param_2=None):\n    for var_0 in param_1:\n        if param_2:\n            if host.broadcast_rpc_address == param_2:\n                return var_0\n        elif host.endpoint == param_0:\n            return var_0\n    return None", "contrast": "def get_host(self, endpoint_or_address):\n        if not isinstance(endpoint_or_address, EndPoint):\n            return self._get_host_by_address(endpoint_or_address)\n        return self._hosts.get(endpoint_or_address)", "label": 1}
{"index": "gp327751", "code": "import numpy as np\nfrom scipy import special\ndef sinusoidAWGN(param_0, param_1):\n    var_0 = np.var(param_0)\n    var_1 = 10** (param_1/10)\n    var_2 = var_0 / var_1\n    var_3 = np.sqrt(var_2/2) * np.random.randn(len(param_0))\n    var_4 = param_0 + var_3\n    return var_4", "contrast": "def sinusoidAWGN(x,SNRdB):\n    x_pwr = np.var(x)\n    noise = np.sqrt(x_pwr/10**(SNRdB/10.))*np.random.randn(len(x));\n    return x + noise", "label": 1}
{"index": "gp119315", "code": "def load_context(param_0, param_1, param_2=None):\n    if param_0 == '-':\n        return loads(sys.stdin.read(), ac_parser=param_1, ac_schema=param_2)\n    return load(param_0, ac_parser=param_1, ac_schema=param_2)", "contrast": "import anyconfig\nimport json\ndef validate_context(ctx_path, ctx_type, scm):\n    if ctx_path == '-':\n        ctx_file = json.load(sys.stdin)\n    else:\n        with open(ctx_path, 'r') as file:\n            if ctx_type == 'json':\n                ctx_file = json.load(file)\n    validator = anyconfig.validate(ctx_file, scm)\n    return validator", "label": 0}
{"index": "gp068662", "code": "def get_dict_tokens_for_termid(param_0, param_1):\n        if self.dict_tokens_for_tid is None:\n            self.dict_tokens_for_tid = {}\n            for var_0 in self.get_terms():\n                self.dict_tokens_for_tid[term.get_id()] = term.get_span().get_span_ids()\n        return self.dict_tokens_for_tid.get(param_1,[])", "contrast": "def get_token_span(term_id):\n    token_ids = term_id_to_token_ids_dict.get(term_id,[])\n    spans = []\n    for token_id in token_ids:\n        spans.append(token_id_to_span_dict.get(token_id))\n    return spans", "label": 0}
{"index": "gp320201", "code": "def get_max_item(param_0):\n    if not param_0:\n        raise ValueError('Tree is empty')\n    var_0 = max(tree.keys())\n    return param_0[var_0]", "contrast": "def max_item(self):\n        if self.is_empty():\n            raise ValueError(\"Tree is empty\")\n        node = self._root\n        while node.right is not None:\n            node = node.right\n        return node.key, node.value", "label": 1}
{"index": "gp207938", "code": "def find_info_by_confidence(param_0, param_1, param_2):\n    if param_1 not in param_0:\n        return None\n    if param_0[param_1]['confidence'] >= param_2:\n        return param_0[param_1]['info']\n    else:\n        return None", "contrast": "def get(self, key, confidence=0):\n        if key not in self.info:\n            return None\n        conf, value = self.info.get(key)\n        if conf >= confidence:\n            return value\n        return None", "label": 1}
{"index": "gp063997", "code": "def filter_geometry(param_0, **param_1):\n    var_0 = geo_field(param_0).name\n    var_1 = {'%s__%s' % (var_0, var_2): var_3 for var_2, var_3 in filters.items()}\n    return queryset.filter(**var_1)", "contrast": "def spatial_lookup_filter(**kwargs):\n    spatial_lookup_map = {\n        'bbcontains': 'bbcontains',\n        'bboverlaps': 'bboverlaps',\n        'contained': 'contained',\n        'contains': 'contains',\n        'disjoint': 'disjoint',\n        'equals': 'equals',\n        'intersects': 'intersects',\n        'overlaps': 'overlaps',\n        'same_as': 'same_as',\n        'touches': 'touches',\n        'within': 'within',\n    }\n    lookup = ''\n    for key, value in kwargs.items():\n        lookup_type = spatial_lookup_map.get(key)\n        if lookup_type:\n            if lookup:\n                lookup += '__'\n            lookup += lookup_type\n    return lookup, value", "label": 0}
{"index": "gp152945", "code": "def prt_num_sig(param_0, param_1=sys.stdout, param_2=0.05):\n        var_0 = self.get_num_sig(param_2)\n        prt.write(\"{N:6,} TOTAL: {TXT}\\n\".format(N=len(self.nts), TXT=\" \".join([\n            \"FDR({FDR:4})\".format(FDR=var_0['FDR']),\n            \"Bonferroni({B:4})\".format(B=var_0['Bonferroni']),\n            \"Benjamini({B:4})\".format(B=var_0['Benjamini']),\n            \"PValue({P:4})\".format(P=var_0['PValue']),\n            os.path.basename(self.fin_davidchart)])))", "contrast": "def print_significant_GO_terms(go_terms):\n    significant_terms = 0\n    for term in go_terms:\n        if term.startswith(\"GO:\") and float(term.split(\":\")[1]) < 0.01:\n            significant_terms += 1\n    print(\"Number of significant GO terms:\", significant_terms)", "label": 0}
{"index": "gp276946", "code": "def get_folder_details(param_0: str) -> File:\n    pass", "contrast": "def get_file_by_id(self, file_id):\n        return self._create_item_response(\n            self.data_service.get_file(file_id),\n            File\n        )", "label": 1}
{"index": "gp057897", "code": "def _drop_empty_props(param_0, param_1):\n        if isinstance(param_1, list):\n            return [self._drop_empty_props(var_0) for var_0 in param_1]\n        if isinstance(param_1, dict):\n            return {\n                var_1: self._drop_empty_props(var_2)\n                for var_1, var_2 in item.items() if var_2 != ''\n            }\n        return param_1", "contrast": "def remove_empty_props(nested_dict):\n    def inner(d):\n        return {\n            k: inner(v) if isinstance(v, dict) else v\n            for k, v in d.items() if v != \"\" and inner(v) != {}\n        }\n    return inner(nested_dict)", "label": 0}
{"index": "gp286071", "code": "def get_vault_token(param_0, param_1):\n    var_0 = determine_token(param_0, param_1) \n    return var_0", "contrast": "def app_token(vault_client, app_id, user_id):\n    resp = vault_client.auth_app_id(app_id, user_id)\n    if 'auth' in resp and 'client_token' in resp['auth']:\n        return resp['auth']['client_token']\n    else:\n        raise aomi.exceptions.AomiCredentials('invalid apptoken')", "label": 1}
{"index": "gp299267", "code": "def get_full_group_path(param_0):\n    var_0 = \"\"\n    var_1 = param_0\n    while var_1:\n        if var_0:\n            var_0 = var_1 + \"/\" + var_0\n        else:\n            var_0 = var_1\n        var_1 = parent_group(var_1)\n    return var_0", "contrast": "def path(self):\n        if self.dataset is self:\n            return ''\n        else:  \n            return self.dataset.path + '/' + self.name", "label": 1}
{"index": "gp177022", "code": "import networkx as nx\ndef generate_dummy_graph(param_0):\n    var_0 = nx.spring_layout(param_0)\n    nx.set_node_attributes(param_0, var_0, name='pos')\n    var_1 = []\n    var_2 = []\n    for var_3 in graph.nodes.data():\n        if '380' in var_3[0]:\n            nodes_380.append(var_3[0])\n        elif '220' in var_3[0]:\n            nodes_220.append(var_3[0])\n    for var_3 in var_1:\n        var_4 = []\n        for var_5 in graph.neighbors(var_3):\n            if '220' in var_5:\n                nodes.append(var_5)\n        for var_5 in var_4:\n            graph.add_edge(var_3, var_5)\n        graph.remove_node(var_3)\n    return param_0", "contrast": "def generate_dummy_graph(network):\n    graph = pypsa.descriptors.OrderedGraph()\n    graph.add_nodes_from([bus for bus in network.buses.index if bus not in buses_to_split])\n    for node in graph.nodes():\n        graph.node[node][\"pos\"] = np.array(network.buses.loc[node,[\"x\",\"y\"]],dtype=float)\n    return graph", "label": 1}
{"index": "gp282151", "code": "import serial.tools.list_ports\nimport pandas as pd\ndef list_available_com_ports():\n    var_0 = [(port.device, port.description) for var_1 in serial.tools.list_ports.comports()]\n    var_2 = pd.DataFrame(var_0, columns=['port', 'descriptor']).set_index('port')\n    return var_2", "contrast": "def _comports():\n    return (pd.DataFrame(list(map(list, serial.tools.list_ports.comports())),\n                         columns=['port', 'descriptor', 'hardware_id'])\n            .set_index('port'))", "label": 1}
{"index": "gp124698", "code": "def rebuild():\n    drop_db()\n    create_db()\n    if StrictVersion(django.get_version()) < StrictVersion('1.7'):\n        local('python{} manage.py syncdb --all --noinput'.format(\n            PYTHON_VERSION))\n        local('python{} manage.py migrate --fake'.format(PYTHON_VERSION))\n    else:\n        local('python{} manage.py migrate'.format(PYTHON_VERSION))", "contrast": "from django.core.management import call_command\ndef reset_db():\n    call_command('reset_db', interactive=False)\n    call_command('migrate')", "label": 0}
{"index": "gp025341", "code": "def peek(param_0, param_1: int) -> memoryview:\n        assert param_1 > 0\n        try:\n            var_0, var_1 = self._buffers[0]\n        except IndexError:\n            return memoryview(b\"\")\n        var_2 = self._first_pos\n        if var_0:\n            return typing.cast(memoryview, var_1[var_2 : var_2 + param_1])\n        else:\n            return memoryview(var_1)[var_2 : var_2 + param_1]", "contrast": "def get_view(buffer, size):\n    return buffer[:size]", "label": 0}
{"index": "gp234582", "code": "import requests\ndef send_http_request():\n    var_0 = requests.get('http://satellite-url/wait_new_conf')\n    if response.status_code == 200:\n        return True\n    else:\n        return False", "contrast": "def wait_new_conf(self):\n        logger.debug(\"Wait new configuration for %s, %s %s\", self.name, self.alive, self.reachable)\n        return self.con.get('_wait_new_conf')", "label": 1}
{"index": "gp198501", "code": "import glob\ndef expand_paths(param_0, param_1):\n    var_0 = []\n    for var_1 in param_0:\n        for var_2 in glob.glob(var_1 + '/' + param_1):\n            expanded_paths.append(var_2)\n    return var_0", "contrast": "def extract_files(files):\n    expanded_files = []\n    legal_extensions = [\".md\", \".txt\", \".rtf\", \".html\", \".tex\", \".markdown\"]\n    for f in files:\n        if os.path.isdir(f):\n            for dir_, _, filenames in os.walk(f):\n                for filename in filenames:\n                    fn, file_extension = os.path.splitext(filename)\n                    if file_extension in legal_extensions:\n                        joined_file = os.path.join(dir_, filename)\n                        expanded_files.append(joined_file)\n        else:\n            expanded_files.append(f)\n    return expanded_files", "label": 1}
{"index": "gp091366", "code": "def extractbpflags(param_0, param_1):\n    var_0 = util.tools.table()\n    tb.open(b(os.path.join(param_0, 'ANTENNA')))\n    var_1 = tb.getcol(b'NAME')\n    tb.close()\n    tb.open(b(param_0))\n    try:\n        var_2 = tb.getkeyword(b'VisCal')\n    except RuntimeError:\n        raise PKError('no \"VisCal\" keyword in %s; it doesn\\'t seem to be a '\n                       'bandpass calibration table', param_0)\n    if var_2 != 'B Jones':\n        raise PKError('table %s doesn\\'t seem to be a bandpass calibration '\n                       'table; its type is \"%s\"', param_0, var_2)\n    def emit(param_0, param_1, param_2, param_3):\n        print(\"antenna='%s&*' spw='%d:%d~%d' reason='BANDPASS_FLAGGED'\" %              (var_1[param_0], param_1, param_2, param_3), file=param_1)\n    for var_3 in range(tb.nrows()):\n        var_4 = tb.getcell(b'ANTENNA1', var_3)\n        var_5 = tb.getcell(b'SPECTRAL_WINDOW_ID', var_3)\n        var_6 = tb.getcell(b'FLAG', var_3)\n        var_7 = ~((~var_6).prod(axis=0, dtype=np.bool))\n        var_8 = None\n        for var_9 in range(sqflag.size):\n            if var_7[var_9]:\n                if var_8 is None:\n                    var_8 = var_9\n            elif var_8 is not None:\n                emit(var_4, var_5, var_8, var_9 - 1)\n                var_8 = None\n        if var_8 is not None:\n            emit(var_4, var_5, var_8, var_9)\n    tb.close()", "contrast": "def make_flags_file(calpath:str, deststream:typing.IO):\n    with open(calpath, 'rb') as f:\n        while True:\n            chunk = f.read(1024)\n            if not chunk:\n                break\n            deststream.write(chunk)", "label": 0}
{"index": "gp110281", "code": "def export(param_0, param_1):\n        var_0, var_1 = self.get_url_rev()\n        logger.notify('Exporting svn repository %s to %s' % (var_0, param_1))\n        logger.indent += 2\n        try:\n            if os.path.exists(param_1):\n                rmtree(param_1)\n            call_subprocess(\n                [self.cmd, 'export', var_0, param_1],\n                filter_stdout=self._filter, show_stdout=False)\n        finally:\n            logger.indent -= 2", "contrast": "import subprocess\ndef export_svn(svn_url, destination_path):\n    subprocess.check_call(['svn', 'export', svn_url, destination_path])", "label": 0}
{"index": "gp130687", "code": "def NoExclusions(param_0):\n        if len(self.start_bounds) + len(self.target_rs) + len(self.ignored_rs) == 0:\n            return BoundaryCheck.chrom == -1\n        return False", "contrast": "def check_exclusion_criteria():\n    return True", "label": 0}
{"index": "gp067192", "code": "def discharge_token(param_0, param_1):\n        var_0 = '{}discharge-token-for-user?username={}'.format(\n            self.url, quote(param_1))\n        logging.debug('Sending identity info to {}'.format(var_0))\n        var_1 = make_request(var_0, method='GET', timeout=self.timeout)\n        try:\n            var_2 = var_1['DischargeToken']\n            var_3 = json.dumps(var_2)\n        except (KeyError, UnicodeDecodeError) as err:\n            raise InvalidMacaroon(\n                'Invalid macaroon from discharger: {}'.format(err.message))\n        return base64.urlsafe_b64encode(\"[{}]\".format(\n            var_3).encode('utf-8'))", "contrast": "import base64\ndef discharge_token(username):\n    try:\n        response_token = 'example_token_generated_for_user_' + username\n        encoded_token = base64.b64encode(response_token.encode('utf-8'))\n        return encoded_token.decode('utf-8')\n    except:\n        raise ServerError('An error occurred in the request process.')", "label": 0}
{"index": "gp128801", "code": "def rollback(param_0):\n        self._state_machine.transition_to_rollback()\n        for var_0 in reversed(self._executed_actions):\n            try:\n                self.execute_with_retries(var_0, lambda a: a.rollback())\n            except:  \n                pass  \n        self._state_machine.transition_to_rollback_complete()", "contrast": "def call_rollback(actions):\n    for action in actions:\n        action.rollback()", "label": 0}
{"index": "gp226603", "code": "def image_provider(param_0):\n    with open(param_0, 'rb') as var_0:\n        var_1 = f.read()\n    var_2 = Image.open(io.BytesIO(var_1))\n    if img.format not in ['JPEG', 'PNG']:\n        raise ValueError('Image format not supported')\n    if img.width > 1920 or img.height > 1080:\n        raise ValueError('Image too big')", "contrast": "def setOverlayFromFile(self, ulOverlayHandle, pchFilePath):\n        fn = self.function_table.setOverlayFromFile\n        result = fn(ulOverlayHandle, pchFilePath)\n        return result", "label": 1}
{"index": "gp254216", "code": "def is_file_like(param_0):\n    try:\n        obj.read()\n    except AttributeError:\n        return False\n    return True", "contrast": "def io_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, io.IOBase):\n            name = type(var).__name__\n            raise IOObjError(\n                f'Function {func} expected file-like object, {name} got instead.')", "label": 1}
{"index": "gp155453", "code": "def get_atlas_zonefile_data( param_0, param_1, param_2=True ):\n    var_0 = atlas_zonefile_path(param_1, param_0)\n    var_1 = atlas_zonefile_path_legacy(param_1, param_0)\n    for var_2 in [var_0, var_1]:\n        if not os.path.exists( var_2 ):\n            continue\n        if param_2:\n            var_3 = _read_atlas_zonefile(var_2, param_0)\n        else:\n            var_3 = _read_atlas_zonefile(var_2, None)\n        if var_3:\n            return var_3\n    return None", "contrast": "import pickle\nimport os\ndef get_cached_zone_file():\n    if os.path.exists('cached_zone_file.pkl'):\n        with open('cached_zone_file.pkl', 'rb') as f:\n            return pickle.load(f)\n    else:\n        return None", "label": 0}
{"index": "gp007395", "code": "def _resolve_ctx(param_0):\n    if not param_0:\n        raise ResolveError(\"Missing node definition.\")\n    if len(param_0) == 1 and param_0[0][0] is None:\n        return param_0[0][1]\n    if any(var_0[0] is None for var_0 in param_0):\n        raise ResolveError(\"Multiple definition, multiple ieml object provided for the same node.\")\n    if any(not isinstance(var_0[0], Path) for var_0 in param_0):\n        raise ResolveError(\"Must have only path instance.\")\n    var_1 = param_0[0]\n    var_2 = _inferred_types(*var_1)\n    for var_0 in param_0[1:]:\n        var_2 = types.intersection(_inferred_types(*var_0))\n    if not var_2:\n        raise ResolveError(\"No definition, no type inferred on rules list.\")\n    if len(var_2) > 1:\n        raise ResolveError(\"Multiple definition, multiple type inferred on rules list.\")\n    var_3 = next(types.__iter__())\n    if var_3 == Topic:\n        var_4, var_5 = _build_deps_topic(param_0)\n        if var_4:\n            return\n        var_6 = None\n        if var_5['f']:\n            var_6 = var_5['f']\n        if not var_5['r']:\n            raise ResolveError(\"No root for the topic node.\")\n        return topic(var_5['r'], var_6)\n    if var_3 == Text:\n        var_4, var_5 = _build_deps_text(param_0)\n        if var_4:\n            return\n        return text(var_5)\n    if var_3 in (Theory, Fact):\n        var_4, var_5 = _build_deps_tree_graph(param_0)\n        if var_4:\n            return\n        if var_3 == Fact:\n            var_7 = []\n            for var_8, var_9, var_10 in var_5:\n                clauses.append((var_8, var_9, var_10))\n            return fact(var_7)\n        else:\n            var_7 = []\n            for var_8, var_9, var_10 in var_5:\n                clauses.append((var_8, var_9, var_10))\n            return theory(var_7)\n    raise ResolveError(\"Invalid type inferred %s\"%type.__name__)", "contrast": "def resolve_context_and_build_ieml(rules):\n    return ieml_element", "label": 0}
{"index": "gp025773", "code": "def utilization(param_0):\n        class GpuUtilizationInfo(Structure):\n            var_0 = [\n                ('gpu', c_uint),\n                ('memory', c_uint),\n            ]\n        var_1 = GpuUtilizationInfo()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetUtilizationRates\")(self.hnd, byref(var_1)))\n        return {'gpu': c_util.gpu, 'memory': c_util.memory}", "contrast": "import pynvml\ndef utilization():\n    pynvml.nvmlInit()\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    gpu_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n    gpu_memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    return {'gpu': gpu_utilization.gpu, 'memory': gpu_memory.percent}\n    pynvml.nvmlShutdown()", "label": 0}
{"index": "gp224981", "code": "def fetch_agent(param_0):\n    return agent", "contrast": "def get_agent(self, agent_id):\n        url = 'agents/%s' % agent_id\n        return Agent(**self._api._get(url))", "label": 1}
{"index": "gp014059", "code": "def odd_even(param_0):\n        return self.select(lambda Z, N: (Z % 2) and not(N % 2), name=self.name)", "contrast": "def odd_even_nuclei(table):\n    odd_nuclei = []\n    even_nuclei = []\n    for row in table:\n        if row[0] % 2 == 0:\n            even_nuclei.append(row)\n        else:\n            odd_nuclei.append(row)\n    return odd_nuclei, even_nuclei", "label": 0}
{"index": "gp004546", "code": "def default(param_0) -> 'PrecalculatedTextMeasurer':\n        if cls._default_cache is not None:\n            return cls._default_cache\n        if pkg_resources.resource_exists(__name__, 'default-widths.json.xz'):\n            import lzma\n            with pkg_resources.resource_stream(__name__,\n                                               'default-widths.json.xz') as var_0:\n                with lzma.open(var_0, \"rt\") as var_1:\n                    cls._default_cache = PrecalculatedTextMeasurer.from_json(\n                        cast(TextIO, var_1))\n                    return cls._default_cache\n        elif pkg_resources.resource_exists(__name__, 'default-widths.json'):\n            with pkg_resources.resource_stream(__name__,\n                                               'default-widths.json') as var_0:\n                cls._default_cache = PrecalculatedTextMeasurer.from_json(\n                    io.TextIOWrapper(var_0, encoding='utf-8'))\n                return cls._default_cache\n        else:\n            raise ValueError('could not load default-widths.json')", "contrast": "def get_default_precalculated_text_measurer():\n    return PrecalculatedTextMeasurer()", "label": 0}
{"index": "gp051260", "code": "def add_to(\n            param_0, param_1, param_2=None, param_3=False, param_4=0):\n        if isinstance(param_1, list):\n            for var_0 in param_1:\n                if isinstance(var_0, str):\n                    var_0 = To(var_0, None)\n                if isinstance(var_0, tuple):\n                    var_0 = To(var_0[0], var_0[1])\n                self._set_emails(var_0, param_2, param_3, param_4)\n        else:\n            if isinstance(param_1, str):\n                param_1 = To(param_1, None)\n            if isinstance(param_1, tuple):\n                param_1 = To(param_1[0], param_1[1])\n            if isinstance(param_1, Email):\n                param_4 = to_email.personalization\n            self._set_emails(param_1, param_2, param_3, param_4)", "contrast": "def add_to_to_personalization(to_emails, global_substitutions=None, is_multiple=False, p=None):\n    if p is None:\n        p = Personalization()\n    if isinstance(to_emails, str):\n        to_emails = [to_emails]\n    if isinstance(to_emails, tuple):\n        to_emails = list(to_emails)\n    if is_multiple:\n        for email in to_emails:\n            new_p = Personalization()\n            new_p.add_to(To(email))\n            if global_substitutions:\n                for key, value in global_substitutions.items():\n                    new_p.add_global_substitution(key, value)\n            if isinstance(p, int):\n                message.personalizations[p] = new_p\n            else:\n                p.add_substitution(new_p)\n    else:\n        new_to = To(to_emails)\n        if global_substitutions:\n            for key, value in global_substitutions.items():\n                p.add_global_substitution(key, value)\n        p.add_to(new_to)\n    return p", "label": 0}
{"index": "gp158624", "code": "def to_gremlin(param_0):\n        self.validate()\n        var_0 = {\n            'direction': self.direction,\n            'edge_name': self.edge_name,\n            'inverse_direction': 'in' if self.direction == 'out' else 'out'\n        }\n        return (u'collectMany{{entry -> entry.{direction}_{edge_name}'\n                u'.collect{{edge -> edge.{inverse_direction}V.next()}}}}'\n                .format(**var_0))", "contrast": "def gremlin_representation(block):\n    return str(block).encode('unicode_escape').decode('utf-8')", "label": 0}
{"index": "gp125057", "code": "def log_iter(param_0, param_1=None, param_2=None, **param_3):\n        param_2 = str(param_2 or self.template)\n        var_0 = self.log(n=param_1, param_2=param_2, **param_3)\n        if not var_0:\n            return\n        print(var_0)\n        for var_1 in itersplit(var_0, self.lsep):\n            var_1 = l.strip()\n            if not var_1:\n                continue\n            try:\n                yield self._parselog(var_1,)\n            except Exception:\n                log.error(\"%s %r\" % (str(param_0), var_1))\n                raise\n        return", "contrast": "def parse_repository_log_command(log_output):\n    for log_entry in log_output.split('\\n'):\n        if not log_entry:\n            continue\n        parts = log_entry.split('\\t')\n        yield parts[0], parts[1], parts[2], parts[3], parts[4], parts[5], parts[6]", "label": 0}
{"index": "gp261592", "code": "def get_id(param_0: str, param_1: str) -> str:\n    return f\"{param_0}:{param_1}\"", "contrast": "def id(self):\n        id = ''\n        if (self.server_and_prefix is not None and\n                self.server_and_prefix != ''):\n            id += self.server_and_prefix + '/'\n        if (self.identifier is not None):\n            id += self.identifier\n        return id", "label": 1}
{"index": "gp110228", "code": "def duplicates(param_0, param_1):\n    for var_0 in param_1:\n        if item.similarity(param_0) and not item.equality(param_0):\n            yield var_0", "contrast": "def similar_items(base, items):\n    def similarity(item):\n        return abs(item - base)\n    sorted_items = sorted(items, key=similarity)\n    for item in sorted_items:\n        if item != base:\n            yield item", "label": 0}
{"index": "gp135317", "code": "def pitch(param_0, param_1, param_2):\n    import numpy\n    return numpy.arctan(param_0, numpy.sqrt(param_1**2+param_2**2))", "contrast": "def get_pitch_angle(ax, ay, az):\n    import numpy as np\n    pitch = np.arctan2(-ax, np.sqrt(ay * ay + az * az))\n    return pitch", "label": 0}
{"index": "gp000940", "code": "def _propagate_mean(param_0, param_1, param_2):\n  return linop.matmul(param_0) + dist.mean()[..., tf.newaxis]", "contrast": "import numpy as np\ndef propagate_mean(mu, A, b):\n    transformed_mean = np.dot(A, mu) + b\n    return transformed_mean", "label": 0}
{"index": "gp087492", "code": "def traverse(proto_file):\n    def _collapse_comments(comments):\n        return '\\n'.join(\n            [c.strip() for c in (comments[\"leading_comments\"] + comments[\"trailing_comments\"]).split('\\n')])\n    def _traverse(package, items, tree):\n        for item_index, item in enumerate(items):\n            item = convert_protodef_to_editable(item)\n            if item_index in tree:\n                comments = tree[item_index]\n                if \"leading_comments\" in comments or \"trailing_comments\" in comments:\n                    item.comment = _collapse_comments(comments)\n                    del comments[\"leading_comments\"]\n                    del comments[\"trailing_comments\"]\n                if item.kind is EnumDescriptorProto:\n                    if 2 in comments: \n                        for k in comments[2]:\n                            value_comment = comments[2][k]\n                            if value_comment != {}:\n                                item.value[k].comment = _collapse_comments(value_comment)\n                elif item.kind is DescriptorProto:\n                    if 2 in comments: \n                        for k in comments[2]:\n                            field_comment = comments[2][k]\n                            if field_comment != {}:\n                                item.field[k].comment = _collapse_comments(field_comment)\n                elif item.kind is ServiceDescriptorProto:\n                    if 2 in comments: \n                        for k in comments[2]:\n                            method_comment = comments[2][k]\n                            if method_comment != {}:\n                                item.method[k].comment = _collapse_comments(method_comment)\n                else:\n                    raise Exception, item.kind\n            yield item, package\n            if item.kind is DescriptorProto:\n                for enum in item.enum_type:\n                    yield enum, package\n                for nested in item.nested_type:\n                    nested_package = package + \".\" + item.name\n                    for nested_item, np in _traverse(nested_package, [nested], tree[item_index]):\n                        yield nested_item, np\n    tree = collections.defaultdict(collections.defaultdict)\n    for loc in proto_file.source_code_info.location:\n        if loc.leading_comments or loc.trailing_comments:\n            place = tree\n            for p in loc.path:\n                if not place.has_key(p):\n                    place[p] = collections.defaultdict(collections.defaultdict)\n                place = place[p]\n            place[\"leading_comments\"] = loc.leading_comments\n            place[\"trailing_comments\"] = loc.trailing_comments\n    if set(tree.keys()).difference(set([4, 5, 6, 7, 8])) != set():\n        raise Exception, tree\n    return {\"types\":\n        list(itertools.chain(\n            _traverse(proto_file.package, proto_file.service, tree[6]), \n            _traverse(proto_file.package, proto_file.enum_type, tree[5]), \n            _traverse(proto_file.package, proto_file.message_type, tree[4]), \n        )),\n        \"file\": [\"\".join(x.leading_detached_comments) for x in proto_file.source_code_info.location if len(x.leading_detached_comments) > 0]\n    }", "contrast": "from google.protobuf.descriptor_pb2 import FileDescriptorProto\ndef flatten_tree(proto_file: FileDescriptorProto) -> dict:\n    flattened_tree = {'messages': [], 'enums': []}\n    def traverse(path, items, is_enum=False):\n        for i, item in enumerate(items):\n            item_path = path[:]\n            item_path.append(i)\n            if 'source_code_info' in item:\n                for location in item['source_code_info'].location:\n                    if location.leading_comments:\n                        comment = location.leading_comments.strip()\n                        if comment:\n                            msg_path = item_path[:-1]\n                            msg_name = items[msg_path[-1]].name\n                            msg = {'path': msg_path, 'name': msg_name, 'comment': comment}\n                            if is_enum:\n                                flattened_tree['enums'].append(msg)\n                            else:\n                                flattened_tree['messages'].append(msg)\n            if 'nested_type' in item:\n                traverse(item_path, item['nested_type'])\n            if 'enum_type' in item:\n                traverse(item_path, item['enum_type'], True)\n    traverse([], proto_file.message_type)\n    return flattened_tree", "label": 0}
{"index": "gp238714", "code": "def get_child_node(param_0):\n    for var_0 in self.children:\n        if child.name == param_0:\n            return var_0\n    return None", "contrast": "def get_child_by_name(self, inst_name):\n        child_inst = self.inst.get_child_by_name(inst_name)\n        if child_inst is None:\n            return None\n        return Node._factory(child_inst, self.env, self)", "label": 1}
{"index": "gp089072", "code": "def reference(param_0, param_1, param_2):\n        self.template(78)\n        print(\"| Total {0} {1} installed and {2} {3} upgraded\".format(\n            len(param_1), self.pkg(len(param_1)),\n            len(param_2), self.pkg(len(param_2))))\n        self.template(78)\n        for var_0, var_1 in itertools.izip_longest(param_1, param_2):\n            if var_1:\n                print(\"| Package {0} upgraded successfully\".format(var_1))\n            if var_0:\n                print(\"| Package {0} installed successfully\".format(var_0))\n        self.template(78)\n        print(\"\")", "contrast": "import subprocess\ndef reference_list():\n    installed_packages = subprocess.check_output(['pip', 'freeze']).decode('utf-8').strip().split('\\n')\n    upgraded_packages = subprocess.check_output(['pip', 'list', '--upgrade']).decode('utf-8').strip().split('\\n')\n    reference_list = []\n    for package in installed_packages:\n        name, version = package.split('==')\n        reference_list.append({'name': name, 'version': version, 'status': 'installed'})\n    for package in upgraded_packages:\n        name, version = package.split()\n        reference_list.append({'name': name, 'version': version, 'status': 'upgraded'})\n    return reference_list", "label": 0}
{"index": "gp190679", "code": "def normalize_sequence(param_0, param_1):\n    var_0 = []\n    for var_1 in param_1:\n        var_2 = [param_0[var_3][var_1] for var_3 in range(len(param_0))]\n        mean.append(sum(var_2)/len(var_2))\n    for var_1 in range(len(param_0)):\n        for var_3 in param_1:\n            param_0[var_1][var_3] -= var_0[considerDimensions.index(var_3)]\n    return param_0", "contrast": "def normalizeSequence(sequence):\n  seq = np.array(sequence).astype('float64')\n  meanSeq = np.mean(seq)\n  stdSeq = np.std(seq)\n  seq = (seq - np.mean(seq)) / np.std(seq)\n  sequence = seq.tolist()\n  return sequence, meanSeq, stdSeq", "label": 1}
{"index": "gp059887", "code": "def adaptive_knn_graph(param_0,param_1): \n    var_0 = np.zeros_like(param_0,dtype=float) \n    var_1=(np.transpose(np.argsort(param_0,0))) \n    for var_2 in range(0,(traj_dist.shape[0])): \n        var_0[var_2,var_1[var_2,range(1,param_1)]]=param_0[var_2,var_1[var_2,range(1,param_1)]]\n    return var_0", "contrast": "def adj_matrix(traj_dist, k):\n    n = traj_dist.shape[0]\n    adj_mat = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i+1, n):\n            dist = traj_dist[i][j]\n            if dist <= k:\n                adj_mat[i][j] = 1\n                adj_mat[j][i] = 1\n    return adj_mat", "label": 0}
{"index": "gp052744", "code": "def plot_compare(param_0, param_1, param_2=True):\n        import matplotlib.lines as mlines\n        var_0 = self.get_plot()\n        var_1 = self.bs_plot_data()\n        var_2 = other_plotter.bs_plot_data()\n        var_3 = 1\n        for var_4 in range(other_plotter._nb_bands):\n            for var_5 in range(len(var_1['distances'])):\n                plt.plot(var_1['distances'][var_5],\n                         [var_6[str(Spin.up)][var_4] for var_6 in var_2['energy']][var_5],\n                         'c-', linewidth=var_3)\n                if other_plotter._bs.is_spin_polarized:\n                    plt.plot(var_1['distances'][var_5],\n                             [var_6[str(Spin.down)][var_4] for var_6 in var_2['energy']][var_5],\n                             'm--', linewidth=var_3)\n        if param_2:\n            var_7 = [mlines.Line2D([], [], linewidth=2,\n                                     color='b', label='bs 1 up'),\n                       mlines.Line2D([], [], linewidth=2,\n                                     color='r', label='bs 1 down',\n                                     linestyle=\"--\"),\n                       mlines.Line2D([], [], linewidth=2,\n                                     color='c', label='bs 2 up'),\n                       mlines.Line2D([], [], linewidth=2,\n                                     color='m', linestyle=\"--\",\n                                     label='bs 2 down')]\n            plt.legend(var_7=var_7)\n        return var_0", "contrast": "import matplotlib.pyplot as plt\ndef plot_two_bandstructures(bandstructure1, bandstructure2):\n    data1 = bandstructure1.as_dict()\n    data2 = bandstructure2.as_dict()\n    ymin = min([data1['energy'][ispin][iband][-1] for ispin in range(data1['nspin'])\n                                                      for iband in range(data1['nb_bands'])])\n    ymax = max([data1['energy'][ispin][iband][0] for ispin in range(data1['nspin'])\n                                                     for iband in range(data1['nb_bands'])])\n    color = 'blue'\n    for ispin in range(data1['nspin']):\n        for iband in range(data1['nb_bands']):\n            plt.plot(data1['distances'], data1['energy'][ispin][iband], color=color)\n    color = 'red'\n    for ispin in range(data2['nspin']):\n        for iband in range(data2['nb_bands']):\n            plt.plot(data2['distances'], data2['energy'][ispin][iband], color=color)\n    plt.xlim(data1['distances'][0], data1['distances'][-1])\n    plt.ylim(ymin, ymax)\n    plt.xlabel('Wave Vector')\n    plt.ylabel('Energy')\n    plt.show()", "label": 0}
{"index": "gp300919", "code": "def delete_rows(param_0):\n    df.drop(index=df.index[param_0], inplace=True)", "contrast": "def delete(self, indexes):\n        indexes = [indexes] if not isinstance(indexes, (list, blist)) else indexes\n        if all([isinstance(i, bool) for i in indexes]):  \n            if len(indexes) != len(self._index):\n                raise ValueError('boolean indexes list must be same size of existing indexes')\n            indexes = [i for i, x in enumerate(indexes) if x]\n        else:\n            indexes = [sorted_index(self._index, x) for x in indexes] if self._sort                else [self._index.index(x) for x in indexes]\n        indexes = sorted(indexes, reverse=True)  \n        for i in indexes:\n            del self._data[i]\n        for i in indexes:\n            del self._index[i]", "label": 1}
{"index": "gp113973", "code": "def of_think(param_0, param_1):\n        return self._compute(\n            duration=think.duration,\n            after=self.continuation)", "contrast": "import time\ndef simulate_worker_process(time_in_seconds):\n    time.sleep(time_in_seconds)", "label": 0}
{"index": "gp068993", "code": "def find_element_by_jquery(param_0, param_1):\n    var_0 = find_elements_by_jquery(param_0, param_1)\n    if not var_0:\n        raise AssertionError(\"No matching element found.\")\n    if len(var_0) > 1:\n        raise AssertionError(\"Multiple matching elements found.\")\n    return var_0[0]", "contrast": "from bs4 import BeautifulSoup\ndef find_html_element(html, selector):\n    soup = BeautifulSoup(html, 'html.parser')\n    element = soup.select_one(selector)\n    return element", "label": 0}
{"index": "gp218969", "code": "def check_order(param_0):\n    var_0 = param_0[0]\n    for var_1 in param_0[1:]:\n        if var_0 > var_1:\n            return False\n        elif var_0 == var_1:\n            continue\n        else:\n            var_0 = var_1\n    return True", "contrast": "def check_dependee_order(depender, dependee, dependee_id):\n shutit_global.shutit_global_object.yield_to_draw()\n if dependee.run_order > depender.run_order:\n\t\treturn 'depender module id:\\n\\n' + depender.module_id + '\\n\\n(run order: ' + str(depender.run_order) + ') ' + 'depends on dependee module_id:\\n\\n' + dependee_id + '\\n\\n(run order: ' + str(dependee.run_order) + ') ' + 'but the latter is configured to run after the former'\n return ''", "label": 1}
{"index": "gp189049", "code": "import os\ndef ensure_directory_does_not_exist(param_0):\n    if os.path.exists(param_0):\n        os.rmdir(param_0)\n    return True", "contrast": "def ensure_dir_does_not_exist(*args):\n    path = os.path.join(*args)\n    if os.path.isdir(path):\n        shutil.rmtree(path)", "label": 1}
{"index": "gp156143", "code": "def overlaps(param_0, param_1):\n        return self.network in param_1 or self.broadcast in param_1 or (\n            other.network in param_0 or other.broadcast in param_0)", "contrast": "def is_partly_contained(self, other):\n    return any(x in other for x in self)", "label": 0}
{"index": "gp239642", "code": "def is_excluded_dir(param_0, param_1):\n    for var_0 in param_1:\n        if directory.startswith(var_0):\n            return True\n    return False", "contrast": "def is_excluded(root, excludes):\n    for exclude in excludes:\n        if fnmatch(root, exclude):\n            return True\n    return False", "label": 1}
{"index": "gp300361", "code": "def apply_quaternion_rotation(param_0, param_1):\n    var_0 = [param_0[3], param_0[0], param_0[1], param_0[2]]\n    var_1 = [0] + param_1\n    var_2 = [var_0[0], -var_0[1], -var_0[2], -var_0[3]]\n    var_3 = quaternion_multiply(quaternion_multiply(var_0, var_1), var_2)\n    return var_3[1:]", "contrast": "def quaternion_rotation(quat, vector):\n    dp = np.dot(quat[1:], vector)\n    cos = (2*quat[0]*quat[0] - 1)\n    return np.array([\n        2 * (quat[0] * (quat[2] * vector[2] - quat[3] * vector[1]) + quat[1] * dp) + cos * vector[0],\n        2 * (quat[0] * (quat[3] * vector[0] - quat[1] * vector[2]) + quat[2] * dp) + cos * vector[1],\n        2 * (quat[0] * (quat[1] * vector[1] - quat[2] * vector[0]) + quat[3] * dp) + cos * vector[2]\n    ], float)", "label": 1}
{"index": "gp081729", "code": "def pop_event(param_0):\n    with self.lock:\n      if not self.events:\n        raise ValueError('no events queued')\n      return self.events.popleft()", "contrast": "def pop_event(queue):\n    if len(queue) == 0:\n        raise ValueError('There is no event queued')\n    else:\n        return queue.pop(0)", "label": 0}
{"index": "gp321745", "code": "def concatenate_criteria(param_0):\n    var_0 = []\n    for var_1, var_2 in criterion.items():\n        if isinstance(var_2, dict):\n            var_3 = concatenate_criteria(var_2).split('__')\n            for var_4 in var_3:\n                criteria.append(var_1 + '__' + var_4)\n        else:\n            criteria.append(var_1)\n    return '__'.join(sorted(var_0))", "contrast": "def name(self):\n        names = (criterion.name() for criterion in self._criteria)\n        return '__'.join(sorted(names))", "label": 1}
{"index": "gp171525", "code": "def get_nth_value(param_0, param_1=1):\n    var_0 = [var_2 for (var_1, var_2) in some_message if var_1 == param_0]\n    if len(var_0) < param_1:\n        return None\n    return var_0[param_1-1]", "contrast": "def get(self, tag, nth=1):\n        tag = fix_tag(tag)\n        nth = int(nth)\n        for t, v in self.pairs:\n            if t == tag:\n                nth -= 1\n                if nth == 0:\n                    return v\n        return None", "label": 1}
{"index": "gp219702", "code": "import logging\ndef log_client_host(param_0):\n    var_0 = logging.getLogger(__name__)\n    logger.info(\"Client host: %s\", param_0, extra={'client_host': param_0})", "contrast": "def my_address_string(self):\n        addr = getattr(self, 'client_address', ('', None))[0]\n        if addr in self.proxy_ips:\n            return self.headers.getheader('x-forwarded-for', addr)\n        return addr", "label": 1}
{"index": "gp100153", "code": "def all_variables(param_0):\n        var_0 = []\n        variables.extend(self.free_variables)\n        variables.append(self.independent['symbol'])\n        variables.extend([ var_1['symbol'] for var_1 in self.fitting_parameters ])\n        variables.extend([ var_1['symbol'] for var_1 in self.fixed_parameters ])\n        variables.extend([ var_2['symbol'] for var_2 in self.constants ])\n        var_3 = []\n        for var_4 in var_0:\n            if isinstance(var_4, str):\n                symbols.append(self.model.symbol(var_4))\n            else:\n                symbols.append(var_4)\n        return tuple(var_3)", "contrast": "def get_all_symbols(fit):\n    return tuple(fit.free_variables + fit.independent + fit.fitting_parameters + fit.fixed_parameters + fit.constants)", "label": 0}
{"index": "gp153205", "code": "def _get_ntgpadnt(param_0, param_1, param_2):\n        var_0 = self.gpad_columns[param_1]\n        if param_2:\n            var_0 = var_0 + ['NS']\n        return cx.namedtuple(\"ntgpadobj\", var_0)", "contrast": "from collections import namedtuple\ndef create_namedtuple(annotation):\n    Annotation = namedtuple('Annotation', ['name', 'value'])\n    ann = Annotation(name=annotation[0], value=annotation[1])\n    return ann", "label": 0}
{"index": "gp310168", "code": "def attribute_probability(param_0, param_1):\n    return probability", "contrast": "def get_value_prob(self, attr_name, value):\n        if attr_name not in self._attr_value_count_totals:\n            return\n        n = self._attr_value_counts[attr_name][value]\n        d = self._attr_value_count_totals[attr_name]\n        return n/float(d)", "label": 1}
{"index": "gp108574", "code": "def _get_select_commands(param_0, param_1, param_2):\n        var_0 = {var_1: self.select_all(var_1, execute=False) for var_1 in\n                       tqdm(param_2, total=len(param_2), desc='Getting {0} select queries'.format(param_1))}\n        for var_1, var_2 in row_queries.items():\n            if isinstance(var_2, str):\n                var_0[var_1] = [var_2]\n        return [(var_1, cmd) for var_1, var_3 in row_queries.items() for cmd in var_3]", "contrast": "def create_select_queries(source, tables):\n    commands = {}\n    for table in tables:\n         commands[table] = f\"SELECT * FROM {source}.{table};\"\n    return commands", "label": 0}
{"index": "gp222493", "code": "def execute_command(param_0):\n    if param_0 == 'help':\n        print('This is the help text')\n    else:\n        pass ", "contrast": "def execute(self):\n        if self.args and self.argument(0) == \"help\":\n            self.error(self.usage() + \"\\n\\n\" + self.help())\n            return False\n        return True", "label": 1}
{"index": "gp258138", "code": "from habanero import Crossref\nimport random\ndef random_dois(param_0=10, **param_1):\n    if param_0 > 100:\n        param_0 = 100\n    var_0 = Crossref()\n    var_1 = \"works?sample=%s&filter=doi\" % param_0\n    for var_2 in param_1:\n        var_1 += \"&%s=%s\" % (var_2, param_1[var_2])\n    var_3 = cr._make_request(var_1)\n    return random.sample(var_3[\"message\"][\"result\"], param_0)", "contrast": "def random_dois(self, sample = 10, **kwargs):\n        res = request(self.mailto, self.base_url, \"/works/\", None,\n            None, None, None, None, sample, None,\n            None, None, None, True, None, None, None, **kwargs)\n        return [ z['DOI'] for z in res['message']['items'] ]", "label": 1}
{"index": "gp106515", "code": "def FunctionalGroupColorMapping(param_0='jet', param_1=False):\n    var_0 = '#f76ab4'\n    var_1 = '#ff7f00'\n    var_2 = '#12ab0d'\n    var_3 = '#84380b'\n    var_4 = '#e41a1c'\n    var_5 = '#972aa8'\n    var_6 = '#3c58e5'\n    var_7 = {'G':var_0, 'A':var_0,\n                 'S':var_1, 'T':var_1, 'C':var_1,\n                 'V':var_2, 'L':var_2, 'I':var_2, 'M':var_2, 'P':var_2,\n                 'F':var_3, 'Y':var_3, 'W':var_3,\n                 'D':var_4, 'E':var_4,\n                 'H':var_6, 'K':var_6, 'R':var_6,\n                 'N':var_5, 'Q':var_5,\n                 '*':'#000000'}\n    return (None, var_7, None)", "contrast": "def amino_acid_group_colors(maptype=None, reverse=False):\n    aa_colors = {\n        'hydrophobic': '#FCA000',\n        'basic': '#0000FF',\n        'acidic': '#FF1493',\n        'polar': '#29B4B4',\n        'cysteine': '#FFD700',\n        'aromatic': '#A020F0',\n        'glycine': '#FFFFFF',\n        'proline': '#C0C0C0'\n    }\n    return aa_colors", "label": 0}
{"index": "gp012714", "code": "def draw_string(param_0, param_1, param_2, param_3, param_4=None, param_5=None, param_6=Alignment.left, param_7=VerticalAlignment.baseline):\n    var_0 = Style(param_0)\n    var_1 = GlyphRun(var_0, param_1)\n    var_2 = GlyphLayout([var_1], param_2, param_3, param_4, param_5, param_6, param_7)\n    draw_glyph_layout(var_2)", "contrast": "def draw_text(font, text, x, y):\n    font_surface = font.render(text, True, (0, 0, 0))\n    screen.blit(font_surface, (x, y))", "label": 0}
{"index": "gp219317", "code": "from django.contrib.admin.options import ModelAdmin\ndef is_modeladmin_derived(param_0):\n    return issubclass(node.__class__, ModelAdmin)", "contrast": "def is_model_admin_subclass(node):\n    if node.name[-5:] != 'Admin' or isinstance(node.parent, ClassDef):\n        return False\n    return node_is_subclass(node, 'django.contrib.admin.options.ModelAdmin')", "label": 1}
{"index": "gp247724", "code": "def normalize_to_bytes(param_0):\n    if isinstance(param_0, str):\n        return data.encode()\n    elif isinstance(param_0, bytes):\n        return param_0\n    else:\n        raise TypeError(\"Input data must be a string or bytes object\")", "contrast": "def to_bytes(data):\n    if isinstance(data, six.string_types) and not isinstance(data, bytes):\n        return codecs.encode(data, TEXT_ENCODING)\n    return data", "label": 1}
{"index": "gp144488", "code": "def functions(param_0):\n        var_0 = ffi.lib.LLVMPY_ModuleFunctionsIter(param_0)\n        return _FunctionsIterator(var_0, dict(module=param_0))", "contrast": "import types\ndef get_functions(module):\n    for _, value in module.__dict__.items():\n        if isinstance(value, types.FunctionType):\n            yield value.__code__", "label": 0}
{"index": "gp093209", "code": "def set_index(param_0, param_1, param_2=True, param_3=False,\n                  param_4=False, param_5=False):\n        if param_2 is True:\n            try:\n                assert type(param_1) is not str\n                var_0 = set(param_1)\n            except (TypeError, AssertionError):\n                var_0 = set([param_1])\n        if not self._required_cols <= (set(self.columns) - set(var_0)):\n            raise PhysicalMeaning('You drop a column that is needed to '\n                                  'be a physical meaningful description '\n                                  'of a molecule.')\n        if param_4:\n            self._frame.set_index(param_1, param_2=param_2, param_3=param_3,\n                                  param_4=param_4,\n                                  param_5=param_5)\n        else:\n            var_1 = self._frame.set_index(param_1, param_2=param_2, param_3=param_3,\n                                        param_4=param_4,\n                                        param_5=param_5)\n            return self.__class__(var_1, _metadata=self._metadata,\n                                  metadata=self.metadata)", "contrast": "def set_dataframe_index(dataframe, columns):\n    return dataframe.set_index(columns)", "label": 0}
{"index": "gp299205", "code": "from IPython.display import HTML\ndef show_preview(param_0):\n    var_0 = {\n        'small': '100px',\n        'med': '250px',\n        'thumb': '50px',\n        'full': '100%'\n    }\n    var_1 = f'<img src=\"preview.png\" style=\"width:{var_0[param_0]}\">'\n    display(HTML(var_1))", "contrast": "def show_images(self, size=\"small\"):\n        d = dict(small=256, med=512, thumb=100, full=1024)\n        try:\n            width = d[size]\n        except KeyError:\n            print(\"Allowed keys:\", d.keys())\n            return\n        img_urls = [i._get_img_url(size) for i in self.obsids]\n        imagesList = \"\".join(\n            [\n                \"<img style='width: {0}px; margin: 0px; float: \"\n                \"left; border: 1px solid black;' \"\n                \"src='{1}' />\".format(width, s)\n                for s in img_urls\n            ]\n        )\n        display(HTML(imagesList))", "label": 1}
{"index": "gp004410", "code": "def delete_lower(param_0, param_1=None):\n    if param_1 is None:\n        param_1 = script.current_layer()\n    if param_1 != 0:\n        change(param_0, 0)\n    for var_0 in range(param_1):\n        delete(param_0, 0)\n    return None", "contrast": "def delete_layers_below(layer_num, meshlab_project_file):\n    with open(meshlab_project_file, 'r') as f:\n        proj_str = f.read()\n    start_idx = proj_str.find(\"<Layer \")\n    new_proj_str = proj_str[:start_idx]\n    new_proj_str += proj_str[start_idx:].split(f'<Layer ID=\"{layer_num}\"')[0]\n    new_proj_str += f'<Layer ID=\"{layer_num}\"'\n    with open(meshlab_project_file, 'w') as f:\n        f.write(new_proj_str)", "label": 0}
{"index": "gp004398", "code": "def mesh2fc(param_0, param_1=False):\n    var_0 = ''.join([\n        '  <filter name=\"Transfer Color: Mesh to Face\">\\n',\n        '    <Param name=\"allVisibleMesh\" ',\n        'value=\"%s\" ' % str(param_1).lower(),\n        'description=\"Apply to all Meshes\" ',\n        'type=\"RichBool\" ',\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(param_0, var_0)\n    return None", "contrast": "faces, otherwise the filter applies color mapping only to the faces of the active mesh.\n    Returns:\n        The created filter.\n    '''\n    def transfer_mesh_colors_to_face_colors(script, all_visible_layers=True):\n        filter = script.createFilter(\"geometry.colors.transferMeshColorsToFaceColors\")\n        filter.setEnabled(True)\n        filter.setBoolProperty(\"all_visible_layers\", all_visible_layers)\n        return filter", "label": 0}
{"index": "gp269106", "code": "import subprocess\ndef globus_task_event_list_executor(param_0):\n    var_0 = ['globus', 'task-event-list', param_0]\n    var_1 = subprocess.run(var_0, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if result.returncode != 0:\n        raise Exception(f\"Command execution failed with error: {result.stderr.decode('utf-8')}\")\n    return result.stdout.decode('utf-8')", "contrast": "def task_event_list(task_id, limit, filter_errors, filter_non_errors):\n    client = get_client()\n    if filter_errors and filter_non_errors:\n        raise click.UsageError(\"Cannot filter by both errors and non errors\")\n    elif filter_errors:\n        filter_string = \"is_error:1\"\n    elif filter_non_errors:\n        filter_string = \"is_error:0\"\n    else:\n        filter_string = \"\"\n    event_iterator = client.task_event_list(\n        task_id, num_results=limit, filter=filter_string\n    )\n    def squashed_json_details(x):\n        is_json = False\n        try:\n            loaded = json.loads(x[\"details\"])\n            is_json = True\n        except ValueError:\n            loaded = x[\"details\"]\n        if is_json:\n            return json.dumps(loaded, separators=(\",\", \":\"), sort_keys=True)\n        else:\n            return loaded.replace(\"\\n\", \"\\\\n\")\n    formatted_print(\n        event_iterator,\n        fields=(\n            (\"Time\", \"time\"),\n            (\"Code\", \"code\"),\n            (\"Is Error\", \"is_error\"),\n            (\"Details\", squashed_json_details),\n        ),\n        json_converter=iterable_response_to_dict,\n    )", "label": 1}
{"index": "gp196283", "code": "class ValidationError(Exception):\n    pass\ndef validate_value(param_0, param_1, param_2):\n    if param_1 in param_0:\n        try:\n            param_2(param_0[param_1])\n        except Exception as e:\n            raise ValidationError(f\"Validation error for {param_1} with value '{param_0[param_1]}': {str(e)}\")\n    for var_0 in obj.values():\n        if isinstance(var_0, dict):\n            validate_value(var_0, param_1, param_2)\n        elif isinstance(var_0, list):\n            for var_1 in var_0:\n                if isinstance(var_1, dict):\n                    validate_value(var_1, param_1, param_2)", "contrast": "def validate_all_values_for_key(obj, key, validation_fun):\n    for vkey, value in obj.items():\n        if vkey == key:\n            validation_fun(value)\n        elif isinstance(value, dict):\n            validate_all_values_for_key(value, key, validation_fun)", "label": 1}
{"index": "gp069735", "code": "def parse_int_string(param_0: str) -> List[int]:\n    var_0 = \" \".join(int_string.strip().split())\n    var_0 = cleaned.replace(\" - \", \"-\")\n    var_0 = cleaned.replace(\",\", \" \")\n    var_1 = cleaned.split(\" \")\n    var_2: Set[int] = set()\n    for var_3 in var_1:\n        if \"-\" in var_3:\n            var_4 = token.split(\"-\")\n            if len(var_4) != 2:\n                LOG.info(f\"Dropping '{var_3}' as invalid - weird range.\")\n                continue\n            var_5 = int(var_4[0])\n            var_6 = int(var_4[1]) + 1\n            var_2 = indices.union(var_2, set(range(var_5, var_6)))\n        else:\n            try:\n                indices.add(int(var_3))\n            except ValueError:\n                LOG.info(f\"Dropping '{var_3}' as invalid - not an int.\")\n    return list(var_2)", "contrast": "def unique_integers(string):\n    numbers = []\n    for group in string.split():\n        if '-' in group:\n            start, end = group.split('-')\n            numbers.extend(range(int(start), int(end)+1))\n        else:\n            try:\n                number = int(group)\n                numbers.append(number)\n            except ValueError:\n                pass\n    return list(set(numbers))", "label": 0}
{"index": "gp195145", "code": "import numpy as np\ndef invert_matrix(param_0, param_1=True):\n    if param_1:\n        param_0 = W.copy()\n    for var_0 in range(W.shape[0]):\n        for var_1 in range(W.shape[1]):\n            param_0[var_0][var_1] = 1 / param_0[var_0][var_1]\n    return param_0", "contrast": "def invert(W, copy=True):\n    if copy:\n        W = W.copy()\n    E = np.where(W)\n    W[E] = 1. / W[E]\n    return W", "label": 1}
{"index": "gp210742", "code": "def append_pending_symbol_to_annotations(param_0):\n    param_0['annotations'].append(param_0['pending_symbol'])\n    return None", "contrast": "def set_annotation(self):\n        assert self.pending_symbol is not None\n        assert not self.value\n        annotations = (_as_symbol(self.pending_symbol, is_symbol_value=False),)  \n        self.annotations = annotations if not self.annotations else self.annotations + annotations\n        self.ion_type = None\n        self.pending_symbol = None  \n        self.quoted_text = False\n        self.line_comment = False\n        self.is_self_delimiting = False\n        return self", "label": 1}
{"index": "gp095721", "code": "def patch(param_0, param_1):\n        with (tempfile.NamedTemporaryFile(prefix='.sync',\n              suffix=os.path.basename(self.path),\n              dir=os.path.dirname(self.path), delete=False)) as var_0:\n            try:\n                with open(self.path, 'rb') as var_1:\n                    var_2 = librsync.patch(var_1, param_1, var_0)\n                    os.rename(output.name, self.path)\n                    return var_2\n            finally:\n                try:\n                    os.remove(output.name)\n                except OSError as e:\n                    if e.errno != errno.ENOENT:\n                        raise", "contrast": "def apply_remote_delta_to_local_file(remote_delta: bytearray, local_file_path: str) -> None:\n    with open(local_file_path, 'r+b') as file:\n        file_content = bytearray(file.read())\n        file_content = apply_delta(file_content, remote_delta)\n        file.seek(0)\n        file.write(file_content)", "label": 0}
{"index": "gp096629", "code": "def make_cmd_invocation(param_0, param_1, param_2):\n    if not invocation.endswith('/'):\n        param_0 += '/'\n    if not invocation.startswith('/'):\n        param_0 = '/' + param_0\n    var_0 = param_0\n    for var_1 in param_1:\n        var_0 += str(var_1) + \"/\"\n    var_2 = []\n    for var_3, var_4 in kwargs.items():\n        rendered_kwargs.append(\"--%s=%s\" % (var_3,var_4))\n    return ['./giotto-cmd', var_0] + var_2", "contrast": "def make_cmd_invocation(program_path, args_list, kwargs_dict):\n    invocation = ['./giotto-cmd', program_path]\n    for arg in args_list:\n        invocation.append(arg)\n    for key, value in kwargs_dict.items():\n        invocation.append('--{}={}'.format(key, value))\n    return invocation", "label": 0}
{"index": "gp205836", "code": "import tqdm\ndef create_progress_bar(param_0, param_1='', param_2=None):\n    return tqdm.tqdm(param_0, param_1=param_1, param_2=param_2, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')", "contrast": "def progress_bar(**kwargs):\n    tqdm_kw = {\n        'desc': 'Processing',\n        'file': sys.stdout,\n        'bar_format': TQDM_BAR_FORMAT,\n    }\n    tqdm_kw.update(kwargs)\n    pbar = tqdm(**tqdm_kw)\n    if not pbar.disable:\n        pbar.desc = pbar.desc.rstrip(': ')\n        pbar.refresh()\n    return pbar", "label": 1}
{"index": "gp236343", "code": "def set_callback_verbosity(param_0):\n    def wrapper(*param_0, **param_1):\n        return param_0(*param_0, **param_1)\n    wrapper.wants_verbosity = True\n    return wrapper", "contrast": "def pass_verbosity(f):\n    def new_func(*args, **kwargs):\n        kwargs['verbosity'] = click.get_current_context().verbosity\n        return f(*args, **kwargs)\n    return update_wrapper(new_func, f)", "label": 1}
{"index": "gp259333", "code": "def meets_search_space_restrictions(param_0, param_1):\n    for var_0, var_1 in instance.items():\n        if var_0 not in param_1 or var_1 not in param_1[var_0]:\n            return False\n    return True", "contrast": "def check_restrictions(restrictions, element, keys, verbose):\n    params = OrderedDict(zip(keys, element))\n    for restrict in restrictions:\n        if not eval(replace_param_occurrences(restrict, params)):\n            if verbose:\n                print(\"skipping config\", get_instance_string(params), \"reason: config fails restriction\")\n            return False\n    return True", "label": 1}
{"index": "gp088188", "code": "def get_api_docs(param_0):\n    param_0 = map(_get_tuple_from_route, param_0)\n    var_0 = []\n    for var_1, var_2, var_3 in sorted(param_0, key=lambda a: a[0]):\n        if issubclass(var_2, APIHandler):\n            documentation.append(_get_route_doc(var_1, var_2, var_3))\n    var_0 = (\n        \"**This documentation is automatically generated.**\\n\\n\" +\n        \"**Output schemas only represent `data` and not the full output; \" +\n        \"see output examples and the JSend specification.**\\n\" +\n        \"\\n<br>\\n<br>\\n\".join(var_0)\n    )\n    return var_0", "contrast": "def generate_api_docs(routes: list) -> str:\n    docs = ''\n    for route in routes:\n        url, handler = route\n        docstring = handler.__doc__\n        if docstring:\n            sig = inspect.signature(handler)\n            args = sig.parameters\n            returns = handler.__annotations__.get('return', None)\n            method = handler.__module__.split('.', 1)[-1]\n            if hasattr(handler, 'schema'):\n                schema = handler.schema\n            else:\n                schema = None\n            docs += f\"### {method.upper()}: {url}\\n\\n\"\n            docs += f\"{docstring.strip()}\\n\\n\"\n            if schema:\n                docs += f\"", "label": 0}
{"index": "gp063762", "code": "def predict_mhcii_binding(param_0, param_1, param_2, param_3, param_4):\n    job.fileStore.logToMaster('Running mhcii on %s:%s' % (param_3['patient'], param_2))\n    var_0 = job.fileStore.getLocalTempDir()\n    var_1 = {\n        'peptfile.faa': param_1}\n    var_1 = get_files_from_filestore(param_0, var_1, var_0, docker=True)\n    var_2 = [param_4['pred'],\n                  param_2,\n                  var_1['peptfile.faa']]\n    with open('/'.join([var_0, 'predictions.tsv']), 'w') as var_3:\n        docker_call(tool='mhcii', tool_parameters=var_2, var_0=var_0,\n                    dockerhub=param_3['dockerhub'], outfile=var_3, interactive=True)\n    var_4 = True\n    with open(predfile.name, 'r') as var_3:\n        for var_5 in var_3:\n            if not line.startswith('HLA'):\n                continue\n            if line.strip().split('\\t')[5] == 'NetMHCIIpan':\n                break\n            elif line.strip().split('\\t')[5] == 'Sturniolo':\n                var_6 = 'Sturniolo'\n            else:\n                var_6 = 'Consensus'\n            var_4 = False\n            break\n    if var_4:\n        var_7 = job.addChildJobFn(predict_netmhcii_binding, param_1, param_2, param_3,\n                                        disk='10G')\n        return NetMHCIIpan.rv()\n    else:\n        var_8 = job.fileStore.writeGlobalFile(predfile.name)\n        return var_8, var_6", "contrast": "def predict_MHC_binding(YY, ALLELE):\n    PREDFILE = \"path/to/prediction/file\"\n    PREDICTOR = \"Consensus\" \n    return PREDFILE, PREDICTOR", "label": 0}
{"index": "gp229416", "code": "import requests\ndef get_pending_txns(param_0):\n    try:\n        var_0 = requests.get(f\"{param_0}/api/transactions/pending\")\n        return len(response.json())\n    except requests.exceptions.RequestException as e:\n        print(f\"Error occurred while fetching pending transactions: {e}\")\n        return None", "contrast": "def pending_transactions(server):\n    namecoind = NamecoindClient(server, NAMECOIND_PORT,\n                                NAMECOIND_USER, NAMECOIND_PASSWD)\n    reply = namecoind.listtransactions(\"\", 10000)\n    counter = 0\n    for i in reply:\n        if i['confirmations'] == 0:\n            counter += 1\n    return counter", "label": 1}
{"index": "gp234985", "code": "import numpy as np\ndef check_time_and_freq(param_0: np.ndarray, param_1: list[np.ndarray], param_2: np.ndarray, param_3: list[np.ndarray]):\n    if not isinstance(param_0, np.ndarray):\n        return False\n    for var_0 in param_1:\n        if not isinstance(var_0, np.ndarray):\n            return False\n    if not isinstance(param_2, np.ndarray):\n        return False\n    for var_0 in param_3:\n        if not isinstance(var_0, np.ndarray):\n            return False\n    return True", "contrast": "def validate(ref_time, ref_freqs, est_time, est_freqs):\n    util.validate_events(ref_time, max_time=MAX_TIME)\n    util.validate_events(est_time, max_time=MAX_TIME)\n    if ref_time.size == 0:\n        warnings.warn(\"Reference times are empty.\")\n    if ref_time.ndim != 1:\n        raise ValueError(\"Reference times have invalid dimension\")\n    if len(ref_freqs) == 0:\n        warnings.warn(\"Reference frequencies are empty.\")\n    if est_time.size == 0:\n        warnings.warn(\"Estimated times are empty.\")\n    if est_time.ndim != 1:\n        raise ValueError(\"Estimated times have invalid dimension\")\n    if len(est_freqs) == 0:\n        warnings.warn(\"Estimated frequencies are empty.\")\n    if ref_time.size != len(ref_freqs):\n        raise ValueError('Reference times and frequencies have unequal '\n                         'lengths.')\n    if est_time.size != len(est_freqs):\n        raise ValueError('Estimate times and frequencies have unequal '\n                         'lengths.')\n    for freq in ref_freqs:\n        util.validate_frequencies(freq, max_freq=MAX_FREQ, min_freq=MIN_FREQ,\n                                  allow_negatives=False)\n    for freq in est_freqs:\n        util.validate_frequencies(freq, max_freq=MAX_FREQ, min_freq=MIN_FREQ,\n                                  allow_negatives=False)", "label": 1}
{"index": "gp205409", "code": "def remove_redundant_segments(param_0):\n    var_0 = []\n    for var_1 in param_0:\n        if not var_0 or var_1 != var_0[-1]:\n            result.append(var_1)\n    return var_0", "contrast": "def reduce(self) -> None:\n        idx = 0\n        while idx < len(self):\n            if idx > 0 and                    self[idx - 1].type == 'text' and self[idx].type == 'text':\n                self[idx - 1].data['text'] += self[idx].data['text']\n                del self[idx]\n            else:\n                idx += 1", "label": 1}
{"index": "gp181932", "code": "def increment_byte_sequence(param_0: bytes) -> bytes:\n    var_0 = int.from_bytes(param_0, byteorder='little', signed=False)\n    var_0 += 1\n    return num.to_bytes((num.bit_length() + 7) // 8, byteorder='little', signed=False)", "contrast": "def sodium_increment(inp):\n    ensure(isinstance(inp, bytes),\n           raising=exc.TypeError)\n    ln = len(inp)\n    buf = ffi.new(\"unsigned char []\", ln)\n    ffi.memmove(buf, inp, ln)\n    lib.sodium_increment(buf, ln)\n    return ffi.buffer(buf, ln)[:]", "label": 1}
{"index": "gp077631", "code": "def get_pdb_id(param_0):\n        if self.pdb_id:\n            return self.pdb_id\n        else:\n            var_0 = self.parsed_lines[\"HEADER\"]\n            assert(len(var_0) <= 1)\n            if var_0:\n                self.pdb_id = var_0[0][62:66]\n                return self.pdb_id\n        return None", "contrast": "class PDBFile:\n    def __init__(self, file_path: str, pdb_id: str = None):\n        self.file_path = file_path\n        self.pdb_id = pdb_id\n    def get_pdb_id(self):\n        if self.pdb_id:\n            return self.pdb_id\n        else:\n            with open(self.file_path) as f:\n                for line in f:\n                    if line.startswith('HEADER'):\n                        header_info = line.split()\n                        for info in header_info:\n                            if len(info) == 4 and info.isalnum():\n                                return info\n                        break\n            return None", "label": 0}
{"index": "gp194330", "code": "def update_income_dist(param_0, param_1):\n    for var_0 in range(param_1[0].size):\n        for var_1 in range(param_0[0].size):\n            var_2 = param_1[1][var_0] * param_0[1][var_1]\n            var_3 = param_1[2][var_0] * param_0[2][var_1]\n            var_4 = param_1[0][var_0] * param_0[0][var_1]\n            param_1[0][var_1 * param_1[0].size + var_0] = var_4\n            param_1[1][var_1 * param_1[0].size + var_0] = var_2\n            param_1[2][var_1 * param_1[0].size + var_0] = var_3\n    return None", "contrast": "def addAggShkDstn(self,AggShkDstn):\n        if len(self.IncomeDstn[0]) > 3:\n            self.IncomeDstn = self.IncomeDstnWithoutAggShocks\n        else:\n            self.IncomeDstnWithoutAggShocks = self.IncomeDstn\n        self.IncomeDstn = [combineIndepDstns(self.IncomeDstn[t],AggShkDstn) for t in range(self.T_cycle)]", "label": 1}
{"index": "gp274189", "code": "def update_state(param_0, param_1, param_2, param_3, param_4):\n    var_0 = dict(param_0)\n    var_1 = len(param_0)\n    for var_2, var_3 in zip(param_1, range(var_1, var_1 + len(param_1))):\n        var_0[var_2] = var_3\n    param_2 = np.concatenate((state_vec, param_2), axis=0)\n    param_3 = np.concatenate((state_cov, param_3), axis=0)\n    param_4 = np.concatenate((noise_var, param_4))\n    return var_0, param_2, param_3, param_4", "contrast": "def add_features(self, kept_indices, new_indices,\n                     new_state_vec, new_state_cov, new_noise_var):\n        assert len(kept_indices) == len(self.state_vec)\n        assert len(new_indices) == len(new_state_vec)\n        assert len(new_indices) == len(new_state_cov)\n        assert len(new_indices) == len(new_noise_var)\n        if self.has_cached_obs_vec:\n            del self.obs_vec\n        if self.has_cached_predicted_state_vec:\n            del self.predicted_obs_vec\n        nfeatures = len(kept_indices) + len(new_indices)\n        next_state_vec = np.zeros((nfeatures, self.state_len))\n        next_state_cov = np.zeros((nfeatures, self.state_len, self.state_len))\n        next_noise_var = np.zeros((nfeatures, self.state_len))\n        if len(kept_indices) > 0:\n            next_state_vec[kept_indices] = self.state_vec\n            next_state_cov[kept_indices] = self.state_cov\n            next_noise_var[kept_indices] = self.noise_var\n            if len(self.state_noise_idx) > 0:\n                self.state_noise_idx = kept_indices[self.state_noise_idx]\n        if len(new_indices) > 0:\n            next_state_vec[new_indices] = new_state_vec\n            next_state_cov[new_indices] = new_state_cov\n            next_noise_var[new_indices] = new_noise_var\n        self.state_vec = next_state_vec\n        self.state_cov = next_state_cov\n        self.noise_var = next_noise_var", "label": 1}
{"index": "gp135337", "code": "def stepback(param_0, param_1=False):\n        if param_1:\n            var_0 = self._buffer[self._index - 1]\n            self._buffer.append(var_0)\n        else:\n            self._index -= 1", "contrast": "def stepback_buffer(buffer, append=False):\n    if not append:\n        return buffer[:-1]\n    else:\n        return buffer[::-1]", "label": 0}
{"index": "gp128814", "code": "def _update_pathway_definitions(param_0,\n                                param_1,\n                                param_2):\n    var_0 = {}\n    for var_1, var_2 in crosstalk_corrected_index_map.items():\n        var_3 = param_2[var_1]\n        var_4 = set([param_1[var_5] for var_5 in list(var_2)])\n        var_0[var_3] = var_4\n    return var_0", "contrast": "def convert_mapping(mapping, pathway_names, gene_names):\n    converted_mapping = {}\n    for pathway_id, gene_ids in mapping.items():\n        pathway_name = pathway_names.get(pathway_id, None)\n        converted_gene_names = []\n        for gene_id in gene_ids:\n            gene_name = gene_names.get(gene_id, None)\n            if gene_name:\n                converted_gene_names.append(gene_name)\n        if pathway_name:\n            converted_mapping[pathway_name] = converted_gene_names\n    return converted_mapping", "label": 0}
{"index": "gp089718", "code": "def _to_dict(param_0):\n        return dict(area= self.area._to_dict(),\n                    earthquakes = [q._to_dict() for var_0 in self.earthquakes],\n                    title = self.title)", "contrast": "class MyClass:\n    def __init__(self, attribute1, attribute2):\n        self.attribute1 = attribute1\n        self.attribute2 = attribute2\n    def to_dict(self):\n        return {'attribute1': self.attribute1, 'attribute2': self.attribute2}", "label": 0}
{"index": "gp018372", "code": "def uniform_int(param_0:int, param_1:int, param_2:Optional[List[int]]=None)->IntOrTensor:\n    return random.randint(param_0,param_1) if param_2 is None else torch.randint(param_0,param_1+1,param_2)", "contrast": "import random\nfrom typing import Union, List\ndef generate_int_or_tensor(size: Union[int, List[int]], low: int, high: int) -> Union[int, List[int]]:\n    if isinstance(size, int):\n        return [random.randint(low, high) for _ in range(size)]\n    else:\n        return [[random.randint(low, high) for _ in range(size[i])] for i in range(len(size))]", "label": 0}
{"index": "gp019723", "code": "def _encode_gif(param_0, param_1):\n  var_0 = WholeVideoWriter(param_1)\n  writer.write_multi(param_0)\n  return writer.finish()", "contrast": "from moviepy.video.io.ImageSequenceClip import ImageSequenceClip\nimport numpy as np\ndef encode_images_to_gif(images, fps):\n    if isinstance(images, list):\n        images = np.array(images)\n    clip = ImageSequenceClip(list(images), fps=fps)\n    gif_string = clip.write_gif('temp.gif', fps=fps)\n    with open('temp.gif', 'rb') as f:\n        gif_bytes = f.read()\n    return gif_bytes", "label": 0}
{"index": "gp081984", "code": "def xcoord(param_0):\n        var_0 = next(self.raw_data.psy.iter_base_variables)\n        return self.decoder.get_x(var_0, coords=self.data.coords)", "contrast": "def x_coordinate(x):\n    return x", "label": 0}
{"index": "gp091779", "code": "def expr_order_key(param_0):\n    if hasattr(param_0, '_order_key'):\n        return expr._order_key\n    try:\n        if isinstance(expr.kwargs, OrderedDict):\n            var_0 = expr.kwargs.values()\n        else:\n            var_0 = [expr.kwargs[var_1] for var_1 in sorted(expr.kwargs)]\n        return KeyTuple((expr.__class__.__name__, ) +\n                        tuple(map(expr_order_key, expr.args)) +\n                        tuple(map(expr_order_key, var_0)))\n    except AttributeError:\n        return str(param_0)", "contrast": "def default_order_key(expr):\n    if isinstance(expr, str):\n        return expr.lower(),  \n    elif isinstance(expr, (int, float)):\n        return expr,\n    elif isinstance(expr, tuple):\n        return tuple(default_order_key(e) for e in expr)\n    elif isinstance(expr, list):\n        return [default_order_key(e) for e in expr]\n    elif isinstance(expr, dict):\n        return {k: default_order_key(v) for k, v in expr.items()}\n    else:\n        return str(expr).lower(),", "label": 0}
{"index": "gp182648", "code": "def get_hash(param_0):\n    var_0 = hash(param_0)\n    return var_0", "contrast": "def Hash(self):\n        if not self.__hash:\n            hashdata = self.RawData()\n            ba = bytearray(binascii.unhexlify(hashdata))\n            hash = bin_dbl_sha256(ba)\n            self.__hash = UInt256(data=hash)\n        return self.__hash", "label": 1}
{"index": "gp144523", "code": "def display(param_0, param_1):\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n        param_1 = self.preprocess(param_1)\n        if self.framebuffer.redraw_required(param_1):\n            var_0, var_1, var_2, var_3 = self.framebuffer.inflate_bbox()\n            var_4 = var_2 - var_0\n            var_5 = var_3 - var_1\n            var_6 = bytearray(var_4 * var_5 >> 1)\n            self._set_position(var_1, var_2, var_3, var_0)\n            self._populate(var_6, self.framebuffer.getdata())\n            self.data(list(var_6))", "contrast": "from PIL import Image\ndef render_to_greyscale_OLED(image):\n    if image.mode == '1' or image.mode == 'L':\n        return image\n    elif image.mode == 'RGB':\n        gray_image = Image.new('L', image.size)\n        for i in range(image.size[0]):\n            for j in range(image.size[1]):\n                pixel = image.getpixel((i, j))\n                gray_value = int(0.299 * pixel[0] + 0.587 * pixel[1] + 0.114 * pixel[2])\n                gray_image.putpixel((i, j), gray_value)\n        return gray_image\n    else:\n        raise ValueError('Unsupported image mode')", "label": 0}
{"index": "gp325539", "code": "def disconnect(param_0=False):\n    if not param_0:\n        var_0 = {'type': 'disconnect'}\n        send_packet(var_0)\n    for var_1 in jobs.values():\n        job.stop()\n    jobs.clear()\n    current_app.socketio.server.disconnect(current_app.client_namespace)", "contrast": "def disconnect(self, silent=False):\n        if not silent:\n            packet = {\"type\": \"disconnect\",\n                      \"endpoint\": self.ns_name}\n            self.socket.send_packet(packet)\n        try:\n            self.socket.remove_namespace(self.ns_name)\n        finally:\n            self.kill_local_jobs()", "label": 1}
{"index": "gp264248", "code": "def filter_listeners_with_hooks(param_0, param_1):\n    var_0 = []\n    for var_1 in param_1:\n        if hasattr(var_1, 'hooks') and svc_event.event_type in listener.hooks:\n            filtered_listeners.append(var_1)\n    return var_0", "contrast": "def _filter_with_hooks(self, svc_event, listeners):\n        svc_ref = svc_event.get_service_reference()\n        hook_refs = self._registry.find_service_references(\n            SERVICE_EVENT_LISTENER_HOOK\n        )\n        if hook_refs:\n            ctx_listeners = {}\n            for listener in listeners:\n                context = listener.bundle_context\n                ctx_listeners.setdefault(context, []).append(listener)\n            shrinkable_ctx_listeners = ShrinkableMap(\n                {\n                    context: ShrinkableList(value)\n                    for context, value in ctx_listeners.items()\n                }\n            )\n            for hook_ref in hook_refs:\n                if not svc_ref == hook_ref:\n                    hook_bundle = hook_ref.get_bundle()\n                    hook_svc = self._registry.get_service(hook_bundle, hook_ref)\n                    if hook_svc is not None:\n                        try:\n                            hook_svc.event(svc_event, shrinkable_ctx_listeners)\n                        except:\n                            self._logger.exception(\n                                \"Error calling EventListenerHook\"\n                            )\n                        finally:\n                            self._registry.unget_service(hook_bundle, hook_ref)\n            ret_listeners = set()\n            for bnd_listeners in shrinkable_ctx_listeners.values():\n                ret_listeners.update(bnd_listeners)\n            return ret_listeners\n        return listeners", "label": 1}
{"index": "gp249374", "code": "def save_modified_batch(param_0, param_1, param_2):\n    server_context.data_client.save_assay_batch(\n        param_1=param_1,\n        param_2=param_2,\n    )", "contrast": "def save_batch(server_context, assay_id, batch):\n    result = save_batches(server_context, assay_id, [batch])\n    if result is not None:\n        return result[0]\n    return None", "label": 1}
{"index": "gp275115", "code": "import numpy as np\nfrom stwcs.wcsutil import HSTWCS\ndef in_science_area(param_0: np.ndarray, param_1: HSTWCS, param_2: np.ndarray, param_3: np.ndarray) -> np.ndarray:\n    var_0, var_1 = wcs.all_pix2world(param_2, param_3, 0)  \n    var_2 = np.logical_not(np.isnan(param_0[0]))  \n    var_3 = (var_0 >= wcs.wcs.crval[0]) & (var_0 <= wcs.wcs.crval[0]+wcs.wcs.cdelt[0]*img.shape[2]) &                    (var_1 >= wcs.wcs.crval[1]) & (var_1 <= wcs.wcs.crval[1]+wcs.wcs.cdelt[1]*img.shape[1])  \n    var_4 = np.zeros(x.shape, dtype=bool)  \n    for var_5 in range(inside_science.size):\n        if var_3[var_5]:\n            var_6, var_7 = wcs.all_world2pix(var_0[var_5], var_1[var_5], 0)  \n            if 0 <= int(var_7) < img.shape[1] and 0 <= int(var_6) < img.shape[2]:\n                if np.isnan(param_0[0][int(var_7),int(var_6)]) == False:\n                    var_4[var_5] = True  \n    return param_2[var_4], param_3[var_4]  ", "contrast": "def within_footprint(img, wcs, x, y):\n    if hasattr(wcs, 'naxis1'):\n        naxis1 = wcs.naxis1\n        naxis2 = wcs.naxis2\n    elif hasattr(wcs, 'pixel_shape'):\n        naxis1, naxis2 = wcs.pixel_shape\n    else:\n        naxis1 = wcs._naxis1\n        naxis2 = wcs._naxis2\n    maskx = np.bitwise_or(x < 0, x > naxis1)\n    masky = np.bitwise_or(y < 0, y > naxis2)\n    mask = ~np.bitwise_or(maskx, masky)\n    x = x[mask]\n    y = y[mask]\n    img_mask = create_image_footprint(img, wcs, border=1.0)\n    inmask = np.where(img_mask[y.astype(np.int32), x.astype(np.int32)])[0]\n    x = x[inmask]\n    y = y[inmask]\n    return x, y", "label": 1}
{"index": "gp015911", "code": "def nice(param_0):\n        var_0 = \"this property is deprecated; use Process.get_nice() method instead\"\n        warnings.warn(var_0, category=DeprecationWarning, stacklevel=2)\n        return self.get_nice()", "contrast": "import os\ndef get_set_process_nice(pid=None, priority=None):\n    if pid is None:\n        pid = os.getpid()\n    if priority is not None:\n        os.nice(priority)\n        return\n    return os.nice(0)", "label": 0}
{"index": "gp199062", "code": "import hashlib\ndef change_avatar(param_0=None):\n    if param_0 is None:\n        param_0 = hashlib.sha256(bytes(str(time.time()), encoding='utf-8')).hexdigest()\n    else:\n        param_0 = str(param_0)\n    print(\"Avatar changed to:\", param_0)", "contrast": "def change(self, inpt, hashfun=DEFAULT_HASHFUN):\n        self.img = self.__create_image(inpt, hashfun)", "label": 1}
{"index": "gp139025", "code": "def _filter_disabled_regions(param_0):\n    param_0 = list(param_0)\n    var_0 = False\n    var_1 = len(param_0)\n    var_2 = 0\n    while var_2 < var_1:\n        var_3 = param_0[var_2]\n        if var_3 == \"`\":\n            if ((var_2 + 2) < var_1 and\n                    \"\".join(param_0[var_2:var_2 + 3]) == \"```\"):\n                var_0 = not var_0\n                var_2 += 3\n                continue\n        if var_0:\n            param_0[var_2] = \" \"\n        var_2 += 1\n    return \"\".join(param_0)", "contrast": "def filter_backtick_regions(regions):\n    filtered_regions = []\n    for region in regions:\n        if '`' not in region:\n            filtered_regions.append(region)\n    return filtered_regions", "label": 0}
{"index": "gp266897", "code": "import numpy as np\ndef bootstrap_parameters(param_0, param_1=True, param_2=1000):\n    var_0 = []\n    var_1 = []\n    for var_2 in param_0:\n        var_3, var_4 = np.linalg.eig(var_2)\n        var_5 = np.argsort(-var_3)\n        T.append(var_3[var_5])\n        V.append(var_4[:, var_5])\n    var_6 = np.mean(var_0, axis=0)\n    var_7 = np.mean(var_1, axis=0)\n    var_8 = []\n    var_9 = []\n    for var_10 in range(param_2):\n        if param_1:\n            var_2 = np.diag(var_6).dot(var_7).dot(np.diag(var_6)).dot(Vmean.T)\n        else:\n            var_2 = np.zeros_like(param_0[0])\n            for var_11 in range(len(var_2)):\n                var_2[var_11, var_11] = var_6[var_11]\n            for var_11 in range(len(var_2)):\n                for var_12 in range(var_11+1, len(var_2)):\n                    var_13 = np.random.normal(0, 1)\n                    var_2[var_11, var_12] = var_13\n                    var_2[var_12, var_11] = var_13\n        var_3, var_4 = np.linalg.eig(var_2)\n        var_5 = np.argsort(-var_3)\n        Taus.append(var_3[var_5])\n        Vs.append(var_4[:, var_5])\n    return var_6, var_7, var_8, var_9", "contrast": "def s_boot(Ss, ipar=0, nb=1000):\n    Ss = np.array(Ss)\n    npts = Ss.shape[0]\n    nf, Sigma, avs = sbar(Ss)\n    Tmean, Vmean = doseigs(avs)  \n    Taus, Vs = [], []  \n    for k in range(int(float(nb))):  \n        BSs = apseudo(Ss, ipar, Sigma)\n        nf, sigma, avbs = sbar(BSs)  \n        tau, Vdirs = doseigs(avbs)  \n        Taus.append(tau)\n        Vs.append(Vdirs)\n    return Tmean, Vmean, Taus, Vs", "label": 1}
{"index": "gp134377", "code": "def _safe_getmodule(param_0):\n    from inspect import getmodule\n    try:\n        return getmodule(param_0)\n    except: \n        msg.err(\"_safe_getmodule: {}\".format(param_0), 2)\n        pass", "contrast": "def get_module(o):\n    return o.__class__.__module__", "label": 0}
{"index": "gp198046", "code": "def _print(param_0, *param_4, param_1=' ', param_2='\\n', param_3=None):\n    if param_3 is None:\n        param_3 = self.channel\n    print(*param_4, param_1=param_1, param_2=param_2, param_3=param_3)", "contrast": "def _print(self, *data, **kw):\n        sep = kw.pop('sep', ' ')\n        end = kw.pop('end', '\\n')\n        _ = kw.pop('file', None)\n        assert not kw, 'Too many keyword-only arguments'\n        data = sep.join(map(str, data))\n        self._chan.write(data + end)", "label": 1}
{"index": "gp037385", "code": "def rm_token(param_0, param_1):\n    var_0 = _redis_client(param_0)\n    if not var_0:\n        return\n    try:\n        redis_client.delete(param_1)\n        return {}\n    except Exception as err:\n        log.warning('Could not remove token %s: %s', param_1, err)", "contrast": "def remove_token(opts, tok):\n    try:\n        return {}\n    except:\n        return None", "label": 0}
{"index": "gp103832", "code": "def get_synidx(param_0, param_1):\n        var_0 = self.cellsim(param_1, return_just_cell=True)\n        var_1 = {}\n        for var_2, var_3 in enumerate(self.X):\n            var_1[var_3] = self.fetchSynIdxCell(var_0=var_0,\n                                             nidx=self.k_yXL[:, var_2],\n                                             synParams=self.synParams.copy())\n        return var_1", "contrast": "import numpy as np\ndef draw_synapse_locations(cellindex):\n    np.random.seed(POPULATIONSEED + cellindex)\n    synidx = dict()\n    return synidx", "label": 0}
{"index": "gp285488", "code": "class Expression:\n    def __init__(param_0, param_1):\n        self.value = param_1\n    def __repr__(param_0):\n        return f\"{self.value}\"\n    def __eq__(param_0, param_1):\n        return type(param_0) == type(param_1) and self.value == other.value\n    def __ne__(param_0, param_1):\n        return not self.__eq__(param_1)\n    def __hash__(param_0):\n        return hash((type(param_0), self.value))\n    @classmethod\n    def create_spl_expression(param_0, param_1):\n        if isinstance(param_1, param_0):\n            return param_0(value.value)\n        return param_0(param_1)", "contrast": "def expression(value):\n        if isinstance(value, Expression):\n            return Expression(value._type, value._value)\n        if hasattr(value, 'spl_json'):\n            sj = value.spl_json()\n            return Expression(sj['type'], sj['value'])\n        return Expression('splexpr', value)", "label": 1}
{"index": "gp268773", "code": "def set_client_certificate(param_0, param_1):\n    self.client_certificate = param_1", "contrast": "def client_certificate(self, client_certificate):\n        if client_certificate is None:\n            raise ValueError(\"Invalid value for `client_certificate`, must not be `None`\")\n        if client_certificate is not None and len(client_certificate) > 3000:\n            raise ValueError(\"Invalid value for `client_certificate`, length must be less than or equal to `3000`\")\n        self._client_certificate = client_certificate", "label": 1}
{"index": "gp252112", "code": "def get_modified_lines(param_0, param_1, param_2):\n    if param_1 is None:\n        return None\n    var_0 = []\n    with open(param_0, 'r') as var_1:\n        var_2 = f.readlines()\n        for var_3, var_4 in enumerate(var_2):\n            if param_2 == extra_data.get((param_0, var_3 + 1)):\n                modified_lines.append(var_4)\n    return var_0 if var_0 else None", "contrast": "def modified_lines(filename, extra_data, commit=None):\n    if extra_data is None:\n        return []\n    if extra_data != 'M':\n        return None\n    command = ['hg', 'diff', '-U', '0']\n    if commit:\n        command.append('--change=%s' % commit)\n    command.append(filename)\n    diff_lines = subprocess.check_output(command).split(\n        os.linesep.encode('utf-8'))\n    diff_line_numbers = utils.filter_lines(\n        diff_lines,\n        br'@@ -\\d+,\\d+ \\+(?P<start_line>\\d+),(?P<lines>\\d+) @@',\n        groups=('start_line', 'lines'))\n    modified_line_numbers = []\n    for start_line, lines in diff_line_numbers:\n        start_line = int(start_line)\n        lines = int(lines)\n        modified_line_numbers.extend(range(start_line, start_line + lines))\n    return modified_line_numbers", "label": 1}
{"index": "gp103511", "code": "def get(param_0, param_1=None, param_2=None, param_3=None):\n        if param_1 is None and param_3 is None:\n            return self._get_all()\n        elif param_3 is not None and param_3 == 1:\n            return self.get_one(param_1)\n        else:\n            return self._get_with_criteria(param_1, param_2=param_2, param_3=param_3)", "contrast": "def get(items, criteria=None):\n    if criteria:\n        return [item for item in items if criteria(item)]\n    else:\n        return items", "label": 0}
{"index": "gp188083", "code": "def formatted_filenames(param_0):\n    var_0 = {}\n    for var_1 in param_0:\n        if filename.endswith('.txt'):\n            var_0[var_1] = filename.upper()\n        elif filename.endswith('.csv'):\n            var_0[var_1] = filename.lower()\n        elif filename.endswith('.docx'):\n            var_0[var_1] = filename.title()\n        else:\n            var_0[var_1] = var_1\n    return var_0", "contrast": "def get_formatted_files(self, net):\n        idx = -1\n        if (\n                self.event_name is not None and\n                net.history\n        ):\n            for i, v in enumerate(net.history[:, self.event_name]):\n                if v:\n                    idx = i\n        return {\n            \"f_params\": self._format_target(net, self.f_params, idx),\n            \"f_optimizer\": self._format_target(net, self.f_optimizer, idx),\n            \"f_history\": self.f_history_,\n            \"f_pickle\": self._format_target(net, self.f_pickle, idx)\n        }", "label": 1}
{"index": "gp162868", "code": "def parameters(param_0):\n        return self.block_tl, self.block_br, self.rows, self.cols, self.cells", "contrast": "def get_selection_params(self):\n    return (self.block_tl, self.block_br, self.rows, self.cols, self.cells)", "label": 0}
{"index": "gp003148", "code": "def get_property_descriptions(param_0):\n        return {var_0: v.as_property_description()\n                for var_0, var_1 in self.properties.items()}", "contrast": "def get_properties(thing):\n    properties = {}\n    for prop in dir(thing):\n        if not prop.startswith(\"__\"):\n            properties[prop] = getattr(thing, prop).__doc__\n    return properties", "label": 0}
{"index": "gp109241", "code": "def get_next_appointment(param_0, param_1=None):\r\n        var_0 = {}\r\n        var_1 = {}\r\n        var_2 = {}\r\n        \"\"\"List of ids of appointment groups to search.\"\"\"\r\n        if param_1 is not None:\r\n            var_2[\"appointment_group_ids\"] = param_1\r\n        self.logger.debug(\"GET /api/v1/appointment_groups/next_appointment with query params: {params} and form data: {data}\".format(var_2=var_2, var_1=var_1, **var_0))\r\n        return self.generic_request(\"GET\", \"/api/v1/appointment_groups/next_appointment\".format(**var_0), var_1=var_1, var_2=var_2, all_pages=True)", "contrast": "def get_next_appointment():\n    appointments = ['2021-10-01', '2021-10-05', '2021-10-06', '2021-10-10']\n    now = datetime.now()\n    next_appointment = []\n    for appointment in appointments:\n        appointment_date = datetime.strptime(appointment, \"%Y-%m-%d\")\n        if appointment_date > now:\n            next_appointment.append(appointment)\n            break\n    return next_appointment", "label": 0}
{"index": "gp133936", "code": "def _check_forward_mode_input_dict(param_0, param_1: dict) -> int:\n        var_0: int = 1\n        for var_1 in param_1:\n            var_2 = param_1[var_1]\n            if isinstance(var_2, scalar_instance_types):\n                var_3 = 1\n            elif isinstance(var_2, np.ndarray):\n                var_3 = self._calc_T_var(var_2)\n            else:\n                raise ValueError(f'val={var_2} in var_tbl; {type(var_2)} not a recognized value type.')\n            if var_3 > 1 and var_0 == 1:\n                var_0 = var_3\n            elif var_3 not in (1,var_0):\n                raise ValueError(f'Bound variable {var_1} has inconsistent shape')\n        return var_0", "contrast": "def check_input_dict_shape(input_dict):\n    T = None\n    for key in input_dict.keys():\n        if isinstance(input_dict[key], (int, float)):\n            if T is None:\n                T = 1\n            else:\n                T *= 1\n        elif isinstance(input_dict[key], (list, tuple)):\n            if T is None:\n                T = len(input_dict[key])\n            else:\n                T *= len(input_dict[key])\n        else:\n            raise ValueError(f\"Invalid shape for {key}\")\n    return T", "label": 0}
{"index": "gp066924", "code": "def create_budget(param_0, param_1, param_2, param_3):\n        var_0 = {\n            'model': param_2,\n            'limit': param_3,\n        }\n        return make_request(\n            '{}wallet/{}/budget'.format(self.url, param_1),\n            method='POST',\n            body=var_0,\n            timeout=self.timeout,\n            client=self._client)", "contrast": "def create_budget(wallet_name, model_uuid, limit):\n    try:\n        response = make_request('POST', 'plans/', {'walletName': wallet_name, 'modelUuid': model_uuid, 'limit': limit})\n        return response['success']\n    except ServerError as e:\n        raise e", "label": 0}
{"index": "gp020619", "code": "def cv_squared(param_0):\n  var_0 = 1e-10\n  var_1 = tf.to_float(tf.size(param_0)) + var_0\n  var_2 = tf.reduce_sum(param_0) / var_1\n  var_3 = tf.reduce_sum(tf.squared_difference(param_0, var_2)) / var_1\n  return var_3 / (tf.square(var_2) + var_0)", "contrast": "import tensorflow as tf\ndef squared_coefficient_variation(x):\n    if tf.size(x) == 0:\n        return 0\n    else:\n        mean = tf.reduce_mean(x)\n        mean_diff = x - mean\n        mean_diff_squared = tf.square(mean_diff)\n        variance = tf.reduce_mean(mean_diff_squared)\n        coefficient_variation = tf.divide(tf.sqrt(variance + 1e-8), mean + 1e-8)\n        squared_coefficient_variation = tf.square(coefficient_variation)\n        return squared_coefficient_variation", "label": 0}
{"index": "gp151125", "code": "def flatten_phases_and_groups(param_0):\n  if isinstance(param_0, PhaseGroup):\n    param_0 = [param_0]\n  var_0 = []\n  for var_1 in param_0:\n    if isinstance(var_1, PhaseGroup):\n      ret.append(phase.flatten())\n    elif isinstance(var_1, collections.Iterable):\n      ret.extend(flatten_phases_and_groups(var_1))\n    else:\n      ret.append(phase_descriptor.PhaseDescriptor.wrap_or_copy(var_1))\n  return var_0", "contrast": "def flatten_list(nested_list):\n    flat_list = []\n    for element in nested_list:\n        if isinstance(element, list):\n            flat_list += flatten_list(element)\n        else:\n            flat_list.append(element)\n    return flat_list", "label": 0}
{"index": "gp320928", "code": "def remove_rows(param_0):\n    for var_0 in sorted(param_0, reverse=True):\n        del data[var_0]", "contrast": "def removeIds(self, idsToRemove):\n    rowsToRemove = [k for k, rowID in enumerate(self._categoryRecencyList)                    if rowID in idsToRemove]\n    self._removeRows(rowsToRemove)", "label": 1}
{"index": "gp215836", "code": "import inspect\nclass ChainedMinimizer:\n    def __init__(param_0, *param_1):\n        self.minimizers = param_1\n    def execute(param_0, param_1, **param_2):\n        var_0 = param_1\n        for var_1 in self.minimizers:\n            var_0 = var_1(var_0, **param_2)\n        return var_0\n    def signature(param_0):\n        var_0 = [inspect.Parameter('x', inspect.Parameter.POSITIONAL_OR_KEYWORD)]\n        for var_1 in self.minimizers:\n            parameters.append(inspect.Parameter(minimizer.__name__, inspect.Parameter.POSITIONAL_OR_KEYWORD))\n        return inspect.Signature(var_0)", "contrast": "def _make_signature(self):\n        name = lambda x: x.__class__.__name__\n        count = Counter(\n            [name(minimizer) for minimizer in self.minimizers]\n        ) \n        parameters = []\n        for minimizer in reversed(self.minimizers):\n            if count[name(minimizer)] == 1:\n                param_name = name(minimizer)\n            else:\n                param_name = '{}_{}'.format(name(minimizer), count[name(minimizer)])\n            count[name(minimizer)] -= 1\n            parameters.append(\n                inspect_sig.Parameter(\n                    param_name,\n                    kind=inspect_sig.Parameter.KEYWORD_ONLY,\n                    default={}\n                )\n            )\n        return inspect_sig.Signature(parameters=reversed(parameters))", "label": 1}
{"index": "gp275764", "code": "def generate_model_maps(param_0=None):\n    if param_0 is not None:\n        pass\n    else:\n        pass", "contrast": "def generate_model(self, model_name=None):\n        for i, c in enumerate(self._components):\n            c.generate_model(model_name=model_name)", "label": 1}
{"index": "gp083116", "code": "def stamp_title(param_0: kb,\n                param_1: Sphinx,\n                param_2: param_2):\n    var_0 = sphinx_app.env.resources\n    var_1 = sphinx_app.confdir\n    var_2 = PurePath(doctree.attributes['source'])\n    var_3 = str(source.relative_to(var_1)).split('.rst')[0]\n    var_4 = resources.get(var_3)\n    if var_4:\n        var_5 = get_rst_title(param_2)\n        resource.title = var_5", "contrast": "def walk_tree(tree):\n    if 'title' in tree:\n        title = tree['title']\n        if isinstance(title, dict):\n            tree['title'] = title.get('text', '')\n        elif isinstance(title, list):\n            for i in title:\n                walk_tree(i)\n    for k, v in tree.items():\n        if isinstance(v, dict):\n            walk_tree(v)\n        elif isinstance(v, list):\n            for i in v:\n                if isinstance(i, dict):\n                    walk_tree(i)\n    return tree", "label": 0}
{"index": "gp286934", "code": "import subprocess\ndef pull_docker_images():\n    var_0 = [\"image_1\", \"image_2\", \"image_3\"]\n    for var_1 in var_0:\n        try:\n            subprocess.check_call([\"docker\", \"pull\", var_1])\n        except subprocess.CalledProcessError as e:\n            print(\"Pulling image failed with error:\", e)", "contrast": "def post_register_hook(self, verbosity=1):\n        if not getattr(settings, 'FLOW_DOCKER_DONT_PULL', False):\n            call_command('list_docker_images', pull=True, verbosity=verbosity)", "label": 1}
{"index": "gp118623", "code": "def get_conv(param_0, param_1, param_2=False, param_3=1.0):\n    var_0 = {}\n    var_0[\"bits\"] = param_0\n    var_0[\"bin_point\"] = param_1\n    var_0[\"signed\"] = param_2\n    var_0[\"scaling\"] = param_3\n    var_0[\"dec_step\"] = 1.0 / (2 ** param_1)\n    var_0[\"dec_mask\"] = sum([2 ** var_1 for var_1 in range(param_1)])\n    if param_0 == 8:\n        var_0[\"fmt\"] = \"B\"\n    elif param_0 == 16:\n        var_0[\"fmt\"] = \"H\"\n    elif param_0 == 32:\n        var_0[\"fmt\"] = \"I\"\n    else:\n        raise ConversionError(\"numer of bits not supported: \" + str(param_0))\n    if param_2:\n        _get_signed_params(var_0)\n    else:\n        _get_unsigned_params(var_0)\n    return var_0", "contrast": "def create_conversion_structure(bits, bin_point, signed, scaling=None):\n    conversion_structure = {\n        'bits': bits,\n        'bin_point': bin_point,\n        'signed': signed\n    }\n    if scaling:\n        conversion_structure['scaling'] = scaling\n    return conversion_structure", "label": 0}
{"index": "gp287461", "code": "def boolean_grammar(param_0: str) -> str:\n    return f\"{param_0}: 'Y' | 'N'\"", "contrast": "def boolean(name=None):\n    if name is None:\n        name = 'Boolean Field'\n    field = pp.Regex('[YN]')\n    field.setParseAction(lambda b: _to_boolean(b[0]))\n    field.setName(name)\n    return field", "label": 1}
{"index": "gp293202", "code": "import hashlib\ndef generate_hash_functions(param_0):\n    def murmur3_64(param_0, param_1=0):\n        var_0 = 0xc6a4a7935bd1e995\n        var_1 = 47\n        var_2 = param_1 ^ (len(param_0) * var_0)\n        for var_3 in range(0, len(param_0) // 8):\n            var_4 = int.from_bytes(param_0[var_3*8:(var_3+1)*8], byteorder='little')\n            var_4 *= var_0\n            var_4 ^= var_4 >> var_1\n            var_4 *= var_0\n            var_2 ^= var_4\n            var_2 *= var_0\n        if len(param_0) % 8 >= 4:\n            var_4 = int.from_bytes(param_0[-4:], byteorder='little')\n            var_4 *= var_0\n            var_4 ^= var_4 >> var_1\n            var_4 *= var_0\n            var_2 ^= var_4\n            var_2 *= var_0\n            param_0 = param_0[:-(len(param_0) % 8)]\n        if len(param_0) % 8 > 0:\n            var_4 = int.from_bytes(param_0[-8:], byteorder='little')\n            var_4 *= var_0\n            var_4 ^= var_4 >> var_1\n            var_4 *= var_0\n            var_2 ^= var_4\n            var_2 *= var_0\n        var_2 ^= var_2 >> var_1\n        var_2 *= var_0\n        var_2 ^= var_2 >> var_1\n        return var_2\n    var_0 = []\n    for var_1 in range(param_0):\n        def new_hash(param_0, param_1=var_1+1):\n            return murmur3_64(param_0, param_1=param_1)\n        hashes.append(new_hash)\n    return var_0", "contrast": "def generate_hashfunctions(nbr_bits, nbr_slices):\n    def _make_hashfuncs(key):\n        if isinstance(key, text_type):\n            key = key.encode('utf-8')\n        else:\n            key = str(key)\n        rval = []\n        current_hash = 0\n        for i in range(nbr_slices):\n            seed = current_hash\n            current_hash = hash64(key, seed)\n            rval.append(current_hash % nbr_bits)\n        return rval\n    return _make_hashfuncs", "label": 1}
{"index": "gp019745", "code": "def get_policy(param_0, param_1, param_2):\n  if not isinstance(param_2, gym.spaces.Discrete):\n    raise ValueError(\"Expecting discrete action space.\")\n  var_0 = common_layers.shape_list(param_0)\n  (var_1, var_2) = var_0[2:4]\n  if hparams.policy_problem_name == \"dummy_policy_problem_ttt\":\n    tf.logging.info(\"Using DummyPolicyProblemTTT for the policy.\")\n    var_3 = tic_tac_toe_env.DummyPolicyProblemTTT()\n  else:\n    tf.logging.info(\"Using DummyPolicyProblem for the policy.\")\n    var_3 = DummyPolicyProblem(param_2, var_1, var_2)\n  trainer_lib.add_problem_hparams(param_1, var_3)\n  hparams.force_full_predict = True\n  var_4 = registry.model(hparams.policy_network)(\n      param_1, tf.estimator.ModeKeys.TRAIN\n  )\n  try:\n    var_5 = hparams.video_num_target_frames\n  except AttributeError:\n    var_5 = 1\n  var_6 = {\n      \"inputs\": param_0,\n      \"input_action\": tf.zeros(var_0[:2] + [1], dtype=tf.int32),\n      \"input_reward\": tf.zeros(var_0[:2] + [1], dtype=tf.int32),\n      \"targets\": tf.zeros(var_0[:1] + [var_5] + var_0[2:]),\n      \"target_action\": tf.zeros(\n          var_0[:1] + [var_5, 1], dtype=tf.int32),\n      \"target_reward\": tf.zeros(\n          var_0[:1] + [var_5, 1], dtype=tf.int32),\n      \"target_policy\": tf.zeros(\n          var_0[:1] + [var_5] + [action_space.n]),\n      \"target_value\": tf.zeros(\n          var_0[:1] + [var_5])\n  }\n  with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    t2t_model.create_dummy_vars()\n    (var_7, var_8) = var_4(var_6)\n  return (var_7[\"target_policy\"][:, 0, :], var_7[\"target_value\"][:, 0])", "contrast": "def get_policy_network(observations, hparams, action_space):\n    logits = \n    value = \n    return logits, value", "label": 0}
{"index": "gp311379", "code": "import datetime\ndef copy_timestamp(param_0):\n    try:\n        var_0 = datetime.datetime.strptime(param_0, '%Y-%m-%d %H:%M:%S.%f%z')\n    except ValueError:\n        var_0 = datetime.datetime.strptime(param_0, '%Y-%m-%d %H:%M:%S.%f')\n        var_0 = dt.replace(tzinfo=datetime.timezone.utc)\n    return dt.timestamp()", "contrast": "def CopyFromDateTimeString(self, time_string):\n    date_time_values = self._CopyDateTimeFromString(time_string)\n    year = date_time_values.get('year', 0)\n    month = date_time_values.get('month', 0)\n    day_of_month = date_time_values.get('day_of_month', 0)\n    hours = date_time_values.get('hours', 0)\n    minutes = date_time_values.get('minutes', 0)\n    seconds = date_time_values.get('seconds', 0)\n    microseconds = date_time_values.get('microseconds', 0)\n    timestamp = self._GetNumberOfSecondsFromElements(\n        year, month, day_of_month, hours, minutes, seconds)\n    timestamp *= definitions.MILLISECONDS_PER_SECOND\n    if microseconds:\n      milliseconds, _ = divmod(\n          microseconds, definitions.MILLISECONDS_PER_SECOND)\n      timestamp += milliseconds\n    self._timestamp = timestamp\n    self.is_local_time = False", "label": 1}
{"index": "gp208562", "code": "def present_proof(param_0):\n    return proof, revealed_attributes", "contrast": "async def presentProof(self, proofRequest: ProofRequest) -> FullProof:\n        claims, requestedProof = await self._findClaims(proofRequest)\n        proof = await self._prepareProof(claims, proofRequest.nonce, requestedProof)\n        return proof", "label": 1}
{"index": "gp192295", "code": "def get_form_kwargs(**param_0):\n    return param_0", "contrast": "def get_form_kwargs(self):\n        kwargs = {\"user\": self.request.user, \"initial\": self.get_initial()}\n        if self.request.method in [\"POST\", \"PUT\"]:\n            kwargs.update({\n                \"data\": self.request.POST,\n                \"files\": self.request.FILES,\n            })\n        return kwargs", "label": 1}
{"index": "gp070865", "code": "def syscal_save_to_config_txt(param_0, param_1, param_2=1):\n    print('Number of measurements: ', configs.shape[0])\n    var_0 = configs.max().astype(int)\n    with open(param_0, 'w') as var_1:\n        _syscal_write_electrode_coords(var_1, param_2, var_0)\n        _syscal_write_quadpoles(var_1, configs.astype(int))", "contrast": "import numpy as np\ndef write_syscal_config(filename: str, configs: np.ndarray) -> None:\n    with open(filename, 'w') as f:\n        for cfg in configs:\n            f.write(f'{cfg[0]:.3f}\\t{cfg[1]:.3f}\\t{cfg[2]:.3f}\\t{cfg[3]:.3f}\\n')", "label": 0}
{"index": "gp034040", "code": "def describe(param_0,\n             param_1=None, param_2=None, param_3=None, param_4=None):\n    var_0 = _get_conn(param_1=param_1, param_2=param_2, param_3=param_3, param_4=param_4)\n    try:\n        var_1 = conn.describe_elasticsearch_domain_config(param_0=param_0)\n        if var_1 and 'DomainConfig' in var_1:\n            var_1 = var_1['DomainConfig']\n            var_2 = ('ElasticsearchClusterConfig', 'EBSOptions', 'AccessPolicies',\n                    'SnapshotOptions', 'AdvancedOptions')\n            return {'domain': dict([(var_3, domain.get(var_3, {}).get('Options')) for var_3 in var_2 if var_3 in var_1])}\n        else:\n            return {'domain': None}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "contrast": "import boto3\ndef describe_domain(domain_name):\n    es = boto3.client('es')\n    response = es.describe_elasticsearch_domain(DomainName=domain_name)\n    return response", "label": 0}
{"index": "gp245008", "code": "import json\nfrom django.core.serializers.json import DjangoJSONEncoder\ndef encode_to_json(param_0):\n    return json.dumps(param_0, cls=DjangoJSONEncoder)", "contrast": "def serialize_instance(instance):\n    ret = dict([(k, v)\n                for k, v in instance.__dict__.items()\n                if not k.startswith('_')])\n    return json.loads(json.dumps(ret, cls=DjangoJSONEncoder))", "label": 1}
{"index": "gp292864", "code": "def validate_data(param_0, param_1):\n    if not isinstance(param_1, dict):\n        raise ValidationError(f\"{param_0} should be a dictionary.\")", "contrast": "def validate(self, name, data):\n        super().validate(name, data)\n        if self.value is not None:\n            try:\n                self.value = self.query.get(self.lookup_field == self.value)\n            except (AttributeError, ValueError, peewee.DoesNotExist):\n                raise ValidationError('related', field=self.lookup_field.name, values=self.value)", "label": 1}
{"index": "gp207251", "code": "def _reparentUnions(param_0):\n    for var_0 in self.namespaces:\n        for var_1 in ns.unions:\n            ns._removeNode(var_1)\n        for var_2 in ns.classes:\n            pyclass._reparentUnions()\n            for var_1 in pyclass.unions:\n                ns._removeNode(var_1)\n                pyclass._addNodeToSelf(var_1)\n        for var_3 in ns.functions:\n            pyfunc._reparentUnions()\n            for var_1 in pyfunc.unions:\n                ns._removeNode(var_1)\n                pyfunc._addNodeToSelf(var_1)\n            for var_4 in pyfunc.nodes:\n                node._reparent(var_0)\n        for var_5 in ns.structs:\n            struct._reparentUnions()\n            for var_1 in struct.unions:\n                ns._removeNode(var_1)\n                struct._addNodeToSelf(var_1)", "contrast": "def reparentUnions(self):\n        removals = []\n        for u in self.unions:\n            parts = u.name.split(\"::\")\n            if len(parts) >= 2:\n                parent_name = \"::\".join(p for p in parts[:-1])\n                reparented  = False\n                for node in itertools.chain(self.class_like, self.namespaces):\n                    if node.name == parent_name:\n                        node.children.append(u)\n                        u.parent = node\n                        reparented = True\n                        break\n                if reparented:\n                    removals.append(u)\n                else:\n                    utils.verbose_log(\n                        \"The union {0} has '::' in its name, but no parent was found!\".format(u.name),\n                        utils.AnsiColors.BOLD_RED\n                    )\n        for rm in removals:\n            self.unions.remove(rm)", "label": 1}
{"index": "gp068141", "code": "def console(param_0, param_1):\n    var_0 = argparse.ArgumentParser(description=console.__doc__)\n    parser.add_argument('--host', default='127.0.0.1', help='IP or FQDN of AVR')\n    parser.add_argument('--port', default='14999', help='Port of AVR')\n    parser.add_argument('--verbose', '-v', action='count')\n    var_1 = parser.parse_args()\n    if args.verbose:\n        var_2 = logging.DEBUG\n    else:\n        var_2 = logging.INFO\n    logging.basicConfig(var_2=var_2)\n    def log_callback(param_0):\n        log.info('Callback invoked: %s' % param_0)\n    var_3 = args.host\n    var_4 = int(args.port)\n    log.info('Connecting to Anthem AVR at %s:%i' % (var_3, var_4))\n    var_5 = yield from anthemav.Connection.create(\n        var_3=var_3, var_4=var_4, param_0=param_0, update_callback=log_callback)\n    log.info('Power state is '+str(conn.protocol.power))\n    conn.protocol.power = True\n    log.info('Power state is '+str(conn.protocol.power))\n    yield from asyncio.sleep(10, param_0=param_0)\n    log.info('Panel brightness (raw) is '+str(conn.protocol.panel_brightness))\n    log.info('Panel brightness (text) is '+str(conn.protocol.panel_brightness_text))", "contrast": "import socket\ndef connect_and_show_events(host, port, verbose=False):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.connect((host, port))\n            if verbose:\n                print(\"Connection established with the receiver.\")\n            while True:\n                data = s.recv(1024)\n                if verbose:\n                    print(\"Received: \", str(data.decode()))\n                if not data:\n                    break\n    except socket.error as err:\n        print(\"Error: \", err)", "label": 0}
{"index": "gp065584", "code": "def rotate(param_0, param_1, param_2='z', param_3=None):\r\n        return Space(param_0).rotate(param_1, param_2, param_3)[0]", "contrast": "def rotate_place(place, angle, direction, axis=None):\n    import numpy as np\n    from pyny import Place\n    if axis is None:\n        axis = place.centroid()[:2]\n    if len(axis) == 2:\n        axis.append(0)\n    if direction == 'x':\n        axis_index = 0\n    elif direction == 'y':\n        axis_index = 1\n    else:\n        axis_index = 2\n    rotation_matrix = np.zeros((3,3))\n    rotation_matrix[(axis_index+1)%3, axis_index] = 1\n    rotation_matrix[axis_index, (axis_index+1)%3] = -1\n    rotation_matrix[(axis_index+2)%3, (axis_index+2)%3] = 1\n    angle_sin = np.sin(angle)\n    angle_cos = np.cos(angle)\n    rotation_matrix[(axis_index+1)%3, (axis_index+1)%3] = angle_cos\n    rotation_matrix[(axis_index+1)%3, (axis_index+2)%3] = -angle_sin\n    rotation_matrix[(axis_index+2)%3, (axis_index+1)%3] = angle_sin\n    rotation_matrix[(axis_index+2)%3, (axis_index+2)%3] = angle_cos\n    points = np.array(place.points)\n    points -= np.array(axis)\n    rotated_points = np.dot(rotation_matrix, points.T).T\n    rotated_points += np.array(axis)\n    return Place(rotated_points.tolist())", "label": 0}
{"index": "gp160826", "code": "def finish(param_0, param_1='\\n', param_2=False):\n        if not param_2:\n            self.end_time = datetime.now()\n            self.update(self.max_value, force=True)\n        StdRedirectMixin.finish(param_0, param_1=param_1)\n        ResizableMixin.finish(param_0)\n        ProgressBarBase.finish(param_0)", "contrast": "def finish_progressbar(end='\\n', dirty=False):\n    if not dirty:\n        sys.stdout.write('\\n')\n    sys.stdout.write(end)\n    sys.stdout.flush()\n    sys.stdout.buffer.flush()", "label": 0}
{"index": "gp270892", "code": "import os\ndef get_file_size(param_0, param_1=False):\n    if param_1:\n        return os.path.getsize(param_0 + '.zip')\n    else:\n        return os.path.getsize(param_0)", "contrast": "def _file_size(file_path, uncompressed=False):\n    _, ext = os.path.splitext(file_path)\n    if uncompressed:\n        if ext in {\".gz\", \".gzip\"}:\n            with gzip.GzipFile(file_path, mode=\"rb\") as fp:\n                try:\n                    fp.seek(0, os.SEEK_END)\n                    return fp.tell()\n                except ValueError:\n                    fp.seek(0)\n                    while len(fp.read(8192)) != 0:\n                        pass\n                    return fp.tell()\n        elif ext in {\".bz\", \".bz2\", \".bzip\", \".bzip2\"}:\n            with bz2.BZ2File(file_path, mode=\"rb\") as fp:\n                fp.seek(0, os.SEEK_END)\n                return fp.tell()\n    return os.path.getsize(file_path)", "label": 1}
{"index": "gp289675", "code": "import random\ndef random_step_self_loop():\n    return random.choice([0, 1])", "contrast": "def pagerank_lazy_push(s, r, w_i, a_i, push_node, rho, lazy):\n    A = rho*r[push_node]\n    B = (1-rho)*(1 - lazy)*r[push_node]\n    C = (1-rho)*lazy*(r[push_node])\n    s[push_node] += A\n    r[push_node] = C\n    r[a_i] += B * w_i", "label": 1}
{"index": "gp316967", "code": "class Population:\n    def __init__(param_0):\n        self.individuals = []\n    def add_individual(param_0, param_1):\n        self.individuals.append(param_1)\n    def get_individuals(param_0):\n        return self.individuals", "contrast": "def population(self):\n        try:\n            return self._p\n        except AttributeError:\n            self._p = self._population_class(base=self,\n                                             tournament_size=self._tournament_size,\n                                             classifier=self.classifier,\n                                             labels=self._labels,\n                                             es_extra_test=self.es_extra_test,\n                                             popsize=self._popsize,\n                                             random_generations=self._random_generations,\n                                             negative_selection=self._negative_selection)\n            return self._p", "label": 1}
{"index": "gp164474", "code": "def to_sky(param_0, param_1, param_2=_DEFAULT_WCS_ORIGIN, param_3=_DEFAULT_WCS_MODE):\n        return SkyCoord.from_pixel(\n            xp=self.x, yp=self.y, param_1=param_1,\n            param_2=param_2, param_3=param_3,\n        )", "contrast": "from astropy.coordinates import SkyCoord\ndef pixcoord_to_skycoord(pixcoord, wcs):\n    return SkyCoord.from_pixel(pixcoord.x, pixcoord.y, wcs)", "label": 0}
{"index": "gp163296", "code": "def hash_data(param_0, param_1):\n        var_0 = \"UserIdHasher is deprecated; use satosa.util.hash_data instead.\"\n        _warnings.warn(var_0, DeprecationWarning)\n        return util.hash_data(param_0, param_1)", "contrast": "import hashlib\ndef hash_with_salt(salt: str, value: str) -> str:\n    salted_value = salt + value\n    hash_obj = hashlib.sha512()\n    hash_obj.update(salted_value.encode())\n    return hash_obj.hexdigest()", "label": 0}
{"index": "gp035383", "code": "def network_interface_get_effective_route_table(param_0, param_1, **param_2):\n    var_0 = __utils__['azurearm.get_client']('network', **param_2)\n    try:\n        var_1 = netconn.network_interfaces.get_effective_route_table(\n            network_interface_name=param_0,\n            resource_group_name=param_1\n        )\n        nic.wait()\n        var_2 = nic.result()\n        var_2 = tables.as_dict()\n        var_3 = var_2['value']\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **param_2)\n        var_3 = {'error': str(exc)}\n    return var_3", "contrast": "def get_route_tables_for_nic(name, resource_group):\n    result = {}\n    return result", "label": 0}
{"index": "gp174048", "code": "from typing import List\nfrom project import Task\ndef profile_task_status(param_0: Task, param_1: str) -> List[str]:\n    return [var_0 for var_0 in task.status if var_0 == param_1]", "contrast": "def Sample(self, task, status):\n    sample_time = time.time()\n    sample = '{0:f}\\t{1:s}\\t{2:s}\\n'.format(\n        sample_time, task.identifier, status)\n    self._WritesString(sample)", "label": 1}
{"index": "gp056847", "code": "def cloud_cover_to_irradiance_liujordan(param_0, param_1, **param_2):\n        var_0 = self.location.get_solarposition(cloud_cover.index)\n        var_1 = get_extra_radiation(cloud_cover.index)\n        var_2 = self.location.get_airmass(cloud_cover.index)\n        var_3 = self.cloud_cover_to_transmittance_linear(param_1,\n                                                                 **param_2)\n        var_4 = liujordan(var_0['apparent_zenith'],\n                           var_3, var_2['airmass_absolute'],\n                           var_1=var_1)\n        var_4 = irrads.fillna(0)\n        return var_4", "contrast": "import pvlib\ndef estimate_irradiance(cloud_cover):\n    transmittance = pvlib.ForecastModel.cloud_cover_to_transmittance_linear(cloud_cover)\n    solar_position = pvlib.solarposition.get_solarposition(time,latitude,longitude)\n    dni_extra = pvlib.irradiance.get_extra_radiation(time)\n    airmass = pvlib.atmosphere.get_relative_airmass(solar_position['apparent_zenith'])\n    pressure = pvlib.atmosphere.alt2pres(altitude)\n    am_abs = pvlib.atmosphere.get_absolute_airmass(airmass,pressure)\n    tl = pvlib.clearsky.lookup_linke_turbidity(time,latitude,longitude)\n    cs = pvlib.clearsky.ineichen(solar_position['apparent_zenith'],am_abs,tl,dni_extra=dni_extra,altitude=altitude)\n    effective_angles = pvlib.irradiance.get_effective_irradiance(solar_position['zenith'],solar_position['azimuth'],aoi, aoi_limit, cs['dni'], cs['ghi'], cs['dhi'], model='haydavies')\n    poa = pvlib.irradiance.poa_irradiance(effective_angles['aoi'], effective_angles['apparent_zenith'], effective_angles['azimuth'], \n                                          dni, ghi, dhi, dni_extra=dni_extra, model='haydavies', airmass_absolute=am_abs, \n                                          pressure=pressure, dni_extra_tracker=dni_extra_tracker, surface_tilt=surface_tilt, \n                                          surface_azimuth=surface_azimuth, albedo=albedo, \n                                          model_perez='allsitescomposite1990')\n    return poa", "label": 0}
{"index": "gp185818", "code": "import numpy as np\ndef derivative(param_0, param_1=1e-7):\n    def derivative_func(param_0):\n        return (param_0(param_0 + param_1) - param_0(param_0)) / param_1\n    return np.vectorize(derivative_func)", "contrast": "def derivative_factory(name):\n    if name == 'sin':\n        def derivative(self, point):\n            return MultiplyOperator(cos(self.domain)(point))\n    elif name == 'cos':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(-sin(self.domain)(point))\n    elif name == 'tan':\n        def derivative(self, point):\n            return MultiplyOperator(1 + self(point) ** 2)\n    elif name == 'sqrt':\n        def derivative(self, point):\n            return MultiplyOperator(0.5 / self(point))\n    elif name == 'square':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(2.0 * point)\n    elif name == 'log':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(1.0 / point)\n    elif name == 'exp':\n        def derivative(self, point):\n            return MultiplyOperator(self(point))\n    elif name == 'reciprocal':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(-self(point) ** 2)\n    elif name == 'sinh':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(cosh(self.domain)(point))\n    elif name == 'cosh':\n        def derivative(self, point):\n            return MultiplyOperator(sinh(self.domain)(point))\n    else:\n        derivative = Operator.derivative\n    return derivative", "label": 1}
{"index": "gp234481", "code": "def add_event_handler(param_0):\n    actions_list.append(param_0) ", "contrast": "def add_event_handler(self, action):\n        if action.uuid in self.actions:\n            logger.info(\"Already existing event handler: %s\", action)\n            return\n        self.actions[action.uuid] = action\n        self.nb_event_handlers += 1", "label": 1}
{"index": "gp019316", "code": "def get_longest_non_repeat_v1(param_0):\n    if param_0 is None:\n        return 0, ''\n    var_0 = ''\n    var_1 = {}\n    var_2 = 0\n    var_3 = 0\n    for var_4 in range(len(param_0)):\n        if param_0[var_4] in var_1:\n            var_3 = max(var_1[param_0[var_4]], var_3)\n        var_1[param_0[var_4]] = var_4 + 1\n        if var_4 - var_3 + 1 > var_2:\n            var_2 = var_4 - var_3 + 1\n            var_0 = param_0[var_3: var_4 + 1]\n    return var_2, var_0", "contrast": "def find_longest_substring(input_str):\n    if not input_str:\n        return 0, \"\"\n    max_len = 0\n    sub_str = \"\"\n    for i in range(len(input_str)):\n        seen_chars = set()\n        j = i\n        while j < len(input_str) and input_str[j] not in seen_chars:\n            seen_chars.add(input_str[j])\n            j += 1\n        if j-i > max_len:\n            max_len = j-i\n            sub_str = input_str[i:j]\n    return max_len, sub_str", "label": 0}
{"index": "gp080776", "code": "def unload(param_0):\n        self.is_loaded = False\n        for var_0 in self._event_cache:\n            self.context.unregister(\n                var_0['event'], var_0['callback'], var_0['selector'])\n        self._event_cache = {}", "contrast": "def on_unload_view(self):\n    for handler in self.event_handlers:\n        self.unsubscribe(handler)", "label": 0}
{"index": "gp304678", "code": "import numpy as np\ndef std(param_0, param_1, param_2, param_3, param_4, param_5):\n    var_0 = np.array([param_0, param_1, param_2])\n    var_1 = np.array([param_3, param_4, param_5])\n    var_2 = np.sqrt(np.sum((var_0 - np.average(var_0, weights=var_1))**2 * var_1) / np.sum(var_1))\n    return var_2", "contrast": "def exampleSignals(std=1, dur1=1, dur2=3, dur3=0.2,\r\n                          n1=0.5, n2=0.5, n3=2):\r\n    np.random.seed(123)\r\n    t = np.linspace(0, 10, 100)\r\n    f0 = _flux(t, n1, dur1, std, offs=0)\r\n    f1 = _flux(t, n2, dur2, std, offs=0)\r\n    f2 = _flux(t, n3, dur3, std, offs=0)\r\n    return t,f0,f1,f2", "label": 1}
{"index": "gp033281", "code": "def _get_tcpip_interface_info(param_0):\n    var_0 = _get_base_interface_info(param_0)\n    if var_0['ipv4']['requestmode'] == 'static':\n        var_1 = _load_config(interface.name, ['IP_Address', 'Subnet_Mask', 'Gateway', 'DNS_Address'])\n        var_0['ipv4']['address'] = var_1['IP_Address']\n        var_0['ipv4']['netmask'] = var_1['Subnet_Mask']\n        var_0['ipv4']['gateway'] = var_1['Gateway']\n        var_0['ipv4']['dns'] = [var_1['DNS_Address']]\n    elif var_0['up']:\n        var_0['ipv4']['address'] = interface.sockaddrToStr(interface.addr)\n        var_0['ipv4']['netmask'] = interface.sockaddrToStr(interface.netmask)\n        var_0['ipv4']['gateway'] = '0.0.0.0'\n        var_0['ipv4']['dns'] = _get_dns_info()\n        with salt.utils.files.fopen('/proc/net/route', 'r') as var_2:\n            var_3 = re.compile(r'^{interface}\\t[0]{{8}}\\t([0-9A-Z]{{8}})'.format(param_0=interface.name),\n                                 re.MULTILINE)\n            var_4 = pattern.search(route_file.read())\n            var_5 = None if not var_4 else match.group(1)\n        if var_5 is not None and len(var_5) == 8:\n            var_0['ipv4']['gateway'] = '.'.join([str(int(var_5[var_6:var_6 + 2], 16))\n                                                            for var_6 in range(6, -1, -2)])\n    return var_0", "contrast": "import socket\nimport psutil\ndef interface_details(interface_name):\n    interfaces = psutil.net_if_addrs()\n    if interface_name in interfaces.keys():\n        interface = interfaces[interface_name]\n        for info in interface:\n            if info.family == socket.AF_INET:\n                ip_address = info.address\n                netmask = info.netmask\n        mac_address = interfaces[interface_name][0].address\n        return {'interface': interface_name, 'ipv4_address': ip_address, 'netmask': netmask, 'mac_address': mac_address}\n    else:\n        return 'Invalid interface provided'", "label": 0}
{"index": "gp284709", "code": "def get_sentence(param_0: int) -> str:\n    var_0 = [\"This is the first sentence.\",\n                 \"This is the second sentence.\",\n                 \"This is the third sentence.\",\n                 \"This is the fourth sentence.\",\n                 \"This is the fifth sentence.\"]\n    return var_0[param_0]", "contrast": "def get_sentence(self, offset: int) -> BioCSentence or None:\n        for sentence in self.sentences:\n            if sentence.offset == offset:\n                return sentence\n        return None", "label": 1}
{"index": "gp279500", "code": "def put_file(param_0, param_1, param_2):\n    try:\n        self._backend.put_file(param_1, param_2)\n    finally:\n        self._cache.pop(param_1, None)", "contrast": "def put_file(self, key, file):\n        try:\n            return self._dstore.put_file(key, file)\n        finally:\n            self.cache.delete(key)", "label": 1}
{"index": "gp264551", "code": "def test_update_state(param_0, param_1):\n    for var_0 in param_1:\n        if not var_0:\n            return False\n    return not param_0", "contrast": "def check_lifecycle(self):\n        with self._lock:\n            was_valid = self.state == StoredInstance.VALID\n            can_validate = self.state not in (\n                StoredInstance.VALIDATING,\n                StoredInstance.VALID,\n            )\n            handlers_valid = self.__safe_handlers_callback(\n                \"is_valid\", break_on_false=True\n            )\n            if was_valid and not handlers_valid:\n                self.invalidate(True)\n            elif (\n                can_validate and handlers_valid and self._ipopo_service.running\n            ):\n                self.validate(True)", "label": 1}
{"index": "gp189961", "code": "def eda_parameter_to_string(param_0, param_1=False, param_2='\"'):\n    if isinstance(param_0, bool):\n        if param_1:\n            return \"true\" if param_0 else \"false\"\n        else:\n            return int(param_0)\n    elif isinstance(param_0, str):\n        return param_2 + param_0 + param_2\n    else:\n        return str(param_0)", "contrast": "def jinja_filter_param_value_str(value, str_quote_style=\"\", bool_is_str=False):\n    if (type(value) == bool) and not bool_is_str:\n        if (value) == True:\n            return '1'\n        else:\n            return '0'\n    elif type(value) == str or ((type(value) == bool) and bool_is_str):\n        return str_quote_style + str(value) + str_quote_style\n    else:\n        return str(value)", "label": 1}
{"index": "gp281540", "code": "def create_new_transition(param_0, param_1, param_2, param_3, param_4=None):\n    if param_0 is None or param_2 is None:\n        raise AttributeError(\"Invalid state id\")\n    if param_1 is None or param_3 is None:\n        raise AttributeError(\"Invalid outcome\")\n    var_0 = 1 \n    return var_0", "contrast": "def create_transition(self, from_state_id, from_outcome, to_state_id, to_outcome, transition_id):\n        if from_state_id is not None:\n            if from_state_id == self.state_id:\n                from_state = self\n            else:\n                from_state = self.states[from_state_id]\n        if from_outcome is not None:\n            if from_outcome in from_state.outcomes:\n                if to_outcome is not None:\n                    if to_outcome in self.outcomes:  \n                        self.transitions[transition_id] =                            Transition(from_state_id, from_outcome, to_state_id, to_outcome, transition_id, self)\n                    else:\n                        raise AttributeError(\"to_state does not have outcome %s\", to_outcome)\n                else:  \n                    self.transitions[transition_id] =                        Transition(from_state_id, from_outcome, to_state_id, to_outcome, transition_id, self)\n            else:\n                raise AttributeError(\"from_state does not have outcome %s\", from_state)\n        else:\n            self.transitions[transition_id] =                Transition(None, None, to_state_id, to_outcome, transition_id, self)\n        self._transitions_cv.acquire()\n        self._transitions_cv.notify_all()\n        self._transitions_cv.release()\n        return transition_id", "label": 1}
{"index": "gp144512", "code": "def element_type(param_0):\n        if not self.is_pointer:\n            raise ValueError(\"Type {} is not a pointer\".format(param_0))\n        return TypeRef(ffi.lib.LLVMPY_GetElementType(param_0))", "contrast": "def pointed_type(tp):\n    if not isinstance(tp, type):\n        raise TypeError('tp must be a type.')\n    if not hasattr(tp, '__name__'):\n        raise ValueError('tp must have a name.')\n    if not tp.__name__.endswith('*'):\n        raise ValueError('tp must be a pointer type.')\n    return type(tp.__name__[:-1])", "label": 0}
{"index": "gp289485", "code": "import heapq\ndef dijkstra(param_0, param_1, param_2):\n    var_0 = {var_1: float('inf') for var_1 in range(len(param_2))}\n    var_0[param_0] = 0\n    var_2 = [(0, param_0)]\n    while var_2:\n        var_3, var_4 = heapq.heappop(var_2)\n        if var_3 > var_0[var_4]:\n            continue\n        for var_5, var_6 in param_1[var_4]:\n            var_7 = var_3 + param_2[var_4][var_5]\n            if var_7 < var_0[var_5]:\n                var_0[var_5] = var_7\n                heapq.heappush(var_2, (var_7, var_5))\n    return var_0", "contrast": "def graph_distances(start, edges, distances):\n    adj = {x: [] for x in range(len(distances))}\n    for n1, n2 in edges:\n        adj[n1].append(n2)\n        adj[n2].append(n1)\n    to_visit = []\n    new_dist = {}\n    for n in adj[start]:\n        heapq.heappush(to_visit, (distances[start, n], n))\n    while to_visit:\n        d, next_node = heapq.heappop(to_visit)\n        if next_node not in new_dist:\n            new_dist[next_node] = d\n        for n in adj[next_node]:\n            if n not in new_dist:\n                heapq.heappush(to_visit, (d + distances[next_node, n], n))\n    return new_dist", "label": 1}
{"index": "gp264330", "code": "def get_instances_by_factory(param_0):\n    var_0 = []\n    for var_1 in stored_objects:\n        if obj.factory == param_0:\n            instances.append(var_1)\n    return var_0", "contrast": "def __get_stored_instances(self, factory_name):\n        with self.__instances_lock:\n            return [\n                stored_instance\n                for stored_instance in self.__instances.values()\n                if stored_instance.factory_name == factory_name\n            ]", "label": 1}
{"index": "gp089962", "code": "def set_plugin_directories(param_0, param_1, param_2=True):\n        self.directory_manager.set_directories(param_1, param_2)", "contrast": "import os\nclass Plugin():\n    def __init__(self):\n        self.plugin_directories = set()\n    def set_plugin_directories(self, directories, except_blacklisted=False, blacklisted=set()):\n        if hasattr(directories, '__iter__'):\n            directories = set(directories)\n        else:\n            directories = {directories}\n        working_dir = os.getcwd()\n        directories = {os.path.abspath(os.path.join(working_dir, directory)) for directory in directories}\n        if except_blacklisted:\n            directories = directories - blacklisted\n        self.plugin_directories = directories", "label": 0}
{"index": "gp087003", "code": "def make_gpg_home(appname, config_dir=None):\n    assert is_valid_appname(appname)\n    config_dir = get_config_dir( config_dir )\n    path = os.path.join( config_dir, \"gpgkeys\", appname )\n    if not os.path.exists(path):\n        os.makedirs( path, 0700 )\n    else:\n        os.chmod( path, 0700 )\n    return path", "contrast": "import os\ndef make_gpg_keyring_dir(app_name):\n    path = os.path.expanduser(f\"~/.{app_name}/gnupg\")\n    os.makedirs(path, exist_ok=True)\n    return path", "label": 0}
{"index": "gp097380", "code": "def _get_statements(param_0, param_1, param_2):\n        var_0 = []\n        var_1 = []\n        var_2 = [l.split(\"!\")[0] for var_3 in param_1]\n        var_4 = 0\n        for var_5 in range(len(var_2)):\n            var_6 = var_2[var_5].strip()\n            var_7 = param_2 + var_5\n            var_4 += len(param_1[var_5]) + 1\n            if len(var_6) == 0 or var_6[-1] != \"&\":\n                statement.append(var_6)\n                result.append((var_7-len(var_1)+1, \n                               \" \".join(var_1), var_4))\n                var_1 = []\n                var_4 = 0\n            else:\n                statement.append(var_6[:len(var_6)-1])\n        return var_0", "contrast": "def fortran_statements(lines, start):\n    statements = []\n    statement = ''\n    original_length = 0\n    line_num = start\n    continuation = False\n    for line in lines[start:]:\n        if not continuation:\n            statement = ''\n            original_length = 0\n        line_label = str(line)\n        stripped_line = line.rstrip()\n        if not stripped_line:\n            continuation = False\n            continue\n        if stripped_line.startswith(('c', 'C', '*')):\n            continuation = False\n            statements.append((line_num, line_label, original_length))\n            line_num += 1\n            continue\n        if stripped_line.endswith('&'):\n            continuation = True\n            statement += stripped_line[:-1].rstrip()\n            original_length += len(line)\n            line_num += 1\n            continue\n        continuation = False\n        statement += stripped_line\n        original_length += len(line)\n        statements.append((line_num, statement, original_length))\n        line_num += 1\n    return statements", "label": 0}
{"index": "gp308951", "code": "def read_topology(param_0):\n    var_0 = {}\n    with open(param_0, 'r') as var_1:\n        pass\n    return var_0", "contrast": "def read(self, stream):\n        def read_it(stream):\n            bytes = stream.read()\n            transportIn = TMemoryBuffer(bytes)\n            protocolIn = TBinaryProtocol.TBinaryProtocol(transportIn)\n            topology = StormTopology()\n            topology.read(protocolIn)\n            return topology\n        if isinstance(stream, six.string_types):\n            with open(stream, 'rb') as f:\n                return read_it(f)\n        else:\n            return read_it(stream)", "label": 1}
{"index": "gp089338", "code": "def getAttribute(param_0, param_1: str) -> _AttrValueType:\n        if param_1 == 'class':\n            if self.classList:\n                return self.classList.toString()\n            return None\n        var_0 = self.getAttributeNode(param_1)\n        if var_0 is None:\n            return None\n        return attr_node.value", "contrast": "def get_attribute_as_string(node, attr):\n    if hasattr(node, attr):\n        return str(getattr(node, attr))\n    else:\n        return None", "label": 0}
{"index": "gp328507", "code": "import subprocess\ndef host_exists(param_0):\n    var_0 = subprocess.run(['ping', '-c', '1', '-W', '1', param_0], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    if result.returncode == 0:\n        return True\n    else:\n        return False", "contrast": "def is_present(self, host=None):\n        r = self.local_renderer\n        r.env.host = host or self.genv.host_string\n        ret = r._local(\"getent hosts {host} | awk '{{ print $1 }}'\", capture=True) or ''\n        if self.verbose:\n            print('ret:', ret)\n        ret = ret.strip()\n        if self.verbose:\n            print('Host %s %s present.' % (r.env.host, 'IS' if bool(ret) else 'IS NOT'))\n        ip = ret\n        ret = bool(ret)\n        if not ret:\n            return False\n        r.env.ip = ip\n        with settings(warn_only=True):\n            ret = r._local('ping -c 1 {ip}', capture=True) or ''\n        packet_loss = re.findall(r'([0-9]+)% packet loss', ret)\n        ip_accessible = packet_loss and int(packet_loss[0]) < 100\n        if self.verbose:\n            print('IP %s accessible: %s' % (ip, ip_accessible))\n        return bool(ip_accessible)", "label": 1}
{"index": "gp099628", "code": "def location_filter(param_0, param_1, param_2):\r\n    var_0 = dict()\r\n    for var_1, var_2 in files_with_tags.items():\r\n        if 'GPS GPSLatitude' in var_2:\r\n            try:\r\n                var_3 = convert_to_decimal(str(var_2['GPS GPSLatitude']))\r\n                var_4 = convert_to_decimal(str(var_2['GPS GPSLongitude']))\r\n            except ValueError:\r\n                print('{0} has invalid gps info'.format(var_1))\r\n            try:\r\n                if haversine(var_3, var_4, param_1['lat'], param_1['long']) < param_2:\r\n                    var_0[var_1] = var_2\r\n            except InvalidCoordinate:\r\n                print('{0} has invalid gps info'.format(var_1))\r\n    return var_0", "contrast": "from typing import List, Tuple\ndef get_photos_within_radius(photos: List[Tuple[float, float]], center: Tuple[float, float], radius: float) -> List[Tuple[float, float]]:\n    result = []\n    for photo in photos:\n        distance = ((center[0] - photo[0])**2 + (center[1] - photo[1])**2)**0.5\n        if distance <= radius:\n            result.append(photo)\n    return result", "label": 0}
{"index": "gp292890", "code": "def get_correspondance(param_0: list, param_1: list) -> dict:\n    return dict(zip(param_0, param_1))", "contrast": "def calculate_iI_correspondence(omega):\n    Ne = len(omega[0])\n    om = omega[0][0]\n    correspondence = []\n    I = 0\n    for i in range(Ne):\n        if omega[i][0] != om:\n            om = omega[i][0]\n            I += 1\n        correspondence += [(i+1, I+1)]\n    Nnd = I+1\n    def I_nd(i):\n        return correspondence[i-1][1]\n    def i_d(I):\n        for i in range(Ne):\n            if correspondence[i][1] == I:\n                return correspondence[i][0]\n    return i_d, I_nd, Nnd", "label": 1}
{"index": "gp308794", "code": "def notUnique(param_0, param_1=None):\n    from collections import Counter\n    var_0 = Counter(param_0)\n    for var_1, var_2 in c.most_common(param_1):\n        if var_2 == 1:\n            break\n        yield var_1", "contrast": "def notUnique(iterable, reportMax=INF):\n    hash = {}\n    n=0\n    if reportMax < 1:\n        raise ValueError(\"`reportMax` must be >= 1 and is %r\" % reportMax)\n    for item in iterable:\n        count = hash[item] = hash.get(item, 0) + 1\n        if count > 1:\n            yield item\n            n += 1\n            if n >= reportMax:\n                return", "label": 1}
{"index": "gp009078", "code": "def listen(param_0):\n        var_0 = 0\n        self._start()\n        while True:\n            var_1 = self._socket.recv_pyobj()\n            if isinstance(var_1, tuple):\n                var_2, var_3 = var_1\n            else:\n                var_2 = var_1\n                var_3 = None\n            if var_2 == self.SPACE:\n                if self.queue.qsize() + var_0 < self.queue_maxsize:\n                    self._socket.send_string(self.SPACE_AVAILABLE)\n                    var_0 += 1\n                else:\n                    self._socket.send_string(self.SPACE_NOT_AVAILABLE)\n            elif var_2 == self.PING:\n                self._socket.send_string(self.PONG)\n            elif var_2 == self.DATA:\n                self._socket.send_string(self.STORING)\n                self.queue.put(var_3)\n                var_0 -= 1\n            elif var_2 == self.DONE:\n                self._socket.send_string(ZMQServer.CLOSED)\n                self.queue.put(('DONE', [], {}))\n                self._close()\n                break\n            else:\n                raise RuntimeError('I did not understand your request %s' % var_2)", "contrast": "def handle_client_request(request_type, data=None):\n    if request_type == 1:\n        pass\n    elif request_type == 2:\n        pass\n    elif request_type == 3:\n        if handle_client_request(1):\n            pass\n        else:\n            return False\n    elif request_type == 4:\n        pass\n    else:\n        raise ValueError(\"Invalid request type\") ", "label": 0}
{"index": "gp295178", "code": "def convert_to_string(param_0):\n    return \"\\n\".join(param_0)", "contrast": "def parse_setup(options: Union[List, str]) -> str:\n    if isinstance(options, str):\n        return options\n    return \"\\n\".join(options)", "label": 1}
{"index": "gp160681", "code": "def clear_calendars(param_0):\n        self._calendars.clear()\n        self._calendar_factories.clear()\n        self._aliases.clear()", "contrast": "def deregister_all_calendars():\n    for cal in calendar.calendar_list:\n        calendar.deregister_calendar(cal)", "label": 0}
{"index": "gp239752", "code": "def list_registered_hooks():\n    var_0 = []\n    for var_1 in sorted(hook_registry.keys()):\n        var_2 = hook_registry[var_1]\n        for var_3 in var_2:\n            if var_3 not in var_0:\n                hook_order.append(var_3)\n    return var_0", "contrast": "def hooks(ctx):\n    from ..hooks.run_hooks_hook import RunHooksHook\n    bundles = _get_bundles(ctx.obj.data['env'])\n    hooks = RunHooksHook(None).collect_from_bundles(bundles)\n    print_table(('Hook Name',\n                 'Default Bundle Module',\n                 'Bundle Module Override Attr',\n                 'Description'),\n                [(hook.name,\n                 hook.bundle_module_name or '(None)',\n                 hook.bundle_override_module_name_attr or '(None)',\n                 format_docstring(hook.__doc__) or '(None)') for hook in hooks])", "label": 1}
{"index": "gp170174", "code": "def run_command(param_0, param_1=True, param_2=False):\n    if param_1 and not session.has_permission():\n        raise PermissionError(\"User doesn't have permission to run this command\")\n    if param_2:\n        if session.check_prerequisites():\n            return True\n        else:\n            return False\n    else:\n        session.run()\n        return True", "contrast": "async def run(self, session, *,\n                  check_perm: bool = True,\n                  dry: bool = False) -> bool:\n        has_perm = await self._check_perm(session) if check_perm else True\n        if self.func and has_perm:\n            if dry:\n                return True\n            if session.current_arg_filters is not None and                    session.current_key is not None:\n                arg = session.current_arg\n                config = session.bot.config\n                for f in session.current_arg_filters:\n                    try:\n                        res = f(arg)\n                        if isinstance(res, Awaitable):\n                            res = await res\n                        arg = res\n                    except ValidateError as e:\n                        if config.MAX_VALIDATION_FAILURES > 0:\n                            session.state['__validation_failure_num'] =                                session.state.get(\n                                    '__validation_failure_num', 0) + 1\n                            if session.state['__validation_failure_num'] >=                                    config.MAX_VALIDATION_FAILURES:\n                                session.finish(render_expression(\n                                    config.TOO_MANY_VALIDATION_FAILURES_EXPRESSION\n                                ), **session._current_send_kwargs)\n                        failure_message = e.message\n                        if failure_message is None:\n                            failure_message = render_expression(\n                                config.DEFAULT_VALIDATION_FAILURE_EXPRESSION\n                            )\n                        session.pause(failure_message,\n                                      **session._current_send_kwargs)\n                session.state[session.current_key] = arg\n            else:\n                if self.args_parser_func:\n                    await self.args_parser_func(session)\n                if session.current_key is not None and                        session.current_key not in session.state:\n                    session.state[session.current_key] = session.current_arg\n            await self.func(session)\n            return True\n        return False", "label": 1}
{"index": "gp128847", "code": "def get_facts_by_name_and_value(param_0=None, param_1=None, param_2=None, param_3=False, param_4=list()):\n    return utils._make_api_request(param_0, '/facts/{0}/{1}'.format(param_1, param_2), param_3, param_4)", "contrast": "import requests\ndef get_facts_by_name_and_value(api_url, fact_name, fact_value):\n    url = api_url + \"/facts/\" + fact_name + \"/\" + fact_value\n    response = requests.get(url)\n    return response.json()", "label": 0}
{"index": "gp082022", "code": "def _re_flatten(param_0):\n    if '(' not in param_0: return param_0\n    return re.sub(r'(\\\\*)(\\(\\?P<[^>]*>|\\((?!\\?))',\n        lambda m: m.group(0) if len(m.group(1)) % 2 else m.group(1) + '(?:', param_0)", "contrast": "import re\ndef turn_capturing_groups_to_noncapturing(pattern):\n    return re.sub(r\"(\\()(\\?P<\\w+>)?\", r\"\\1?:\", pattern)", "label": 0}
{"index": "gp304827", "code": "import cv2\nimport numpy as np\ndef polar_to_cartesian(param_0):\n    var_0, var_1, var_2 = polar_array.shape\n    var_3 = var_1/2\n    var_4 = var_0/2\n    var_5 = min(var_0, var_1)/2\n    var_6 = np.zeros((var_0, var_1), dtype=np.float32)\n    var_7 = np.zeros((var_0, var_1), dtype=np.float32)\n    for var_8 in range(var_1):\n        for var_9 in range(var_0):\n            var_10, var_11 = param_0[var_9, var_8]\n            var_12 = var_11*np.pi/180\n            var_13 = var_3 + var_10*np.sin(var_12)\n            var_14 = var_4 + var_10*np.cos(var_12)\n            var_6[var_9, var_8] = var_13/var_1\n            var_7[var_9, var_8] = var_14/var_0\n    var_15 = np.zeros((var_0, var_1, 2), dtype=np.float32)\n    var_15[:, :, 0] = var_6\n    var_15[:, :, 1] = var_7\n    return cv2.remap(param_0, var_15, None, cv2.INTER_LINEAR)", "contrast": "def polarToLinear(img, shape=None, center=None, maps=None,\r\n                  interpolation=cv2.INTER_AREA,\r\n                  borderValue=0, borderMode=cv2.BORDER_REFLECT, **opts):\r\n    if maps is None:\r\n        mapY, mapX = polarToLinearMaps(img.shape[:2], shape, center)\r\n    else:\r\n        mapY, mapX = maps\r\n    o = {'interpolation': interpolation,\r\n         'borderValue': borderValue,\r\n         'borderMode': borderMode}\r\n    o.update(opts)\r\n    return cv2.remap(img, mapY, mapX, **o)", "label": 1}
{"index": "gp009740", "code": "def db_stats(param_0):\n        var_0 = dict(action='db-stats')\n        var_1 = self._api_request(params=var_0)\n        var_2 = DBStats(total_clicks=int(var_1['db-stats']['total_clicks']),\n                        total_links=int(var_1['db-stats']['total_links']))\n        return var_2", "contrast": "import requests\ndef get_db_stats():\n    url = 'https://example.com/db_stats'\n    response = requests.get(url)\n    response.raise_for_status()\n    db_stats = response.json()\n    return db_stats", "label": 0}
{"index": "gp002721", "code": "def topological_nodes(param_0):\n        return nx.lexicographical_topological_sort(self._multi_graph,\n                                                   key=lambda x: str(x.qargs))", "contrast": "def topological_sort(graph):\n    incoming_edges = {v: 0 for v in graph}\n    for v in graph:\n        for w in graph[v]:\n            incoming_edges[w] += 1\n    queue = [v for v in incoming_edges if incoming_edges[v] == 0]\n    while queue:\n        v = queue.pop(0)\n        yield v\n        for w in graph[v]:\n            incoming_edges[w] -= 1\n            if incoming_edges[w] == 0:\n                queue.append(w)", "label": 0}
{"index": "gp198606", "code": "def start_serialized_action(param_0, param_1):\n    return eliot.start_action(param_0=param_0, action_type='serialized-action', param_1=param_1)", "contrast": "def continue_task(cls, logger=None, task_id=_TASK_ID_NOT_SUPPLIED):\n        if task_id is _TASK_ID_NOT_SUPPLIED:\n            raise RuntimeError(\"You must supply a task_id keyword argument.\")\n        if isinstance(task_id, bytes):\n            task_id = task_id.decode(\"ascii\")\n        uuid, task_level = task_id.split(\"@\")\n        action = cls(\n            logger, uuid, TaskLevel.fromString(task_level),\n            \"eliot:remote_task\")\n        action._start({})\n        return action", "label": 1}
{"index": "gp137022", "code": "def process_check(param_0,param_1,param_2,param_3):\n  try: var_0=param_1[param_2]\n  except pg.FieldError: return ['?field']\n  if not isinstance(var_0,syncschema.Syncable): return ['here',None,var_0]\n  if syncable.version()>param_3: \n    return ['here',syncable.version(),syncable.generate()] \n  elif syncable.version()==param_3: return ['ok',param_3]\n  elif syncable.version()<param_3: return ['upload',syncable.version()]\n  else: raise RuntimeError(\"shouldn't get here\")", "contrast": "def do_check_helper(version):\n    if version is None:\n        return None\n    else:\n        return \"Helper for do_check, version {0}\".format(version)", "label": 0}
{"index": "gp188869", "code": "import requests\nimport time\nRATE_LIMIT_DELAY = 5  \ndef call_zendesk_api(param_0, param_1, **param_2):\n    var_0 = requests.request(param_0, param_1, **param_2)\n    if response.status_code == 429:  \n        var_1 = int(response.headers.get('Retry-After', RATE_LIMIT_DELAY))\n        time.sleep(var_1)\n        var_0 = requests.request(param_0, param_1, **param_2)\n    response.raise_for_status()\n    try:\n        return response.json()\n    except ValueError:\n        raise ValueError(\"Invalid JSON response from Zendesk API\")", "contrast": "def _call_api(self, http_method, url, **kwargs):\n        log.debug(\"{}: {} - {}\".format(http_method.__name__.upper(), url, kwargs))\n        if self.ratelimit is not None:\n            response = self._ratelimit(http_method=http_method, url=url, **kwargs)\n        else:\n            response = http_method(url, **kwargs)\n        if response.status_code == 429:\n            while 'retry-after' in response.headers and int(response.headers['retry-after']) > 0:\n                retry_after_seconds = int(response.headers['retry-after'])\n                log.warning(\n                    \"Waiting for requested retry-after period: %s seconds\" % retry_after_seconds\n                )\n                while retry_after_seconds > 0:\n                    retry_after_seconds -= 1\n                    self.check_ratelimit_budget(1)\n                    log.debug(\"    -> sleeping: %s more seconds\" % retry_after_seconds)\n                    sleep(1)\n                response = http_method(url, **kwargs)\n        self._check_response(response)\n        self._update_callsafety(response)\n        return response", "label": 1}
{"index": "gp079280", "code": "def validate(param_0, param_1):\n        try:\n            self._choice = IPAddress(param_1)\n        except (ValueError, AddrFormatError):\n            self.error_message = '%s is not a valid IP address.' % param_1\n            return False\n        if self._choice.is_netmask():\n            return True\n        else:\n            self.error_message = '%s is not a valid IP netmask.' % param_1\n            return False", "contrast": "def is_valid_netmask(netmask):\n    octets = netmask.split('.')\n    if len(octets) != 4:\n        return False\n    for octet in octets:\n        try:\n            octet = int(octet)\n            if octet < 0 or octet > 255:\n                return False\n        except ValueError:\n            return False\n    for i in range(1, 4):\n        if octets[i] < octets[i-1]:\n            return False\n    return True", "label": 0}
{"index": "gp184460", "code": "def set_path(param_0, param_1, param_2):\n    if len(param_1) == 1:\n        if param_1[0] in param_0:\n            if isinstance(param_0[param_1[0]], list):\n                param_0[param_1[0]].append(param_2)\n            else:\n                param_0[param_1[0]] = [param_0[param_1[0]], param_2]\n        else:\n            param_0[param_1[0]] = [param_2]\n    else:\n        if param_1[0] not in param_0:\n            param_0[param_1[0]] = dict()\n        set_path(param_0[param_1[0]], param_1[1:], param_2)", "contrast": "def set_path(dicts, keys, v):\n    for key in keys[:-1]:\n        dicts = dicts.setdefault(key, dict())\n    dicts = dicts.setdefault(keys[-1], list())\n    dicts.append(v)", "label": 1}
{"index": "gp160771", "code": "def head_request(\n            param_0,\n            param_1,\n            param_2,\n            param_3=None,\n            param_4=None,\n            param_5=None):\n        var_0 = self._cache.switch(param_1)\n        var_1 = False if param_4 is None else param_4\n        var_2 = self._head_request(var_0, param_2, param_3, var_1, param_5)\n        logger.info('Head Request using : alias=%s, uri=%s, headers=%s, \\\n        allow_redirects=%s ' % (param_1, param_2, param_3, var_1))\n        return var_2", "contrast": "import requests\ndef send_head_request(alias, uri, allow_redirects=False, headers=None):\n    session = requests.Session()\n    session_object = session.cache.get(alias)\n    if session_object:\n        response = session_object.head(uri, allow_redirects=allow_redirects, headers=headers)\n        return response\n    else:\n        print(f\"No session object found with alias '{alias}' in the cache.\")", "label": 0}
{"index": "gp005714", "code": "def record_consumption_rate(param_0, param_1, param_2):\n        if self._last_time is None:\n            self._last_time = param_2\n            self._current_rate = 0.0\n            return\n        self._current_rate = self._calculate_exponential_moving_average_rate(\n            param_1, param_2)\n        self._last_time = param_2", "contrast": "def record_consumption_rate(amt: int, time_at_consumption: float):\n    consumption_rate = amt / time_at_consumption\n    return consumption_rate", "label": 0}
{"index": "gp274913", "code": "def get_mandate(param_0):\n    return mandate", "contrast": "def mandate(self):\n        return self.client.customer_mandates.with_parent_id(self.customer_id).get(self.mandate_id)", "label": 1}
{"index": "gp326968", "code": "import datetime\nimport matplotlib.pyplot as plt\nimport gtfspy\ndef plot_dates(param_0: gtfspy.GTFS, param_1=None, param_2=[], param_3=[], param_4=False):\n    var_0 = []\n    for var_1 in G.services:\n        date_ranges.append([service.start_date, service.end_date])\n    date_ranges.sort()\n    var_2 = datetime.datetime.strptime(var_0[0][0], \"%Y%m%d\")\n    var_3 = datetime.datetime.strptime(var_0[-1][1], \"%Y%m%d\")\n    var_4 = []\n    while var_2 <= var_3:\n        dates.append(var_2)\n        var_2 += datetime.timedelta(days=1)\n    var_5 = []\n    var_6 = []\n    for var_7 in var_4:\n        var_8 = 0\n        for var_1 in G.services:\n            if date.strftime(\"%Y%m%d\") in service.active_dates:\n                var_8 += 1\n        x.append(var_7)\n        y.append(var_8)\n    param_1 = param_1 or plt.axes()\n    ax.plot(var_5, var_6)\n    if param_2:\n        for var_7, var_9 in zip(param_2, param_3):\n            if isinstance(var_7, str):\n                var_7 = datetime.datetime.strptime(var_7, \"%Y%m%d\")\n            ax.axvline(var_5=var_7, c='r', ls='--', lw=1)\n            ax.text(var_7, ax.get_ylim()[1], var_9, fontsize=8, ha='center', va='top', rotation=45)\n    if param_4:\n        plt.show()\n    return param_1", "contrast": "def plot_trip_counts_per_day(G, ax=None, highlight_dates=None, highlight_date_labels=None, show=False):\n    daily_trip_counts = G.get_trip_counts_per_day()\n    if ax is None:\n        _fig, ax = plt.subplots()\n    daily_trip_counts[\"datetime\"] = pandas.to_datetime(daily_trip_counts[\"date_str\"])\n    daily_trip_counts.plot(\"datetime\", \"trip_counts\", kind=\"line\", ax=ax, marker=\"o\", color=\"C0\", ls=\":\",\n                           label=\"Trip counts\")\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Trip counts per day\")\n    if highlight_dates is not None:\n        assert isinstance(highlight_dates, list)\n        if highlight_date_labels is not None:\n            assert isinstance(highlight_date_labels, list)\n            assert len(highlight_dates) == len(highlight_date_labels), \"Number of highlight date labels do not match\"\n        else:\n            highlight_date_labels = [None] * len(highlight_dates)\n        for i, (highlight_date, label) in enumerate(zip(highlight_dates, highlight_date_labels)):\n            color = \"C\" + str(int(i % 8 + 1))\n            highlight_date = pandas.to_datetime(highlight_date)\n            ax.axvline(highlight_date, color=color, label=label)\n    ax.legend(loc=\"best\")\n    ax.grid()\n    if show:\n        plt.show()\n    return ax", "label": 1}
{"index": "gp327638", "code": "def compute_angular_mass_within_ellipse(param_0, param_1, param_2):\n    return profiles.mass_profiles.angualr_mass_within_ellipse(param_0, param_1, param_2)", "contrast": "def mass_within_ellipse_in_units(self, major_axis, unit_mass='angular', kpc_per_arcsec=None, critical_surface_density=None):\n        if self.has_mass_profile:\n            return sum(map(lambda p: p.mass_within_ellipse_in_units(major_axis=major_axis, unit_mass=unit_mass,\n                                                                    kpc_per_arcsec=kpc_per_arcsec,\n                                                                    critical_surface_density=critical_surface_density),\n                           self.mass_profiles))\n        else:\n            return None", "label": 1}
{"index": "gp121517", "code": "def _send(param_0, param_1):\n        var_0 = self._talk.put(param_1)\n        if not var_0:\n            self._logger.error('Failed to send \"%s\"' % param_1)\n        return var_0", "contrast": "def return_response_message():\n    return \"Response message to client\"", "label": 0}
{"index": "gp153259", "code": "def get_init(param_0, param_1=\"__init__.py\"):\n        import ast\n        with open(param_1) as var_0:\n            var_1 = ast.parse(init_file.read())\n        var_2 = lambda x: (ast.literal_eval(node.value) for var_3 in ast.walk(var_1)            if isinstance(var_3, ast.Assign) and node.targets[0].id == x)\n        try:\n            return next(var_2(\"__author__\")),                   next(var_2(\"__email__\")),                   next(var_2(\"__license__\")),                   next(var_2(\"__version__\"))\n        except StopIteration:\n            raise ValueError(\"One of author, email, license, or version\"\n                        \" cannot be found in {}\".format(param_1))", "contrast": "def get_package_info(package_name):\n    try:\n        import pkg_resources\n        package_info = pkg_resources.get_distribution(package_name)\n        return {'version': package_info.version, \n                'author': package_info.author, \n                'license': package_info.license,\n                'description': package_info.get_metadata('DESCRIPTION'),\n                'keywords': package_info.keywords,\n                'classifiers': package_info.classifiers,\n                'provides': package_info.provides\n                }\n    except Exception as e:\n        print(str(e))\n        return {}", "label": 0}
{"index": "gp291103", "code": "def get_file_contents_as_memoryview(param_0):\n    with open(param_0, 'rb') as var_0:\n        var_1 = memoryview(f.read())\n    return var_1", "contrast": "def get_file(hash):\n    stmt = _get_sql('get-file.sql')\n    args = dict(hash=hash)\n    with db_connect() as db_conn:\n        with db_conn.cursor() as cursor:\n            cursor.execute(stmt, args)\n            try:\n                file, _ = cursor.fetchone()\n            except TypeError:\n                raise FileNotFound(hash)\n    return memoryview(file[:])", "label": 1}
{"index": "gp085362", "code": "def update_ips(param_0):\n        self.ips = self._cloud_provider.get_ips(self.instance_id)\n        return self.ips[:]", "contrast": "import time\ndef get_instance_ips():\n    public_ip = None\n    private_ip = None\n    timeout = 60 \n    cloud_provider_response = call_cloud_provider_api()\n    if cloud_provider_response:\n        if \"public_ip\" in cloud_provider_response:\n            public_ip = cloud_provider_response[\"public_ip\"]\n        if \"private_ip\" in cloud_provider_response:\n            private_ip = cloud_provider_response[\"private_ip\"]\n        while not public_ip and timeout > 0:\n            time.sleep(5) \n            cloud_provider_response = call_cloud_provider_api()\n            if \"public_ip\" in cloud_provider_response:\n                public_ip = cloud_provider_response[\"public_ip\"]\n            timeout -= 5\n    return public_ip, private_ip\ndef call_cloud_provider_api():\n    return {\"public_ip\": \"1.2.3.4\", \"private_ip\": \"192.168.1.1\"}", "label": 0}
{"index": "gp195999", "code": "import random\ndef predict_next_action(param_0, param_1):\n    var_0 = float(\"-inf\")\n    var_1 = None\n    for var_2 in param_1:\n        if q_table[param_0][var_2] > var_0:\n            var_0 = q_table[param_0][var_2]\n            var_1 = var_2\n    var_3 = [var_2 for var_2 in param_1 if q_table[param_0][var_2] == var_0]\n    var_4 = random.choice(var_3)\n    return var_4", "contrast": "def predict_next_action(self, state_key, next_action_list):\n        if self.q_df is not None:\n            next_action_q_df = self.q_df[self.q_df.state_key == state_key]\n            next_action_q_df = next_action_q_df[next_action_q_df.action_key.isin(next_action_list)]\n            if next_action_q_df.shape[0] == 0:\n                return random.choice(next_action_list)\n            else:\n                if next_action_q_df.shape[0] == 1:\n                    max_q_action = next_action_q_df[\"action_key\"].values[0]\n                else:\n                    next_action_q_df = next_action_q_df.sort_values(by=[\"q_value\"], ascending=False)\n                    max_q_action = next_action_q_df.iloc[0, :][\"action_key\"]\n                return max_q_action\n        else:\n            return random.choice(next_action_list)", "label": 1}
{"index": "gp077994", "code": "def generate_pymol_session(param_0, param_1 = 'pymol', param_2 = {}):\n        if not self.fixed:\n            self.fix()\n        var_0 = BatchBuilder(param_1 = param_1)\n        for var_1 in self.structures:\n            s.add_residues_of_interest(self.get_differing_atom_residue_ids(s.structure_name))\n        var_2 = b.run(MultiStructureBuilder, [self.structures], param_2 = param_2)\n        return var_2[0], b.PSE_scripts[0]", "contrast": "def generate_pymol_session(scaffold_structure, model_structure, design_structure):\n    pymol_session = f'''\n    # Load structures\n    load {scaffold_structure}, Scaffold\n    load {model_structure}, Model\n    load {design_structure}, Design\n    # Create the required selections\n    select scaffold, Scaffold\n    select model, Model\n    select design, Design\n    # Color the structures\n    color gray, scaffold\n    color teal, model\n    color red, design\n    # Set the representations\n    hide everything\n    show cartoon, scaffold\n    show sticks, model\n    show sticks, design and not resn hoh\n    # Align the structures\n    align model, scaffold\n    # Save the session\n    save session.pse\n    quit\n    '''\n    return pymol_session, \" \".join(pymol_session.split())", "label": 0}
{"index": "gp061863", "code": "def cookie(\n        param_0,\n        param_1,\n        param_2,\n        param_3=None):\n    var_0 = urlparse(param_0)\n    var_1 = u.hostname\n    if '.' not in var_1 and not _is_ip_addr(var_1):\n        var_1 += \".local\"\n    var_2 = str(u.port) if u.port is not None else None\n    var_3 = u.scheme == 'https'\n    if param_3 is not None:\n        if expires.tzinfo is not None:\n            raise ValueError('Cookie expiration must be a naive datetime')\n        param_3 = (param_3 - datetime(1970, 1, 1)).total_seconds()\n    return http_cookiejar.Cookie(\n        version=0,\n        param_1=param_1,\n        param_2=param_2,\n        var_2=var_2,\n        port_specified=var_2 is not None,\n        var_1=var_1,\n        domain_specified=True,\n        domain_initial_dot=False,\n        path=u.path,\n        path_specified=True,\n        var_3=var_3,\n        param_3=param_3,\n        discard=False,\n        comment=None,\n        comment_url=None,\n        rest=None,\n        rfc2109=False,\n    )", "contrast": "import datetime\nfrom http.cookiejar import Cookie\ndef create_cookie(name: str, value: str, url: str, expires: datetime.datetime = None) -> Cookie:\n    if expires is not None:\n        expires = int((expires - datetime.datetime(1970, 1, 1)).total_seconds())\n    cookie = Cookie(\n        version=0,\n        name=name,\n        value=value,\n        port=None,\n        port_specified=False,\n        domain='',\n        domain_specified=False,\n        domain_initial_dot=False,\n        path=url,\n        path_specified=True,\n        secure=False,\n        expires=expires,\n        discard=True,\n        comment=None,\n        comment_url=None,\n        rest=None,\n        rfc2109=False\n    )\n    return cookie", "label": 0}
{"index": "gp122003", "code": "def done(param_0):\n    def done(param_0, param_1, **param_2):\n        return Done(param_0, param_0)\n    return done", "contrast": "def deleted_response_builder(message):\n    return {'status': 'deleted', 'message': message}", "label": 0}
{"index": "gp231065", "code": "def set_cmake_flags(param_0):\n    var_0 = details.get_custom_cmake_flags()\n    if var_0:\n        var_1 = details.get_cmake_file_path()\n        with open(var_1, 'r') as var_2:\n            var_3 = fd.read()\n        var_3 = content.replace('##CUSTOM_CMAKE_FLAGS##', var_0)\n        with open(var_1, 'w') as var_2:\n            fd.write(var_3)", "contrast": "def processLibraryDetails(details):\n  for includeDir in details.includeDirs:\n\t\t\tfor pattern in CUSTOM_FLAGS_FOR_INCLUDE_DIRS:\n\t\t\t\tif pattern in includeDir:\n\t\t\t\t\tflag = '-D' + CUSTOM_FLAGS_FOR_INCLUDE_DIRS[pattern] + '=' + includeDir\n     details.cmakeFlags.append(flag)\n  for lib in details.libs:\n\t\t\tfilename = os.path.basename(lib)\n   (name, ext) = os.path.splitext(filename)\n   libName = name.replace('lib', '') if name.startswith('lib') else name\n   libName = libName.rstrip('_-1234567890')\n   if libName in CUSTOM_FLAGS_FOR_LIBS:\n\t\t\t\tflag = '-D' + CUSTOM_FLAGS_FOR_LIBS[libName] + '=' + lib\n    details.cmakeFlags.append(flag)", "label": 1}
{"index": "gp051683", "code": "def event_handlers(param_0):\n        if self.is_flow:\n            return self._event_handlers\n        try:\n            return self._event_handlers\n        except AttributeError:\n            return self.flow._event_handlers", "contrast": "def registered_handlers(self):\n    if self.handlers:\n        return self.handlers\n    elif self.flow:\n        return self.flow.handlers\n    else:\n        return []", "label": 0}
{"index": "gp117759", "code": "def get_comments_by_genus_type(param_0, param_1):\n        var_0 = JSONClientValidated('commenting',\n                                         var_0='Comment',\n                                         runtime=self._runtime)\n        var_1 = collection.find(\n            dict({'genusTypeId': str(param_1)},\n                 **self._view_filter())).sort('_id', DESCENDING)\n        return objects.CommentList(var_1, runtime=self._runtime, proxy=self._proxy)", "contrast": "def get_comments_by_genus_type(self, comment_genus_type):\n    if not comment_genus_type:\n        raise NullArgument('comment_genus_type is null')\n    return CommentList() ", "label": 0}
{"index": "gp105230", "code": "def _get_clstr_outfile(self):\n        if self.Parameters['-o'].isOn():\n            return ''.join([self.Parameters['-o'].Value, '.clstr'])\n        else:\n            raise ValueError, \"No output file specified\"", "contrast": "import os\ndef absolute_path_to_clstr_outfile(filepath: str) -> str:\n    return os.path.abspath(filepath)", "label": 0}
{"index": "gp173765", "code": "def create_signature_scanner(param_0):\n    var_0 = pysigscan.scanner()\n    for var_1 in param_0:\n        scanner.add_signature(specification.signature, specification.offset, specification.description)\n    return var_0", "contrast": "def CreateSignatureScanner(cls, specification_store):\n    scanner_object = pysigscan.scanner()\n    for format_specification in specification_store.specifications:\n      for signature in format_specification.signatures:\n        pattern_offset = signature.offset\n        if pattern_offset is None:\n          signature_flags = pysigscan.signature_flags.NO_OFFSET\n        elif pattern_offset < 0:\n          pattern_offset *= -1\n          signature_flags = pysigscan.signature_flags.RELATIVE_FROM_END\n        else:\n          signature_flags = pysigscan.signature_flags.RELATIVE_FROM_START\n        scanner_object.add_signature(\n            signature.identifier, pattern_offset, signature.pattern,\n            signature_flags)\n    return scanner_object", "label": 1}
{"index": "gp002184", "code": "def __trim_beats(param_0, param_1, param_2):\n    var_0 = scipy.signal.convolve(param_0[param_1],\n                                       scipy.signal.hann(5),\n                                       'same')\n    if param_2:\n        var_1 = 0.5 * ((var_0**2).mean()**0.5)\n    else:\n        var_1 = 0.0\n    var_2 = np.argwhere(var_0 > var_1)\n    return param_1[valid.min():valid.max()]", "contrast": "def final_post_processing(data):\n    while data[0] == 'beat':\n        del data[0]\n    while data[-1] == 'beat':\n        del data[-1]\n    return data", "label": 0}
{"index": "gp310935", "code": "def monolithic_job():\n    var_0 = download_data()\n    var_1 = convert_data(var_0)\n    var_2 = transform_data(var_1)\n    upload_data(var_2)", "contrast": "def download_run_and_upload(job, master_ip, inputs, spark_on_toil):\n    master_ip = MasterAddress(master_ip)\n    bam_name = inputs.sample.split('://')[-1].split('/')[-1]\n    sample_name = \".\".join(os.path.splitext(bam_name)[:-1])\n    hdfs_subdir = sample_name + \"-dir\"\n    if inputs.run_local:\n        inputs.local_dir = job.fileStore.getLocalTempDir()\n        if inputs.native_adam_path is None:\n            hdfs_dir = \"/data/\"\n        else:\n            hdfs_dir = inputs.local_dir\n    else:\n        inputs.local_dir = None\n        hdfs_dir = \"hdfs://{0}:{1}/{2}\".format(master_ip, HDFS_MASTER_PORT, hdfs_subdir)\n    try:\n        hdfs_prefix = hdfs_dir + \"/\" + sample_name\n        hdfs_bam = hdfs_dir + \"/\" + bam_name\n        hdfs_snps = hdfs_dir + \"/\" + inputs.dbsnp.split('://')[-1].split('/')[-1]\n        if not inputs.run_local:\n            download_data(job, master_ip, inputs, inputs.dbsnp, inputs.sample, hdfs_snps, hdfs_bam)\n        else:\n            copy_files([inputs.sample, inputs.dbsnp], inputs.local_dir)\n        adam_input = hdfs_prefix + \".adam\"\n        adam_snps = hdfs_dir + \"/snps.var.adam\"\n        adam_convert(job, master_ip, inputs, hdfs_bam, hdfs_snps, adam_input, adam_snps, spark_on_toil)\n        adam_output = hdfs_prefix + \".processed.bam\"\n        adam_transform(job, master_ip, inputs, adam_input, adam_snps, hdfs_dir, adam_output, spark_on_toil)\n        out_file = inputs.output_dir + \"/\" + sample_name + inputs.suffix + \".bam\"\n        if not inputs.run_local:\n            upload_data(job, master_ip, inputs, adam_output, out_file, spark_on_toil)\n        else:\n            local_adam_output = \"%s/%s.processed.bam\" % (inputs.local_dir, sample_name)\n            move_files([local_adam_output], inputs.output_dir)\n        remove_file(master_ip, hdfs_subdir, spark_on_toil)\n    except:\n        remove_file(master_ip, hdfs_subdir, spark_on_toil)\n        raise", "label": 1}
{"index": "gp226070", "code": "import logging\ndef log_batch(param_0):\n    for var_0 in param_0:\n        var_1 = var_0['time']\n        var_2 = var_0['message']\n        var_3 = var_0['level']\n        var_4 = var_0['attachment']\n        var_5 = var_4['name']\n        var_6 = var_4['data']\n        var_7 = var_4['mime']\n        var_8 = logging.getLogger()\n        logger.setLevel(level.upper())\n        var_9 = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n        var_10 = logging.FileHandler(f'{var_1}.log')\n        file_handler.setLevel(level.upper())\n        file_handler.setFormatter(var_9)\n        logger.addHandler(var_10)\n        if isinstance(var_6, str):\n            logger.log(level.upper(), var_2, extra={'attachment_name': var_5, 'attachment_mime': var_7, 'attachment_content': var_6})\n        else:\n            logger.log(level.upper(), var_2, extra={'attachment_name': var_5, 'attachment_mime': var_7}, stack_info=True)", "contrast": "def log_batch(self, log_data):\n        url = uri_join(self.base_url, \"log\")\n        attachments = []\n        for log_item in log_data:\n            log_item[\"item_id\"] = self.stack[-1]\n            attachment = log_item.get(\"attachment\", None)\n            if \"attachment\" in log_item:\n                del log_item[\"attachment\"]\n            if attachment:\n                if not isinstance(attachment, collections.Mapping):\n                    attachment = {\"data\": attachment}\n                name = attachment.get(\"name\", str(uuid.uuid4()))\n                log_item[\"file\"] = {\"name\": name}\n                attachments.append((\"file\", (\n                    name,\n                    attachment[\"data\"],\n                    attachment.get(\"mime\", \"application/octet-stream\")\n                )))\n        files = [(\n            \"json_request_part\", (\n                None,\n                json.dumps(log_data),\n                \"application/json\"\n            )\n        )]\n        files.extend(attachments)\n        from reportportal_client import POST_LOGBATCH_RETRY_COUNT\n        for i in range(POST_LOGBATCH_RETRY_COUNT):\n            try:\n                r = self.session.post(\n                    url=url,\n                    files=files,\n                    verify=self.verify_ssl\n                )\n            except KeyError:\n                if i < POST_LOGBATCH_RETRY_COUNT - 1:\n                    continue\n                else:\n                    raise\n            break\n        logger.debug(\"log_batch - Stack: %s\", self.stack)\n        logger.debug(\"log_batch response: %s\", r.text)\n        return _get_data(r)", "label": 1}
{"index": "gp303144", "code": "def create_access_token_entry(param_0):\n    return True", "contrast": "def save_token(self, access_token):\n        access_token_id = self.execute(self.create_access_token_query,\n                                       access_token.client_id,\n                                       access_token.grant_type,\n                                       access_token.token,\n                                       access_token.expires_at,\n                                       access_token.refresh_token,\n                                       access_token.refresh_expires_at,\n                                       access_token.user_id)\n        for key, value in list(access_token.data.items()):\n            self.execute(self.create_data_query, key, value,\n                         access_token_id)\n        for scope in access_token.scopes:\n            self.execute(self.create_scope_query, scope, access_token_id)\n        return True", "label": 1}
{"index": "gp306772", "code": "def calculate_good_piece_size(param_0):\n    if param_0 <= 0:\n        return 0\n    elif param_0 <= 100:\n        return 10\n    elif param_0 <= 1000:\n        return 100\n    else:\n        return 1000", "contrast": "def calc_piece_size(size, min_piece_size=20, max_piece_size=29, max_piece_count=1000):\n    logger.debug('Calculating piece size for %i' % size)\n    for i in range(min_piece_size, max_piece_size): \n        if size / (2**i) < max_piece_count:\n            break\n    return 2**i", "label": 1}
{"index": "gp258516", "code": "import pint\ndef infer_unit_entity(param_0: str) -> str:\n    var_0 = pint.UnitRegistry(autoconvert_offset_to_baseunit=True, auto_reduce_dimensions=True)\n    var_1 = str(ureg.parse_expression(param_0).dimensionality)\n    return entity.split('[')[0].strip()", "contrast": "def get_entity_from_dimensions(dimensions, text):\n    new_dimensions = [{'base': l.NAMES[i['base']].entity.name,\n                       'power': i['power']} for i in dimensions]\n    final_dimensions = sorted(new_dimensions, key=lambda x: x['base'])\n    key = l.get_key_from_dimensions(final_dimensions)\n    try:\n        if clf.USE_CLF:\n            ent = clf.disambiguate_entity(key, text)\n        else:\n            ent = l.DERIVED_ENT[key][0]\n    except IndexError:\n        logging.debug(u'\\tCould not find entity for: %s', key)\n        ent = c.Entity(name='unknown', dimensions=new_dimensions)\n    return ent", "label": 1}
{"index": "gp274926", "code": "def cancel_subscription(param_0):\n    subscription.status = 'canceled'\n    return param_0", "contrast": "def delete(self, subscription_id, data=None):\n        if not subscription_id or not subscription_id.startswith(self.RESOURCE_ID_PREFIX):\n            raise IdentifierError(\n                \"Invalid subscription ID: '{id}'. A subscription ID should start with '{prefix}'.\".format(\n                    id=subscription_id, prefix=self.RESOURCE_ID_PREFIX)\n            )\n        result = super(CustomerSubscriptions, self).delete(subscription_id, data)\n        return self.get_resource_object(result)", "label": 1}
{"index": "gp222154", "code": "def output_info(param_0):\n    for var_0, var_1 in data.items():\n        print(var_0 + ':', var_1)", "contrast": "def output_sub_generic(gandi, data, output_keys, justify=10):\n    for key in output_keys:\n        if key in data:\n            output_sub_line(gandi, key, data[key], justify)", "label": 1}
{"index": "gp023100", "code": "def _reader(param_0, param_1, param_2, param_3):\n        while True:\n            var_0 = stream.readline()\n            if not var_0:\n                break\n            var_0 = s.decode('utf-8').rstrip()\n            outbuf.append(var_0)\n            logger.debug('%s: %s' % (param_1, var_0))\n        stream.close()", "contrast": "def read_subprocess_stream(name, stream, outbuf):\n    for line in iter(stream.readline, b''):\n        outbuf.append(line.decode('utf-8'))", "label": 0}
{"index": "gp237074", "code": "from typing import Tuple\nfrom device_adapter import AbstractDeviceAdapter, DeviceServerError, DeviceAdapterError\ndef send_script(param_0: str, param_1: str, param_2: bytes) -> Tuple[int, str]:\n    try:\n        var_0 = AbstractDeviceAdapter(param_1)\n        var_1 = adapter.send_script(param_0, param_2)\n        return var_1\n    except DeviceServerError as e:\n        raise e\n    except DeviceAdapterError as e:\n        raise e", "contrast": "async def send_script(self, client_id, conn_string, script):\n        conn_id = self._client_connection(client_id, conn_string)\n        await self.adapter.send_script(conn_id, script)", "label": 1}
{"index": "gp177082", "code": "def map_paths(param_0, param_1, param_2):\n    var_0 = param_1 + param_2\n    return var_0", "contrast": "def process_IN_MOVED_TO(self, raw_event):\n        watch_ = self._watch_manager.get_watch(raw_event.wd)\n        path_ = watch_.path\n        dst_path = os.path.normpath(os.path.join(path_, raw_event.name))\n        mv_ = self._mv_cookie.get(raw_event.cookie)\n        to_append = {'cookie': raw_event.cookie}\n        if mv_ is not None:\n            self._mv[mv_[0]] = (dst_path, datetime.now())\n            to_append['src_pathname'] = mv_[0]\n        elif (raw_event.mask & IN_ISDIR and watch_.auto_add and\n              not watch_.exclude_filter(dst_path)):\n            self._watch_manager.add_watch(dst_path, watch_.mask,\n                                          proc_fun=watch_.proc_fun,\n                                          rec=True, auto_add=True,\n                                          exclude_filter=watch_.exclude_filter)\n        return self.process_default(raw_event, to_append)", "label": 1}
{"index": "gp098249", "code": "def monitor_instances(param_0, param_1):\n        var_0 = {}\n        self.build_list_params(var_0, param_1, 'InstanceId')\n        return self.get_list('MonitorInstances', var_0,\n                             [('item', InstanceInfo)], verb='POST')", "contrast": "import boto.ec2\ndef enable_cloudwatch_monitoring(instance_id):\n    conn = boto.ec2.connect_to_region('us-east-1')\n    reservations = conn.get_all_instances(instance_ids=instance_id)\n    instances = [i for r in reservations for i in r.instances]\n    return conn.monitor_instances(instances)", "label": 0}
{"index": "gp321163", "code": "def print_float_array(param_0, param_1):\n    for var_0 in range(min(param_1, len(param_0))):\n        print(\"{:.2f}\".format(param_0[var_0]), end=\" \")\n    print()", "contrast": "def printColConfidence(self, aState, maxCols = 20):\n    def formatFPRow(var):\n      s = ''\n      for c in range(min(maxCols, self.numberOfCols)):\n        if c > 0 and c % 10 == 0:\n          s += '   '\n        s += ' %5.3f' % var[c]\n      s += ' '\n      return s\n    print formatFPRow(aState)", "label": 1}
{"index": "gp164700", "code": "def search_line(param_0, param_1, param_2):\n    if param_2 == 're' or param_2 == 'word':\n        return re.search(param_1, param_0)  \n    elif param_2 == 'pos':\n        return searcher.search_out(param_0, param_1)\n    elif param_2 == 'hyper':\n        return searcher.hypernym_search(param_0, param_1)", "contrast": "def search_line(line, search_term):\n    if search_term in line:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp240068", "code": "def lookup_widget(param_0):\n    return pos.get('widget', None)", "contrast": "def _confirm_pos(self, pos):\n        candidate = None\n        if self._get_node(self._treelist, pos) is not None:\n            candidate = pos\n        return candidate", "label": 1}
{"index": "gp070842", "code": "def popvalue(param_0, param_1, param_2=None):\n        if param_1 not in self._col_dict:\n            return param_2\n        var_0 = self._col_dict.pop(param_1)\n        self._col_list.remove(var_0)\n        return var_0", "contrast": "def popvalue(D, k, d=None):\n    try:\n        v = D.pop(k)\n    except KeyError:\n        if d is None:\n            raise\n        else:\n            v = d\n    return v", "label": 0}
{"index": "gp088199", "code": "def coroutine(param_0, param_1=True):\n    if TORNADO_MAJOR != 4:\n        var_0 = gen.coroutine(param_0)\n    else:\n        var_0 = gen.coroutine(param_0, param_1)\n    wrapper.__argspec_args = inspect.getargspec(param_0).args\n    return var_0", "contrast": "from tornado import gen\ndef tornado_coroutine_compat(func):\n    @gen.coroutine\n    def wrapper(*args, **kwargs):\n        return (yield func(*args, **kwargs))\n    wrapper.__argspec_args = func.__code__.co_varnames[:func.__code__.co_argcount]\n    return wrapper", "label": 0}
{"index": "gp271956", "code": "def get_key_hash(param_0):\n    return hash(param_0)", "contrast": "def get_hash(key: str) -> int:\n    return int(hashlib.sha1(key.encode('utf8')).hexdigest(), 16) % 4294967295", "label": 1}
{"index": "gp175419", "code": "def find_file_handler(param_0, param_1, param_2):\n    if param_0 not in param_2:\n        raise KeyError('No handler for the given requirements is available.')\n    if param_2[param_0](param_1):\n        return param_2[param_0]\n    else:\n        raise RuntimeError('Handler for the given requirements is available but it doesn\\'t match the filename info.')", "contrast": "def find_required_filehandlers(self, requirements, filename_info):\n        req_fh = []\n        filename_info = set(filename_info.items())\n        if requirements:\n            for requirement in requirements:\n                for fhd in self.file_handlers[requirement]:\n                    if set(fhd.filename_info.items()).issubset(filename_info):\n                        req_fh.append(fhd)\n                        break\n                else:\n                    raise RuntimeError(\"No matching requirement file of type \"\n                                       \"{}\".format(requirement))\n        return req_fh", "label": 1}
{"index": "gp257268", "code": "def label_list(param_0, param_1):\n    if isinstance(param_1, str):\n        label_list.append(param_1)\n    elif isinstance(param_1, int):\n        label_list.append(str(param_1))\n    else:\n        raise TypeError(\"Value must be a string or integer\")\n    return param_0", "contrast": "def p_label_list_list(p):\n    p[0] = p[1]\n    entry = check_and_make_label(p[3], p.lineno(3))\n    p[1].append(entry)", "label": 1}
{"index": "gp084716", "code": "def follow_cf(param_0, param_1, param_2, param_3, param_4=5.0, param_5=None):\n    if param_5 == None:\n        param_5 = Spinon(slaves=6, orbitals=3, avg_particles=param_4,\n                       hopping=[0.5]*6, populations = np.asarray([param_4]*6)/6)\n    var_0, var_1, var_2, var_3 = [], [], [], []\n    for var_4 in param_1:\n        print('U=', var_4, 'del=', param_2)\n        var_5=root(targetpop, param_3[-1],(var_4,param_2,param_5, param_4))\n        print(res.x)\n        if res.x>param_3[-1]: break\n        nup.append(res.x)\n        slsp.param['populations']=population_distri(param_3[-1])\n        mean_f.append(slsp.mean_field())\n        zet.append(slsp.quasiparticle_weight())\n        lam.append(slsp.param['lambda'])\n        mu.append(orbital_energies(slsp.param, var_0[-1]))\n    var_6 = save.createGroup('cf={}'.format(param_2))\n    var_7 = st.setgroup(var_6)\n    st.storegroup(var_7, param_1[:len(var_0)], var_0, var_1, var_2, param_3[1:],param_2,var_3)", "contrast": "def quasiparticle_weight(N):\n    return 1 / (1 + N)", "label": 0}
{"index": "gp187794", "code": "import requests\ndef check_and_visit_login_url(param_0):\n    var_0 = requests.get(param_0)\n    if response.status_code == 200:\n        print(\"Login URL is working fine!\")\n    else:\n        print(\"Error accessing the login URL.\")", "contrast": "def visit_loginurl(self):\n        url = self.config[\"loginurl\"]\n        if not url:\n            return\n        user, password = self.config.get_user_password(url)\n        session = requests.Session()\n        response = session.get(url)\n        cgiuser = self.config[\"loginuserfield\"]\n        cgipassword = self.config[\"loginpasswordfield\"]\n        form = formsearch.search_form(response.content, cgiuser, cgipassword,\n              encoding=response.encoding)\n        form.data[cgiuser] = user\n        form.data[cgipassword] = password\n        for key, value in self.config[\"loginextrafields\"].items():\n            form.data[key] = value\n        formurl = urlparse.urljoin(url, form.url)\n        response = session.post(formurl, data=form.data)\n        self.cookies = session.cookies\n        if len(self.cookies) == 0:\n            raise LinkCheckerError(\"No cookies set by login URL %s\" % url)", "label": 1}
{"index": "gp035944", "code": "def destination_absent(param_0, param_1=None):\n    var_0 = {'name': param_0, 'result': None, 'comment': None, 'changes': {}}\n    var_1 = _do_element_absent(param_0, 'admin_object_resource', {}, param_1)\n    if not var_1['error']:\n        if __opts__['test'] and var_1['delete']:\n            var_0['comment'] = 'JMS Queue set to be deleted'\n        elif var_1['delete']:\n            var_0['result'] = True\n            var_0['comment'] = 'JMS Queue deleted'\n        else:\n            var_0['result'] = True\n            var_0['comment'] = 'JMS Queue doesn\\'t exist'\n    else:\n        var_0['result'] = False\n        var_0['comment'] = 'Error: {0}'.format(var_1['error'])\n    return var_0", "contrast": "import os\ndef ensure_destination_doesnt_exist(name):\n    if os.path.exists(name):\n        raise Exception(f\"JMS Destination '{name}' already exists\")\n    else:\n        print(f\"JMS Destination '{name}' doesn't exist\")", "label": 0}
{"index": "gp211100", "code": "def add_api_call_to_cache(param_0, param_1, param_2):\n    var_0 = {}\n    if param_0 in var_0:\n        var_0[param_0][param_1] = param_2\n    else:\n        var_0[param_0] = {}\n        var_0[param_0][param_1] = param_2\n    return var_0", "contrast": "def lookup_value(self, api_name, key):\n        if api_name in self._cache:\n            return self._cache[api_name].get(key, None)\n        return None", "label": 1}
{"index": "gp313829", "code": "def load_genD(param_0):\n    with open(param_0, 'r') as var_0:\n        var_1 = f.readlines()\n    var_2 = []\n    for var_3 in var_1:\n        if line.startswith(\"D_\"):\n            var_3 = line.strip().split()\n            genD.append([var_3[0], var_3[1]])\n    return var_2", "contrast": "def read_igor_D_gene_parameters(params_file_name):\n    params_file = open(params_file_name, 'r')\n    D_gene_info = {}\n    in_D_gene_sec = False\n    for line in params_file:\n        if line.startswith('#GeneChoice;D_gene;'):\n            in_D_gene_sec = True\n        elif in_D_gene_sec:\n            if line[0] == '%':\n                split_line = line[1:].split(';')\n                D_gene_info[split_line[0]] = [split_line[1] , int(split_line[2])]\n            else:\n                break\n    params_file.close()\n    genD = [[]]*len(D_gene_info.keys())\n    for D_gene in D_gene_info.keys():\n        genD[D_gene_info[D_gene][1]] = [D_gene, D_gene_info[D_gene][0]]\n    return genD", "label": 1}
{"index": "gp240454", "code": "def set_canvas_format(param_0):\n    if param_0 == 'png':\n        var_0 = 'png'\n    elif param_0 == 'jpeg':\n        var_0 = 'jpeg'\n    else:\n        raise ValueError(\"Invalid canvas format. Must be 'png' or 'jpeg'.\")", "contrast": "def set_html5_canvas_format(self, fmt):\n        fmt = fmt.lower()\n        if fmt not in ('jpeg', 'png'):\n            raise ValueError(\"Format must be one of {jpeg|png} not '%s'\" % (\n                fmt))\n        settings = self.get_settings()\n        settings.set(html5_canvas_format=fmt)", "label": 1}
{"index": "gp004054", "code": "def classe(param_0, param_1):\n        for var_0 in self.classes():\n            if klass.node.name == param_1:\n                return var_0\n        raise KeyError(param_1)", "contrast": "def get_class_by_name(class_name):\n    try:\n        return globals()[class_name]\n    except KeyError:\n        raise KeyError(f\"No such class found: {class_name}\")", "label": 0}
{"index": "gp083764", "code": "def hittime(param_0, param_1=None, param_2=None, param_3=None):\n        if isinstance(param_1, (int, float)):\n            return int(Time.milliseconds_offset(Time.from_unix(param_1, param_3=param_3)))\n        if isinstance(param_1, datetime.datetime):\n            return int(Time.milliseconds_offset(param_1))\n        if isinstance(param_2, (int, float)):\n            return int(param_2 * 1000) + (param_3 or 0)", "contrast": "import time\ndef get_hit_offset(hit_timestamp):\n    current_time_milliseconds = int(round(time.time() * 1000))\n    return current_time_milliseconds - hit_timestamp", "label": 0}
{"index": "gp188235", "code": "def split_attrgetter_atom_tokens(param_0):\n    var_0 = param_0[0]\n    if len(param_0) > 1:\n        var_1 = tuple(param_0[1:])\n    else:\n        var_1 = None\n    return (var_0, var_1)", "contrast": "def attrgetter_atom_split(tokens):\n    if len(tokens) == 1:  \n        return tokens[0], None\n    elif len(tokens) >= 2 and tokens[1] == \"(\":  \n        if len(tokens) == 2:  \n            return tokens[0], \"\"\n        elif len(tokens) == 3:  \n            return tokens[0], tokens[2]\n        else:\n            raise CoconutInternalException(\"invalid methodcaller literal tokens\", tokens)\n    else:\n        raise CoconutInternalException(\"invalid attrgetter literal tokens\", tokens)", "label": 1}
{"index": "gp212477", "code": "def get_volume_per_neurite(param_0):\n    return [var_0/len(param_0) for var_0 in param_0]", "contrast": "def total_volume_per_neurite(neurites, neurite_type=NeuriteType.all):\n    return list(sum(s.volume for s in n.iter_sections())\n                for n in iter_neurites(neurites, filt=is_type(neurite_type)))", "label": 1}
{"index": "gp028077", "code": "def relaxNGValidatePushElement(param_0, param_1, param_2):\n        if param_1 is None: var_0 = None\n        else: var_0 = doc._o\n        if param_2 is None: var_1 = None\n        else: var_1 = elem._o\n        var_2 = libxml2mod.xmlRelaxNGValidatePushElement(self._o, var_0, var_1)\n        return var_2", "contrast": "def push_to_relaxng_stack(start, stack):\n    stack.append(start)\n    return stack", "label": 0}
{"index": "gp016294", "code": "def load_default_config(param_0=None):\n    if param_0 is None:\n        param_0 = get_ipython_dir()\n    var_0 = os.path.join(param_0, 'profile_default')\n    var_1 = PyFileConfigLoader(default_config_file_name, var_0)\n    try:\n        var_2 = cl.load_config()\n    except ConfigFileNotFound:\n        var_2 = Config()\n    return var_2", "contrast": "from IPython import get_ipython\ndef load_default_config():\n    ip = get_ipython()\n    if ip is not None:\n        ip.magic('%config IPCompleter.use_jedi=False')\n        ip.magic('%config InlineBackend.figure_format=\"retina\"')\n    else:\n        print(\"IPython shell not found. Please run this command in an IPython shell.\")", "label": 0}
{"index": "gp097827", "code": "def ne(param_0, param_1, param_2=None):\n        param_2 = param_2 or DEFAULT_RANKS\n        if isinstance(param_1, Card):\n            if ranks.get(\"suits\"):\n                return (\n                    param_2[\"values\"][self.value] !=\n                    param_2[\"values\"][other.value] or\n                    param_2[\"suits\"][self.suit] !=\n                    param_2[\"suits\"][other.suit]\n                )\n            else:\n                return param_2[self.value] != param_2[other.value]\n        else:\n            return False", "contrast": "def compare_cards(card, other, ranks):\n    return ranks[card.get_rank()] != ranks[other.get_rank()]", "label": 0}
{"index": "gp058001", "code": "def _wrap_response(param_0, param_1=None, **param_2):\n        param_2['status'] = param_1 if param_1 is not None else self._status.OK\n        return param_2", "contrast": "def wrap_status(status='OK', **kwargs):\n    return {'status': status, **kwargs}", "label": 0}
{"index": "gp173511", "code": "def ExtractTerminalServerClientEvents(param_0, param_1):\n    var_0 = {}\n    var_0['Terminal Server Client Name'] = registry_key.name\n    var_0['Product ID'] = registry_key.GetValueByName('ProductId').data\n    var_0['Company Name'] = registry_key.GetValueByName('CompanyName').data\n    var_0['File Version'] = registry_key.GetValueByName('FileVersion').data\n    var_0['Product Version'] = registry_key.GetValueByName('ProductVersion').data\n    var_0['Client Name'] = registry_key.GetValueByName('ClientName').data\n    var_0['Client Build Number'] = registry_key.GetValueByName('ClientBuildNumber').data\n    var_0['Client DLL Name'] = registry_key.GetValueByName('ClientDll').data\n    var_0['Last Successful Connection Time'] = registry_key.GetValueByName('LastSuccessfulConnectTime').data\n    var_0['Last Connection Time'] = registry_key.GetValueByName('LastConnectTime').data\n    var_0['Connection Statistics'] = registry_key.GetValueByName('ConnectionStatistics').data\n    var_0['Connection Bar Data'] = registry_key.GetValueByName('ConnectionBarData').data\n    var_0['Negotiated Protocol'] = registry_key.GetValueByName('NegotiatedProtocol').data\n    var_0['Client Address'] = registry_key.GetValueByName('ClientAddress').data\n    var_1 = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(var_1, var_0)", "contrast": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    mru_values_dict = {}\n    for subkey in registry_key.GetSubkeys():\n      username_value = subkey.GetValueByName('UsernameHint')\n      if (username_value and username_value.data and\n          username_value.DataIsString()):\n        username = username_value.GetDataAsObject()\n      else:\n        username = 'N/A'\n      mru_values_dict[subkey.name] = username\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = subkey.path\n      event_data.offset = subkey.offset\n      event_data.regvalue = {'Username hint': username}\n      event_data.source_append = self._SOURCE_APPEND\n      event = time_events.DateTimeValuesEvent(\n          subkey.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = mru_values_dict\n    event_data.source_append = self._SOURCE_APPEND\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "label": 1}
{"index": "gp165075", "code": "def createDaemon():\n   try:\n      pid = os.fork()\n   except OSError, e:\n      raise Exception, \"%s [%d]\" % (e.strerror, e.errno)\n   if (pid == 0):   \n      os.setsid()\n      try:\n         pid = os.fork()    \n      except OSError, e:\n         raise Exception, \"%s [%d]\" % (e.strerror, e.errno)\n      if (pid == 0):    \n         os.chdir(WORKDIR)\n         os.umask(UMASK)\n      else:\n         os._exit(0)    \n   else:\n      os._exit(0)   \n   import resource      \n   maxfd = resource.getrlimit(resource.RLIMIT_NOFILE)[1]\n   if (maxfd == resource.RLIM_INFINITY):\n      maxfd = MAXFD\n   os.open(REDIRECT_TO, os.O_RDWR)  \n   os.dup2(0, 1)            \n   os.dup2(0, 2)            \n   return(0)", "contrast": "Here's one way to implement the function:\nimport os\nimport sys\ndef detach_process_from_terminal():\n    pid = os.fork()\n    if pid > 0:\n        sys.exit(0)\n    os.setsid()\n    pid = os.fork()\n    if pid > 0:\n        sys.exit(0)\n    os.chdir('/')\n    for fd in range(0, 3):\n        try:\n            os.close(fd)\n        except OSError:\n            pass\n    os.open('/dev/null', os.O_RDWR)  \n    os.dup2(0, 1)  \n    os.dup2(0, 2)  ", "label": 0}
{"index": "gp083064", "code": "def _invertMapping(param_0):\n    var_0 = ddict(set)\n    for var_1, var_2 in viewitems(param_0):\n        for var_3 in var_2:\n            var_0[var_3].add(var_1)\n    return var_0", "contrast": "def convert_mapping(mapping):\n    inverted_mapping = {}\n    for key, values in mapping.items():\n        for value in values:\n            if value not in inverted_mapping:\n                inverted_mapping[value] = set()\n            inverted_mapping[value].add(key)\n    return inverted_mapping", "label": 0}
{"index": "gp269429", "code": "from datetime import datetime\nfrom flask import request, make_response\nfrom werkzeug.exceptions import NotModified, UnprocessableEntity\ndef get_record(param_0, param_1, param_2):\n    if not read_permission_factory.can():\n        raise UnprocessableEntity(\"Unable to read record\")\n    var_0 = record.get_etag()\n    var_1 = record.get_last_modified()\n    try:\n        var_2 = datetime.strptime(request.headers['If-Modified-Since'], '%a, %d %b %Y %H:%M:%S %Z')\n        if var_2 >= var_1:\n            raise NotModified\n    except (KeyError, ValueError):\n        pass\n    if request.headers.get('If-None-Match') == var_0:\n        raise NotModified\n    var_3 = make_response(record.get_data())\n    response.headers['Content-Type'] = record.get_content_type()\n    response.headers['ETag'] = var_0\n    response.headers['Last-Modified'] = last_modified.strftime('%a, %d %b %Y %H:%M:%S %Z')\n    response.headers['Link'] = '<{}>; rel=\"self\"'.format(request.url)\n    return var_3", "contrast": "def get(self, pid, record, **kwargs):\n        etag = str(record.revision_id)\n        self.check_etag(str(record.revision_id))\n        self.check_if_modified_since(record.updated, etag=etag)\n        return self.make_response(\n            pid, record, links_factory=self.links_factory\n        )", "label": 1}
{"index": "gp174359", "code": "async def read_packet_id(param_0):\n    var_0 = await reader.readexactly(2)\n    var_1 = int.from_bytes(var_0, byteorder='big', signed=False)\n    return var_1", "contrast": "def decode_packet_id(reader) -> int:\n    packet_id_bytes = yield from read_or_raise(reader, 2)\n    packet_id = unpack(\"!H\", packet_id_bytes)\n    return packet_id[0]", "label": 1}
{"index": "gp257870", "code": "def find_or_generate_getter(param_0, param_1, param_2):\n    try:\n        var_0 = getattr(param_0, 'get_' + param_1)\n        if getter_fn.__code__.co_argcount == 1:\n            return var_0\n    except AttributeError:\n        pass\n    def getter(param_0):\n        return getattr(param_0, param_2)\n    return getter", "contrast": "def _get_getter_fun(object_type,           \n                    parameter,             \n                    private_property_name  \n                    ):\n    property_name = parameter.name\n    overridden_getters = getmembers(object_type, predicate=_has_annotation(__GETTER_OVERRIDE_ANNOTATION, property_name))\n    if len(overridden_getters) > 0:\n        if len(overridden_getters) > 1:\n            raise DuplicateOverrideError('Getter is overridden more than once for attribute name : ' + property_name)\n        getter_fun = overridden_getters[0][1]\n        s = signature(getter_fun)\n        if not ('self' in s.parameters.keys() and len(s.parameters.keys()) == 1):\n            raise IllegalGetterSignatureException('overridden getter must only have a self parameter, found ' +\n                                                  str(len(s.parameters.items()) - 1) + ' for function ' + str(\n                getter_fun.__qualname__))\n        property_obj = property(getter_fun)\n    else:\n        def autoprops_generated_getter(self):\n            return getattr(self, private_property_name)\n        getter_fun = autoprops_generated_getter\n        try:\n            annotations = getter_fun.__annotations__\n        except AttributeError:\n            pass\n        else:\n            annotations['return'] = parameter.annotation  \n    return getter_fun", "label": 1}
{"index": "gp165316", "code": "def i2c_write_request(param_0, param_1, param_2):\n        var_0 = asyncio.ensure_future(self.core.i2c_write_request(param_1, param_2))\n        self.loop.run_until_complete(var_0)", "contrast": "import smbus\ndef write_i2c(address, *args):\n    bus = smbus.SMBus(1)\n    bus.write_i2c_block_data(address, 0, list(args))", "label": 0}
{"index": "gp269789", "code": "def create_chunked_ending():\n    return b\"\\r\\n0\\r\\n\\r\\n\"", "contrast": "def create_chunked_body_end(trailers=None):\n    chunk = []\n    chunk.append('0\\r\\n')\n    if trailers:\n        for name, value in trailers:\n            chunk.append(name)\n            chunk.append(': ')\n            chunk.append(value)\n            chunk.append('\\r\\n')\n    chunk.append('\\r\\n')\n    return s2b(''.join(chunk))", "label": 1}
{"index": "gp103032", "code": "def _update_version(param_0, param_1):\n    if connection.engine.name == 'sqlite':\n        connection.execute('PRAGMA user_version = {}'.format(param_1))\n    elif connection.engine.name == 'postgresql':\n        connection.execute(DDL('CREATE SCHEMA IF NOT EXISTS {};'.format(POSTGRES_SCHEMA_NAME)))\n        connection.execute(DDL('CREATE SCHEMA IF NOT EXISTS {};'.format(POSTGRES_PARTITION_SCHEMA_NAME)))\n        connection.execute('CREATE TABLE IF NOT EXISTS {}.user_version(version INTEGER NOT NULL);'\n                           .format(POSTGRES_SCHEMA_NAME))\n        if connection.execute('SELECT * FROM {}.user_version;'.format(POSTGRES_SCHEMA_NAME)).fetchone():\n            connection.execute('UPDATE {}.user_version SET version = {};'\n                               .format(POSTGRES_SCHEMA_NAME, param_1))\n        else:\n            connection.execute('INSERT INTO {}.user_version (version) VALUES ({})'\n                               .format(POSTGRES_SCHEMA_NAME, param_1))\n    else:\n        raise DatabaseMissingError('Do not know how to migrate {} engine.'\n                                   .format(connection.engine.driver))", "contrast": "def update_version(connection, version):\n    query = \"UPDATE migrations SET version = :version\"\n    connection.execute(query, version=version)", "label": 0}
{"index": "gp065951", "code": "def _parse_methods(param_0, param_1):\n        if param_1 is None:\n            return APIServer.DEFAULT_METHODS\n        var_0 = list_string.replace(\"'\", '\"')\n        return json.loads(var_0)", "contrast": "import json\ndef get_http_methods():\n    http_methods = [\"GET\", \"HEAD\", \"POST\", \"PUT\", \"DELETE\", \"CONNECT\", \"OPTIONS\", \"TRACE\", \"PATCH\"]\n    return json.dumps(http_methods)", "label": 0}
{"index": "gp132554", "code": "def _randomize_speed(param_0: int, param_1: int = None) -> int:\n        if param_1 is None:\n            var_0 = int(param_0 / 4)\n        else:\n            var_0 = param_1\n        var_1 = MissionWeather._gauss(param_0, var_0)\n        if var_1 < 0:\n            return 0\n        return min(var_1, 50)", "contrast": "import random\nfrom math import exp, sqrt\ndef create_variation(base_speed, sigma):\n    return base_speed * random.gauss(1, sigma)", "label": 0}
{"index": "gp058241", "code": "def download_feed_posts(param_0, param_1: int = None, param_2: bool = False,\n                            param_3: Optional[Callable[[Post], bool]] = None) -> None:\n        self.context.log(\"Retrieving pictures from your feed...\")\n        var_0 = 1\n        for var_1 in self.get_feed_posts():\n            if param_1 is not None and var_0 > param_1:\n                break\n            var_2 = post.owner_username\n            if param_3 is not None and not param_3(var_1):\n                self.context.log(\"<pic by %s skipped>\" % var_2, flush=True)\n                continue\n            self.context.log(\"[%3i] %s \" % (var_0, var_2), end=\"\", flush=True)\n            var_0 += 1\n            with self.context.error_catcher('Download feed'):\n                var_3 = self.download_post(var_1, target=':feed')\n                if param_2 and not var_3:\n                    break", "contrast": "from instaloader import Instaloader\ndef download_user_feed(max_count=20, fast_update=True, post_filter=lambda post: post.viewer_has_liked):\n    loader = Instaloader()\n    loader.load_session_from_file('USER')\n    loader.download_feed_posts(max_count=max_count, fast_update=fast_update, post_filter=post_filter)", "label": 0}
{"index": "gp160522", "code": "def _ModifyInterface(\n      param_0, param_1, param_2, param_3, param_4=False):\n    var_0 = '%s=%s' % (param_2, param_3)\n    if not open(param_1).read().count(param_2):\n      with open(param_1, 'a') as var_1:\n        config.write('%s\\n' % var_0)\n    elif param_4:\n      for var_2 in fileinput.input(param_1, inplace=True):\n        print(re.sub(r'%s=.*' % param_2, var_0, line.rstrip()))", "contrast": "import configparser\ndef write_config_value(interface_config, config_key, config_value, replace):\n    parser = configparser.ConfigParser()\n    parser.read(interface_config)\n    if not parser.has_section(config_key):\n        parser.add_section(config_key)\n    if replace or not parser.has_option(config_key, config_value):\n        parser.set(config_key, config_value)\n        with open(interface_config, 'w') as config_file:\n            parser.write(config_file)", "label": 0}
{"index": "gp139798", "code": "def _in_batches_cmdidx(param_0):\n        var_0 = {}\n        for var_1, var_2 in enumerate(param_0):\n            var_3 = BatchCommand.in_batches_pat.match(var_2)\n            if var_3:\n                var_4 = int(mat.group(1))\n                if var_4 in var_0:\n                    raise IndexError(\n                        'IN_BATCH%d is used multiple times in command below, while IN_BATCH0 - IN_BATCH%d must be used:%s$ %s' %\n                        (var_4, len(var_0) - 1, os.linesep, list2cmdline(param_0)))\n                var_0[var_4] = var_1\n        var_5 = []\n        for var_4 in range(len(var_0)):\n            try:\n                var_1 = var_0[var_4]\n                in_batches_cmdidx.append(var_1)\n            except KeyError:\n                raise IndexError('IN_BATCH%d is not found in command below, while IN_BATCH0 - IN_BATCH%d must be used:%s$ %s' %\n                                 (var_4, len(var_0) - 1, os.linesep, list2cmdline(param_0)))\n        return tuple(var_5)", "contrast": "def get_in_batch_indices(cmd_array):\n    in_batch_count = 0\n    in_batch_indices = []\n    for i, cmd in enumerate(cmd_array):\n        if cmd.startswith(\"IN_BATCH\"):\n            expected = \"IN_BATCH{}\".format(in_batch_count)\n            if cmd != expected:\n                raise IndexError\n            in_batch_count += 1\n            in_batch_indices.append(i)\n    return tuple(in_batch_indices)", "label": 0}
{"index": "gp334916", "code": "def generate_policy(param_0, param_1):\n    var_0 = {\n        'Version': '2012-10-17',\n        'Statement': [\n            {\n                'Effect': 'Allow',\n                'Action': '*',\n                'Resource': '*',\n                'Condition': param_0\n            },\n            {\n                'Effect': 'Deny',\n                'Action': '*',\n                'Resource': '*',\n                'Condition': param_1\n            }\n        ]\n    }\n    return var_0", "contrast": "def build(self):\n        if ((self.allowMethods is None or len(self.allowMethods) == 0) and\n                (self.denyMethods is None or len(self.denyMethods) == 0)):\n            raise NameError('No statements defined for the policy')\n        policy = {\n            'principalId': self.principal_id,\n            'policyDocument': {\n                'Version': self.version,\n                'Statement': []\n            }\n        }\n        policy['policyDocument']['Statement'].extend(\n            self._get_effect_statement('Allow', self.allowMethods))\n        policy['policyDocument']['Statement'].extend(\n            self._get_effect_statement('Deny', self.denyMethods))\n        return policy", "label": 1}
{"index": "gp018680", "code": "def res_block(param_0, param_1:bool=False, param_2:Optional[NormType]=NormType.Batch, param_3:bool=False, **param_4):\n    var_0 = param_2\n    if not param_1 and (param_2==NormType.Batch): var_0 = NormType.BatchZero\n    var_1 = param_0 \n    return SequentialEx(conv_layer(param_0, var_1, param_2=param_2, **param_4),\n                      conv_layer(var_1, param_0, param_2=var_0, **param_4),\n                      MergeLayer(param_1))", "contrast": "def resnet_block(nf, conv_layer, conv_kwargs):\n    block = []\n    block += [conv_layer(nf, **conv_kwargs), nn.ReLU()]\n    block += [conv_layer(nf, **conv_kwargs), nn.ReLU()]\n    return nn.Sequential(*block)", "label": 0}
{"index": "gp336646", "code": "import os\nimport configparser\ndef files_to_ini(param_0, param_1, param_2=None, param_3=None, param_4=None):\n    var_0 = configparser.ConfigParser()\n    for var_1, var_2, var_3 in os.walk(param_0):\n        for var_4 in var_3:\n            if param_3 and fnmatch.fnmatch(var_4, param_3):\n                continue\n            if param_4 and not fnmatch.fnmatch(var_4, param_4):\n                continue\n            var_5 = os.path.join(var_1, var_4)\n            var_6 = var_4\n            if param_2:\n                var_6 = base.strip(\"/\") + \"/\" + var_6\n            var_0[var_6] = {\"hash\": hashlib.sha1(open(var_5, 'rb').read()).hexdigest()}\n    config.write(param_1)", "contrast": "def DumpDirHashToStringIO(directory, stringio, base='', exclude=None, include=None):\n    import fnmatch\n    import os\n    files = [(os.path.join(directory, i), i) for i in os.listdir(directory)]\n    files = [i for i in files if os.path.isfile(i[0])]\n    for fullname, filename in files:\n        if include is not None:\n            if not fnmatch.fnmatch(fullname, include):\n                continue\n        if exclude is not None:\n            if fnmatch.fnmatch(fullname, exclude):\n                continue\n        md5 = Md5Hex(fullname)\n        if base:\n            stringio.write('%s/%s=%s\\n' % (base, filename, md5))\n        else:\n            stringio.write('%s=%s\\n' % (filename, md5))", "label": 1}
{"index": "gp221949", "code": "import sys\nimport os\ndef set_environment():\n    os.environ['SCOOP_MAIN_MODULE'] = __file__\n    sys.argv[0] = __file__\n    sys.path.append(os.path.dirname(__file__))\n    import scoop.MAIN_MODULE", "contrast": "def setupEnvironment(self=None):\n        sys.path.append(os.path.dirname(os.path.abspath(scoop.MAIN_MODULE)))\n        sys.argv = sys.argv[:1]\n        if self:\n            sys.argv += self.args.args\n        try:\n            if scoop.IS_ORIGIN:\n                _ = open(scoop.MAIN_MODULE, 'r')\n                user_module = None\n            else:\n                user_module = importFunction(\n                    \"SCOOP_WORKER\",\n                    scoop.MAIN_MODULE,\n                )\n        except FileNotFoundError as e:\n            sys.stderr.write('{0}\\nFile: {1}\\nIn path: {2}\\n'.format(\n                    str(e),\n                    scoop.MAIN_MODULE,\n                    sys.path[-1],\n                )\n            )\n            sys.stderr.flush()\n            sys.exit(-1)\n        globs = {}\n        try:\n            attrlist = user_module.__all__\n        except AttributeError:\n            attrlist = dir(user_module)\n        for attr in attrlist:\n            globs[attr] = getattr(user_module, attr)\n        if self and scoop.IS_ORIGIN:\n            return {}\n        elif self:\n            return globs\n        return user_module", "label": 1}
{"index": "gp188014", "code": "def reset_auth():\n    if RFID.auth:\n        stop_crypto()", "contrast": "def deauth(self):\n        self.method = None\n        self.key = None\n        self.last_auth = None\n        if self.debug:\n            print(\"Changing auth key and method to None\")\n        if self.rfid.authed:\n            self.rfid.stop_crypto()\n            if self.debug:\n                print(\"Stopping crypto1\")", "label": 1}
{"index": "gp306738", "code": "import importlib\ndef get_class_from_name(param_0, param_1, param_2='facsimile.base.Facsimile'):\n    var_0, param_0 = class_name.rsplit('.', 1)\n    var_1 = importlib.import_module(var_0)\n    var_2 = getattr(var_1, param_0)(param_1)\n    if param_2:\n        var_3, param_2 = base_class.rsplit('.', 1)\n        var_3 = importlib.import_module(var_3)\n        param_2 = getattr(var_3, param_2)\n        for var_4 in dir(param_2):\n            if not attr.startswith('__'):\n                setattr(var_2, var_4, getattr(param_2, var_4))\n    return var_2", "contrast": "def get_cls(project_name, project_data):\n    if project_name:\n        cls = getattr(facsimile.base, project_data.get('class', 'Facsimile'))\n        cls.name = project_name\n    else:\n        cls = facsimile.base.Facsimile\n    return cls", "label": 1}
{"index": "gp127771", "code": "def auto_load_model_menu(param_0):\n        from trionyx.trionyx.apps import BaseConfig\n        var_0 = 0\n        for var_1 in apps.get_app_configs():\n            if not isinstance(var_1, BaseConfig) or getattr(var_1, 'no_menu', False):\n                continue\n            var_2 = app.name.split('.')[-1]\n            var_3 = 0\n            for var_4 in app.get_models():\n                var_5 = models_config.get_config(var_4)\n                if config.menu_exclude:\n                    continue\n                var_6 = None\n                var_7 = '{}/{}'.format(var_2, config.model_name)\n                if config.menu_root:\n                    var_0 += 10\n                    var_8 = var_0\n                    var_6 = config.menu_icon\n                    var_7 = config.model_name\n                else:\n                    var_3 += 10\n                    var_8 = var_3\n                self.add_item(\n                    path=var_7,\n                    name=config.menu_name if config.menu_name else model._meta.verbose_name_plural.capitalize(),\n                    var_0=config.menu_order if config.menu_order else var_8,\n                    icon=var_6,\n                    url=reverse(\n                        \"trionyx:model-list\",\n                        kwargs={\n                            'app': model._meta.app_label,\n                            'model': model._meta.model_name,\n                        }\n                    )\n                )\n            if var_3 > 0:\n                var_0 += 10\n                self.add_item(\n                    path=var_2,\n                    name=getattr(var_1, 'menu_name', app.verbose_name),\n                    icon=getattr(var_1, 'menu_icon', None),\n                    var_0=getattr(var_1, 'menu_order', var_0),\n                )", "contrast": "def auto_load_models():\n    from trionyx.config import ModelConfig\n    configs = ModelConfig.get_all_configs()\n    menu_entries = []\n    for config in configs:\n        entry = dict(\n            menu_name=config.get(\"menu_name\", \"\"),\n            menu_icon=config.get(\"menu_icon\", \"\"),\n            menu_order=config.get(\"menu_order\", \"\"),\n        )\n        menu_entries.append(entry)\n    return menu_entries", "label": 0}
{"index": "gp175452", "code": "import logging\ndef get_logger(param_0):\n    var_0 = logging.getLogger(param_0)\n    if not logger.handlers:\n        logger.addHandler(logging.NullHandler())\n    return var_0", "contrast": "def get_logger(name):\n    if not hasattr(logging.Logger, 'trace'):\n        logging.addLevelName(TRACE_LEVEL, 'TRACE')\n        def trace(self, message, *args, **kwargs):\n            if self.isEnabledFor(TRACE_LEVEL):\n                self._log(TRACE_LEVEL, message, args, **kwargs)\n        logging.Logger.trace = trace\n    log = logging.getLogger(name)\n    if not log.handlers:\n        log.addHandler(logging.NullHandler())\n    return log", "label": 1}
{"index": "gp132227", "code": "def _json_clean(param_0):\n    var_0 = {}\n    var_1 = {}\n    for var_2, var_3 in d.items():\n        if not isinstance(var_2, tuple):\n            var_0[var_2] = var_3\n        else:\n            var_4 = \"c.{}\".format(id(var_2))\n            var_0[var_4] = var_3\n            var_1[var_4] = var_2\n    return (var_0, var_1)", "contrast": "def clean_dict(d):\n    cleaned_dict = {}\n    for key, value in d.items():\n        if isinstance(key, tuple):\n            cleaned_key = str(key)\n        else:\n            cleaned_key = key\n        if isinstance(value, dict):\n            cleaned_dict[cleaned_key] = clean_dict(value)\n        else:\n            cleaned_dict[cleaned_key] = value\n    return cleaned_dict", "label": 0}
{"index": "gp008756", "code": "def register_link(param_0, param_1):\n        var_0 = tuple((var_1, link.initial_hook_value) for var_1 in link.hook_references)\n        for var_2 in var_0:\n            if var_2 in self._record_hooks:\n                link.set_target(target_record=self._record_hooks[var_2].target_record)\n                break\n        else:\n            for var_2 in var_0:\n                if var_2 in self._table_hooks:\n                    link.set_target(target_table=self._table_hooks[var_2])\n                    break\n            else:\n                var_3 = link.source_record.get_field_descriptor(link.source_index)\n                raise FieldValidationError(\n                    f\"No object found with any of given references : {var_0}. \"\n                    f\"{field_descriptor.get_error_location_message(link.initial_hook_value)}\"\n                )\n        if link.source_record not in self._links_by_source:\n            self._links_by_source[link.source_record] = set()\n        self._links_by_source[link.source_record].add(param_1)\n        if link.target not in self._links_by_target:\n            self._links_by_target[link.target] = set()\n        self._links_by_target[link.target].add(param_1)", "contrast": "def check_record_and_index(source_record, index):\n    if source_record is not None and index is not None:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp286924", "code": "from django.core.paginator import Paginator\ndef chunked_delete(param_0, param_1=1000):\n    var_0 = Paginator(param_0, param_1)\n    for var_1 in paginator.page_range:\n        var_2 = paginator.page(var_1)\n        for var_3 in page.object_list.iterator():\n            obj.delete()", "contrast": "def _delete_chunked(queryset, chunk_size=500):\n        while True:\n            with transaction.atomic():\n                offset = queryset.order_by('pk')[:chunk_size].count()\n                if not offset:\n                    break\n                last_instance = queryset.order_by('pk')[offset - 1]\n                queryset.filter(pk__lte=last_instance.pk).delete()", "label": 1}
{"index": "gp253257", "code": "import json\nfrom molotov import request\nasync def request_json(*param_0, **param_1):\n    var_0 = await request(*param_0, **param_1)\n    return json.loads(response.text)", "contrast": "def json_request(endpoint, verb='GET', session_options=None, **options):\n    req = functools.partial(_request, endpoint, verb, session_options,\n                            json=True, **options)\n    return _run_in_fresh_loop(req)", "label": 1}
{"index": "gp252363", "code": "from typing import List, Callable\ndef chain_functions(param_0: List[Callable]) -> Callable:\n    def chained_function(param_0):\n        for var_0 in param_0:\n            param_0 = var_0(param_0)\n        return param_0\n    return chained_function", "contrast": "def chain_functions(functions):\n    functions = list(functions)\n    if not functions:\n        return _no_op\n    elif len(functions) == 1:\n        return functions[0]\n    else:\n        return partial(reduce, lambda res, f: f(res), functions)", "label": 1}
{"index": "gp190366", "code": "def get_network_envelope(param_0, param_1):\n    var_0 = []\n    if param_1:\n        var_0 = [1] * param_0\n    else:\n        for var_1 in range(param_0):\n            if var_1 <= param_0 // 2:\n                envelope.append(float((var_1 + 1) / (param_0 // 2 + 1)))\n            else:\n                envelope.append(float((param_0 - var_1) / (param_0 // 2 + 1)))\n    return var_0", "contrast": "def create_envelope(periodic,N):\n  kappa = 0.3; \n  a0 = 30;    \n  if periodic==0:\n      A = np.zeros(N)\n      for m in range(N):\n          r = np.abs(m-N/2);\n          if r<kappa*N:\n              A[m] = 1\n          else:\n              A[m] = np.exp(-a0*((r-kappa*N)/((1-kappa)*N))**2)\n  else:\n      A = np.ones((1,N));\n  return A", "label": 1}
{"index": "gp115121", "code": "def set_distribute_compositions(param_0, param_1):\n        if self.get_distribute_compositions_metadata().is_read_only():\n            raise errors.NoAccess()\n        if not self._is_valid_boolean(param_1):\n            raise errors.InvalidArgument()\n        self._my_map['distributeCompositions'] = param_1", "contrast": "def set_distribution_rights(distribute_comps):\n    if not isinstance(distribute_comps, bool):\n        raise InvalidArgument(\"distribute_comps is invalid\")\n    distribute_verbatim = True\n    return distribute_verbatim", "label": 0}
{"index": "gp248565", "code": "def is_valid_prefix(param_0: str) -> bool:\n    if path.startswith(\"/\") and path.endswith(\"/\"):\n        return True\n    else:\n        return False", "contrast": "def is_prefix(cls, path):\n        lagofile = paths.Paths(path).prefix_lagofile()\n        return os.path.isfile(lagofile)", "label": 1}
{"index": "gp176214", "code": "def set_realized_target_count(param_0, param_1):\n    run_tracker.daemon_stats['realized_target_count'] = param_1", "contrast": "def _set_affected_target_count_in_runtracker(self):\n    target_count = len(self.build_graph)\n    self.run_tracker.pantsd_stats.set_affected_targets_size(target_count)\n    return target_count", "label": 1}
{"index": "gp146296", "code": "def _load_file(param_0, param_1):\n        param_1 = os.path.abspath(os.path.expanduser(param_1))\n        if not os.path.isfile(param_1):\n            raise Exception('File %s does not exist' % param_1)\n        var_0 = vtki.get_ext(param_1)\n        if var_0 == '.ply':\n            var_1 = vtk.vtkPLYReader()\n        elif var_0 == '.stl':\n            var_1 = vtk.vtkSTLReader()\n        elif var_0 == '.vtk':\n            var_1 = vtk.vtkPolyDataReader()\n        elif var_0 == '.vtp':\n            var_1 = vtk.vtkXMLPolyDataReader()\n        elif var_0 == '.obj':\n            var_1 = vtk.vtkOBJReader()\n        else:\n            raise TypeError('Filetype must be either \"ply\", \"stl\", \"vtk\", \"vtp\", or \"obj\".')\n        reader.SetFileName(param_1)\n        reader.Update()\n        self.ShallowCopy(reader.GetOutput())\n        if not np.any(self.points):\n            raise AssertionError('Empty or invalid file')", "contrast": "import os\nimport trimesh\ndef load_mesh_from_file(filename):\n    extension = os.path.splitext(filename)[1].lower()\n    valid_extensions = ['.ply', '.stl', '.vtk']\n    if extension not in valid_extensions:\n        raise ValueError(f'Invalid file type: {extension}')\n    try:\n        mesh = trimesh.load(filename)\n    except ValueError as e:\n        raise ValueError(f'Could not load mesh from file {filename}: {e}')\n    return mesh", "label": 0}
{"index": "gp075538", "code": "def sign(param_0, param_1=None, param_2=None, param_3=None):\n    if param_3 is None:\n        param_3 = Session()\n    else:\n        param_3 = param_3\n    if param_2 is None:\n        param_2 = date.today()\n    else:\n        param_2 = param_2\n    var_0 = (\n        session\n        .query(User)\n        .filter(User.user_id == param_0)\n        .one_or_none()\n    )\n    if var_0:\n        var_1 = (\n            user\n            .entries\n            .filter(Entry.date == param_2)\n            .filter(Entry.time_out.is_(None))\n            .all()\n        )\n        if not var_1:\n            var_2 = sign_in(var_0, param_1=param_1)\n            session.add(var_2)\n            var_3 = Status(\n                valid=True,\n                in_or_out='in',\n                user_name=get_user_name(var_0),\n                param_1=new_entry.user_type,\n                entry=var_2\n            )\n        else:\n            for var_4 in var_1:\n                var_5 = sign_out(var_4)\n                session.add(var_5)\n                var_3 = Status(\n                    valid=True,\n                    in_or_out='out',\n                    user_name=get_user_name(var_0),\n                    param_1=signed_out_entry.user_type,\n                    var_4=var_5\n                )\n        session.commit()\n    else:\n        raise UnregisteredUser(\n            '{} not registered. Please register at the front desk.'.format(\n                param_0\n            )\n        )\n    logger.debug(var_3)\n    return var_3", "contrast": "from datetime import datetime\nfrom collections import namedtuple\nStatus = namedtuple('Status', ['success', 'message'])\ndef sign_user(user_id, user_type=None, today=None, session=None):\n    if not isinstance(user_id, int):\n        return Status(False, 'Invalid user id')\n    if today is None:\n        today = datetime.now().date()\n    if session is None:\n        session = get_session()\n    user = session.query(User).filter(User.id == user_id).first()\n    if user is None:\n        return Status(False, 'User does not exist with given id')\n    if user.signed_in:\n        user.signed_in = False\n        session.commit()\n        return Status(True, 'User signed out successfully')\n    else:\n        user.signed_in = True\n        user.last_login = today\n        session.commit()\n        return Status(True, 'User signed in successfully')", "label": 0}
{"index": "gp084381", "code": "def census(param_0, *param_1):\n        var_0 = {'mode': 'score+rank+rrank+prank+prrank'}\n        if param_1:\n            var_0['scale'] = '+'.join(str(var_1) for var_1 in param_1)\n        @api_query('census', **var_0)\n        async def result(param_0, param_1):\n            return [\n                CensusScaleCurrent(var_0)\n                for var_0 in root.find('CENSUS')\n            ]\n        return result(param_0)", "contrast": "def census(*scales):\n    if not scales:\n        scales = [81]\n    url = f\"https://www.nationstates.net/cgi-bin/api.cgi?q=census;scale={','.join(map(str, scales))}\"\n    response = requests.get(url)\n    root = ET.fromstring(response.text)\n    return ApiQuery([CensusScaleCurrent(child) for child in root.findall(\".//SCALE\")])", "label": 0}
{"index": "gp306591", "code": "def post_to_all_outputs(*param_1, param_0=''):\n    return new_record", "contrast": "def send(\n            self,\n            *args: str,\n            text: str=None,\n    ) -> IterationRecord:\n        if text is not None:\n            final_text = text\n        else:\n            if len(args) == 0:\n                raise BotSkeletonException((\"Please provide text either as a positional arg or \"\n                                            \"as a keyword arg (text=TEXT)\"))\n            else:\n                final_text = args[0]\n        record = IterationRecord(extra_keys=self.extra_keys)\n        for key, output in self.outputs.items():\n            if output[\"active\"]:\n                self.log.info(f\"Output {key} is active, calling send on it.\")\n                entry: Any = output[\"obj\"]\n                output_result = entry.send(text=final_text)\n                record.output_records[key] = output_result\n            else:\n                self.log.info(f\"Output {key} is inactive. Not sending.\")\n        self.history.append(record)\n        self.update_history()\n        return record", "label": 1}
{"index": "gp046450", "code": "def find_pids(param_0, param_1, param_2, param_3, param_4=True):\n        if not self.should_refresh_pid_cache(param_1):\n            return self.pid_cache[param_1]\n        var_0 = self.log.debug\n        if not param_4:\n            var_0 = self.log.error\n        var_1 = self.should_refresh_ad_cache(param_1)\n        var_2 = set()\n        for var_3 in psutil.process_iter():\n            if not var_1 and proc.pid in self.ad_cache:\n                continue\n            var_4 = False\n            for var_5 in param_2:\n                try:\n                    if var_5 == 'All':\n                        var_4 = True\n                    if param_3:\n                        if os.name == 'nt':\n                            if proc.name().lower() == string.lower():\n                                var_4 = True\n                        else:\n                            if proc.name() == var_5:\n                                var_4 = True\n                    else:\n                        var_6 = proc.cmdline()\n                        if os.name == 'nt':\n                            var_7 = string.lower()\n                            if re.search(var_7, ' '.join(var_6).lower()):\n                                var_4 = True\n                        else:\n                            if re.search(var_5, ' '.join(var_6)):\n                                var_4 = True\n                except psutil.NoSuchProcess:\n                    self.log.warning('Process disappeared while scanning')\n                except psutil.AccessDenied as e:\n                    var_0('Access denied to process with PID {}'.format(proc.pid))\n                    var_0('Error: {}'.format(e))\n                    if var_1:\n                        self.ad_cache.add(proc.pid)\n                    if not param_4:\n                        raise\n                else:\n                    if var_1:\n                        self.ad_cache.discard(proc.pid)\n                    if var_4:\n                        matching_pids.add(proc.pid)\n                        break\n        self.pid_cache[param_1] = var_2\n        self.last_pid_cache_ts[param_1] = time.time()\n        if var_1:\n            self.last_ad_cache_ts[param_1] = time.time()\n        return var_2", "contrast": "import psutil\ndef find_pids(search_string):\n    pids = set()\n    for process in psutil.process_iter(['pid', 'name']):\n        if search_string in process.info['name']:\n            pids.add(process.info['pid'])\n    return pids", "label": 0}
{"index": "gp285581", "code": "def merge_streams(param_0):\n    var_0 = []\n    for var_1 in param_0:\n        result.extend(var_1)\n    return var_0", "contrast": "def union(self, streamSet):\n        if(not isinstance(streamSet,set)) :\n            raise TypeError(\"The union operator parameter must be a set object\")\n        if(len(streamSet) == 0):\n            return self        \n        op = self.topology.graph.addOperator(\"$Union$\")\n        op.addInputPort(outputPort=self.oport)\n        for stream in streamSet:\n            op.addInputPort(outputPort=stream.oport)\n        oport = op.addOutputPort(schema=self.oport.schema)\n        return Stream(self.topology, oport)", "label": 1}
{"index": "gp151033", "code": "def read_until_close(param_0, param_1=None):\n    while True:\n      try:\n        yield self.read(param_1=param_1)\n      except usb_exceptions.AdbStreamClosedError:\n        break", "contrast": "def yield_data_until_closed(timeout_ms):\n    start_time = time.monotonic()\n    while True:\n        if timeout_ms is not None:\n            elapsed_time = (time.monotonic() - start_time) * 1000\n            time_left_ms = timeout_ms - elapsed_time\n            if time_left_ms <= 0:\n                raise AdbTimeoutError('Timeout waiting for data')\n            else:\n                timeout = PolledTimeout.from_millis(time_left_ms)\n        else:\n            timeout = None\n        try:\n            data = self.read(timeout)\n            if data:\n                yield data\n            else:\n                return\n        except AdbConnectionResetError as e:\n            raise AdbConnectionResetError(f'Read failed: {e!r}')\n        except AdbTimeoutError as e:\n            raise AdbTimeoutError(f'Read timed out: {e!r}')\n        except IOError as e:\n            raise AdbError(f'Read failed: {e!r}') from e", "label": 0}
{"index": "gp275976", "code": "import numpy as np\ndef make_wcs_mapping(param_0, param_1):\n    var_0, var_1 = wcs.array_shape\n    var_2 = np.full((var_0, var_1), -1, dtype=int)\n    var_3 = np.ones((var_0, var_1))\n    var_4 = (var_0, var_1)\n    for var_5 in range(var_0):\n        for var_6 in range(var_1):\n            var_7, var_8, var_9 = wcs.pixel_to_world_values(var_6 + 0.5, var_5 + 0.5, 0)\n            var_10 = hpx.lonlat_to_healpix(var_7, var_8)\n            var_2[var_5, var_6] = var_10\n    return var_2, var_3, var_4", "contrast": "def make_hpx_to_wcs_mapping_centers(hpx, wcs):\n    npix = (int(wcs.wcs.crpix[0] * 2), int(wcs.wcs.crpix[1] * 2))\n    mult_val = np.ones(npix).T.flatten()\n    sky_crds = hpx.get_sky_coords()\n    pix_crds = wcs.wcs_world2pix(sky_crds, 0).astype(int)\n    ipixs = -1 * np.ones(npix, int).T.flatten()\n    pix_index = npix[1] * pix_crds[0:, 0] + pix_crds[0:, 1]\n    if hpx._ipix is None:\n        for ipix, pix_crd in enumerate(pix_index):\n            ipixs[pix_crd] = ipix\n    else:\n        for pix_crd, ipix in zip(pix_index, hpx._ipix):\n            ipixs[pix_crd] = ipix\n    ipixs = ipixs.reshape(npix).T.flatten()\n    return ipixs, mult_val, npix", "label": 1}
{"index": "gp284536", "code": "def send_apdu(param_0):\n    return response_apdu", "contrast": "def InternalSendApdu(self, apdu_to_send):\n    response = None\n    if not self.use_legacy_format:\n      response = apdu.ResponseApdu(self.transport.SendMsgBytes(\n          apdu_to_send.ToByteArray()))\n      if response.sw1 == 0x67 and response.sw2 == 0x00:\n        self.use_legacy_format = True\n        return self.InternalSendApdu(apdu_to_send)\n    else:\n      response = apdu.ResponseApdu(self.transport.SendMsgBytes(\n          apdu_to_send.ToLegacyU2FByteArray()))\n    return response", "label": 1}
{"index": "gp112565", "code": "def itemfreq(param_0):\n    var_0 = pstat.unique(param_0)\n    scores.sort()\n    var_1 = []\n    for var_2 in var_0:\n        freq.append(inlist.count(var_2))\n    return zip(var_0, var_1)", "contrast": "def litemfreq(inlist):\n    freq_dict = {}\n    for score in inlist:\n        if score in freq_dict:\n            freq_dict[score] += 1\n        else:\n            freq_dict[score] = 1\n    freq_table = [[score, freq_dict[score]] for score in freq_dict]\n    freq_table.sort()\n    freq_table.append(['Total', len(inlist)])\n    return freq_table", "label": 0}
{"index": "gp305929", "code": "def create_tweet_listener():\n    class TweetListener(tweepy.StreamListener):\n        def on_status(param_0, param_1):\n            print(status.text)\n    var_0 = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n    auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n    var_1 = TweetListener()\n    var_2 = tweepy.Stream(var_0=var_0, var_1=var_1)\n    return var_2", "contrast": "def construct_listener(outfile=None):\n    if outfile is not None:\n        if os.path.exists(outfile):\n            raise IOError(\"File %s already exists\" % outfile)\n        outfile = open(outfile, 'wb')\n    return PrintingListener(out=outfile)", "label": 1}
{"index": "gp312439", "code": "def munin_plugin_fetch():\n    var_0 = [1, 2, 3, 4, 5]  \n    for var_1 in var_0:\n        print(var_1)", "contrast": "def fetch(self):\n        self.retrieveVals()\n        for parent_name in self._graphNames:\n            graph = self._graphDict[parent_name]\n            if self.isMultigraph:\n                print \"multigraph %s\" % self._getMultigraphID(parent_name)\n            print self._formatVals(graph.getVals())\n            print\n        if (self.isMultigraph and self._nestedGraphs \n            and self._subgraphDict and self._subgraphNames):\n            for (parent_name, subgraph_names) in self._subgraphNames.iteritems():\n                for graph_name in subgraph_names:\n                    graph = self._subgraphDict[parent_name][graph_name]\n                    print \"multigraph %s\" % self.getMultigraphID(parent_name, \n                                                                 graph_name)\n                    print self._formatVals(graph.getVals())\n                    print\n        return True", "label": 1}
{"index": "gp165172", "code": "def send(param_0):\n        for var_0, var_1 in iteritems(param_0):\n            var_2 = c_int(0)\n            gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM, byref(var_2))\n            if shader_id.value == 0:\n                raise UnboundLocalError(\"\"\"Shader not bound to OpenGL context--uniform cannot be sent.\n                ------------ Tip -------------\n                with ratcave.default_shader:\n                    mesh.draw()\n                ------------------------------\n                \"\"\")\n            try:\n                var_3, var_4 = array.loc\n                if shader_id.value != var_4:\n                    raise Exception('Uniform location bound to a different shader')\n            except (AttributeError, Exception) as e:\n                array.loc = (gl.glGetUniformLocation(shader_id.value, name.encode('ascii')), shader_id.value)\n            if array.ndim == 2:  \n                try:\n                    var_5 = array.pointer\n                except AttributeError:\n                    array.pointer = array.ctypes.data_as(POINTER(c_float * 16)).contents\n                    var_5 = array.pointer\n                gl.glUniformMatrix4fv(array.loc[0], 1, True, var_5)\n            else:\n                var_6 = self._sendfuns[array.dtype.kind][len(var_1) - 1]  \n                var_6(array.loc[0], *var_1)", "contrast": "def send_uniforms_to_graphics_card(uniforms_dict):\n    for key, value in uniforms_dict.items():\n        send_uniform_variable_to_graphics_card(key, value)", "label": 0}
{"index": "gp085331", "code": "def list_all_categories(param_0, **param_1):\n        param_1['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._list_all_categories_with_http_info(**param_1)\n        else:\n            (var_0) = cls._list_all_categories_with_http_info(**param_1)\n            return var_0", "contrast": "def list_categories(async=False, page=None, size=None, sort=None):\n    if async:\n        thread = api.list_all_categories(async=True)\n        result = thread.get()\n        return result\n    else:\n        pass", "label": 0}
{"index": "gp293017", "code": "def isCLIExecutable(filename):\ndef getCLIExecutables(baseDir):\n    paths = []\n    for file in os.listdir(baseDir):\n        path = os.path.join(baseDir, file)\n        if isCLIExecutable(path):\n            paths.append(path)\n    return paths", "contrast": "def listCLIExecutables(baseDir):\n    return [path for path in glob.glob(os.path.join(os.path.normpath(baseDir), '*'))\n            if isCLIExecutable(path)]", "label": 1}
{"index": "gp277526", "code": "from xml.etree.ElementTree import Element, ElementTree\nfrom typing import Union\nclass SubSection:\n    ALLOWED_SUBSECTIONS = {'subsection1', 'subsection2', 'subsection3'}\n    def __init__(param_0, param_1: Union[str, None], param_2: Union[str, None]):\n        self.md_ref = param_1\n        self.md_wrap = param_2\n    @staticmethod\n    def from_xml(param_0: Union[Element, ElementTree]) -> 'SubSection':\n        if root.tag not in SubSection.ALLOWED_SUBSECTIONS:\n            raise exceptions.ParseError(\"root's tag is not in allowed sub-sections\")\n        if param_0[0].tag != 'mdRef' and param_0[0].tag != 'mdWrap':\n            raise exceptions.ParseError(\"the first child of root is not mdRef or mdWrap\")\n        return SubSection(param_0[0].get('href'), param_0[0].text if param_0[0].tag == 'mdWrap' else None)", "contrast": "def parse(cls, root):\n        subsection = root.tag.replace(utils.lxmlns(\"mets\"), \"\", 1)\n        if subsection not in cls.ALLOWED_SUBSECTIONS:\n            raise exceptions.ParseError(\n                \"SubSection can only parse elements with tag in %s with METS namespace\"\n                % (cls.ALLOWED_SUBSECTIONS,)\n            )\n        section_id = root.get(\"ID\")\n        created = root.get(\"CREATED\", \"\")\n        status = root.get(\"STATUS\", \"\")\n        child = root[0]\n        if child.tag == utils.lxmlns(\"mets\") + \"mdWrap\":\n            mdwrap = MDWrap.parse(child)\n            obj = cls(subsection, mdwrap, section_id)\n        elif child.tag == utils.lxmlns(\"mets\") + \"mdRef\":\n            mdref = MDRef.parse(child)\n            obj = cls(subsection, mdref, section_id)\n        else:\n            raise exceptions.ParseError(\n                \"Child of %s must be mdWrap or mdRef\" % subsection\n            )\n        obj.created = created\n        obj.status = status\n        return obj", "label": 1}
{"index": "gp194138", "code": "def interpolate_timestamps(param_0):\n    var_0 = []\n    for var_1 in range(len(param_0)):\n        if var_1 > 0 and param_0[var_1] == param_0[var_1-1]:\n            var_2 = param_0[var_1+1] - param_0[var_1]\n            interpolated_timestamps.append(param_0[var_1] + var_2/2)\n        else:\n            interpolated_timestamps.append(param_0[var_1])\n    return var_0", "contrast": "def interpolate_timestamp(capture_times):\n    timestamps = []\n    num_file = len(capture_times)\n    time_dict = OrderedDict()\n    if num_file < 2:\n        return capture_times\n    time_dict = OrderedDict()\n    for i, t in enumerate(capture_times):\n        if t not in time_dict:\n            time_dict[t] = {\n                \"count\": 0,\n                \"pointer\": 0\n            }\n            interval = 0\n            if i != 0:\n                interval = (t - capture_times[i - 1]).total_seconds()\n                time_dict[capture_times[i - 1]][\"interval\"] = interval\n        time_dict[t][\"count\"] += 1\n    if len(time_dict) >= 2:\n        time_dict[time_dict.keys()[-1]\n                  ][\"interval\"] = time_dict[time_dict.keys()[-2]][\"interval\"]\n    else:\n        time_dict[time_dict.keys()[0]][\"interval\"] = time_dict[time_dict.keys()[\n            0]][\"count\"] * 1.\n    for t in capture_times:\n        d = time_dict[t]\n        s = datetime.timedelta(\n            seconds=d[\"pointer\"] * d[\"interval\"] / float(d[\"count\"]))\n        updated_time = t + s\n        time_dict[t][\"pointer\"] += 1\n        timestamps.append(updated_time)\n    return timestamps", "label": 1}
{"index": "gp292104", "code": "class intrange:\n    def __init__(param_0, param_1, param_2):\n        self.start = param_1\n        self.end = param_2\n    def endsbefore(param_0, param_1):\n        if isinstance(param_1, int):\n            return self.end <= param_1\n        elif isinstance(param_1, intrange):\n            return self.end <= other.end\n        else:\n            raise TypeError(\"other must be an integer or an intrange object\")", "contrast": "def endsbefore(self, other):\n        if self.is_valid_range(other):\n            if self.upper == other.upper:\n                return not self.upper_inc or other.upper_inc\n            elif self.upper_inf:\n                return False\n            elif other.upper_inf:\n                return True\n            else:\n                return self.upper <= other.upper\n        elif self.is_valid_scalar(other):\n            return self.upper <= other\n        else:\n            raise TypeError(\n                \"Unsupported type to test for ends before '{}'\".format(\n                    other.__class__.__name__))", "label": 1}
{"index": "gp105978", "code": "def get_breakpoint_graph(param_0, param_1=True):\n        var_0 = BreakpointGraph()\n        var_1 = None\n        var_2 = {}\n        for var_3 in param_0:\n            var_3 = line.strip()\n            if len(var_3) == 0:\n                continue\n            if GRIMMReader.is_genome_declaration_string(data_string=var_3):\n                var_1 = GRIMMReader.parse_genome_declaration_string(data_string=var_3)\n                var_2 = {}\n            elif GRIMMReader.is_comment_string(data_string=var_3):\n                if GRIMMReader.is_comment_data_string(string=var_3):\n                    var_4, (var_5, var_6) = GRIMMReader.parse_comment_data_string(comment_data_string=var_3)\n                    if len(var_4) > 0 and var_4[0] == \"fragment\":\n                        add_to_dict_with_path(destination_dict=var_2, var_5=var_5, var_6=var_6, var_4=var_4)\n                else:\n                    continue\n            elif var_1 is not None:\n                var_7 = GRIMMReader.parse_data_string(data_string=var_3)\n                var_8 = GRIMMReader.get_edges_from_parsed_data(var_7=var_7)\n                for var_9, var_10 in var_8:\n                    var_11 = {\n                        \"fragment\": {\n                            \"forward_orientation\": (var_9, var_10)\n                        }\n                    }\n                    var_12 = BGEdge(vertex1=var_9, vertex2=var_10, multicolor=Multicolor(var_1), data=deepcopy(var_2))\n                    edge.update_data(source=var_11)\n                    result.add_bgedge(bgedge=var_12,\n                                      merge=param_1)\n        return var_0", "contrast": "from typing import IO, List\nfrom bg import BreakpointGraph\ndef transform_gene_order_data(merge_edges: bool, stream: IO[str]) -> BreakpointGraph:\n    gene_order_data = [line.strip() for line in stream if line.strip()]\n    block_sizes = [len(block) for block in gene_order_data]\n    block_ends = [sum(block_sizes[:i]) + size for i, size in enumerate(block_sizes)]\n    adjacencies = []\n    for i in range(len(gene_order_data) - 1):\n        adjacencies.append((block_ends[i], -block_ends[i+1]))\n        adjacencies.append((block_ends[i+1], -block_ends[i]))\n    return BreakpointGraph(adjacencies, merge_edges)", "label": 0}
{"index": "gp261399", "code": "def remove_keys(param_0, *param_1):\n    var_0 = dct.copy()\n    for var_1 in param_1:\n        new_dct.pop(var_1, None)\n    return var_0", "contrast": "def _delete_keys(dct, keys):\n    c = deepcopy(dct)\n    assert isinstance(keys, list)\n    for k in keys:\n        c.pop(k)\n    return c", "label": 1}
{"index": "gp164839", "code": "def main(param_0=None):\n    if param_0 is None:\n        param_0 = sys.argv\n    var_0 = argparse.ArgumentParser(\n        fromfile_prefix_chars='@',\n        description=\n        \"\"\"Execute a command with resource limits and measurements.\n           Command-line parameters can additionally be read from a file if file name prefixed with '@' is given as argument.\n           Part of BenchExec: https://github.com/sosy-lab/benchexec/\"\"\")\n    var_1 = parser.add_argument_group(\"optional arguments for resource limits\")\n    resource_args.add_argument(\"--memlimit\", type=util.parse_memory_value, metavar=\"BYTES\",\n        help=\"memory limit in bytes\")\n    resource_args.add_argument(\"--timelimit\", type=util.parse_timespan_value, metavar=\"SECONDS\",\n        help=\"CPU time limit in seconds\")\n    resource_args.add_argument(\"--softtimelimit\", type=util.parse_timespan_value, metavar=\"SECONDS\",\n        help='\"soft\" CPU time limit in seconds (command will be send the TERM signal at this time)')\n    resource_args.add_argument(\"--walltimelimit\", type=util.parse_timespan_value, metavar=\"SECONDS\",\n        help='wall time limit in seconds (default is CPU time limit plus a few seconds)')\n    resource_args.add_argument(\"--cores\", type=util.parse_int_list, metavar=\"N,M-K\",\n        help=\"list of CPU cores to use\")\n    resource_args.add_argument(\"--memoryNodes\", type=util.parse_int_list, metavar=\"N,M-K\",\n        help=\"list of memory nodes to use\")\n    var_2 = parser.add_argument_group(\"optional arguments for run I/O\")\n    io_args.add_argument(\"--input\", metavar=\"FILE\",\n        help=\"name of file used as stdin for command \"\n            \"(default: /dev/null; use - for stdin passthrough)\")\n    io_args.add_argument(\"--output\", default=\"output.log\", metavar=\"FILE\",\n        help=\"name of file where command output (stdout and stderr) is written\")\n    io_args.add_argument(\"--maxOutputSize\", type=util.parse_memory_value, metavar=\"BYTES\",\n        help=\"shrink output file to approximately this size if necessary \"\n            \"(by removing lines from the middle of the output)\")\n    io_args.add_argument(\"--filesCountLimit\", type=int, metavar=\"COUNT\",\n        help=\"maximum number of files the tool may write to (checked periodically, counts only files written in container mode or to temporary directories, only supported with --no-tmpfs)\")\n    io_args.add_argument(\"--filesSizeLimit\", type=util.parse_memory_value, metavar=\"BYTES\",\n        help=\"maximum size of files the tool may write (checked periodically, counts only files written in container mode or to temporary directories, only supported with --no-tmpfs)\")\n    io_args.add_argument(\"--skip-cleanup\", action=\"store_false\", dest=\"cleanup\",\n        help=\"do not delete files created by the tool in temp directory\")\n    var_3 = parser.add_argument_group(\"optional arguments for run container\")\n    var_4 = container_args.add_mutually_exclusive_group()\n    container_on_args.add_argument(\"--container\", action='store_true',\n        help=\"force isolation of run in container (future default starting with BenchExec 2.0)\")\n    container_on_args.add_argument(\"--no-container\", action='store_true',\n        help=\"disable use of containers for isolation of runs (current default)\")\n    containerexecutor.add_basic_container_args(var_3)\n    containerexecutor.add_container_output_args(var_3)\n    var_5 = parser.add_argument_group(\"optional arguments for run environment\")\n    environment_args.add_argument(\"--require-cgroup-subsystem\", action=\"append\", default=[], metavar=\"SUBSYSTEM\",\n        help=\"additional cgroup system that should be enabled for runs \"\n            \"(may be specified multiple times)\")\n    environment_args.add_argument(\"--set-cgroup-value\", action=\"append\", dest=\"cgroup_values\", default=[],\n        metavar=\"SUBSYSTEM.OPTION=VALUE\",\n        help=\"additional cgroup values that should be set for runs (e.g., 'cpu.shares=1000')\")\n    environment_args.add_argument(\"--dir\", metavar=\"DIR\",\n        help=\"working directory for executing the command (default is current directory)\")\n    environment_args.add_argument(\"--user\", metavar=\"USER\",\n        help=\"execute tool under given user account (needs password-less sudo setup, \"\n            \"not supported in combination with --container)\")\n    baseexecutor.add_basic_executor_options(var_0)\n    var_6 = parser.parse_args(param_0[1:])\n    baseexecutor.handle_basic_executor_options(var_6, var_0)\n    if options.container:\n        if options.user is not None:\n            sys.exit(\"Cannot use --user in combination with --container.\")\n        var_7 = containerexecutor.handle_basic_container_args(var_6, var_0)\n        var_8 = containerexecutor.handle_container_output_args(var_6, var_0)\n        if var_7['container_tmpfs'] and (options.filesCountLimit or options.filesSizeLimit):\n            parser.error(\"Files-count limit and files-size limit are not supported if tmpfs is used in container. Use --no-tmpfs to make these limits work or disable them (typically they are unnecessary if a tmpfs is used).\")\n    else:\n        var_7 = {}\n        var_8 = {}\n        if options.user is not None:\n            logging.warning(\n                \"Executing benchmarks at another user with --user is deprecated and may be removed in the future. \"\n                \"Consider using the container mode instead for isolating runs \"\n                \"(cf. https://github.com/sosy-lab/benchexec/issues/215).\")\n        elif not options.no_container:\n            logging.warning(\n                \"Neither --container or --no-container was specified, \"\n                \"not using containers for isolation of runs. \"\n                \"Either specify --no-container to silence this warning, \"\n                \"or specify --container to use containers for better isolation of runs \"\n                \"(this will be the default starting with BenchExec 2.0). \"\n                \"Please read https://github.com/sosy-lab/benchexec/blob/master/doc/container.md \"\n                \"for more information.\")\n    var_9 = {}\n    if len(options.args) == 1 and options.args[0].startswith(\"{\"):\n        var_10 = eval(options.args[0])\n        options.args = var_10[\"args\"]\n        var_9 = data.get(\"env\", {})\n        options.debug = data.get(\"debug\", options.debug)\n        if \"maxLogfileSize\" in var_10:\n            try:\n                options.maxOutputSize = int(var_10[\"maxLogfileSize\"]) * _BYTE_FACTOR * _BYTE_FACTOR \n            except ValueError:\n                options.maxOutputSize = util.parse_memory_value(var_10[\"maxLogfileSize\"])\n    if options.input == '-':\n        var_11 = sys.stdin\n    elif options.input is not None:\n        if options.input == options.output:\n            parser.error(\"Input and output files cannot be the same.\")\n        try:\n            var_11 = open(options.input, 'rt')\n        except IOError as e:\n            parser.error(e)\n    else:\n        var_11 = None\n    var_12 = set(options.require_cgroup_subsystem)\n    var_13 = {}\n    for var_14 in options.cgroup_values:\n        try:\n            var_15, var_16 = arg.split(\"=\", 1)\n            var_17, var_18 = key.split(\".\", 1)\n            if not var_17 or not var_18:\n                raise ValueError()\n        except ValueError:\n            parser.error(\n                'Cgroup value \"{}\" has invalid format, needs to be \"subsystem.option=value\".'\n                    .format(var_14))\n        var_13[(var_17, var_18)] = var_16\n        cgroup_subsystems.add(var_17)\n    var_19 = RunExecutor(user=options.user, cleanup_temp_dir=options.cleanup,\n                           additional_cgroup_subsystems=list(var_12),\n                           use_namespaces=options.container, **var_7)\n    def signal_handler_kill(param_0, param_1):\n        executor.stop()\n    signal.signal(signal.SIGTERM, signal_handler_kill)\n    signal.signal(signal.SIGINT,  signal_handler_kill)\n    var_20 = \" \".join(map(util.escape_string_shell, options.args))\n    logging.info('Starting command %s', var_20)\n    if options.container and options.output_directory and options.result_files:\n        logging.info('Writing output to %s and result files to %s',\n                     util.escape_string_shell(options.output),\n                     util.escape_string_shell(options.output_directory))\n    else:\n        logging.info('Writing output to %s', util.escape_string_shell(options.output))\n    try:\n        var_21 = executor.execute_run(\n                            args=options.args,\n                            output_filename=options.output,\n                            var_11=var_11,\n                            hardtimelimit=options.timelimit,\n                            softtimelimit=options.softtimelimit,\n                            walltimelimit=options.walltimelimit,\n                            cores=options.cores,\n                            memlimit=options.memlimit,\n                            memory_nodes=options.memoryNodes,\n                            cgroupValues=var_13,\n                            environments=var_9,\n                            workingDir=options.dir,\n                            maxLogfileSize=options.maxOutputSize,\n                            files_count_limit=options.filesCountLimit,\n                            files_size_limit=options.filesSizeLimit,\n                            **var_8)\n    finally:\n        if var_11:\n            stdin.close()\n    executor.check_for_new_files_in_home()\n    var_22 = util.ProcessExitCode.from_raw(var_21['exitcode'])\n    def print_optional_result(param_0, param_1=''):\n        if param_0 in var_21:\n            print(param_0 + \"=\" + str(var_21[param_0]).replace(\"'u\", '') + param_1)\n    print_optional_result('terminationreason')\n    print(\"exitcode=\" + str(exit_code.raw))\n    if exit_code.value is not None:\n        print(\"returnvalue=\" + str(exit_code.value))\n    if exit_code.signal is not None:\n        print(\"exitsignal=\" + str(exit_code.signal))\n    print(\"walltime=\" + str(var_21['walltime']) + \"s\")\n    print(\"cputime=\" + str(var_21['cputime']) + \"s\")\n    for var_15 in sorted(result.keys()):\n        if key.startswith('cputime-'):\n            print(\"{}={:.9f}s\".format(var_15, var_21[var_15]))\n    print_optional_result('memory')\n    print_optional_result('blkio-read', 'B')\n    print_optional_result('blkio-write', 'B')\n    var_23 = intel_cpu_energy.format_energy_results(result.get('cpuenergy'))\n    for var_24, var_25 in energy.items():\n        print('{}={}J'.format(var_24, var_25))", "contrast": "import argparse\nfrom benchexec import runexecutor\ndef main():\n    parser = argparse.ArgumentParser(description='A simple command-line interface for BenchExec runexecutor module')\n    parser.add_argument('xml_file', type=str, help='Path to the XML file containing the benchmark definition')\n    parser.add_argument('tool', type=str, help='Name of the tool to run')\n    parser.add_argument('output_dir', type=str, help='Path to the output directory')\n    args = parser.parse_args()\n    run_set = runexecutor.RunSet(args.xml_file, args.tool, args.output_dir)\n    run_executor = runexecutor.RunExecutor()\n    run_executor.execute_run_set(run_set)\nif __name__ == '__main__':\n    main()", "label": 0}
{"index": "gp012529", "code": "def state(param_0):\n        return {'c': self.c, 's0': self.s0, 's1': self.s1, 's2': self.s2}", "contrast": "def get_internal_state():\n    return internal_state", "label": 0}
