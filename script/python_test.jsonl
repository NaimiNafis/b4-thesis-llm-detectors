{"index": "gp333488", "code": "import xml.etree.ElementTree as ET\ndef strip_namespace(root):\n    for elem in root.getiterator():\n        elem.tag = elem.tag.split('}')[-1]\n        ET.cleanup_namespaces(root)\n    return root", "contrast": "def strip_xml_namespace(root):\n    try:\n        root.tag = root.tag.split('}')[1]\n    except IndexError:\n        pass\n    for element in root.getchildren():\n        strip_xml_namespace(element)", "label": 1}
{"index": "gp254519", "code": "def time_to_deplete_stock(V_stock):\n    if V_stock <= 0:\n        return 0\n    else:\n        depletion_rate = 10.0  \n        time_to_deplete = V_stock / depletion_rate\n        return time_to_deplete", "contrast": "def T_stock(self, V_stock):\n        return Stock.T_stock(self, V_stock, self.Q_stock()).to(u.hr)", "label": 1}
{"index": "gp320381", "code": "import cv2\nimport numpy as np\ndef resize_with_keypoints(image, annos, mask, zoom_range):\n    h, w, _ = image.shape\n    zoom_factor = np.random.uniform(*zoom_range)\n    new_h = int(h * zoom_factor)\n    new_w = int(w * zoom_factor)\n    image = cv2.resize(image, (new_w, new_h))\n    annos = np.array(annos) * [new_w / w, new_h / h]\n    if mask is not None:\n        mask = cv2.resize(mask, (new_w, new_h))\n    return image, annos, mask", "contrast": "def keypoint_random_resize(image, annos, mask=None, zoom_range=(0.8, 1.2)):\n    height = image.shape[0]\n    width = image.shape[1]\n    _min, _max = zoom_range\n    scalew = np.random.uniform(_min, _max)\n    scaleh = np.random.uniform(_min, _max)\n    neww = int(width * scalew)\n    newh = int(height * scaleh)\n    dst = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)\n    if mask is not None:\n        mask = cv2.resize(mask, (neww, newh), interpolation=cv2.INTER_AREA)\n    adjust_joint_list = []\n    for joint in annos:  \n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n    if mask is not None:\n        return dst, adjust_joint_list, mask\n    else:\n        return dst, adjust_joint_list, None", "label": 1}
{"index": "gp162698", "code": "def libvlc_media_player_get_chapter_count_for_title(p_mi, i_title):\n    f = _Cfunctions.get('libvlc_media_player_get_chapter_count_for_title', None) or        _Cfunction('libvlc_media_player_get_chapter_count_for_title', ((1,), (1,),), None,\n                    ctypes.c_int, MediaPlayer, ctypes.c_int)\n    return f(p_mi, i_title)", "contrast": "def get_title_chapter_count(p_mi, i_title):\n    if p_mi == None or i_title < 0:\n        return -1\n    try:\n        p_title = p_mi.getTitle(i_title)\n        if not p_title:\n            return -1\n        i_chapter_count = p_title.getChapterCount()\n        return i_chapter_count\n    except:\n        return -1", "label": 0}
{"index": "gp187093", "code": "def depth_dimension_tuple(lst):\n    for i in lst:\n        if isinstance(i, list):\n            yield 1, len(lst)\n            yield from depth_dimension_tuple(i)\n        else:\n            yield 0, len(lst)\n            break", "contrast": "def get_depths_and_dimensions(data, depth):\n    if not isinstance(data, (list, tuple)):\n        return ()\n    yield depth, len(data)\n    for item in data:\n        yield from get_depths_and_dimensions(item, depth + 1)", "label": 1}
{"index": "gp158805", "code": "def _update_context_field_expression(present_locations, expression):\n    no_op_blocks = (ContextField, Literal, LocalField, UnaryTransformation, Variable)\n    if isinstance(expression, BinaryComposition):\n        if isinstance(expression.left, ContextField) or isinstance(expression.right, ContextField):\n            return _update_context_field_binary_composition(present_locations, expression)\n        else:\n            return _simplify_non_context_field_binary_composition(expression)\n    elif isinstance(expression, TernaryConditional):\n        return _simplify_ternary_conditional(expression)\n    elif isinstance(expression, BetweenClause):\n        lower_bound = expression.lower_bound\n        upper_bound = expression.upper_bound\n        if isinstance(lower_bound, ContextField) or isinstance(upper_bound, ContextField):\n            raise AssertionError(u'Found BetweenClause with ContextFields as lower/upper bounds. '\n                                 u'This should never happen: {}'.format(expression))\n        return expression\n    elif isinstance(expression, (OutputContextField, FoldedContextField)):\n        raise AssertionError(u'Found unexpected expression of type {}. This should never happen: '\n                             u'{}'.format(type(expression).__name__, expression))\n    elif isinstance(expression, no_op_blocks):\n        return expression\n    raise AssertionError(u'Found unhandled expression of type {}. This should never happen: '\n                         u'{}'.format(type(expression).__name__, expression))", "contrast": "def simplify_expressions(expr):\n    if isinstance(expr, dict):\n        if 'type' in expr:\n            if expr['type'] == 'ContextField' and 'name' in expr:\n                return {'type': 'TrueLiteral'}\n            elif expr['type'] == 'Expression':\n                expr['body'] = simplify_expressions(expr['body'])\n            elif expr['type'] == 'UnaryExpression' and 'argument' in expr:\n                expr['argument'] = simplify_expressions(expr['argument'])\n    elif isinstance(expr, list):\n        for i in range(len(expr)):\n            expr[i] = simplify_expressions(expr[i])\n    return expr", "label": 0}
{"index": "gp184758", "code": "import json\ndef write_json(file_path: str, js: dict):\n    with open(file_path, 'w', encoding='utf-8') as f:\n        json.dump(js, f, ensure_ascii=False, indent=4)", "contrast": "def _write_plain_json(file_path, js):\n    if file_path.endswith('.bz2'):\n        with bz2.open(file_path, 'wt', encoding=_default_encoding) as f:\n            json.dump(js, f, indent=2, ensure_ascii=False)\n    else:\n        with open(file_path, 'w', encoding=_default_encoding) as f:\n            json.dump(js, f, indent=2, ensure_ascii=False)", "label": 1}
{"index": "gp006931", "code": "def push(self, move):\n        self.move_number += 1\n        captured_piece = self.piece_type_at(move.to_square) if move else NONE\n        self.captured_piece_stack.append(captured_piece)\n        self.move_stack.append(move)\n        if not move:\n            self.turn ^= 1\n            return\n        if move.drop_piece_type:\n            piece_type = move.drop_piece_type\n            from_hand = True\n        else:\n            piece_type = self.piece_type_at(move.from_square)\n            from_hand = False\n            if move.promotion:\n                piece_type = PIECE_PROMOTED[piece_type]\n            self.remove_piece_at(move.from_square, False)\n        self.set_piece_at(move.to_square, Piece(piece_type, self.turn), from_hand, True)\n        self.turn ^= 1\n        self.transpositions.update((self.zobrist_hash(), ))", "contrast": "def update_position(move, board, stack):\n    if move is None:\n        board.turn = not board.turn\n        if board.turn:\n            board.fullmove_number += 1\n        stack.append(board.copy(stack[-1]))\n        return\n    board.push(move)\n    stack.append(board.copy(stack[-1]))\n    if move.is_capture or board.piece_type_at(move.to_square) == chess.PAWN:\n        board.halfmove_clock = 0\n    else:\n        board.halfmove_clock += 1\n    if board.is_en_passant(move):\n        stack.pop()\n        board.pop()\n    board.turn = not board.turn\n    if board.turn:\n        board.fullmove_number += 1", "label": 0}
{"index": "gp063003", "code": "def _set_rp_address(self, v, load=False):\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=YANGListType(\"rp_ip_addr\",rp_address.rp_address, yang_name=\"rp-address\", rest_name=\"rp-address\", parent=self, is_container='list', user_ordered=False, path_helper=self._path_helper, yang_keys='rp-ip-addr', extensions={u'tailf-common': {u'cli-compact-syntax': None, u'cli-suppress-mode': None, u'callpoint': u'PimStaticRpCfgCallpoint', u'info': u'Static RP', u'cli-suppress-list-no': None}}), is_container='list', yang_name=\"rp-address\", rest_name=\"rp-address\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-compact-syntax': None, u'cli-suppress-mode': None, u'callpoint': u'PimStaticRpCfgCallpoint', u'info': u'Static RP', u'cli-suppress-list-no': None}}, namespace='urn:brocade.com:mgmt:brocade-pim', defining_module='brocade-pim', yang_type='list', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': \"\"\"rp_address must be of a type compatible with list\"\"\",\n          'defined-type': \"list\",\n          'generated-type': \"\"\"YANGDynClass(base=YANGListType(\"rp_ip_addr\",rp_address.rp_address, yang_name=\"rp-address\", rest_name=\"rp-address\", parent=self, is_container='list', user_ordered=False, path_helper=self._path_helper, yang_keys='rp-ip-addr', extensions={u'tailf-common': {u'cli-compact-syntax': None, u'cli-suppress-mode': None, u'callpoint': u'PimStaticRpCfgCallpoint', u'info': u'Static RP', u'cli-suppress-list-no': None}}), is_container='list', yang_name=\"rp-address\", rest_name=\"rp-address\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-compact-syntax': None, u'cli-suppress-mode': None, u'callpoint': u'PimStaticRpCfgCallpoint', u'info': u'Static RP', u'cli-suppress-list-no': None}}, namespace='urn:brocade.com:mgmt:brocade-pim', defining_module='brocade-pim', yang_type='list', is_config=True)\"\"\",\n        })\n    self.__rp_address = t\n    if hasattr(self, '_set'):\n      self._set()", "contrast": "def _set_rp_address(self, rp_address):\n    self.__rp_address = rp_address", "label": 0}
{"index": "gp177794", "code": "import paramiko\nimport time\ndef wait_for_ssh(box, port=22, max_attempts=30):\n    attempts = 0\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    while attempts < max_attempts:\n        try:\n            ssh.connect(box, port=port)\n            ssh.close()\n            return attempts\n        except:\n            attempts += 1\n            time.sleep(1)\n    raise Exception(\"Could not connect to box after %d attempts\" % max_attempts)", "contrast": "def _waitForSSHPort(self):\n        logger.debug('Waiting for ssh port to open...')\n        for i in count():\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            try:\n                s.settimeout(a_short_time)\n                s.connect((self.effectiveIP, 22))\n                logger.debug('...ssh port open')\n                return i\n            except socket.error:\n                pass\n            finally:\n                s.close()", "label": 1}
{"index": "gp146461", "code": "def reload_erase_combos(self, btn=None):\n        combo = self.get_widget('backspace-binding-combobox')\n        binding = self.settings.general.get_string('compat-backspace')\n        for i in combo.get_model():\n            if ERASE_BINDINGS.get(i[0]) == binding:\n                combo.set_active_iter(i.iter)\n                break\n        combo = self.get_widget('delete-binding-combobox')\n        binding = self.settings.general.get_string('compat-delete')\n        for i in combo.get_model():\n            if ERASE_BINDINGS.get(i[0]) == binding:\n                combo.set_active_iter(i.iter)\n                break", "contrast": "import subprocess\ndef read_compat_var(var_name):\n    process = subprocess.Popen(['dconf', 'read', '/org/gnome/desktop/input-sources/'+var_name], stdout=subprocess.PIPE)\n    output, error = process.communicate()\n    return output.decode('utf-8').strip()\ndef select_right_option():\n    compat_backspace = read_compat_var('compat-backspace')\n    compat_delete = read_compat_var('compat-delete')\n    if compat_backspace == \"'guess'\":\n        combo_box_backspace.select('guess')\n    elif compat_backspace == \"'ascii'\":\n        combo_box_backspace.select('ascii')\n    if compat_delete == \"'guess'\":\n        combo_box_delete.select('guess')\n    elif compat_delete == \"'ascii'\":\n        combo_box_delete.select('ascii')", "label": 0}
{"index": "gp156942", "code": "def get_anki_phrases_english(limit=None):\n    texts = set()\n    for lang in ANKI_LANGUAGES:\n        df = get_data(lang)\n        phrases = df.eng.str.strip().values\n        texts = texts.union(set(phrases))\n        if limit and len(texts) >= limit:\n            break\n    return sorted(texts)", "contrast": "import sqlite3\ndef get_anki_phrases_english(limit):\n    conn = sqlite3.connect('anki_flashcards.db')  \n    cursor = conn.cursor()\n    cursor.execute(\"SELECT english FROM flashcards LIMIT ?\", (limit,))\n    phrases = [row[0] for row in cursor.fetchall()]\n    conn.close()\n    return phrases", "label": 0}
{"index": "gp052541", "code": "def set_vars(self, *args, **kwargs):\n        kwargs.update(dict(*args))\n        old_values = {vname: self.input.get(vname) for vname in kwargs}\n        self.input.set_vars(**kwargs)\n        if kwargs or old_values:\n            self.history.info(\"Setting input variables: %s\" % str(kwargs))\n            self.history.info(\"Old values: %s\" % str(old_values))\n        return old_values", "contrast": "def set_abinit_variables(input_file_path, abinit_variables_dict):\n    old_abinit_variables_dict = {}\n    with open(input_file_path, 'r') as input_file:\n        input_file_content = input_file.read()\n    for key in abinit_variables_dict:\n        old_abinit_variables_dict[key] = re.search(f\"{key}=(\\S+)\",\n                                                    input_file_content).group(1)\n        input_file_content = re.sub(f\"{key}=(\\S+)\",\n                                    f\"{key}={abinit_variables_dict[key]}\",\n                                    input_file_content)\n    with open(input_file_path, 'w') as input_file:\n        input_file.write(input_file_content)\n    return old_abinit_variables_dict", "label": 0}
{"index": "gp125164", "code": "def rpc_handler(request):\n    global ip\n    if len(request.body):\n        ip = request.META[\"REMOTE_ADDR\"]\n        response = HttpResponse(mimetype=\"application/xml\")\n        data = dispatcher._marshaled_dispatch(request.body)\n        response.write(data)\n    else:\n        response = HttpResponse()\n        response.write(\"<h1>This is an XML-RPC Service.</h2>\")\n        response.write(\"<p>You need to invoke it using an XML-RPC Client!</p>\")\n        response.write(\"The following methods are available:<ul>\")\n        methods = dispatcher.system_listMethods()\n        for method in methods:\n            sig = dispatcher.system_methodSignature(method)\n            help = dispatcher.system_methodHelp(method)\n            response.write(\"<li><b>%s</b>: [%s] %s\" % (method, sig, help))\n        response.write(\"</ul>\")\n    response['Content-length'] = str(len(response.content))\n    return response", "contrast": "from django.http import HttpResponse\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.views.decorators.http import require_POST\nfrom django.utils import six\nfrom xmlrpc.server import SimpleXMLRPCDispatcher\n@csrf_exempt\n@require_POST\ndef actual_handler(request):\n    if len(request.body) == 0:\n        msg = 'This is an XML-RPC Service. You need to invoke it using an XML-RPC Client.'\n        return HttpResponse(msg, content_type='text/plain')\n    try:\n        data = six.BytesIO(request.body)\n        data.seek(0)\n        dispatcher = SimpleXMLRPCDispatcher(allow_none=True, encoding=None)\n        response_string = dispatcher._marshaled_dispatch(\n            data.read().decode('utf-8'))\n        response = HttpResponse(response_string,\n                                content_type='application/xml')\n        return response\n    except Exception as e:\n        return HttpResponse(content='Error: %s' % str(e), status=400)", "label": 0}
{"index": "gp217851", "code": "from functools import wraps\nclass InvalidVersion(Exception):\n    pass\ndef version_checker(min_version=None, max_version=None):\n    if min_version is None and max_version is None:\n        raise ValueError(\"At least one version boundary must be provided\")\n    if min_version and max_version and min_version > max_version:\n        raise ValueError(f\"min_version ({min_version}) cannot be greater than max_version ({max_version})\")\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            version = args[0].api.version\n            if min_version and version < min_version:\n                raise InvalidVersion(f\"Minimum version requirement not met. Server version: {version}, Minimum version: {min_version}\")\n            elif max_version and version > max_version:\n                raise InvalidVersion(f\"Maximum version requirement exceeded. Server version: {version}, Maximum version: {max_version}\")\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "contrast": "def requires_swimlane_version(min_version=None, max_version=None):\n    if min_version is None and max_version is None:\n        raise ValueError('Must provide either min_version, max_version, or both')\n    if min_version and max_version and compare_versions(min_version, max_version) < 0:\n        raise ValueError('min_version must be <= max_version ({}, {})'.format(min_version, max_version))\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            swimlane = self._swimlane\n            if min_version and compare_versions(min_version, swimlane.build_version, True) < 0:\n                raise InvalidSwimlaneBuildVersion(swimlane, min_version, max_version)\n            if max_version and compare_versions(swimlane.build_version, max_version, True) < 0:\n                raise InvalidSwimlaneBuildVersion(swimlane, min_version, max_version)\n            return func(self, *args, **kwargs)\n        return wrapper\n    return decorator", "label": 1}
{"index": "gp329375", "code": "def start_sasl_authentication(username: str, authzid: str, mechanism: str) -> None:\n    pass  ", "contrast": "def _sasl_authenticate(self, stream, username, authzid):\n        if not stream.initiator:\n            raise SASLAuthenticationFailed(\"Only initiating entity start\"\n                                                        \" SASL authentication\")\n        if stream.features is None or not self.peer_sasl_mechanisms:\n            raise SASLNotAvailable(\"Peer doesn't support SASL\")\n        props = dict(stream.auth_properties)\n        if not props.get(\"service-domain\") and (\n                                        stream.peer and stream.peer.domain):\n            props[\"service-domain\"] = stream.peer.domain\n        if username is not None:\n            props[\"username\"] = username\n        if authzid is not None:\n            props[\"authzid\"] = authzid\n        if \"password\" in self.settings:\n            props[\"password\"] = self.settings[\"password\"]\n        props[\"available_mechanisms\"] = self.peer_sasl_mechanisms\n        enabled = sasl.filter_mechanism_list(\n                            self.settings['sasl_mechanisms'], props,\n                                            self.settings['insecure_auth'])\n        if not enabled:\n            raise SASLNotAvailable(\n                                \"None of SASL mechanism selected can be used\")\n        props[\"enabled_mechanisms\"] = enabled\n        mechanism = None\n        for mech in enabled:\n            if mech in self.peer_sasl_mechanisms:\n                mechanism = mech\n                break\n        if not mechanism:\n            raise SASLMechanismNotAvailable(\"Peer doesn't support any of\"\n                                                    \" our SASL mechanisms\")\n        logger.debug(\"Our mechanism: {0!r}\".format(mechanism))\n        stream.auth_method_used = mechanism\n        self.authenticator = sasl.client_authenticator_factory(mechanism)\n        initial_response = self.authenticator.start(props)\n        if not isinstance(initial_response, sasl.Response):\n            raise SASLAuthenticationFailed(\"SASL initiation failed\")\n        element = ElementTree.Element(AUTH_TAG)\n        element.set(\"mechanism\", mechanism)\n        if initial_response.data:\n            if initial_response.encode:\n                element.text = initial_response.encode()\n            else:\n                element.text = initial_response.data\n        stream.write_element(element)", "label": 1}
{"index": "gp071338", "code": "def retry_failed_logs_action(self, request, queryset):\n        count = 0\n        for trigger_log in queryset:\n            retried = _retry_failed_log(trigger_log)\n            if retried:\n                count += 1\n        self.message_user(\n            request,\n            _('{count} failed trigger logs retried.').format(count=count),\n        )", "contrast": "from django_celery_beat.models import PeriodicTask, PeriodicTasks\ndef retry_failed_trigger_actions(queryset):\n    trigger_ids = queryset.values_list('id', flat=True)\n    tasks = PeriodicTask.objects.filter(name__in=trigger_ids)\n    for task in tasks:\n        task.enabled = True\n        task.save()\n    PeriodicTasks.changed()", "label": 0}
{"index": "gp294042", "code": "import os\ndef _load_envars(items):\n    for item in items:\n        key, value = item\n        os.environ[key] = value", "contrast": "def load_envars(self, items):\n    for item in items:\n        envar = item[0]\n        key = item[1]\n        value = self._get_and_update_setting(envar)\n        if value != None:\n            self.data[key] = value\n            self.config.remove_option(self.name, key)", "label": 1}
{"index": "gp040196", "code": "def complete(self):\n        if not self._techniques:\n            return False\n        if not any(tech._is_overriden('complete') for tech in self._techniques):\n            return False\n        return self.completion_mode(tech.complete(self) for tech in self._techniques if tech._is_overriden('complete'))", "contrast": "def is_completed(manager):\n    return manager.status == \"completed\"", "label": 0}
{"index": "gp064741", "code": "def _print_parsed_webpage(\n            self):\n        self.log.debug('starting the ``_print_parsed_webpage()`` method')\n        from polyglot import htmlCleaner\n        cleaner = htmlCleaner(\n            log=self.log,\n            settings=self.settings,\n            url=self.url,\n            outputDirectory=self.folderpath,\n            title=self.title,  \n            style=True,  \n            metadata=True,  \n            h1=True  \n        )\n        htmlFile = cleaner.clean()\n        if not htmlFile:\n            return\n        pdfPath = htmlFile.replace(\".html\", self.append + \".pdf\")\n        electron = self.settings[\"executables\"][\"electron path\"]\n        cmd = \"\"\"%(electron)s -i \"%(htmlFile)s\" -o \"%(pdfPath)s\" \"\"\" % locals()\n        p = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)\n        stdout, stderr = p.communicate()\n        if len(stderr):\n            print stderr\n        self.log.debug('output: %(stdout)s' % locals())\n        os.remove(htmlFile)\n        exists = os.path.exists(pdfPath)\n        if not exists:\n            print \"%(pdfPath)s was not generated for some reason - please investigate\" % locals()\n            sys.exit(0)\n        self.log.debug('completed the ``_print_parsed_webpage()`` method')\n        return pdfPath", "contrast": "def print_parsed_webpage(webpage):\n    cleaned_webpage = parse_and_clean_webpage(webpage) \n    print(cleaned_webpage)\n    pdfPath = generate_pdf(cleaned_webpage) \n    return pdfPath", "label": 0}
{"index": "gp093776", "code": "def _get_info_pv(info):\n        search = re.search(pattern=PV_REGEX, string=info)\n        return {\"pv\": search.group(\"move_list\")}", "contrast": "def extract_pv(bestmove_info):\n    return bestmove_info['pv']", "label": 0}
{"index": "gp015779", "code": "def _process_execute_payload(self, item):\n        handler = self._payload_handlers.get(item['source'])\n        if handler is None:\n            return False\n        else:\n            handler(item)\n            return True", "contrast": "def dispatch_payloads(payloads, handlers):\n    results = []\n    for payload in payloads:\n        handler = handlers.get(payload['key'])\n        if handler:\n            result = handler(payload)\n            results.append(result)\n    return results", "label": 0}
{"index": "gp255702", "code": "def report_errors_warnings(document):\n    num_errors = document.getNumErrors()\n    num_warnings = document.getNumWarnings()\n    lines = ['SBML document summary:',\n             f'    number of errors: {num_errors}',\n             f'    number of warnings: {num_warnings}']\n    for i in range(len(lines)):\n        print(lines[i])\n    for i in range(num_errors):\n        error = document.getError(i)\n        print(f'Error: {error.getMessage()}')\n    for i in range(num_warnings):\n        warning = document.getWarning(i)\n        print(f'Warning: {warning.getMessage()}')", "contrast": "def run_sbml_validation(document, notifications):\n    validator = libsbml.SBMLValidator()\n    validator.validate(document)\n    for i in range(document.getNumErrors()):\n        notifications['errors'].append(format_failure(document.getError(i)))\n    for i in range(validator.getNumFailures()):\n        failure = validator.getFailure(i)\n        if failure.isWarning():\n            notifications['warnings'].append(format_failure(failure))\n        else:\n            notifications['errors'].append(format_failure(failure))", "label": 1}
{"index": "gp090526", "code": "def get_state_actions(self, state, **kwargs):\n        if state.config_flags & ConfigFlags.DEPENDENT or state.config_id.config_type != ItemType.CONTAINER:\n            return super(ScriptActionGenerator, self).get_state_actions(state, **kwargs)\n        if state.base_state == State.ABSENT:\n            actions = []\n        else:\n            log.debug(\"Found existing script containers: %s\", state.config_id)\n            if not self.remove_existing_before:\n                config_id = state.config_id\n                c_name = self._policy.cname(config_id.map_name, config_id.config_name, config_id.instance_name)\n                if state.client_name == self._policy.default_client_name:\n                    error_msg = \"Container {0} existed prior to running the script.\".format(c_name)\n                else:\n                    error_msg = (\"Container {0} existed on client {1} prior to running the \"\n                                 \"script.\").format(c_name, state.client_name)\n                raise ScriptActionException(error_msg)\n            if state.base_state == State.RUNNING or state.state_flags & StateFlags.RESTARTING:\n                log.debug(\"Preparing shutdown of existing container: %s\", state.config_id)\n                actions = [ItemAction(state, DerivedAction.SHUTDOWN_CONTAINER)]\n            else:\n                log.debug(\"Preparing removal existing container: %s\", state.config_id)\n                actions = [ItemAction(state, Action.REMOVE)]\n        actions.append(ItemAction(state, ContainerUtilAction.SCRIPT, extra_data=kwargs))\n        return actions", "contrast": "from dockermap.map.action import resume\ndef resume_or_remove(state, **kwargs):\n    try:\n        resume.ResumeActionGenerator(state)\n    except ValueError:\n        if state.container.exists():\n            if kwargs.get('remove_existing_before'):\n                state.container.remove(v=True, force=True)\n            else:\n                raise ValueError('Container already exists and could not be removed.')\n        state.script.run()\n    return state.map.build_actions(item_actions=[])", "label": 0}
{"index": "gp173030", "code": "from functools import wraps\nfrom django.shortcuts import redirect\ndef subscription_required(view_func):\n    @wraps(view_func)\n    def wrapper(request, *args, **kwargs):\n        user = request.user\n        if not user.is_authenticated:\n            return redirect('login')\n        elif not user.is_subscribed:\n            return redirect('pay_page')\n        else:\n            return view_func(request, *args, **kwargs)\n    return wrapper", "contrast": "def subscription_payment_required(\n function=None, plan=None, pay_page=SUBSCRIPTION_REDIRECT\n):\n actual_decorator = subscriber_passes_pay_test(\n  subscriber_has_active_subscription, plan=plan, pay_page=pay_page\n )\n if function:\n\t\treturn actual_decorator(function)\n return actual_decorator", "label": 1}
{"index": "gp311146", "code": "def get_last_2_errors(stack):\n    if len(stack) < 2:\n        return stack\n    else:\n        return stack[-2:]", "contrast": "def compact_error(err):\n  def err2(e):\n    if isinstance(e, exceptions.EvaluationError) and e.inner:\n      message, i = err2(e.inner)\n      if i == 1:\n        return ', '.join([e.args[0], str(e.inner)]), i + 1\n      else:\n        return message, i + 1\n    else:\n      return str(e), 1\n  return err2(err)[0]", "label": 1}
{"index": "gp157920", "code": "def countedArray( expr, intExpr=None ):\r\n    arrayExpr = Forward()\r\n    def countFieldParseAction(s,l,t):\r\n        n = t[0]\r\n        arrayExpr << (n and Group(And([expr]*n)) or Group(empty))\r\n        return []\r\n    if intExpr is None:\r\n        intExpr = Word(nums).setParseAction(lambda t:int(t[0]))\r\n    else:\r\n        intExpr = intExpr.copy()\r\n    intExpr.setName(\"arrayLen\")\r\n    intExpr.addParseAction(countFieldParseAction, callDuringTry=True)\r\n    return ( intExpr + arrayExpr )", "contrast": "def counted_list(tokens):\n    count = int(tokens[0])\n    return tokens[1 : count + 1]", "label": 0}
{"index": "gp029077", "code": "def issue_tags(issue):\n    labels = issue.get('labels', [])\n    return [label['name'].replace('tag: ', '') for label in labels if label['name'].startswith('tag: ')]", "contrast": "def get_issue_tags(issue):\n    tags = issue.tags \n    return tags", "label": 0}
{"index": "gp244347", "code": "def search_dashboard_for_facets(async_req: bool, body: FacetsSearchRequestContainer) -> ResponseContainerFacetsResponseContainer:\n    pass", "contrast": "def search_dashboard_for_facets(self, **kwargs):  \n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.search_dashboard_for_facets_with_http_info(**kwargs)  \n        else:\n            (data) = self.search_dashboard_for_facets_with_http_info(**kwargs)  \n            return data", "label": 1}
{"index": "gp151979", "code": "def copy(self, sleep=_unset, stop=_unset, wait=_unset,\n             retry=_unset, before=_unset, after=_unset, before_sleep=_unset,\n             reraise=_unset):\n        if before_sleep is _unset:\n            before_sleep = self.before_sleep\n        return self.__class__(\n            sleep=self.sleep if sleep is _unset else sleep,\n            stop=self.stop if stop is _unset else stop,\n            wait=self.wait if wait is _unset else wait,\n            retry=self.retry if retry is _unset else retry,\n            before=self.before if before is _unset else before,\n            after=self.after if after is _unset else after,\n            before_sleep=before_sleep,\n            reraise=self.reraise if after is _unset else reraise,\n        )", "contrast": "def copy_with_changes(obj, **changes):\n    copied_obj = obj.__class__()\n    for key, value in obj.__dict__.items():\n        setattr(copied_obj, key, value)\n    for key, value in changes.items():\n        setattr(copied_obj, key, value)\n    return copied_obj", "label": 0}
{"index": "gp256405", "code": "import transmissionrpc\ndef delete_torrents(infohash_list):\n    client = transmissionrpc.Client('localhost', port=9091)\n    if isinstance(infohash_list, list):\n        for infohash in infohash_list:\n            client.remove_torrent(infohash, delete_data=True)\n    else:\n        client.remove_torrent(infohash_list, delete_data=True)", "contrast": "def delete_permanently(self, infohash_list):\n        data = self._process_infohash_list(infohash_list)\n        return self._post('command/deletePerm', data=data)", "label": 1}
{"index": "gp327938", "code": "def get_or_create_track(name):\n    track, _ = Track.objects.get_or_create(name=name)\n    return track", "contrast": "def get(self, name) -> Track:\n        name = name.lower()\n        track = self.track_map.get(name)\n        if not track:\n            track = Track(name)\n            self.tacks.append(track)\n            self.track_map[name] = track\n        return track", "label": 1}
{"index": "gp267632", "code": "def create_workflow_temp_project(enabled_regions):\n    workflow_ids = {}\n    project_ids = {}\n    for region in enabled_regions:\n        project_id = create_temp_project(region)\n        project_ids[region] = project_id\n        workflow_id = create_workflow(region, project_id)\n        workflow_ids[region] = workflow_id\n    return (workflow_ids, project_ids)", "contrast": "def _build_underlying_workflows(enabled_regions, json_spec, args):\n    projects_by_region = _create_temporary_projects(enabled_regions, args)\n    workflows_by_region = {}\n    try:\n        for region, project in projects_by_region.items():\n            json_spec['project'] = project\n            workflow_id = _build_regular_workflow(json_spec)\n            logger.debug(\"Created workflow \" + workflow_id + \" successfully\")\n            workflows_by_region[region] = workflow_id\n    except:\n        if projects_by_region:\n            dxpy.executable_builder.delete_temporary_projects(projects_by_region.values())\n        raise\n    return workflows_by_region, projects_by_region", "label": 1}
{"index": "gp190581", "code": "import numpy as np\ndef match_vectors(kw, kv, n, theta):\n    match_count = 0\n    total_count = 0\n    for i in range(kw):\n        for j in range(kv):\n            total_count += 1\n            if np.dot(kw[i], kv[j]) >= theta:\n                match_count += 1\n    percent_match = match_count / total_count if total_count else 0\n    return percent_match, match_count, total_count", "contrast": "def returnMatches(kw, kv, n, theta, inputScaling=1.0):\n  m1 = 4\n  m2 = 1000\n  weights = getSparseTensor(kw, n, m1, fixedRange=1.0 / kw)\n  inputVectors = getSparseTensor(kv, n, m2,\n                                 onlyPositive=True,\n                                 fixedRange= 2*inputScaling / kw,\n                                 )\n  dot = inputVectors.matmul(weights.t())\n  numMatches = ((dot >= theta).sum()).item()\n  pctMatches = numMatches / float(m1*m2)\n  return pctMatches, numMatches, m1*m2", "label": 1}
{"index": "gp142276", "code": "def net(narr,nnet):\n mat = np.zeros((nnet,nnet))\n for i in range(len(narr)):\n\t\tnarr[i] = np.sort(narr[i])\n  mat[narr[i][0]][narr[i][1]] = 1\n return mat", "contrast": "import numpy as np\ndef net(edges, N):\n    Q = np.zeros((N, N))\n    for edge in edges:\n        i, j = edge\n        Q[i][j] = 1\n        Q[j][i] = 1\n    return Q", "label": 0}
{"index": "gp308646", "code": "def getElementsByAttr(attrName, attrValue):\n    return [element for element in self.children if element.has_attr(attrName) and element[attrName] == attrValue]", "contrast": "def getElementsByAttr(self, attrName, attrValue):\n        elements = []\n        for child in self.children:\n            if child.getAttribute(attrName) == attrValue:\n                elements.append(child)\n            elements += child.getElementsByAttr(attrName, attrValue)\n        return TagCollection(elements)", "label": 1}
{"index": "gp022160", "code": "def _create_kvstore(kvstore, num_device, arg_params):\n    update_on_kvstore = bool(int(os.getenv('MXNET_UPDATE_ON_KVSTORE', \"1\")))\n    if kvstore is None:\n        kv = None\n    elif isinstance(kvstore, kvs.KVStore):\n        kv = kvstore\n    elif isinstance(kvstore, str):\n        if num_device == 1 and 'dist' not in kvstore:\n            kv = None\n        else:\n            kv = kvs.create(kvstore)\n            if kvstore == 'local':\n                max_size = max(np.prod(param.shape) for param in\n                               arg_params.values())\n                if max_size > 1024 * 1024 * 16:\n                    update_on_kvstore = False\n    else:\n        raise TypeError('kvstore must be KVStore, str or None')\n    if kv is None:\n        update_on_kvstore = False\n    return (kv, update_on_kvstore)", "contrast": "from mxnet import kvstore\ndef create_kvstore(kvstore, num_device, arg_params):\n    if isinstance(kvstore, str):\n        return kvstore.create(kvstore, num_device)\n    elif isinstance(kvstore, kvstore.KVStore):\n        return kvstore\n    else:\n        raise ValueError(\"Invalid kvstore: {}\".format(kvstore))", "label": 0}
{"index": "gp284086", "code": "def encode(value):\n    return bytearray(value.encode()) ", "contrast": "def encode(self, value):\n        if type(value) is not datetime.datetime:\n            raise TypeError('encode() argument must be a Python datetime')\n        coarse = Time32Type().encode(value)\n        fine   = Time8Type() .encode(value.microsecond / 1e6)\n        return coarse + fine", "label": 1}
{"index": "gp091825", "code": "def _render_str(self, string):\n        if isinstance(string, StrLabel):\n            string = string._render(string.expr)\n        string = str(string)\n        if len(string) == 0:\n            return ''\n        name, supers, subs = split_super_sub(string)\n        return render_unicode_sub_super(\n            name, subs, supers, sub_first=True, translate_symbols=True,\n            unicode_sub_super=self._settings['unicode_sub_super'])", "contrast": "def unicodify(string):\n    return string.encode('utf-8')", "label": 0}
{"index": "gp162066", "code": "def add_all(self, items):\n        check_not_none(items, \"Value can't be None\")\n        data_items = []\n        for item in items:\n            check_not_none(item, \"Value can't be None\")\n            data_items.append(self._to_data(item))\n        return self._encode_invoke(queue_add_all_codec, data_list=data_items)", "contrast": "def enqueue_items(items):\n    initial_size = len(this.queue)\n    for item in items:\n        this.queue.append(item)\n    return len(this.queue) > initial_size", "label": 0}
{"index": "gp193144", "code": "def rename_ids(gff3_file:str, switch_file:str) -> None:\n    with open(gff3_file, 'r') as f:\n        gff3_lines = f.readlines()\n    with open(switch_file, 'r') as f:\n        switch_lines = f.readlines()\n    id_map = {}\n    for i in range(len(gff3_lines)):\n        line = gff3_lines[i]\n        if line.startswith('#'):\n            continue\n        fields = line.strip().split('\\t')\n        old_id = fields[0]\n        if old_id not in id_map:\n            new_id = switch_lines[len(id_map)].strip()\n            id_map[old_id] = new_id\n        new_id = id_map[old_id]\n        fields[0] = new_id\n        gff3_lines[i] = '\\t'.join(fields) + '\\n'\n    with open('reindexed.gff3', 'w') as f:\n        f.writelines(gff3_lines)", "contrast": "def rename(args):\n    p = OptionParser(rename.__doc__)\n    opts, args = p.parse_args(args)\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n    ingff3, switch = args\n    switch = DictFile(switch)\n    gff = Gff(ingff3)\n    for g in gff:\n        id, = g.attributes[\"ID\"]\n        newname = switch.get(id, id)\n        g.attributes[\"ID\"] = [newname]\n        if \"Parent\" in g.attributes:\n            parents = g.attributes[\"Parent\"]\n            g.attributes[\"Parent\"] = [switch.get(x, x) for x in parents]\n        g.update_attributes()\n        print(g)", "label": 1}
{"index": "gp035028", "code": "def _convert_range_to_list(tgt, range_server):\n    r = seco.range.Range(range_server)\n    try:\n        return r.expand(tgt)\n    except seco.range.RangeException as err:\n        log.error('Range server exception: %s', err)\n        return []", "contrast": "def range_to_list(seco_range):\n    target = list(seco_range)\n    return target", "label": 0}
{"index": "gp026736", "code": "def table_data_client(self):\n        if self._table_data_client is None:\n            self._table_data_client = _create_gapic_client(bigtable_v2.BigtableClient)(\n                self\n            )\n        return self._table_data_client", "contrast": "from google.cloud.bigtable_v2 import BigtableClient\ndef get_table_admin_grpc_stub(project_id, instance_id, channel=None):\n    client = BigtableClient(project=project_id, channel=channel)\n    return client.table_admin_client(instance_id)", "label": 0}
{"index": "gp000016", "code": "def matchall(text, patterns):\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n    return ret", "contrast": "import re\ndef scan_for_patterns(text, patterns):\n    matches = []\n    for pattern in patterns:\n        matches += re.findall(pattern, text)\n    return matches", "label": 0}
{"index": "gp180780", "code": "def generate_django_service_url(cas_url, service_url):\n    return cas_url.rstrip('/') + '/login/?service=' + service_url", "contrast": "def get_service_url(request, redirect_to=None):\n    if hasattr(django_settings, 'CAS_ROOT_PROXIED_AS'):\n        service = django_settings.CAS_ROOT_PROXIED_AS + request.path\n    else:\n        protocol = get_protocol(request)\n        host = request.get_host()\n        service = urllib_parse.urlunparse(\n            (protocol, host, request.path, '', '', ''),\n        )\n    if not django_settings.CAS_STORE_NEXT:\n        if '?' in service:\n            service += '&'\n        else:\n            service += '?'\n        service += urllib_parse.urlencode({\n            REDIRECT_FIELD_NAME: redirect_to or get_redirect_url(request)\n        })\n    return service", "label": 1}
{"index": "gp266492", "code": "def read_set_ids(objects):\n    set_ids = set()\n    for obj in objects:\n        set_ids.add(obj.id)\n    return set_ids", "contrast": "def learn_ids(self, item_list):\n        self._reset_sequence()\n        for item in item_list:\n            key = self.nondup_key_for_item(item)\n            self.ids[key] = item[self.id_key]", "label": 1}
{"index": "gp199563", "code": "import sys\ndef get_version_info() -> tuple:\n    version_info = sys.version_info\n    if hasattr(version_info, 'releaselevel'):\n        if version_info.releaselevel == 'alpha':\n            version_type = 64\n        elif version_info.releaselevel == 'beta':\n            version_type = 128\n        elif version_info.releaselevel == 'candidate':\n            version_type = 192\n        else:\n            version_type = 255\n    else:\n        version_type = 0\n    return (version_info.major, version_info.minor, version_info.micro, version_type)", "contrast": "def get_version_info(self, key_name='ver_sw_release'):\n        if key_name in self._msg_info_dict:\n            val = self._msg_info_dict[key_name]\n            return ((val >> 24) & 0xff, (val >> 16) & 0xff, (val >> 8) & 0xff, val & 0xff)\n        return None", "label": 1}
{"index": "gp023682", "code": "def get_shell(pid=None, max_depth=6):\n    if not pid:\n        pid = os.getpid()\n    processes = dict(_iter_process())\n    def check_parent(pid, lvl=0):\n        ppid = processes[pid].get('parent_pid')\n        shell_name = _get_executable(processes.get(ppid))\n        if shell_name in SHELL_NAMES:\n            return (shell_name, processes[ppid]['executable'])\n        if lvl >= max_depth:\n            return None\n        return check_parent(ppid, lvl=lvl + 1)\n    shell_name = _get_executable(processes.get(pid))\n    if shell_name in SHELL_NAMES:\n        return (shell_name, processes[pid]['executable'])\n    try:\n        return check_parent(pid)\n    except KeyError:\n        return None", "contrast": "import os\ndef get_shell(pid=None):\n    if pid is None:\n        pid = os.getpid()\n    with open(f\"/proc/{pid}/stat\") as f:\n        stat = f.read().split()[2]\n        if stat.startswith('(') and stat.endswith(')s'):\n            return stat[1:-2]\n        else:\n            return None", "label": 0}
{"index": "gp267496", "code": "def clone_to_folder(project: str, folder: str) -> DXDataObject:\n    from dxpy import DXDataObject\n    dx_obj = DXDataObject(dxid='object-id', project='source-project')\n    new_dx_obj = dx_obj.clone(project=project, folder=folder)\n    return new_dx_obj", "contrast": "def clone(self, project, folder=\"/\", **kwargs):\n        if self._proj is None:\n            raise DXError(\"Clone called when a project ID was not associated with this object handler\")\n        dxpy.api.project_clone(self._proj,\n                               {\"objects\": [self._dxid],\n                                \"project\": project,\n                                \"destination\": folder},\n                               **kwargs)\n        cloned_copy = copy.copy(self)\n        cloned_copy.set_ids(cloned_copy.get_id(), project)\n        return cloned_copy", "label": 1}
{"index": "gp152788", "code": "def top_stream(cache, user_id):\n    if not user_id:\n        return None\n    stack = cache.get(user_id)\n    if stack is None:\n        return None\n    return stack.pop()", "contrast": "def peek_cache_top(cache, user_id):\n    stack = cache.get(user_id)\n    if stack:\n        return stack[-1]\n    else:\n        return None", "label": 0}
{"index": "gp250317", "code": "import zipfile\ndef read_archive_entry(archive_name, entry_name=None, max_size=None):\n    with zipfile.ZipFile(archive_name, 'r') as archive:\n        if entry_name:\n            with archive.open(entry_name) as entry:\n                if max_size:\n                    return entry.read(max_size)\n                else:\n                    return entry.read()\n        else:\n            entries = {}\n            for entry in archive.infolist():\n                if max_size:\n                    entries[entry.filename] = archive.read(entry, max_size)\n                else:\n                    entries[entry.filename] = archive.read(entry)\n            return entries", "contrast": "def read(self, cnt=None):\n        if cnt is None or cnt < 0:\n            cnt = self._remain\n        elif cnt > self._remain:\n            cnt = self._remain\n        if cnt == 0:\n            return EMPTY\n        data = self._read(cnt)\n        if data:\n            self._md_context.update(data)\n            self._remain -= len(data)\n        if len(data) != cnt:\n            raise BadRarFile(\"Failed the read enough data\")\n        if not data or self._remain == 0:\n            self._check()\n        return data", "label": 1}
{"index": "gp272290", "code": "import matplotlib.colors as mcolors\ndef _make_cmap(colors, bit=False):\n    if bit:\n        colors = [(r/255, g/255, b/255) for (r,g,b) in colors]\n    return mcolors.LinearSegmentedColormap.from_list('custom cmap', colors)", "contrast": "def _make_cmap(colors, position=None, bit=False):\n    bit_rgb = np.linspace(0,1,256)\n    if position == None:\n        position = np.linspace(0,1,len(colors))\n    else:\n        if len(position) != len(colors):\n            sys.exit(\"position length must be the same as colors\")\n        elif position[0] != 0 or position[-1] != 1:\n            sys.exit(\"position must start with 0 and end with 1\")\n    palette = [(i, (float(r), float(g), float(b), float(a))) for\n    i, (r, g, b, a) in enumerate(colors)]\n    cmap = Colormap(*palette)\n    return cmap", "label": 1}
{"index": "gp319870", "code": "import importlib\ndef import_module(module: str):\n    return importlib.import_module(module)", "contrast": "def maybe_dotted(module, throw=True):\n    try:\n        return Configurator().maybe_dotted(module)\n    except ImportError as e:\n        err = '%s not found. %s' % (module, e)\n        if throw:\n            raise ImportError(err)\n        else:\n            log.error(err)\n            return None", "label": 1}
{"index": "gp209243", "code": "import numpy as np\ndef cartesian_to_spherical(vectors):\n    x, y, z = np.transpose(vectors)\n    r = np.sqrt(x**2 + y**2 + z**2)\n    lat = np.arcsin(z/r) * 180/np.pi\n    lon = np.arctan2(y, x) * 180/np.pi\n    dep = r - 6371 \n    return lon, lat, dep", "contrast": "def cartesian_to_spherical(vectors):\n    rr = numpy.sqrt(numpy.sum(vectors * vectors, axis=-1))\n    xx, yy, zz = vectors.T\n    lats = numpy.degrees(numpy.arcsin((zz / rr).clip(-1., 1.)))\n    lons = numpy.degrees(numpy.arctan2(yy, xx))\n    depths = EARTH_RADIUS - rr\n    return lons.T, lats.T, depths", "label": 1}
{"index": "gp173457", "code": "def update_event_tags(number_of_consumed_event_tags, number_of_produced_event_tags, previous_update):\n    if number_of_consumed_event_tags < previous_update[0] or number_of_produced_event_tags < previous_update[1]:\n        raise ValueError(\"The consumed or produced number of event tags is smaller than the previous update.\")\n    elif number_of_consumed_event_tags > previous_update[0] or number_of_produced_event_tags > previous_update[1]:\n        return True\n    else:\n        return False", "contrast": "def UpdateNumberOfEventTags(\n      self, number_of_consumed_event_tags, number_of_produced_event_tags):\n    consumed_event_tags_delta = 0\n    if number_of_consumed_event_tags is not None:\n      if number_of_consumed_event_tags < self.number_of_consumed_event_tags:\n        raise ValueError(\n            'Number of consumed event tags smaller than previous update.')\n      consumed_event_tags_delta = (\n          number_of_consumed_event_tags - self.number_of_consumed_event_tags)\n      self.number_of_consumed_event_tags = number_of_consumed_event_tags\n      self.number_of_consumed_event_tags_delta = consumed_event_tags_delta\n    produced_event_tags_delta = 0\n    if number_of_produced_event_tags is not None:\n      if number_of_produced_event_tags < self.number_of_produced_event_tags:\n        raise ValueError(\n            'Number of produced event tags smaller than previous update.')\n      produced_event_tags_delta = (\n          number_of_produced_event_tags - self.number_of_produced_event_tags)\n      self.number_of_produced_event_tags = number_of_produced_event_tags\n      self.number_of_produced_event_tags_delta = produced_event_tags_delta\n    return consumed_event_tags_delta > 0 or produced_event_tags_delta > 0", "label": 1}
{"index": "gp235921", "code": "from django import template\nregister = template.Library()\n@register.filter\ndef is_parent_of(page, potential_child):\n    return potential_child.is_ancestor_of(page)", "contrast": "def is_parent_of(page1, page2):\n    try:\n        return page1.tree_id == page2.tree_id and page1.lft < page2.lft and page1.rght > page2.rght\n    except AttributeError:\n        return False", "label": 1}
{"index": "gp140868", "code": "def taskotron_changed_outcome(config, message):\n    if not taskotron_result_new(config, message):\n        return False\n    outcome = message['msg']['result'].get('outcome')\n    prev_outcome = message['msg']['result'].get('prev_outcome')\n    return prev_outcome is not None and outcome != prev_outcome", "contrast": "def is_outcome_changed(task_results):\n    return True if task_results[-1]['outcome'] != task_results[-2]['outcome'] else False", "label": 0}
{"index": "gp079907", "code": "def gene_name(name):\n        if 'Van' in name or 'mcr' in name or 'aph' in name or 'ddlA' in name or 'ant' in name or 'aadE_Cc' in name:\n            try:\n                if name == \"ant(3'')_Ih_aac(6')_IId_1_AF453998\":\n                    gene1, version1, gene2, version2, allele, accession = name.split('_')\n                    gname = '{g1}-{v1}-{g2}-{v2}'.format(g1=gene1,\n                                                         v1=version1,\n                                                         g2=gene2,\n                                                         v2=version2)\n                    genename = gname\n                elif name == 'ant(3'')_Ia_1_X02340':\n                    gene, version, allele, accession = name.split('_')\n                    gname = '{g}-{v}'.format(g=gene,\n                                             v=version)\n                    genename = gname\n                elif 'mcr_3' in name or 'mcr_2' in name or 'mcr_1.10' in name:\n                    gene, combinedversion, allele, accession = name.split('_')\n                    version = combinedversion.split('.')[0]\n                    gname = '{gene}-{version}'.format(gene=gene,\n                                                      version=version)\n                    genename = gname\n                else:\n                    try:\n                        pregene, postgene, allele, accession = name.split('_')\n                        gname = '{pre}-{post}'.format(pre=pregene,\n                                                      post=postgene)\n                        genename = gname\n                    except ValueError:\n                        pregene, postgene, allele, preaccession, postaccession = name.split('_')\n                        genename = '{pre}-{post}'.format(pre=pregene,\n                                                         post=postgene)\n                        accession = '{pre}_{post}'.format(pre=preaccession,\n                                                          post=postaccession)\n                        gname = pregene\n            except ValueError:\n                genename, allele, accession = name.split('_')\n                gname = genename\n        else:\n            if 'bla' in name or 'aac' in name or 'ARR' in name or 'POM' in name:\n                if 'OKP' in name or 'CTX' in name or 'OXY' in name:\n                    gene, version1, version2, allele, accession = name.split('_')\n                    gname = '{g}-{v1}-{v2}'.format(g=gene,\n                                                   v1=version1,\n                                                   v2=version2)\n                    genename = gname\n                elif 'CMY' in name:\n                    try:\n                        gname, allele, version, accession = name.split('_')\n                    except ValueError:\n                        gname, allele, version, pre_accession, post_accession = name.split('_')\n                        accession = '{pre}_{post}'.format(pre=pre_accession,\n                                                          post=post_accession)\n                    genename = gname\n                elif name == \"aac(3)_Ib_aac(6')_Ib_1_AF355189\":\n                    gene1, version1, gene2, version2, allele, accession = name.split('_')\n                    gname = '{g1}-{v1}-{g2}-{v2}'.format(g1=gene1,\n                                                         v1=version1,\n                                                         g2=gene2,\n                                                         v2=version2)\n                    genename = gname\n                elif 'alias' in name:\n                    gene1, version1, alias, gene2, version2, allele, accession = name.split('_')\n                    gname = '{g1}-{v1}'.format(g1=gene1,\n                                               v1=version1)\n                    genename = gname\n                else:\n                    try:\n                        genename, allele, accession = name.split('_')\n                        gname = genename\n                    except ValueError:\n                        try:\n                            genename, version, allele, accession = name.split('_')\n                            gname = '{g}-{v}'.format(g=genename,\n                                                     v=version)\n                        except ValueError:\n                            genename, version, allele, preaccession, postaccession = name.split('_')\n                            gname = '{g}-{v}'.format(g=genename,\n                                                     v=version)\n                            genename = gname\n                            accession = '{preaccess}_{postaccess}'.format(preaccess=preaccession,\n                                                                          postaccess=postaccession)\n            else:\n                try:\n                    genename, allele, accession = name.split('_')\n                    gname = genename\n                except ValueError:\n                    genename, allele, preaccession, postaccession = name.split('_')\n                    accession = '{preaccess}_{postaccess}'.format(preaccess=preaccession,\n                                                                  postaccess=postaccession)\n                    gname = genename\n        return gname, genename, accession, allele", "contrast": "def split_fasta_header(name):\n    header_components = name.split('|')\n    gname = header_components[0]\n    genename = header_components[1].split('.')[0]\n    accession = header_components[1]\n    allele = header_components[2] if len(header_components) == 3 else None\n    return gname, genename, accession, allele", "label": 0}
{"index": "gp284753", "code": "def safe_get(dict_obj, keys_lst, default_val=None):\n    try:\n        for key in keys_lst:\n            dict_obj = dict_obj[key]\n        return dict_obj\n    except (KeyError, TypeError):\n        return default_val", "contrast": "def safe_get(data, key_list):\n        for key in key_list:\n            data = data.get(key, {})\n        return data if data else 'plugin_failed'", "label": 1}
{"index": "gp301716", "code": "def arithmetic_mean(data):\n    return sum(data) / len(data) if len(data) > 0 else 0", "contrast": "def mean(data):\n    n = len(data)\n    if n < 1:\n        raise ValueError('mean requires at least one data point')\n    return sum(data)/n", "label": 1}
{"index": "gp069880", "code": "def delete_boot_script(self):\n        j, r = self.datacenter.request('DELETE', self.path + \n                '/metadata/user-script')\n        r.raise_for_status()\n        self.boot_script = None", "contrast": "def delete_boot_script(login, machine_id):\n    url = f\"/{login}/machines/{machine_id}/metadata/user-script\"", "label": 0}
{"index": "gp120407", "code": "def get_kerberos_subs(netid):\n    subs = get_netid_subscriptions(netid, Subscription.SUBS_CODE_KERBEROS)\n    if subs is not None:\n        for subscription in subs:\n            if subscription.subscription_code == Subscription.SUBS_CODE_KERBEROS:\n                return subscription\n    return None", "contrast": "def get_subscription(uwnetid):\n    return subscription_object", "label": 0}
{"index": "gp313499", "code": "def count_events():\n    all_events_count = 30\n    movie_events_count = 12\n    gig_events_count = 10\n    counts_dict = {'counts': {'all': all_events_count, 'movie': movie_events_count, 'gig': gig_events_count}}\n    return counts_dict", "contrast": "def get_event_counts(self):\n        counts = {'all': Event.objects.count(),}\n        for k,v in Event.KIND_CHOICES:\n            counts[k] = Event.objects.filter(kind=k).count()\n        return {'counts': counts,}", "label": 1}
{"index": "gp155662", "code": "def price_name_stacks(name, namespace, block_height):\n    if namespace['version'] in [NAMESPACE_VERSION_PAY_WITH_STACKS]:\n        return price_name(name, namespace, block_height)\n    else:\n        btc_price = price_name(name, namespace, block_height)\n        btc_price = int(btc_price)\n        return (btc_price * MICROSTACKS_PER_SATOSHI_NUM) / MICROSTACKS_PER_SATOSHI_DEN", "contrast": "def get_name_price_in_stacks(name: str, namespace_version_bits: int, btc_price: float) -> int:\n    if namespace_version_bits < 0:\n        raise ValueError(\"Namespace version bits cannot be negative\")\n    if btc_price <= 0:\n        raise ValueError(\"BTC price should be a positive value\")\n    if len(name) == 0:\n        return 0\n    if namespace_version_bits == 0:\n        return 0\n    if len(name) > 255:\n        raise ValueError(\"Name length cannot be greater than 255 characters\")\n    btc_to_stacks_conversion_factor = 1250000000            \n    namespace_multiplier = 0\n    if namespace_version_bits < 3:\n        namespace_multiplier = 1000000000                   \n    elif namespace_version_bits == 3:\n        namespace_multiplier = 1000000000000                \n    btc_per_microstack = btc_price / btc_to_stacks_conversion_factor\n    if btc_per_microstack == 0:\n        return 0\n    if namespace_multiplier == 0:\n        return int(round(btc_price / btc_per_microstack))\n    bytes_hex = name.encode(\"utf-8\").hex()\n    bytes_dec = int(bytes_hex, 16)\n    amount = max(bytes_dec * namespace_multiplier, 1)\n    return int(round(amount / btc_per_microstack))", "label": 0}
{"index": "gp176466", "code": "def main_entrypoint_class_runner():\n    running_nailgun_server = create_running_nailgun_server()\n    client = create_client()\n    return client", "contrast": "def _get_nailgun_client(self, jvm_options, classpath, stdout, stderr, stdin):\n    classpath = self._nailgun_classpath + classpath\n    new_fingerprint = self._fingerprint(jvm_options, classpath, self._distribution.version)\n    with self._NAILGUN_SPAWN_LOCK:\n      running, updated = self._check_nailgun_state(new_fingerprint)\n      if running and updated:\n        logger.debug('Found running nailgun server that needs updating, killing {server}'\n                     .format(server=self._identity))\n        self.terminate()\n      if (not running) or (running and updated):\n        return self._spawn_nailgun_server(new_fingerprint, jvm_options, classpath, stdout, stderr, stdin)\n    return self._create_ngclient(self.socket, stdout, stderr, stdin)", "label": 1}
{"index": "gp324680", "code": "def get_adapter(model):\n    return adapter", "contrast": "def get_adapter(self, model):\n        if not self.is_registered(model):\n            raise RegistrationError(\n                '{} is not registered with Algolia engine'.format(model))\n        return self.__registered_models[model]", "label": 1}
{"index": "gp293531", "code": "from typing import List, Tuple\nfrom pybel.language import Version\nfrom pybel.parser import BELSyntaxError\nfrom pybel.parser.parse_results import CitationDict, NamespaceDict\nfrom pybel.parser.modifiers import modification_parser, activity_parser\ndef validate_semantics(bo, error_level: str) -> Tuple[bool, List[Tuple[str, str]]]:\n    is_valid = True\n    messages = []\n    for ns, values in bo.namespace_dict.items():\n        for v in values:\n            try:\n                Version.parse(v)\n            except ValueError as e:\n                messages.append((bo.document, f\"Invalid namespace value {v} in {ns}: {str(e)}\"))\n                is_valid = False\n    for a, values in bo.annotation_dict.items():\n        for v in values:\n            try:\n                Version.parse(v)\n            except ValueError as e:\n                messages.append((bo.document, f\"Invalid annotation value {v} in {a}: {str(e)}\"))\n                is_valid = False\n    for cp in bo.citation_dict.values():\n        if cp.type not in CitationDict.valid_types:\n            messages.append((bo.document, f\"Invalid citation type {cp.type} in {cp}\"))\n            is_valid = False\n    for _, statement in bo.statements.items():\n        objects = statement.obj.flatten()\n        for obj in objects:\n            try:\n                modification_parser.parseString(obj.modification)\n                activity_parser.parseString(obj.activity)\n            except BELSyntaxError as e:\n                messages.append((bo.document, f\"Syntax error in {obj}: {str(e)}\"))\n                is_valid = False\n    if error_level == \"WARNING\":\n        return is_valid, messages\n    else:\n        return is_valid, [msg for msg in messages if \"ERROR\" in msg[1]]", "contrast": "def validate(bo, error_level: str = \"WARNING\") -> Tuple[bool, List[Tuple[str, str]]]:\n    if bo.ast:\n        bo = validate_functions(bo.ast, bo)  \n        if error_level == \"WARNING\":\n            bo = validate_arg_values(bo.ast, bo)  \n    else:\n        bo.validation_messages.append((\"ERROR\", \"Invalid BEL Statement - cannot parse\"))\n    for msg in bo.validation_messages:\n        if msg[0] == \"ERROR\":\n            bo.parse_valid = False\n            break\n    return bo", "label": 1}
{"index": "gp285289", "code": "def vertices_and_faces(mesh):\n    vertices = mesh.vertices\n    faces = mesh.polygons\n    return vertices, faces", "contrast": "def isosurface_from_data(data, isolevel, origin, spacing):\n    spacing = np.array(extent/resolution)\n    if isolevel >= 0:\n        triangles = marching_cubes(data, isolevel)\n    else: \n        triangles = marching_cubes(-data, -isolevel)\n    faces = []\n    verts = []\n    for i, t in enumerate(triangles):\n       faces.append([i * 3, i * 3 +1, i * 3 + 2])\n       verts.extend(t)\n    faces = np.array(faces)\n    verts = origin + spacing/2 + np.array(verts)*spacing\n    return verts, faces", "label": 1}
{"index": "gp114847", "code": "def maximum_variation(arr):\n  return np.max(arr, axis=0) - np.min(arr, axis=0)", "contrast": "import numpy as np\ndef range_by_columns(arr):\n    return np.max(arr, axis=0) - np.min(arr, axis=0)", "label": 0}
{"index": "gp168817", "code": "def replace_states(workflow_statuses, target_states, replacement):\n    return [replacement if status in target_states else status for status in workflow_statuses]", "contrast": "def fix_workflow_transitions(portal):\n    logger.info(\"Fixing workflow transitions...\")\n    tochange = [\n        {'wfid': 'bika_duplicateanalysis_workflow',\n         'trid': 'submit',\n         'changes': {\n             'new_state_id': 'to_be_verified',\n             'guard_expr': ''\n            },\n         'update': {\n             'catalog': CATALOG_ANALYSIS_LISTING,\n             'portal_type': 'DuplicateAnalysis',\n             'status_from': 'attachment_due',\n             'status_to': 'to_be_verified'\n            }\n         }\n    ]\n    wtool = api.get_tool('portal_workflow')\n    for item in tochange:\n        wfid = item['wfid']\n        trid = item['trid']\n        workflow = wtool.getWorkflowById(wfid)\n        transitions = workflow.transitions\n        transition = transitions[trid]\n        changes = item.get('changes', {})\n        if 'new_state_id' in changes:\n            new_state_id = changes['new_state_id']\n            oldstate = transition.new_state_id\n            logger.info(\n                \"Replacing target state '{0}' from '{1}.{2}' to {3}\"\n                    .format(oldstate, wfid, trid, new_state_id)\n            )\n            transition.new_state_id = new_state_id\n        if 'guard_expr' in changes:\n            new_guard = changes['guard_expr']\n            if not new_guard:\n                transition.guard = None\n                logger.info(\n                    \"Removing guard expression from '{0}.{1}'\"\n                        .format(wfid, trid))\n            else:\n                guard = transition.getGuard()\n                guard.expr = Expression(new_guard)\n                transition.guard = guard\n                logger.info(\n                    \"Replacing guard expression from '{0}.{1}' to {2}\"\n                        .format(wfid, trid, new_guard))\n        update = item.get('update', {})\n        if update:\n            catalog_id = update['catalog']\n            portal_type = update['portal_type']\n            catalog = api.get_tool(catalog_id)\n            brains = catalog(portal_type=portal_type)\n            for brain in brains:\n                obj = api.get_object(brain)\n                if 'status_from' in update and 'status_to' in update:\n                    status_from = update['status_from']\n                    status_to = update['status_to']\n                    if status_from == brain.review_state:\n                        logger.info(\n                            \"Changing status for {0} from '{1} to {2}\"\n                                .format(obj.getId(), status_from, status_to))\n                        changeWorkflowState(obj, wfid, status_to)\n                workflow.updateRoleMappingsFor(obj)\n                obj.reindexObject()", "label": 1}
{"index": "gp179169", "code": "def add_paths_to_library_search_paths(paths, recursive=False, escape=False, target_name=None, configuration_name=None):\n    if isinstance(paths, str):\n        paths = [paths]\n    flag = \"-L\"\n    if recursive:\n        flag += \"R\"\n    if escape:\n        paths = [p.replace(\" \", \"\\ \") for p in paths]\n    for path in paths:\n        flag += f\" {path}\"\n    if target_name is not None:\n        if isinstance(target_name, str):\n            target_name = [target_name]\n        flags[target_name] = flags.get(target_name, {})  \n        if configuration_name is not None:\n            flags[target_name][configuration_name] = flags[target_name].get(configuration_name, []) + [flag]\n        else:\n            for config in valid_configs:\n                flags[target_name][config] = flags[target_name].get(config, []) + [flag]\n    else:\n        for target in valid_targets:\n            flags[target] = flags.get(target, {})  \n            if configuration_name is not None:\n                flags[target][configuration_name] = flags[target].get(configuration_name, []) + [flag]\n            else:\n                for config in valid_configs:\n                    flags[target][config] = flags[target].get(config, []) + [flag]", "contrast": "def add_library_search_paths(self, paths, recursive=True, escape=False, target_name=None, configuration_name=None):\n        self.add_search_paths(XCBuildConfigurationFlags.LIBRARY_SEARCH_PATHS, paths, recursive, escape, target_name,\n                              configuration_name)", "label": 1}
{"index": "gp062545", "code": "def set_coords(self, x=0, y=0, z=0, t=0):\n        self.coords = {}\n        self.coords['x'] = x\n        self.coords['y'] = y\n        self.coords['z'] = z\n        self.coords['t'] = t", "contrast": "def set_agent_coords(world, agent, x, y):\n    world[agent]['x'] = x\n    world[agent]['y'] = y", "label": 0}
{"index": "gp103029", "code": "def get_stored_version(connection):\n    if connection.engine.name == 'sqlite':\n        version = connection.execute('PRAGMA user_version').fetchone()[0]\n        if version == 0:\n            raise VersionIsNotStored\n        return version\n    elif connection.engine.name == 'postgresql':\n        try:\n            r = connection                .execute('SELECT version FROM {}.user_version;'.format(POSTGRES_SCHEMA_NAME))                .fetchone()\n            if not r:\n                raise VersionIsNotStored\n            version = r[0]\n        except ProgrammingError:\n            raise VersionIsNotStored\n        return version\n    else:\n        raise DatabaseError('Do not know how to get version from {} engine.'.format(connection.engine.name))", "contrast": "def get_database_version(connection):\n    if connection.engine.dialect.name == 'postgresql': \n        query = \"select userversion from pg_class where relname = 'user_version';\"\n        result = connection.execute(query).fetchone()\n        return result[0]\n    elif connection.engine.dialect.name == 'sqlite':\n        query = \"PRAGMA user_version;\"\n        result = connection.execute(query).fetchone()\n        return result[0]\n    else:\n        raise ValueError(\"Unsupported database.\") ", "label": 0}
{"index": "gp050769", "code": "def ms_to_datetime(ms, tzinfo=None):\n    if not isinstance(ms, (int, long)):\n        raise TypeError('expected integer, not %s' % type(ms))\n    if tzinfo is None:\n        tzinfo = mktz()\n    return datetime.datetime.fromtimestamp(ms * 1e-3, tzinfo)", "contrast": "from datetime import datetime, timezone, timedelta\ndef ms_to_datetime(ms_value):\n    return datetime.fromtimestamp(ms_value/1000, tz=timezone.utc)", "label": 0}
{"index": "gp121096", "code": "def upstream(self, url, form_data):\n        response = requests.post(\n            self.host + url,\n            files=form_data['files'] if 'files' in form_data else {},\n            headers={\n                'Authorization': 'Bearer ' + self.token,\n                'x-falkonry-source':self.sourceHeader\n            },\n            verify=False\n        )\n        if response.status_code == 202 or response.status_code == 200:\n            try:\n                return json.loads(response._content.decode('utf-8'))\n            except Exception as e:\n                return json.loads(response.content)\n        elif response.status_code == 401:\n            raise Exception(json.dumps({'message':'Unauthorized Access'}))\n        else:\n            raise Exception(response.content)", "contrast": "import requests\ndef post_form_data(url: str, form_data: dict) -> requests.Response:\n    headers = {'Content-Type': 'multipart/form-data'}\n    response = requests.post(url, headers=headers, data=form_data, stream=True)\n    return response", "label": 0}
{"index": "gp239023", "code": "def get_dataset_header(dataset):\n    subj_id = dataset['subj_id']\n    start_time = dataset['start_time']\n    s_freq = dataset['s_freq']\n    chan_name = dataset['chan_name']\n    n_samples = dataset['n_samples']\n    orig = dataset['orig']\n    return subj_id, start_time, s_freq, chan_name, n_samples, orig", "contrast": "def return_hdr(self):\n        subj_id = self.task.subject\n        sampling_freq = set(self.task.channels.get(map_lambda=lambda x: x['sampling_frequency']))\n        if len(sampling_freq) > 1:\n            raise ValueError('Multiple sampling frequencies not supported')\n        s_freq = float(next(iter(sampling_freq)))\n        chan_name = self.task.channels.get(map_lambda=lambda x: x['name'])\n        self.chan_name = array(chan_name)\n        orig = self.baseformat.header\n        start_time = orig['start_time']\n        n_samples = orig['n_samples']\n        return subj_id, start_time, s_freq, chan_name, n_samples, orig", "label": 1}
{"index": "gp183031", "code": "def check_ellipsoids(x, ellipsoids, j):\n    count = 0\n    for i in range(len(ellipsoids)):\n        if i != j:\n            if ((x[0]-ellipsoids[i][0])/ellipsoids[i][3])**2 + ((x[1]-ellipsoids[i][1])/ellipsoids[i][4])**2 + ((x[2]-ellipsoids[i][2])/ellipsoids[i][5])**2 <= 1:\n                count += 1\n    return count", "contrast": "def overlap(self, x, j=None):\n        q = len(self.within(x, j=j))\n        return q", "label": 1}
{"index": "gp178827", "code": "import twilio.rest.api.v2010.account.usage.record.today as TodayInstance\ndef build_today_instance(payload: dict) -> TodayInstance:\n    today_instance = TodayInstance(payload)\n    return today_instance", "contrast": "def get_instance(self, payload):\n        return TodayInstance(self._version, payload, account_sid=self._solution['account_sid'], )", "label": 1}
{"index": "gp180059", "code": "import numpy as np\ndef downside_deviation(rets, mar, full=True, expanding=False, ann=False):\n    if full:\n        mar_rets = np.ones(rets.shape[0]) * mar\n        if expanding:\n            dd_series = np.sqrt(((np.minimum(rets, mar_rets) - mar_rets).cumsum() ** 2) / np.arange(1, rets.shape[0] + 1))\n        else:\n            dd_series = np.sqrt(((np.minimum(rets, mar_rets) - mar_rets) ** 2).mean())\n    else:\n        mask = rets < mar\n        if expanding:\n            dd_series = np.sqrt(((rets[mask] - mar) ** 2).cumsum() / np.arange(1, mask.sum() + 1))\n        else:\n            dd_series = np.sqrt(((rets[mask] - mar) ** 2).mean())\n    if ann:\n        dd_series *= np.sqrt(252)\n    return dd_series", "contrast": "def downside_deviation(rets, mar=0, expanding=0, full=0, ann=0):\n    below = rets[rets < mar]\n    if expanding:\n        n = pd.expanding_count(rets)[below.index] if full else pd.expanding_count(below)\n        dd = np.sqrt(((below - mar) ** 2).cumsum() / n)\n        if ann:\n            dd *= np.sqrt(periods_in_year(rets))\n        return dd.reindex(rets.index).ffill()\n    else:\n        n = rets.count() if full else below.count()\n        dd = np.sqrt(((below - mar) ** 2).sum() / n)\n        if ann:\n            dd *= np.sqrt(periods_in_year(rets))\n        return dd", "label": 1}
{"index": "gp048918", "code": "def CountFlowResultsByType(self, client_id, flow_id, cursor=None):\n    query = (\"SELECT type, COUNT(*) FROM flow_results \"\n             \"FORCE INDEX (flow_results_by_client_id_flow_id_timestamp) \"\n             \"WHERE client_id = %s AND flow_id = %s \"\n             \"GROUP BY type\")\n    args = [db_utils.ClientIDToInt(client_id), db_utils.FlowIDToInt(flow_id)]\n    cursor.execute(query, args)\n    return dict(cursor.fetchall())", "contrast": "def count_flow_results(flow_results):\n    return {result_type: flow_results.count(result_type) for result_type in set(flow_results)}", "label": 0}
{"index": "gp037072", "code": "def reinstall_ruby(ruby, runas=None, env=None):\n    return _rvm(['reinstall', ruby], runas=runas, env=env)", "contrast": "def reinstall_ruby(version, runas=None):\n    cmd = 'rvm reinstall {0}'.format(version)\n    return __salt__['cmd.run'](cmd, runas=runas)", "label": 0}
{"index": "gp091933", "code": "def _split_sympy_quantum_factor(expr):\n    from qnet.algebra.core.abstract_quantum_algebra import (\n        QuantumExpression, ScalarTimesQuantumExpression)\n    from qnet.algebra.core.scalar_algebra import ScalarValue, ScalarTimes, One\n    if isinstance(expr, ScalarTimesQuantumExpression):\n        sympy_factor, quantum_factor = _split_sympy_quantum_factor(expr.coeff)\n        quantum_factor *= expr.term\n    elif isinstance(expr, ScalarValue):\n        sympy_factor = expr.val\n        quantum_factor = expr._one\n    elif isinstance(expr, ScalarTimes):\n        sympy_factor = sympy.S(1)\n        quantum_factor = expr._one\n        for op in expr.operands:\n            op_sympy, op_quantum = _split_sympy_quantum_factor(op)\n            sympy_factor *= op_sympy\n            quantum_factor *= op_quantum\n    elif isinstance(expr, sympy.Basic):\n        sympy_factor = expr\n        quantum_factor = One\n    else:\n        sympy_factor = sympy.S(1)\n        quantum_factor = expr\n    assert isinstance(sympy_factor, sympy.Basic)\n    assert isinstance(quantum_factor, QuantumExpression)\n    return sympy_factor, quantum_factor", "contrast": "from sympy import symbols\nfrom qnet.algebra import split_sympy_qnet_product\ndef split_product(expr):\n    sympy_factor, quantum_factor = split_sympy_qnet_product(expr, return_remaining=True)\n    return sympy_function(sympy_factor) * quantum_factor", "label": 0}
{"index": "gp032673", "code": "def get_routes(iface):\n    filename = os.path.join(_DEB_NETWORK_UP_DIR, 'route-{0}'.format(iface))\n    results = _read_file(filename)\n    filename = os.path.join(_DEB_NETWORK_DOWN_DIR, 'route-{0}'.format(iface))\n    results += _read_file(filename)\n    return results", "contrast": "def get_routes(interface):\n    import subprocess\n    cmd = \"ip route show dev {}\".format(interface)\n    return subprocess.check_output(cmd, shell=True).decode()", "label": 0}
{"index": "gp086225", "code": "def _parse_nested_interval(self, tokens):\n        if tokens[0].isdigit():\n            return self._parse_interval(tokens)\n        elif tokens[0] in ['join', 'order']:\n            return self._parse_join(tokens)\n        elif tokens[0] == 'complement':\n            return self._parse_complement(tokens)\n        raise ValueError('interval {} does not fit pattern.'.format(tokens))", "contrast": "class SuperRange:\n    def __init__(self, *args):\n        self.ranges = []\n        for arg in args:\n            if isinstance(arg, tuple):\n                self.ranges.append(range(arg[0], arg[1]))\n            else:\n                self.ranges.append(arg)\n    def __repr__(self):\n        return f'SuperRange({self.ranges})'\n    def __iter__(self):\n        for r in self.ranges:\n            yield from r\nclass Range:\n    def __init__(self, start, stop=None, step=1):\n        if stop is None:\n            stop = start\n            start = 0\n        self.range = range(start, stop, step)\n    def __repr__(self):\n        return f'Range({self.range.start}, {self.range.stop}, {self.range.step})'\nclass Join:\n    def __init__(self, *ranges):\n        self.ranges = ranges\n    def __repr__(self):\n        return f'Join({self.ranges})'\n    def __iter__(self):\n        for r in self.ranges:\n            yield from r\nclass Complement:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n    def __repr__(self):\n        return f'Complement({self.a}, {self.b})'\n    def __iter__(self):\n        s = set(iter(self.b))\n        for x in iter(self.a):\n            if x not in s:\n                yield x", "label": 0}
{"index": "gp077053", "code": "def normalize_cert_dir():\n    current_cn = get_crt_common_name()\n    if not os.path.isdir(COZY_CONFIG_PATH):\n        print 'Need to create {}'.format(COZY_CONFIG_PATH)\n        os.mkdir(COZY_CONFIG_PATH, 0755)\n    if not os.path.isdir(CERTIFICATES_PATH):\n        print 'Need to create {}'.format(CERTIFICATES_PATH)\n        os.mkdir(CERTIFICATES_PATH, 0755)\n    if not os.path.isdir(ACME_PRIVATE_PATH):\n        print 'Need to create {}'.format(ACME_PRIVATE_PATH)\n        os.mkdir(ACME_PRIVATE_PATH, 0700)\n    if os.path.isfile(OLD_CERTIFICATE_PATH) and            not os.path.islink(OLD_CERTIFICATE_PATH):\n        target = '{}/{}.crt'.format(CERTIFICATES_PATH, current_cn)\n        print 'Move {} to {}'.format(CERTIFICATES_PATH, target)\n        os.rename(OLD_CERTIFICATE_PATH, target)\n    else:\n        print 'Nothing to do for {}'.format(OLD_CERTIFICATE_PATH)\n    if os.path.isfile(OLD_PRIVATE_KEY_PATH) and            not os.path.islink(OLD_PRIVATE_KEY_PATH):\n        target = '{}/{}.key'.format(CERTIFICATES_PATH, current_cn)\n        print 'Move {} to {}'.format(OLD_PRIVATE_KEY_PATH, target)\n        os.rename(OLD_PRIVATE_KEY_PATH, target)\n    else:\n        print 'Nothing to do for {}'.format(OLD_PRIVATE_KEY_PATH)\n    if current_cn:\n        make_links(current_cn)", "contrast": "def convert_old_to_new_certificate(old_cert):\n    return new_cert", "label": 0}
{"index": "gp228119", "code": "import json\ndef dump_json(content):\n    return json.dumps(content)", "contrast": "def dumps(self) -> str:\n        return json.dumps(self.data, sort_keys=True, indent=4)", "label": 1}
{"index": "gp303188", "code": "def create_shipment_data():\n    shipment_data = {\n        'name': 'John Doe',\n        'address': '123 Main Street',\n        'city': 'Anytown',\n        'state': 'CA',\n        'zip': '12345',\n        'weight': '12 lbs'\n    }\n    return shipment_data", "contrast": "def _prepare_wsdl_objects(self):\n        self.RequestedShipment = self.client.factory.create('RequestedShipment')\n        self.RequestedShipment.ShipTimestamp = datetime.datetime.now()\n        total_weight = self.client.factory.create('Weight')\n        total_weight.Value = 0.0\n        total_weight.Units = 'LB'\n        self.RequestedShipment.TotalWeight = total_weight\n        shipper_party = self.client.factory.create('Party')\n        shipper_party.Address = self.client.factory.create('Address')\n        shipper_party.Contact = self.client.factory.create('Contact')\n        self.RequestedShipment.Shipper = shipper_party\n        recipient_party = self.client.factory.create('Party')\n        recipient_party.Contact = self.client.factory.create('Contact')\n        recipient_party.Address = self.client.factory.create('Address')\n        self.RequestedShipment.Recipient = recipient_party\n        payor = self.client.factory.create('Payor')\n        payor.ResponsibleParty = self.client.factory.create('Party')\n        payor.ResponsibleParty.Address = self.client.factory.create('Address')\n        payor.ResponsibleParty.Address.CountryCode = 'US'\n        shipping_charges_payment = self.client.factory.create('Payment')\n        shipping_charges_payment.Payor = payor\n        shipping_charges_payment.PaymentType = 'SENDER'\n        self.RequestedShipment.ShippingChargesPayment = shipping_charges_payment\n        self.RequestedShipment.LabelSpecification = self.client.factory.create('LabelSpecification')\n        self.RequestedShipment.RateRequestTypes = ['PREFERRED']\n        self.RequestedShipment.PackageCount = 0\n        self.RequestedShipment.RequestedPackageLineItems = []\n        self.logger.debug(self.RequestedShipment)", "label": 1}
{"index": "gp221862", "code": "def instantiate_response_model(response, d_attrs):\n    return response(**d_attrs)", "contrast": "def _instantiate_model(self, response, attrs, additional_properties=None):\n        if callable(response):\n            subtype = getattr(response, '_subtype_map', {})\n            try:\n                readonly = [k for k, v in response._validation.items()\n                            if v.get('readonly')]\n                const = [k for k, v in response._validation.items()\n                         if v.get('constant')]\n                kwargs = {k: v for k, v in attrs.items()\n                          if k not in subtype and k not in readonly + const}\n                response_obj = response(**kwargs)\n                for attr in readonly:\n                    setattr(response_obj, attr, attrs.get(attr))\n                if additional_properties:\n                    response_obj.additional_properties = additional_properties\n                return response_obj\n            except TypeError as err:\n                msg = \"Unable to deserialize {} into model {}. \".format(\n                    kwargs, response)\n                raise DeserializationError(msg + str(err))\n        else:\n            try:\n                for attr, value in attrs.items():\n                    setattr(response, attr, value)\n                return response\n            except Exception as exp:\n                msg = \"Unable to populate response model. \"\n                msg += \"Type: {}, Error: {}\".format(type(response), exp)\n                raise DeserializationError(msg)", "label": 1}
{"index": "gp079039", "code": "def take_damage(self, c, dmg):\n        if c.name == self.c1.name:\n            self.c1.stats['Health'] = self.c1.stats['Health'] - dmg\n        else:\n            self.c2.stats['Health'] = self.c2.stats['Health'] - dmg", "contrast": "def apply_damage(character, damage):\n    character['HP'] -= damage\n    return character", "label": 0}
{"index": "gp087836", "code": "def convert_attribute_to_string(value):\n    if value is None:\n        return value\n    elif (sys.hexversion >= 0x03000000 and isinstance(value, str))            or (sys.hexversion < 0x03000000            and isinstance(value, unicode)):\n        return value\n    elif isinstance(value, bytes):\n        return value.decode()\n    elif isinstance(value, np.unicode_):\n        return str(value)\n    elif isinstance(value, np.bytes_):\n        return value.decode()\n    else:\n        return None", "contrast": "def attribute_value_to_string(value):\n    if isinstance(value, str):\n        return value\n    elif hasattr(value, '__str__'):\n        return str(value)\n    else:\n        return None", "label": 0}
{"index": "gp327497", "code": "def join(index, *groupnames):\n    group = set()\n    for name in groupnames:\n        if name in index.groups:\n            group.update(index.groups[name])\n    return group", "contrast": "def join(self, *groupnames):\n        return self._sum([self[k] for k in groupnames if k in self])", "label": 1}
{"index": "gp129040", "code": "def data_dir(default='data'):\n    if 'OPENSHIFT_DATA_DIR' in os.environ:\n        return os.environ['OPENSHIFT_DATA_DIR']\n    if 'GONDOR_DATA_DIR' in os.environ:\n        return os.environ['GONDOR_DATA_DIR']\n    if provider.detect() == provider.DOTCLOUD:\n        return os.path.expanduser('~/data')\n    return default", "contrast": "import os\ndef get_persistent_data_dir(default='default'):\n    dir_path = os.path.expanduser('~/.local/share')\n    if os.path.isdir(dir_path):\n        return dir_path\n    else:\n        return default", "label": 0}
{"index": "gp320992", "code": "def performTimeStep(activeColumns, learn):\n    activateCells(activeColumns)\n    activateDendrites()\n    if learn:\n        updatePermanences()\n    return None ", "contrast": "def compute(self, activeColumns, learn=True):\n    self.activateCells(sorted(activeColumns), learn)\n    self.activateDendrites(learn)", "label": 1}
{"index": "gp231339", "code": "import numpy as np\nimport gzip\nimport dill\nfrom typing import Tuple\nfrom cmapPy.pandasGEXpress import GCToo\ndef write_gctoo_to_file(gctoo_object: GCToo, out_file_name: str, convert_back_to_neg_666: bool,\n                        gzip_compression_level: int = 6, max_chunk_kb: int = 1024, \n                        matrix_dtype: np.dtype = np.float32) -> None:\n    with gzip.open(out_file_name, 'wb', compresslevel=gzip_compression_level) as fout:\n        if convert_back_to_neg_666:\n            gctoo_object.data_df.replace(np.nan, -666, inplace=True)\n            metadata = {k: str(v) if isinstance(v, np.ndarray) else v for k, v in gctoo_object.metadata_df.to_dict('dict').items()}\n            metadata['col_metadata']['profile_id'] = str(metadata['col_metadata']['profile_id'])\n            metadata['data_df'] = None\n            dill.dump(metadata, fout)\n            data = gctoo_object.data_df.values.astype(matrix_dtype)\n            fout.write(data.tobytes())\n        else:\n            metadata = dill.dumps(gctoo_object.to_dict())\n            fout.write(metadata)\n            data = gctoo_object.data_df.values\n            chunk_size = max_chunk_kb * 1024\n            num_chunks = int(np.ceil(data.nbytes / chunk_size))\n            for i in range(num_chunks):\n                chunk_start = i * chunk_size\n                chunk_end = chunk_start + chunk_size\n                if chunk_end > data.nbytes:\n                    chunk_end = data.nbytes\n                fout.write(data[chunk_start:chunk_end].astype(matrix_dtype).tobytes())", "contrast": "def write(gctoo_object, out_file_name, convert_back_to_neg_666=True, gzip_compression_level=6,\n    max_chunk_kb=1024, matrix_dtype=numpy.float32):\n    gctx_out_name = add_gctx_to_out_name(out_file_name)\n    hdf5_out = h5py.File(gctx_out_name, \"w\")\n    write_version(hdf5_out)\n    write_src(hdf5_out, gctoo_object, gctx_out_name)\n    elem_per_kb = calculate_elem_per_kb(max_chunk_kb, matrix_dtype)\n    chunk_size = set_data_matrix_chunk_size(gctoo_object.data_df.shape, max_chunk_kb, elem_per_kb)\n    hdf5_out.create_dataset(data_matrix_node, data=gctoo_object.data_df.transpose().values,\n        dtype=matrix_dtype)\n    write_metadata(hdf5_out, \"col\", gctoo_object.col_metadata_df, convert_back_to_neg_666,\n        gzip_compression=gzip_compression_level)\n    write_metadata(hdf5_out, \"row\", gctoo_object.row_metadata_df, convert_back_to_neg_666,\n        gzip_compression=gzip_compression_level)\n    hdf5_out.close()", "label": 1}
{"index": "gp252712", "code": "def parse_affine(header_data):\n    import numpy as np\n    affine = np.zeros((4, 4))\n    for i in range(4):\n        for j in range(4):\n            affine[i, j] = header_data.get(f\"q{i+1}_{j+1}\", 0)\n    return affine[:3,:3], affine[:3,3]", "contrast": "def parse_affine(self, hdat, dataobj=None):\n        if 'affine' in hdat: return to_affine(hdat['affine'])\n        else:                return to_affine(self.default_affine())", "label": 1}
{"index": "gp177681", "code": "def replace_variable_name(lhsAST, rhsAST, es):\n    def rv(name):\n        return getattr(lhsAST, name)\n    return rv(es)", "contrast": "def parse_declaration_expressn_memberaccess(self, lhsAST, rhsAST, es):\n        if isinstance(lhsAST, wdl_parser.Terminal):\n            es = es + lhsAST.source_string\n        elif isinstance(lhsAST, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(lhsAST, wdl_parser.AstList):\n            raise NotImplementedError\n        es = es + '_'\n        if isinstance(rhsAST, wdl_parser.Terminal):\n            es = es + rhsAST.source_string\n        elif isinstance(rhsAST, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(rhsAST, wdl_parser.AstList):\n            raise NotImplementedError\n        return es", "label": 1}
{"index": "gp264651", "code": "    def access_for(self, actor=None, roles=None):\n        if roles is None:\n            roles = self.roles_for(actor=actor)\n        class AccessControlledObject(object):\n            __slots__ = ('_obj', '_read', '_write')\n            def __init__(self, obj):\n                self._obj = obj\n                self._read = set()\n                self._write = set()\n            def __getattr__(self, name):\n                if name in self._read and name not in self._write:\n                    return getattr(self._obj, name)\n                raise AttributeError(name)\n            def __setattr__(self, name, value):\n                if name in self._write:\n                    setattr(self._obj, name, value)\n                elif name in self._read:\n                    raise AttributeError(\"can't set attribute\")\n                else:\n                    raise AttributeError(name)\n        return AccessControlledObject(self)._obj", "contrast": "def access_for(self, roles=None, actor=None, anchors=[]):\n        if roles is None:\n            roles = self.roles_for(actor=actor, anchors=anchors)\n        elif actor is not None or anchors:\n            raise TypeError('If roles are specified, actor/anchors must not be specified')\n        return RoleAccessProxy(self, roles=roles)", "label": 1}
{"index": "gp056792", "code": "def transmit_tc_bpdu(self):\n        if not self.send_tc_flg:\n            timer = datetime.timedelta(seconds=self.port_times.max_age\n                                       + self.port_times.forward_delay)\n            self.send_tc_timer = datetime.datetime.today() + timer\n            self.send_tc_flg = True", "contrast": "def set_send_tc_flg():\n    send_tc_flg = True\n    return send_tc_flg", "label": 0}
{"index": "gp271632", "code": "def assure_window_area(previous_window, new_window):\n    if previous_window[0] < new_window[0]:\n        new_window[0] = previous_window[0]\n    if previous_window[1] < new_window[1]:\n        new_window[1] = previous_window[1]\n    if previous_window[2] > new_window[2]:\n        new_window[2] = previous_window[2]\n    if previous_window[3] > new_window[3]:\n        new_window[3] = previous_window[3]\n    return new_window", "contrast": "def align(self):\n        nofs = align_to_mmap(self.ofs, 0)\n        self.size += self.ofs - nofs    \n        self.ofs = nofs\n        self.size = align_to_mmap(self.size, 1)", "label": 1}
{"index": "gp192576", "code": "import matplotlib.pyplot as plt\ndef visualize_state():\n    fig = plt.figure()\n    return fig", "contrast": "def plot(self):\n        width = 10\n        height = width / 1.618\n        f = plt.figure(figsize=(width, height))\n        ax = f.add_subplot(111, projection=\"3d\")\n        self.plot_state_histogram(ax)\n        return f", "label": 1}
{"index": "gp058389", "code": "def _refresh_authentication_token(self):\n        if self.retry == self._MAX_RETRIES:\n            raise GeocoderAuthenticationFailure(\n                'Too many retries for auth: %s' % self.retry\n            )\n        token_request_arguments = {\n            'username': self.username,\n            'password': self.password,\n            'referer': self.referer,\n            'expiration': self.token_lifetime,\n            'f': 'json'\n        }\n        url = \"?\".join((self.auth_api, urlencode(token_request_arguments)))\n        logger.debug(\n            \"%s._refresh_authentication_token: %s\",\n            self.__class__.__name__, url\n        )\n        self.token_expiry = int(time()) + self.token_lifetime\n        response = self._base_call_geocoder(url)\n        if 'token' not in response:\n            raise GeocoderAuthenticationFailure(\n                'Missing token in auth request.'\n                'Request URL: %s; response JSON: %s' %\n                (url, json.dumps(response))\n            )\n        self.retry = 0\n        self.token = response['token']", "contrast": "import requests\ndef get_new_token(username, password, server):\n    payload = {\n        'f': 'json',\n        'username': username,\n        'password': password,\n        'referer': server\n    }\n    response = requests.post(server + '/arcgis/tokens/generateToken', data=payload)\n    token = response.json().get('token')\n    return token", "label": 0}
{"index": "gp185723", "code": "import numpy as np\ndef pre_process(x, out=None, c2c=None, r2c=None, halfc=None):\n    if out is None:\n        out = np.empty_like(x)\n    if c2c:\n        tmp = np.empty_like(x)\n        np.copyto(tmp, x)\n        tmp_r = np.fft.ifft2(tmp)\n        np.copyto(out, np.fft.fft2(tmp_r))\n    elif r2c:\n        tmp_f = np.fft.rfft2(x)\n        np.copyto(out, tmp_f)\n    elif halfc:\n        tmp_r = np.fft.irfft2(x)\n        np.copyto(out, tmp_r)\n    return out", "contrast": "def _preprocess(self, x, out=None):\n        if out is None:\n            if self.domain.field == ComplexNumbers():\n                out = self._tmp_r if self._tmp_r is not None else self._tmp_f\n            elif self.domain.field == RealNumbers() and not self.halfcomplex:\n                out = self._tmp_f\n            else:\n                out = self._tmp_r\n        return dft_preprocess_data(\n            x, shift=self.shifts, axes=self.axes, sign=self.sign,\n            out=out)", "label": 1}
{"index": "gp336183", "code": "def total_propulsion_power(propulsion_eff, sea_margin):\n    return propulsion_eff * sea_margin", "contrast": "def prop_power(self, propulsion_eff=0.7, sea_margin=0.2):\n        PP = (1 + sea_margin) * self.resistance() * self.speed/propulsion_eff\n        return PP", "label": 1}
{"index": "gp243999", "code": "import signal\ndef catch_signals(callback):\n    def signal_handler(signum, frame):\n        callback(signum)\n    signal.signal(signal.SIGINT, signal_handler) \n    signal.signal(signal.SIGTERM, signal_handler) ", "contrast": "def signal_catcher(callback):\n    def _catch_exit_signal(sig_num, _frame):\n        print('Received signal {:d} invoking callback...'.format(sig_num))\n        callback()\n    signal.signal(signal.SIGINT, _catch_exit_signal)\n    signal.signal(signal.SIGQUIT, _catch_exit_signal)\n    signal.signal(signal.SIGTERM, _catch_exit_signal)\n    yield", "label": 1}
{"index": "gp064800", "code": "def render(cls, data={}, view_template=None, layout=None, **kwargs):\n        if not view_template:\n            stack = inspect.stack()[1]\n            module = inspect.getmodule(cls).__name__\n            module_name = module.split(\".\")[-1]\n            action_name = stack[3]      \n            view_name = cls.__name__    \n            if view_name.endswith(\"View\"):\n                view_name = view_name[:-4]\n            view_template = \"%s/%s.html\" % (view_name, action_name)\n        data = data if data else dict()\n        data[\"__\"] = cls._context if cls._context else {}\n        if kwargs:\n            data.update(kwargs)\n        data[\"__view_template__\"] = view_template\n        return render_template(layout or cls.LAYOUT, **data)", "contrast": "def render_data_to_template(data, view_template=None, layout=None):\n    if not view_template:\n        view_template = f\"{type(data).__name__.lower()}/action.html\"\n    if not layout:\n        layout = \"{% include __view_template__ %}\"\n    template = Template(layout)\n    return template.render(data=data, __view_template__=view_template)", "label": 0}
{"index": "gp229121", "code": "@property\ndef file_name(self):\n    return self._file_name\n@file_name.setter\ndef file_name(self, value):\n    self._file_name = value", "contrast": "def file_name(self, value):\n        if value == self._defaults['fileName'] and 'fileName' in self._values:\n            del self._values['fileName']\n        else:\n            self._values['fileName'] = value", "label": 1}
{"index": "gp212338", "code": "import base64\nimport Crypto\nfrom Crypto.Cipher import AES\ndef decrypt_content(document_id: str, encrypted_content: str, account: Account) -> str:\n    key = account.get_secret_key(document_id)\n    cipher = AES.new(key, AES.MODE_CBC, IV=key[:16])\n    encrypted_content_bytes = base64.b64decode(encrypted_content)\n    decrypted_content_bytes = cipher.decrypt(encrypted_content_bytes)\n    return decrypted_content_bytes.decode('utf-8')", "contrast": "def decrypt(self, document_id, encrypted_content, account):\n        return self._secret_store(account).decrypt_document(document_id, encrypted_content)", "label": 1}
{"index": "gp157015", "code": "def set_location(self, latitude, longitude, altitude=10):\n        driver = self._current_application()\n        driver.set_location(latitude,longitude,altitude)", "contrast": "def set_location(latitude, longitude, altitude=10):\n    if platform.system() == 'Windows':\n        raise Exception('This function can only be used in Android devices')\n    driver = BuiltIn().get_library_instance('SeleniumLibrary').driver\n    driver.execute_script(f'geo_position = {{{{ \"latitude\": \"{latitude}\", \"longitude\": \"{longitude}\", \"altitude\": \"{altitude}\" }}}}; return navigator.geolocation.setCurrentPosition((position) => console.log(position.coords, position.timestamp), (error) => console.error(error), geo_position);')", "label": 0}
{"index": "gp268871", "code": "def create_network(config_file_path):\n    with open(config_file_path, 'r') as f:\n        network = json.load(f)\n    return network", "contrast": "def create_network(self):\n        class_ = getattr(networks, self.network_class)\n        return class_(max_size=self.quorum)", "label": 1}
{"index": "gp012364", "code": "def get_widgets_that_need_update(self):\n        result = []\n        for widget_name, widget in self.get_widgets().items():\n            if widget.should_update():\n                result.append(widget)\n        return result", "contrast": "def get_updated_widgets():\n    updated_widgets = [widget1, widget2, widget3] \n    return updated_widgets", "label": 0}
{"index": "gp153136", "code": "def _chk_qualifier(self, qualifiers, flds, lnum):\n        for qual in qualifiers:\n            if qual not in AnnoReaderBase.exp_qualifiers:\n                errname = 'UNEXPECTED QUALIFIER({QUAL})'.format(QUAL=qual)\n                self.illegal_lines[errname].append((lnum, \"\\t\".join(flds)))", "contrast": "def check_qualifiers(qualifiers, expected_values):\n    for qual in qualifiers:\n        if qual not in expected_values:\n            return False\n    return True", "label": 0}
{"index": "gp104319", "code": "def delete_url(self, url):\n        for decompress in [False, True]:\n            key = (url, decompress)\n            if key in self._local_paths:\n                path = self._local_paths[key]\n                remove(path)\n                del self._local_paths[key]\n            path = self.local_path(\n                url, decompress=decompress, download=False)\n            if exists(path):\n                remove(path)", "contrast": "import os\ndef delete_local_files(url):\n    for root, dirs, files in os.walk(\".\"):\n        for file in files:\n            if file.startswith(url.rsplit('/', 1)[-1]):\n                os.remove(os.path.join(root,file))\n                print(f\"{os.path.join(root,file)} deleted successfully!\")", "label": 0}
{"index": "gp210807", "code": "def get_signal_header():\n    header = {'name': 'Signal Name', 'type': 'Signal Type', 'units': 'Signal Units'}\n    return header", "contrast": "def getSignalHeader(self, chn):\n        return {'label': self.getLabel(chn),\n                'dimension': self.getPhysicalDimension(chn),\n                                 'sample_rate': self.getSampleFrequency(chn),\n                'physical_max':self.getPhysicalMaximum(chn),\n                'physical_min': self.getPhysicalMinimum(chn),\n                'digital_max': self.getDigitalMaximum(chn),\n                'digital_min': self.getDigitalMinimum(chn),\n                'prefilter':self.getPrefilter(chn),\n                'transducer': self.getTransducer(chn)}", "label": 1}
{"index": "gp324499", "code": "def remove_groups(group_list):\n    for group in group_list:\n        for m in group.metabolites:\n            if m.id in model.metabolites:\n                continue\n            else:\n                group.remove_member(m)\n        for r in group.reactions:\n            if r.id in model.reactions:\n                continue\n            else:\n                group.remove_member(r)\n        for g in group.genes:\n            if g.id in model.genes:\n                continue\n            else:\n                group.remove_member(g)\n    return model", "contrast": "def remove_groups(self, group_list):\n        if isinstance(group_list, string_types) or                hasattr(group_list, \"id\"):\n            warn(\"need to pass in a list\")\n            group_list = [group_list]\n        for group in group_list:\n            if group.id not in self.groups:\n                LOGGER.warning(\"%r not in %r. Ignored.\", group, self)\n            else:\n                self.groups.remove(group)\n                group._model = None", "label": 1}
{"index": "gp313367", "code": "import functools\ndef defer_sync(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        return result\n    return wrapper", "contrast": "def defer_entity_syncing(wrapped, instance, args, kwargs):\n    sync_entities.defer = True\n    try:\n        return wrapped(*args, **kwargs)\n    finally:\n        sync_entities.defer = False\n        model_objs = list(sync_entities.buffer.values())\n        if None in sync_entities.buffer:\n            model_objs = list()\n        if len(sync_entities.buffer):\n            sync_entities(*model_objs)\n        sync_entities.buffer = {}", "label": 1}
{"index": "gp305268", "code": "from pybel import readstring\ndef convert_to_pybel(data, infer_bonds=True):\n    mol_str = f\"{data['molecule']['atoms']}\\n{data['molecule']['bonds']}\"\n    if infer_bonds:\n        mol_str += 'InChI=1S/'\n        for atom in data['molecule']['atoms']:\n            mol_str += f\"{atom['element']}\"\n            if 'charge' in atom:\n                mol_str += f\"{abs(atom['charge'])}{'+-'[atom['charge'] < 0]}\"\n        read_string = readstring(\"smi\", mol_str)\n        return read_string\n    else:\n        read_string = readstring(\"json\", json.dumps(data))\n        return read_string", "contrast": "def json_to_pybel(data, infer_bonds=False):\n    obmol = ob.OBMol()\n    obmol.BeginModify()\n    for atom in data['atoms']:\n        obatom = obmol.NewAtom()\n        obatom.SetAtomicNum(table.GetAtomicNum(str(atom['element'])))\n        obatom.SetVector(*atom['location'])\n        if 'label' in atom:\n            pd = ob.OBPairData()\n            pd.SetAttribute('_atom_site_label')\n            pd.SetValue(atom['label'])\n            obatom.CloneData(pd)\n    if 'bonds' not in data or not data['bonds']:\n        if infer_bonds:\n            obmol.ConnectTheDots()\n            obmol.PerceiveBondOrders()\n    else:\n        for bond in data['bonds']:\n            if 'atoms' not in bond:\n                continue\n            obmol.AddBond(bond['atoms'][0] + 1, bond['atoms'][1] + 1,\n                          bond['order'])\n    if 'unitcell' in data:\n        uc = ob.OBUnitCell()\n        uc.SetData(*(ob.vector3(*v) for v in data['unitcell']))\n        uc.SetSpaceGroup('P1')\n        obmol.CloneData(uc)\n    obmol.EndModify()\n    mol = pybel.Molecule(obmol)\n    if 'charge' in data['atoms'][0]:\n        mol.OBMol.SetPartialChargesPerceived()\n        for atom, pyatom in zip(data['atoms'], mol.atoms):\n            pyatom.OBAtom.SetPartialCharge(atom['charge'])\n    return mol", "label": 1}
{"index": "gp017931", "code": "def masked_rec_array_to_mgr(data, index, columns, dtype, copy):\n    fill_value = data.fill_value\n    fdata = ma.getdata(data)\n    if index is None:\n        index = get_names_from_index(fdata)\n        if index is None:\n            index = ibase.default_index(len(data))\n    index = ensure_index(index)\n    if columns is not None:\n        columns = ensure_index(columns)\n    arrays, arr_columns = to_arrays(fdata, columns)\n    new_arrays = []\n    for fv, arr, col in zip(fill_value, arrays, arr_columns):\n        mask = ma.getmaskarray(data[col])\n        if mask.any():\n            arr, fv = maybe_upcast(arr, fill_value=fv, copy=True)\n            arr[mask] = fv\n        new_arrays.append(arr)\n    arrays, arr_columns = reorder_arrays(new_arrays, arr_columns, columns)\n    if columns is None:\n        columns = arr_columns\n    mgr = arrays_to_mgr(arrays, arr_columns, index, columns, dtype)\n    if copy:\n        mgr = mgr.copy()\n    return mgr", "contrast": "import numpy.ma as ma\nimport pandas as pd\ndef create_manager(masked_array):\n    mask = masked_array.mask\n    unmasked_array = ma.masked_array(masked_array.data, mask=mask)\n    df = pd.DataFrame(unmasked_array)\n    if hasattr(masked_array, 'dtype'):\n        fields = masked_array.dtype.names\n        if fields:\n            df.columns = fields\n    return pd.DataFrame(data=df).manager", "label": 0}
{"index": "gp298574", "code": "def parse_accept_header(accept_header: str, context_aware: bool = False) -> Tuple[str, str]:\n    if not accept_header:\n        return \"application/rdf+xml\", \"xml\"\n    accept_values = [a.strip() for a in accept_header.split(\",\")]\n    if \"*/*\" in accept_values:\n        return \"application/rdf+xml\", \"xml\"\n    known_mimetypes = {\n        \"application/json\": \"json\",\n        \"application/n-triples\": \"nt\",\n        \"application/ld+json\": \"json-ld\",\n        \"application/rdf+xml\": \"xml\",\n        \"text/turtle\": \"ttl\"\n    }\n    best_mime = None\n    best_format = None\n    best_quality = 0.0\n    for av in accept_values:\n        parts = av.split(\";\")\n        mimetype = parts[0].strip()\n        if mimetype in known_mimetypes:\n            fmt = known_mimetypes.get(mimetype)\n            quality = 1.0\n            for p in parts[1:]:\n                p = p.strip()\n                if p.startswith(\"q=\"):\n                    quality = float(p[2:])\n            if quality == 1.0:\n                return mimetype, fmt\n            if best_mime is None or quality > best_quality:\n                best_mime = mimetype\n                best_format = fmt\n                best_quality = quality\n    if best_mime is None:\n        return None, None\n    else:\n        return best_mime, best_format if not context_aware else best_format+\".nq\"", "contrast": "def decide(self, accepts, context_aware=False):\n  mimetype = self.decide_mimetype(accepts, context_aware)\n  if mimetype is not None:\n\t\t\treturn (mimetype, self.get_serialize_format(mimetype))\n  else:\n\t\t\treturn (None, None)", "label": 1}
{"index": "gp156443", "code": "def _on_event(self, event):\n        if self.has_option(event):\n            on_event = self.get_option(event).upper()\n            if on_event not in [\"NO ACTION\", \"RESTRICT\"]:\n                return on_event\n        return False", "contrast": "def get_referential_action(event):\n    referential_actions = {'INSERT': 'CASCADE', 'UPDATE': 'CASCADE', 'DELETE': 'CASCADE'}\n    return referential_actions.get(event.upper())", "label": 0}
{"index": "gp276722", "code": "def delete_project(project_name):\n    try:\n        return \"Project {} deleted successfully.\".format(project_name)\n    except:\n        raise ItemNotFound(\"Project {} not found.\".format(project_name))", "contrast": "def delete_project(self, project_name):\n        project = self._get_project_for_name(project_name)\n        project.delete()\n        self.clear_project_cache()", "label": 1}
{"index": "gp167663", "code": "def DbGetDeviceAttributeList(self, argin):\n        self._log.debug(\"In DbGetDeviceAttributeList()\")\n        dev_name = argin[0]\n        wildcard = argin[1]\n        if not wildcard:\n            wildcard = \"%\"\n        else:\n            wildcard = replace_wildcard(wildcard)\n        return self.db.get_device_attribute_list(dev_name, wildcard)", "contrast": "import tango\ndef get_matching_attributes(argin):\n    device_name, wildcard = argin\n    device_proxy = tango.DeviceProxy(device_name)\n    attribute_list = device_proxy.get_attribute_list()\n    matching_attributes = [attribute for attribute in attribute_list if wildcard in attribute]\n    return tango.DevVarStringArray(matching_attributes)", "label": 0}
{"index": "gp174639", "code": "import yaml\ndef load_parameters(fname):\n    with open(fname, 'r') as f:\n        parameters = yaml.safe_load(f)\n    return parameters", "contrast": "def load_params(fname: str) -> Tuple[Dict[str, mx.nd.NDArray], Dict[str, mx.nd.NDArray]]:\n    save_dict = mx.nd.load(fname)\n    arg_params = {}\n    aux_params = {}\n    for k, v in save_dict.items():\n        tp, name = k.split(':', 1)\n        if tp == 'arg':\n            if \"att_enc_kv2h_weight\" in name:\n                logger.info(\"Splitting '%s' parameters into separate k & v matrices.\", name)\n                v_split = mx.nd.split(v, axis=0, num_outputs=2)\n                arg_params[name.replace('kv2h', \"k2h\")] = v_split[0]\n                arg_params[name.replace('kv2h', \"v2h\")] = v_split[1]\n            else:\n                arg_params[name] = v\n        if tp == 'aux':\n            aux_params[name] = v\n    return arg_params, aux_params", "label": 1}
{"index": "gp078954", "code": "def is_numeric(obj):\n    try:\n        obj+obj, obj-obj, obj*obj, obj**obj, obj/obj\n    except ZeroDivisionError:\n        return True\n    except Exception:\n        return False\n    else:\n        return True", "contrast": "def is_numeric(obj):\n    try:\n        float(obj)\n        return True\n    except ValueError:\n        return False", "label": 0}
{"index": "gp111586", "code": "def get_plugs_mail_classes(self, app):\n        classes = []\n        members = self.get_members(app)\n        for member in members:\n            name, cls = member\n            if inspect.isclass(cls) and issubclass(cls, PlugsMail) and name != 'PlugsMail':\n                files_ = self.get_template_files(app.__file__, name)\n                for file_ in files_:\n                    try:\n                        description = cls.description\n                        location = file_\n                        language = self.get_template_language(location)\n                        classes.append((name, location, description, language))\n                    except AttributeError:\n                        raise AttributeError('Email class must specify email description.')\n        return classes", "contrast": "def convert_list_of_tuples_to_list_of_dicts(tuples_list):\n    return [dict(t) for t in tuples_list]", "label": 0}
{"index": "gp247491", "code": "def direct_children(obj):\n    for child in obj.children:\n        yield child", "contrast": "def _children(self):\n        if self.method_of:\n            yield self.method_of\n        for codeobj in self.arguments:\n            if isinstance(codeobj, CodeExpression):\n                yield codeobj", "label": 1}
{"index": "gp206629", "code": "def check_axis(obj):\n    return hasattr(obj, 'axis') and hasattr(obj, 'sel_axis')", "contrast": "def require_axis(f):\n    @wraps(f)\n    def _wrapper(self, *args, **kwargs):\n        if None in (self.axis, self.sel_axis):\n            raise ValueError('%(func_name) requires the node %(node)s '\n                    'to have an axis and a sel_axis function' %\n                    dict(func_name=f.__name__, node=repr(self)))\n        return f(self, *args, **kwargs)\n    return _wrapper", "label": 1}
{"index": "gp137227", "code": "def list_route_advertised_from_bgp_speaker(self, speaker_id, **_params):\n        return self.get((self.bgp_speaker_path % speaker_id) +\n                        \"/get_advertised_routes\", params=_params)", "contrast": "def fetch_bgp_routes(bgp_speaker_ip):\n    routes = []\n    bgp_speaker = connect_to_bgp_speaker(bgp_speaker_ip)\n    routes = bgp_speaker.get_routes()\n    return routes", "label": 0}
{"index": "gp280239", "code": "def list_type(field_type, *annotations):\n    return List[field_type, *annotations]", "contrast": "def p_list_type(self, p):\n        p[0] = ast.ListType(value_type=p[3], annotations=p[5])", "label": 1}
{"index": "gp128157", "code": "def _parse_auth_message(self, auth_message):\n        result = {}\n        has_matched = False\n        for regex in REGEXES_INVALID_USER:\n            m = re.search(regex, auth_message)\n            if m and not has_matched:\n                has_matched = True\n                result['username'] = m.group('user')\n                result['ip'] = m.group('ip')\n        for regex in REGEXES_INVALID_IP:\n            m = re.search(regex, auth_message)\n            if m and not has_matched:\n                has_matched = True\n                result['ip'] = m.group('ip')                        \n        for regex in REGEXES_IGNORE:\n            m = re.search(regex, auth_message)\n            if m and not has_matched:\n                has_matched = True\n        if not has_matched:\n            sys.stderr.write(\"Unhandled auth message: %s\\n\" % auth_message)\n        return result", "contrast": "import re\ndef parse_message(auth_message):\n    pattern = r'((?:\\d{1,3}\\.){3}\\d{1,3})|@(\\w+)'\n    matches = re.findall(pattern, auth_message)\n    result = {}\n    for match in matches:\n        if match[0]:\n            result['ip'] = match[0]\n        elif match[1]:\n            result['user'] = match[1]\n    return result", "label": 0}
{"index": "gp105800", "code": "def get_in_segmentlistdict(self, process_ids = None):\n  seglists = segments.segmentlistdict()\n  for row in self:\n\t\t\tifos = row.instruments or (None,)\n   if process_ids is None or row.process_id in process_ids:\n\t\t\t\tseglists.extend(dict((ifo, segments.segmentlist([row.in_segment])) for ifo in ifos))\n  return seglists", "contrast": "def segmentlistdict(instruments_table, process_ids=None):\n    segmentlistdict = {}\n    for row in instruments_table:\n        if process_ids is None or row['process_id'] in process_ids:\n            instrument = row['instrument']\n            segment = row['segment']\n            if instrument not in segmentlistdict:\n                segmentlistdict[instrument] = []\n            segmentlistdict[instrument].append(segment)\n    return segmentlistdict", "label": 0}
{"index": "gp154569", "code": "def remove_invalid_fields(self, queryset, fields, view, request):\n        valid_fields = [\n            item[0] for item in self.get_valid_fields(queryset, view,\n                                                      {'request': request})\n        ]\n        bad_terms = [\n            term for term in fields\n            if format_value(term.replace(\".\", \"__\").lstrip('-'), \"underscore\") not in valid_fields\n        ]\n        if bad_terms:\n            raise ValidationError('invalid sort parameter{}: {}'.format(\n                ('s' if len(bad_terms) > 1 else ''), ','.join(bad_terms)))\n        underscore_fields = []\n        for item in fields:\n            item_rewritten = item.replace(\".\", \"__\")\n            if item_rewritten.startswith('-'):\n                underscore_fields.append(\n                    '-' + format_value(item_rewritten.lstrip('-'), \"underscore\"))\n            else:\n                underscore_fields.append(format_value(item_rewritten, \"underscore\"))\n        return super(OrderingFilter, self).remove_invalid_fields(\n            queryset, underscore_fields, view, request)", "contrast": "from rest_framework.exceptions import ValidationError\nfrom rest_framework.filters import OrderingFilter\nclass CustomOrderingFilter(OrderingFilter):\n    def remove_invalid_fields(self, queryset, fields, view, request):\n        valid_fields = self.get_valid_fields(queryset, view, {'request': request})\n        invalid_fields = set(fields) - set(valid_fields)\n        if invalid_fields:\n            error_message = 'Invalid sort parameter(s): {}'.format(', '.join(invalid_fields))\n            raise ValidationError({'detail': error_message})\n        return super().remove_invalid_fields(queryset, fields, view, request)", "label": 0}
{"index": "gp054705", "code": "def get_traffic(self, subreddit):\n        url = self.config['subreddit_traffic'].format(\n            subreddit=six.text_type(subreddit))\n        return self.request_json(url)", "contrast": "import requests\ndef get_traffic_stats(subreddit):\n    url = f\"https://www.reddit.com/r/{subreddit}/about/traffic.json\"\n    headers = {'User-Agent': 'Mozilla/5.0'}\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()", "label": 0}
{"index": "gp331347", "code": "import dateutil.parser\nfrom datetime import datetime\ndef determine_timestamp_format(timestamp):\n    formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%d/%m/%Y %H:%M:%S', '%d/%m/%Y']\n    for format in formats:\n        try:\n            datetime.strptime(timestamp, format)\n            return format\n        except ValueError:\n            continue\n    try:\n        dateutil.parser.parse(timestamp)\n        return \"unknown format, parsed by dateutil.parser\"\n    except ValueError:\n        return \"unable to determine format\"", "contrast": "def detect_timestamp_format(timestamp):\n  time_formats = {\n      'epoch': re.compile(r'^[0-9]{10}$'),\n      'epoch_ms': re.compile(r'^[0-9]{13}$'),\n      'epoch_fraction': re.compile(r'^[0-9]{10}\\.[0-9]{3,9}$'),\n      '%Y-%m-%d %H:%M:%S': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y-%m-%dT%H:%M:%S': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y-%m-%d_%H:%M:%S': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y-%m-%d %H:%M:%S.%f': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y-%m-%dT%H:%M:%S.%f': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y-%m-%d_%H:%M:%S.%f': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y%m%d %H:%M:%S': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y%m%dT%H:%M:%S': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y%m%d_%H:%M:%S': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%Y%m%d %H:%M:%S.%f': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y%m%dT%H:%M:%S.%f': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y%m%d_%H:%M:%S.%f': re.compile(r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%H:%M:%S': re.compile(r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$'),\n      '%H:%M:%S.%f': re.compile(r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$'),\n      '%Y-%m-%dT%H:%M:%S.%f%z': re.compile(r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+[+-][0-9]{4}$')\n  }\n  for time_format in time_formats:\n    if re.match(time_formats[time_format], timestamp):\n      return time_format\n  return 'unknown'", "label": 1}
{"index": "gp312588", "code": "def get_stream_info(size, frame_num, info_msgs):\n    info_str = f\"Size: {size}, Frame Number: {frame_num}, Info Messages:\\n\"\n    for msg in info_msgs:\n        if msg and frame_num != -1:\n            info_str += f\"\\t- {msg}\\n\"\n    return info_str", "contrast": "def info_string(self, size=None, message='', frame=-1):\n        info = []\n        if size is not None:\n            info.append('Size: {1}x{0}'.format(*size))\n        elif self.size is not None:\n            info.append('Size: {1}x{0}'.format(*self.size))\n        if frame >= 0:\n            info.append('Frame: {}'.format(frame))\n        if message != '':\n            info.append('{}'.format(message))\n        return ' '.join(info)", "label": 1}
{"index": "gp186150", "code": "import boto3\nfrom botocore.exceptions import ClientError\ndef send_email_with_ses(recipients, subject, body_html, body_text):\n    AWS_REGION = 'us-east-1'\n    CHARSET = 'UTF-8'\n    SENDER = 'sender@example.com'\n    client = boto3.client('ses', region_name=AWS_REGION)\n    try:\n        response = client.send_email(\n            Destination={\n                'ToAddresses': recipients,\n            },\n            Message={\n                'Body': {\n                    'Html': {\n                        'Charset': CHARSET,\n                        'Data': body_html,\n                    },\n                    'Text': {\n                        'Charset': CHARSET,\n                        'Data': body_text,\n                    },\n                },\n                'Subject': {\n                    'Charset': CHARSET,\n                    'Data': subject,\n                },\n            },\n            Source=SENDER,\n        )\n    except ClientError as e:\n        print(e.response['Error']['Message'])\n    else:\n        return None", "contrast": "def __send_ses_email(self, recipients, subject, body_html, body_text):\n        source_arn = dbconfig.get('source_arn', NS_EMAIL)\n        return_arn = dbconfig.get('return_path_arn', NS_EMAIL)\n        session = get_local_aws_session()\n        ses = session.client('ses', region_name=dbconfig.get('ses_region', NS_EMAIL, 'us-west-2'))\n        body = {}\n        if body_html:\n            body['Html'] = {\n                'Data': body_html\n            }\n        if body_text:\n            body['Text'] = {\n                'Data': body_text\n            }\n        ses_options = {\n            'Source': self.sender,\n            'Destination': {\n                'ToAddresses': recipients\n            },\n            'Message': {\n                'Subject': {\n                    'Data': subject\n                },\n                'Body': body\n            }\n        }\n        if source_arn and return_arn:\n            ses_options.update({\n                'SourceArn': source_arn,\n                'ReturnPathArn': return_arn\n            })\n        ses.send_email(**ses_options)", "label": 1}
{"index": "gp310165", "code": "from math import log2\ndef calculate_entropy(data, attribute, value):\n    total_occurrences = 0\n    value_occurrences = 0\n    for row in data:\n        if row[attribute] == value:\n            total_occurrences += 1\n            if row[-1] == 'yes':\n                value_occurrences += 1\n    if total_occurrences == 0:\n        return 0\n    if value_occurrences == 0 or value_occurrences == total_occurrences:\n        return 0\n    value_probability = value_occurrences / total_occurrences\n    value_complement_probability = 1 - value_probability\n    entropy = -(value_probability * log2(value_probability) + value_complement_probability * log2(value_complement_probability))\n    return entropy", "contrast": "def get_entropy(self, attr_name=None, attr_value=None):\n        is_con = self.tree.data.is_continuous_class\n        if is_con:\n            if attr_name is None:\n                var = self._class_cdist.variance\n            else:\n                var = self._attr_value_cdist[attr_name][attr_value].variance\n            if self.tree.metric == VARIANCE1 or attr_name is None:\n                return var\n            elif self.tree.metric == VARIANCE2:\n                unique_value_count = len(self._attr_value_counts[attr_name])\n                attr_total = float(self._attr_value_count_totals[attr_name])\n                return var*(unique_value_count/attr_total)\n        else:\n            if attr_name is None:\n                total = float(self._class_ddist.total)\n                counts = self._class_ddist.counts\n                unique_value_count = len(self._class_ddist.counts)\n                attr_total = total\n            else:\n                total = float(self._attr_value_counts[attr_name][attr_value])\n                counts = self._attr_class_value_counts[attr_name][attr_value]\n                unique_value_count = len(self._attr_value_counts[attr_name])\n                attr_total = float(self._attr_value_count_totals[attr_name])\n            assert total, \"There must be at least one non-zero count.\"\n            n = max(2, len(counts))\n            if self._tree.metric == ENTROPY1:\n                return -sum(\n                    (count/total)*math.log(count/total, n)\n                    for count in itervalues(counts)\n                )\n            elif self._tree.metric == ENTROPY2:\n                return -sum(\n                    (count/total)*math.log(count/total, n)\n                    for count in itervalues(counts)\n                ) + (unique_value_count/attr_total)\n            elif self._tree.metric == ENTROPY3:\n                return -sum(\n                    (count/total)*math.log(count/total, n)\n                    for count in itervalues(counts)\n                ) + 100*(unique_value_count/attr_total)", "label": 1}
{"index": "gp295820", "code": "def convert_string_to_number(string_list):\n    num_list = []\n    for string in string_list:\n        if string.isdigit():\n            num_list.append(int(string))\n        else:\n            num_list.append(string)\n    return num_list", "contrast": "def atoi(text: str) -> Union[int, str]:\n    return int(text) if text.isdigit() else text", "label": 1}
{"index": "gp184665", "code": "from warcio.archiveiterator import ArchiveIterator\nimport requests\ndef load_and_parse_record(url:str, offset:int, length:int, record_type:str):\n    resp = requests.get(url, headers={'Range': f'bytes={offset}-{offset+length-1}'})\n    content = resp.content\n    if record_type == 'warc':\n        record = next(ArchiveIterator(stream=content, arc2warc=False))\n    elif record_type == 'arc':\n        record = next(ArchiveIterator(stream=content, arc2warc=True))\n    else:\n        raise ValueError(f\"Invalid record type: {record_type}\")\n    return record", "contrast": "def load(self, url, offset, length, no_record_parse=False):\n        try:\n            length = int(length)\n        except:\n            length = -1\n        stream = self.loader.load(url, int(offset), length)\n        decomp_type = 'gzip'\n        stream = DecompressingBufferedReader(stream=stream,\n                                             decomp_type=decomp_type,\n                                             block_size=self.block_size)\n        return self.parse_record_stream(stream, no_record_parse=no_record_parse)", "label": 1}
{"index": "gp278166", "code": "def send_vnic_event(port, net_uuid, segmentation_id, status):\n    payload = {\n        \"port_id\": port.vif_id,\n        \"net_uuid\": net_uuid,\n        \"tag\": segmentation_id,\n        \"status\": status\n    }", "contrast": "def send_vdp_port_event(self, port_uuid, mac, net_uuid,\n                            segmentation_id, status, oui):\n        try:\n            with self.ovs_vdp_lock:\n                ret = self.send_vdp_port_event_internal(port_uuid, mac,\n                                                        net_uuid,\n                                                        segmentation_id,\n                                                        status, oui)\n                return ret\n        except Exception as e:\n            LOG.error(\"Exception in send_vdp_port_event %s\" % str(e))\n            return {'result': False, 'fail_reason': str(e)}", "label": 1}
{"index": "gp274544", "code": "def add_health_monitor(load_balancer, monitor_settings):\n    if 'health_monitor' in load_balancer:\n        load_balancer['health_monitor'].update(monitor_settings)\n    else:\n        load_balancer['health_monitor'] = monitor_settings\n    return load_balancer", "contrast": "def add_health_monitor(self, loadbalancer, type, delay=10, timeout=10,\n            attemptsBeforeDeactivation=3, path=\"/\", statusRegex=None,\n            bodyRegex=None, hostHeader=None):\n        uri = \"/loadbalancers/%s/healthmonitor\" % utils.get_id(loadbalancer)\n        req_body = {\"healthMonitor\": {\n                \"type\": type,\n                \"delay\": delay,\n                \"timeout\": timeout,\n                \"attemptsBeforeDeactivation\": attemptsBeforeDeactivation,\n                }}\n        uptype = type.upper()\n        if uptype.startswith(\"HTTP\"):\n            lb = self._get_lb(loadbalancer)\n            if uptype != lb.protocol:\n                raise exc.ProtocolMismatch(\"Cannot set the Health Monitor type \"\n                        \"to '%s' when the Load Balancer's protocol is '%s'.\" %\n                        (type, lb.protocol))\n            if not all((path, statusRegex, bodyRegex)):\n                raise exc.MissingHealthMonitorSettings(\"When creating an HTTP(S) \"\n                        \"monitor, you must provide the 'path', 'statusRegex' and \"\n                        \"'bodyRegex' parameters.\")\n            body_hm = req_body[\"healthMonitor\"]\n            body_hm[\"path\"] = path\n            body_hm[\"statusRegex\"] = statusRegex\n            body_hm[\"bodyRegex\"] = bodyRegex\n            if hostHeader:\n                body_hm[\"hostHeader\"] = hostHeader\n        resp, body = self.api.method_put(uri, body=req_body)\n        return body", "label": 1}
{"index": "gp035765", "code": "def exists(instance_id=None, name=None, tags=None, region=None, key=None,\n           keyid=None, profile=None, in_states=None, filters=None):\n    instances = find_instances(instance_id=instance_id, name=name, tags=tags,\n                               region=region, key=key, keyid=keyid,\n                               profile=profile, in_states=in_states, filters=filters)\n    if instances:\n        log.info('Instance exists.')\n        return True\n    else:\n        log.warning('Instance does not exist.')\n        return False", "contrast": "import boto3\nfrom botocore.exceptions import ClientError\ndef exists(instance_id):\n    try:\n        ec2 = boto3.resource('ec2')\n        instances = ec2.instances.filter(Filters=[{'Name': 'instance-id', 'Values': [instance_id]}])\n        for instance in instances:\n            return True\n        return False\n    except ClientError:\n        return False", "label": 0}
{"index": "gp172259", "code": "def get_serialization_method(data_type):\n    if data_type == str:\n        return str.encode, bytes.decode\n    elif data_type == int:\n        return int.to_bytes, int.from_bytes\n    elif data_type == float:\n        return float.to_bytes, float.from_bytes\n    elif data_type == bool:\n        return bool.__int__.to_bytes, bool.from_bytes\n    else:\n        raise TypeError('Unsupported data type')", "contrast": "def _fmt_serialization_call(self, data_type, input_value, serialize, depth=0):\n        data_type, _ = unwrap_nullable(data_type)\n        serializer_func = 'serialize' if serialize else 'deserialize'\n        serializer_args = []\n        if is_primitive_type(data_type):\n            return input_value\n        if is_list_type(data_type) or is_map_type(data_type):\n            serializer_args.append(('value', input_value))\n            elem_data_type = (data_type.value_data_type if\n                is_map_type(data_type) else data_type.data_type)\n            serialization_call = self._fmt_serialization_call(\n                elem_data_type, 'elem{}'.format(depth), serialize, depth + 1)\n            data_struct_block = '^id(id elem{}) {{ return {}; }}'.format(\n                depth, serialization_call)\n            serializer_args.append(('withBlock', data_struct_block))\n        elif is_timestamp_type(data_type):\n            serializer_args.append(('value', input_value))\n            serializer_args.append(('dateFormat',\n                                    '@\"{}\"'.format(data_type.format)))\n        else:\n            serializer_args.append(('value', input_value))\n        return '{}'.format(\n            fmt_func_call(\n                caller=fmt_serial_obj(data_type),\n                callee=serializer_func,\n                args=fmt_func_args(serializer_args)))", "label": 1}
{"index": "gp272390", "code": "def add_suffix_to_filename(path, suffix):\n    name, ext = os.path.splitext(path)\n    return f\"{name}{suffix}{ext}\"", "contrast": "def add_suffix(path, suffix=\"\"):\n    return join(dirname(path), basename(path, ext=False) + suffix + extname(path))", "label": 1}
{"index": "gp194643", "code": "def mass_from_composition(composition):\n    atomic_weights = {\n        0: 0.000549, 1: 1.008, 2: 4.003, 3: 6.941,\n        4: 9.012, 5: 10.81, 6: 12.01, 7: 14.01,\n        8: 16.00, 9: 19.00, 10: 20.18, 11: 22.99,\n        12: 24.31, 13: 26.98, 14: 28.09, 15: 30.97,\n        16: 32.07, 17: 35.45, 18: 39.94, 19: 39.10,\n        20: 40.08, 21: 44.96, 22: 47.87, 23: 50.94,\n        24: 52.00, 25: 54.94, 26: 55.85, 27: 58.93,\n        28: 58.69, 29: 63.55, 30: 65.38, 31: 69.72,\n        32: 72.63, 33: 74.92, 34: 78.96, 35: 79.90,\n        36: 83.80, 37: 85.47, 38: 87.62, 39: 88.91,\n        40: 91.22, 41: 92.91, 42: 95.94, 43: 98.91,\n        44: 101.07, 45: 102.91, 46: 106.42, 47: 107.87,\n        48: 112.41, 49: 114.82, 50: 118.71, 51: 121.76,\n        52: 127.60, 53: 126.90, 54: 131.29, 55: 132.91,\n        56: 137.33, 57: 138.91, 58: 140.12, 59: 140.91,\n        60: 144.24, 61: 145.00, 62: 150.36, 63: 151.96,\n        64: 157.25, 65: 158.93, 66: 162.50, 67: 164.93,\n        68: 167.26, 69: 168.93, 70: 173.05, 71: 174.97,\n        72: 178.49, 73: 180.95, 74: 183.84, 75: 186.21,\n        76: 190.23, 77: 192.22, 78: 195.08, 79: 196.97,\n        80: 200.59, 81: 204.38, 82: 207.2, 83: 208.98,\n        84: 209, 85: 210, 86: 222, 87: 223, 88: 226,\n        89: 227, 90: 232.04, 91: 231.04, 92: 238.03,\n        93: 237.05, 94: 244.06, 95: 243.06, 96: 247.07,\n        97: 247.07, 98: 251.08, 99: 252.08, 100: 257.10,\n        101: 258.10, 102: 259.10, 103: 262.11, 104: 261.11,\n        105: 262.11, 106: 266.12, 107: 264.12, 108: 277.13\n    }\n    total_mass = 0.0\n    for atomic_number, coefficient in composition.items():\n        total_mass += atomic_weights[atomic_number] * coefficient\n    return total_mass", "contrast": "def mass_from_composition(composition):\n    mass = 0.0\n    for k, v in composition.items():\n        if k == 0:  \n            mass -= v*5.489e-4\n        else:\n            mass += v*relative_atomic_masses[k-1]\n    return mass", "label": 1}
{"index": "gp268852", "code": "def recruit_participants(num_participants, current_participants):\n    if num_participants > len(current_participants):\n        remaining_participants = num_participants - len(current_participants)\n        print(f'{remaining_participants} participants needed. Please recruit more participants.')\n    else:\n        print('Sufficient number of participants available.')", "contrast": "def recruit(self):\n        num_approved = len(Participant.query.filter_by(status=\"approved\").all())\n        end_of_generation = num_approved % self.generation_size == 0\n        complete = num_approved >= (self.generations * self.generation_size)\n        if complete:\n            self.log(\"All networks full: closing recruitment\", \"-----\")\n            self.recruiter.close_recruitment()\n        elif end_of_generation:\n            self.log(\"generation finished, recruiting another\")\n            self.recruiter.recruit(n=self.generation_size)", "label": 1}
{"index": "gp227600", "code": "def establish_session(message):\n    if not all(param in message for param in [\"dh_server_public\", \"dh_consumer_public\"]):\n        raise ProtocolError(\"Parameters required to establish session are missing\")\n    return DiffieHellmanSHA1ServerSession(message.get(\"dh_server_public\"), message.get(\"dh_consumer_public\"))", "contrast": "def fromMessage(cls, message):\n        dh_modulus = message.getArg(OPENID_NS, 'dh_modulus')\n        dh_gen = message.getArg(OPENID_NS, 'dh_gen')\n        if (dh_modulus is None and dh_gen is not None or\n            dh_gen is None and dh_modulus is not None):\n            if dh_modulus is None:\n                missing = 'modulus'\n            else:\n                missing = 'generator'\n            raise ProtocolError(message,\n                                'If non-default modulus or generator is '\n                                'supplied, both must be supplied. Missing %s'\n                                % (missing,))\n        if dh_modulus or dh_gen:\n            dh_modulus = cryptutil.base64ToLong(dh_modulus)\n            dh_gen = cryptutil.base64ToLong(dh_gen)\n            dh = DiffieHellman(dh_modulus, dh_gen)\n        else:\n            dh = DiffieHellman.fromDefaults()\n        consumer_pubkey = message.getArg(OPENID_NS, 'dh_consumer_public')\n        if consumer_pubkey is None:\n            raise ProtocolError(message, \"Public key for DH-SHA1 session \"\n                                \"not found in message %s\" % (message,))\n        consumer_pubkey = cryptutil.base64ToLong(consumer_pubkey)\n        return cls(dh, consumer_pubkey)", "label": 1}
{"index": "gp236186", "code": "def decide_best_venv(matching_venvs):\n    matching_venvs.sort(key=lambda venv: len(venv.packages), reverse=True)\n    if len(matching_venvs) == 1:\n        return matching_venvs[0]\n    matching_venvs.sort(key=lambda venv: venv.last_accessed, reverse=True)\n    return matching_venvs[0]", "contrast": "def _select_better_fit(self, matching_venvs):\n        venvs = []\n        to_compare = []\n        for matching, venv in matching_venvs:\n            to_compare.append(sorted(matching, key=lambda req: getattr(req, 'key', '')))\n            venvs.append(venv)\n        scores = [0] * len(venvs)\n        for dependencies in zip(*to_compare):\n            if not isinstance(dependencies[0], Distribution):\n                continue\n            winner = dependencies.index(max(dependencies))\n            scores[winner] = scores[winner] + 1\n        winner_pos = None\n        winner_score = -1\n        for i, score in enumerate(scores):\n            if score >= winner_score:\n                winner_score = score\n                winner_pos = i\n        return venvs[winner_pos]", "label": 1}
{"index": "gp242453", "code": "import json\ndef convert_to_writable(file_format: str, metadata: dict) -> str:\n    if file_format.lower() == 'json':\n        return json.dumps(metadata)\n    elif file_format.lower() == 'xml':\n        pass\n    else:\n        raise ValueError(\"Unsupported file format, must be 'json' or 'xml'\")", "contrast": "def get_writable_metadata(self, file_format):\n        if file_format == 'json':\n            metadata = self.json\n        elif file_format == 'xml':\n            metadata = self.xml\n        else:\n            raise TypeError('The requested file type (%s) is not yet supported'\n                            % file_format)\n        return metadata", "label": 1}
{"index": "gp184797", "code": "def format_contraction(element: str, contraction: str) -> str:\n    contractions = contraction.split(',')\n    formatted_contractions = []\n    for c in contractions:\n        count, orbital = c[:-1], c[-1:]\n        formatted_contractions.append(f\"{int(int(count)/4)}{orbital}\")\n    return f\"[{','.join(formatted_contractions)}]\"", "contrast": "def contraction_string(element):\n    if 'electron_shells' not in element:\n        return \"\"\n    cont_map = dict()\n    for sh in element['electron_shells']:\n        nprim = len(sh['exponents'])\n        ngeneral = len(sh['coefficients'])\n        is_spdf = len(sh['angular_momentum']) > 1\n        for am in sh['angular_momentum']:\n            ncont = ngeneral if not is_spdf else 1\n            if am not in cont_map:\n                cont_map[am] = (nprim, ncont)\n            else:\n                cont_map[am] = (cont_map[am][0] + nprim, cont_map[am][1] + ncont)\n    primstr = \"\"\n    contstr = \"\"\n    for am in sorted(cont_map.keys()):\n        nprim, ncont = cont_map[am]\n        if am != 0:\n            primstr += ','\n            contstr += ','\n        primstr += str(nprim) + lut.amint_to_char([am])\n        contstr += str(ncont) + lut.amint_to_char([am])\n    return \"({}) -> [{}]\".format(primstr, contstr)", "label": 1}
{"index": "gp306993", "code": "import pandas as pd\nfrom pandas_functions import summary_df\ndef get_error_summary_stats(error_values: pd.DataFrame, summary_df_kwargs: dict = None) -> pd.DataFrame:\n    if summary_df_kwargs is None:\n        summary_df_kwargs = {}\n    means = error_values.mean(axis=1)\n    stds = error_values.std(axis=1)\n    num_runs = error_values.shape[1] // 2\n    mean_rel_errs = means.iloc[num_runs:] / means.iloc[:num_runs] - 1\n    std_rel_errs = stds.iloc[num_runs:] / stds.iloc[:num_runs] - 1\n    results = pd.concat([means, stds, mean_rel_errs, std_rel_errs], axis=1)\n    results.columns = [\"mean\", \"std\", \"mean_rel_err\", \"std_rel_err\"]\n    diagnostics = error_values.iloc[:, num_runs:]\n    return summary_df(results, diagnostics, **summary_df_kwargs)", "contrast": "def error_values_summary(error_values, **summary_df_kwargs):\n    df = pf.summary_df_from_multi(error_values, **summary_df_kwargs)\n    imp_std, imp_std_unc, imp_frac, imp_frac_unc =        nestcheck.error_analysis.implementation_std(\n            df.loc[('values std', 'value')],\n            df.loc[('values std', 'uncertainty')],\n            df.loc[('bootstrap std mean', 'value')],\n            df.loc[('bootstrap std mean', 'uncertainty')])\n    df.loc[('implementation std', 'value'), df.columns] = imp_std\n    df.loc[('implementation std', 'uncertainty'), df.columns] = imp_std_unc\n    df.loc[('implementation std frac', 'value'), :] = imp_frac\n    df.loc[('implementation std frac', 'uncertainty'), :] = imp_frac_unc\n    if 'values rmse' in set(df.index.get_level_values('calculation type')):\n        imp_rmse, imp_rmse_unc, imp_frac, imp_frac_unc =            nestcheck.error_analysis.implementation_std(\n                df.loc[('values rmse', 'value')],\n                df.loc[('values rmse', 'uncertainty')],\n                df.loc[('bootstrap std mean', 'value')],\n                df.loc[('bootstrap std mean', 'uncertainty')])\n        df.loc[('implementation rmse', 'value'), df.columns] = imp_rmse\n        df.loc[('implementation rmse', 'uncertainty'), df.columns] =            imp_rmse_unc\n        df.loc[('implementation rmse frac', 'value'), :] = imp_frac\n        df.loc[('implementation rmse frac', 'uncertainty'), :] = imp_frac_unc\n    calcs_to_keep = ['true values', 'values mean', 'values std',\n                     'values rmse', 'bootstrap std mean',\n                     'implementation std', 'implementation std frac',\n                     'implementation rmse', 'implementation rmse frac',\n                     'thread ks pvalue mean', 'bootstrap ks distance mean',\n                     'bootstrap energy distance mean',\n                     'bootstrap earth mover distance mean']\n    df = pd.concat([df.xs(calc, level='calculation type', drop_level=False) for\n                    calc in calcs_to_keep if calc in\n                    df.index.get_level_values('calculation type')])\n    return df", "label": 1}
{"index": "gp193862", "code": "async def process_phantomjs(url: str, phantomjs_path: str, args: List[str] = None, timeout: int = 30) -> Tuple[int, str, str]:\n    process_args = [phantomjs_path]\n    if args:\n        process_args.extend(args)\n    process_args.append(url)\n    process = await asyncio.create_subprocess_exec(\n        *process_args,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    )\n    try:\n        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout)\n    except asyncio.TimeoutError:\n        process.kill()\n        stdout, stderr = await process.communicate()\n    return process.returncode, stdout.decode().strip(), stderr.decode().strip()", "contrast": "def process(self, item_session: ItemSession, request, response, file_writer_session):\n        if response.status_code != 200:\n            return\n        if not HTMLReader.is_supported(request=request, response=response):\n            return\n        _logger.debug('Starting PhantomJS processing.')\n        self._file_writer_session = file_writer_session\n        attempts = int(os.environ.get('WPULL_PHANTOMJS_TRIES', 5))\n        for dummy in range(attempts):\n            try:\n                yield from self._run_driver(item_session, request, response)\n            except asyncio.TimeoutError:\n                _logger.warning(_('Waiting for page load timed out.'))\n                break\n            except PhantomJSCrashed as error:\n                _logger.exception(__('PhantomJS crashed: {}', error))\n            else:\n                break\n        else:\n            _logger.warning(__(\n                _('PhantomJS failed to fetch \u2018{url}\u2019. I am sorry.'),\n                url=request.url_info.url\n            ))", "label": 1}
{"index": "gp268551", "code": "import os\ndef ensure_dir_exists(path):\n    if not os.path.exists(path):\n        os.makedirs(path)", "contrast": "def make_all_dirs(path, mode=0o777):\n  try:\n    os.makedirs(path, mode=mode)\n  except OSError as e:\n    if e.errno == errno.EEXIST and os.path.isdir(path):\n      pass\n    else:\n      raise\n  return path", "label": 1}
{"index": "gp289955", "code": "def ensure_unicode_keys(dictionary):\n    return {key.decode('utf-8') if isinstance(key, bytes) else key: value for key, value in dictionary.items()}", "contrast": "def normalize_string_keys(old):\n    new = {}\n    for key, value in old.items():\n        if isinstance(key, bytes_):\n            new[key.decode('utf8')] = value\n        else:\n            new[key] = value\n    return new", "label": 1}
{"index": "gp235476", "code": "from django.views import View\nfrom django.utils.decorators import method_decorator\nfrom django.contrib.auth.decorators import login_required\nfrom django.utils.translation import gettext_lazy as _\nclass LogoutView(View):\n    @method_decorator(login_required)\n    def dispatch(self, request, *args, **kwargs):\n        self.next_page = request.GET.get(\"next_page\", None)\n        self.extra_context = {\n            'page_title': _('Log out'),\n            'button_text': _('Logout'),\n        }\n        return super(LogoutView, self).dispatch(request, *args, **kwargs)", "contrast": "def init_get(self, request):\n        self.request = request\n        self.service = request.GET.get('service')\n        self.url = request.GET.get('url')\n        self.ajax = settings.CAS_ENABLE_AJAX_AUTH and 'HTTP_X_AJAX' in request.META", "label": 1}
{"index": "gp055936", "code": "def GetSitelinksFromFeed(client, feed):\n  feed_mappings = GetFeedMapping(client, feed, PLACEHOLDER_TYPE_SITELINKS)\n  feed_items = {}\n  for feed_item in GetFeedItems(client, feed):\n    site_link_from_feed = {}\n    for attribute_value in feed_item['attributeValues']:\n      if attribute_value['feedAttributeId'] in feed_mappings:\n        for field_id in feed_mappings[attribute_value['feedAttributeId']]:\n          if field_id == SITE_LINK_FIELDS['TEXT']:\n            site_link_from_feed['text'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['URL']:\n            site_link_from_feed['url'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['FINAL_URLS']:\n            site_link_from_feed['finalUrls'] = attribute_value['stringValues']\n          elif field_id == SITE_LINK_FIELDS['FINAL_MOBILE_URLS']:\n            site_link_from_feed['finalMobileUrls'] = attribute_value[\n                'stringValues']\n          elif field_id == SITE_LINK_FIELDS['TRACKING_URL_TEMPLATE']:\n            site_link_from_feed['trackingUrlTemplate'] = attribute_value[\n                'stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE2']:\n            site_link_from_feed['line2'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE3']:\n            site_link_from_feed['line3'] = attribute_value['stringValue']\n          else:\n            print 'No applicable Site Link Field found for Id: %s' % field_id\n    feed_items[feed_item['feedItemId']] = site_link_from_feed\n  return feed_items", "contrast": "def get_sitelinks(client, feed):\n    selector = {\n        'fields': ['FeedItemId', 'LinkText', 'Line1', 'Line2', 'FinalUrls',\n                   'Scheduling'],\n        'predicates': [{\n            'field': 'FeedId',\n            'operator': 'EQUALS',\n            'values': [feed['id']]\n        }]\n    }\n    feed_items = client.GetService('FeedItemService').get(selector)\n    sitelinks = {}\n    for item in feed_items:\n        sitelink = client.factory.create('SiteLinkFromFeed')\n        sitelink.link_text = item['attributeValues'][0]['value']\n        sitelink.line1 = item['attributeValues'][1]['value']\n        sitelink.line2 = item['attributeValues'][2]['value']\n        sitelink.final_urls = item['attributeValues'][3]['value']\n        sitelink.scheduling = item['scheduling']\n        sitelinks[item['feedItemId']] = sitelink\n    return sitelinks", "label": 0}
{"index": "gp037435", "code": "def get_chassis_location(host=None,\n                         admin_username=None,\n                         admin_password=None):\n    return system_info(host=host,\n                       admin_username=admin_username,\n                       admin_password=admin_password)['Chassis Information']['Chassis Location']", "contrast": "import requests\ndef get_chassis_location(host: str, admin_username: str, admin_password: str) -> str:\n    url = f\"https://{host}/some_path\"\n    response = requests.get(url, auth=(admin_username, admin_password))\n    return response.json()[\"chassis_location\"]", "label": 0}
{"index": "gp016352", "code": "def recv(self, socket, mode=zmq.NOBLOCK, content=True, copy=True):\n        if isinstance(socket, ZMQStream):\n            socket = socket.socket\n        try:\n            msg_list = socket.recv_multipart(mode, copy=copy)\n        except zmq.ZMQError as e:\n            if e.errno == zmq.EAGAIN:\n                return None,None\n            else:\n                raise\n        idents, msg_list = self.feed_identities(msg_list, copy)\n        try:\n            return idents, self.unserialize(msg_list, content=content, copy=copy)\n        except Exception as e:\n            raise e", "contrast": "def receive_and_unpack_message(socket):\n    msg = socket.recv_multipart()\n    return msg[:-1], json.loads(msg[-1].decode('utf-8'))", "label": 0}
{"index": "gp285246", "code": "def format_value(value):\n    try:\n        formatted_value = format(value, ',.2f')\n    except ValueError:\n        raise ValidationError(\"An error occurred while formatting the value\")\n    return formatted_value", "contrast": "def _validated(self, value):\n        if value is None:\n            return None\n        if isinstance(value, bson.ObjectId):\n            return value\n        try:\n            return bson.ObjectId(value)\n        except (ValueError, AttributeError):\n            self.fail('invalid_object_id')", "label": 1}
{"index": "gp291454", "code": "def get_repository(auth, username, repo_name):\n    url = f\"{auth.url}/repos/{username}/{repo_name}\"\n    headers = {'Authorization': f'token {auth.token}'}\n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        repo = GogsRepo(**response.json())\n        return repo\n    except requests.exceptions.RequestException as err:\n        raise NetworkFailure(f\"Error communicating with server: {err}\")\n    except (ValueError, KeyError) as err:\n        raise ApiFailure(f\"Error retrieving repository: {err}\")", "contrast": "def get_repo(self, auth, username, repo_name):\n        path = \"/repos/{u}/{r}\".format(u=username, r=repo_name)\n        response = self.get(path, auth=auth)\n        return GogsRepo.from_json(response.json())", "label": 1}
{"index": "gp323058", "code": "def get_license_identifier(license):\n    if license is not None:\n        return license.get('Identifier', None)\n    return None", "contrast": "def get_extr_license_ident(self, extr_lic):\n        identifier_tripples = list(self.graph.triples((extr_lic, self.spdx_namespace['licenseId'], None)))\n        if not identifier_tripples:\n            self.error = True\n            msg = 'Extracted license must have licenseId property.'\n            self.logger.log(msg)\n            return\n        if len(identifier_tripples) > 1:\n            self.more_than_one_error('extracted license identifier_tripples')\n            return\n        identifier_tripple = identifier_tripples[0]\n        _s, _p, identifier = identifier_tripple\n        return identifier", "label": 1}
{"index": "gp156121", "code": "def forward(self, search: str):\n        url_path = \"/api/forward/{search}\".format(search=search)\n        return self._request(path=url_path)", "contrast": "import dns.resolver\ndef get_dns_lookup_history(ip_address):\n    try:\n        answers = dns.resolver.query(ip_address, 'PTR')\n        return [str(rdata) for rdata in answers]\n    except dns.resolver.NoAnswer:\n        return None", "label": 0}
{"index": "gp235999", "code": "def handle_500_error():\n    return render_template('500.html'), 500", "contrast": "def server_error(request, template_name='500.html'):\n    response = render_in_page(request, template_name)\n    if response:\n        return response\n    try:\n        template = loader.get_template(template_name)\n    except TemplateDoesNotExist:\n        return http.HttpResponseServerError('<h1>Server Error (500)</h1>', content_type='text/html')\n    return http.HttpResponseServerError(template.render(Context({})))", "label": 1}
{"index": "gp079456", "code": "def copy_style():\n  import os\n  import matplotlib\n  styles = {}\n  styles['goose.mplstyle'] = '''\nfigure.figsize       : 8,6\nfont.weight          : normal\nfont.size            : 16\naxes.labelsize       : medium\naxes.titlesize       : medium\nxtick.labelsize      : small\nytick.labelsize      : small\nxtick.top            : True\nytick.right          : True\naxes.facecolor       : none\naxes.prop_cycle      : cycler('color',['k', 'r', 'g', 'b', 'y', 'c', 'm'])\nlegend.fontsize      : medium\nlegend.fancybox      : true\nlegend.columnspacing : 1.0\nlegend.handletextpad : 0.2\nlines.linewidth      : 2\nimage.cmap           : afmhot\nimage.interpolation  : nearest\nimage.origin         : lower\nsavefig.facecolor    : none\nfigure.autolayout    : True\nerrorbar.capsize     : 2\n'''\n  styles['goose-tick-in.mplstyle'] = '''\nxtick.direction      : in\nytick.direction      : in\n'''\n  styles['goose-tick-lower.mplstyle'] = '''\nxtick.top            : False\nytick.right          : False\naxes.spines.top      : False\naxes.spines.right    : False\n'''\n  if find_latex_font_serif() is not None:\n    styles['goose-latex.mplstyle'] = r'''\nfont.family          : serif\nfont.serif           : {serif:s}\nfont.weight          : bold\nfont.size            : 18\ntext.usetex          : true\ntext.latex.preamble  : \\usepackage{{amsmath}},\\usepackage{{amsfonts}},\\usepackage{{amssymb}},\\usepackage{{bm}}\n'''.format(serif=find_latex_font_serif())\n  else:\n    styles['goose-latex.mplstyle'] = r'''\nfont.family          : serif\nfont.weight          : bold\nfont.size            : 18\ntext.usetex          : true\ntext.latex.preamble  : \\usepackage{{amsmath}},\\usepackage{{amsfonts}},\\usepackage{{amssymb}},\\usepackage{{bm}}\n'''\n  dirname = os.path.abspath(os.path.join(matplotlib.get_configdir(), 'stylelib'))\n  if not os.path.isdir(dirname): os.makedirs(dirname)\n  for fname, style in styles.items():\n    open(os.path.join(dirname, fname),'w').write(style)", "contrast": "import os\ndef write_goosestyles_to_matplotlib_config():\n    goosestyles_path = '/path/to/goosestyles'  \n    matplotlib_config_dir = os.path.join(os.path.expanduser('~'), '.config', 'matplotlib')\n    if not os.path.exists(matplotlib_config_dir):\n        os.makedirs(matplotlib_config_dir)\n    for filename in os.listdir(goosestyles_path):\n        if filename.endswith('.mplstyle'):\n            source_path = os.path.join(goosestyles_path, filename)\n            destination_path = os.path.join(matplotlib_config_dir, filename)\n            os.replace(source_path, destination_path)\n    print('All goose-styles have been written to the matplotlib configuration directory')", "label": 0}
{"index": "gp074386", "code": "def send_voice(self, chat_id, voice, duration=None, reply_to_message_id=None, reply_markup=None):\n        payload = dict(chat_id=chat_id,\n                       duration=duration,\n                       reply_to_message_id=reply_to_message_id,\n                       reply_markup=reply_markup)\n        files = dict(voice=open(voice, 'rb'))\n        return Message.from_api(self, **self._post('sendVoice', payload, files))", "contrast": "def send_voice(chat_id, audio_file, duration=None, caption=None, parse_mode=None, reply_to_message_id=None, reply_markup=None):\n    url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendVoice'\n    files = {'voice': open(audio_file, 'rb')}\n    data = {'chat_id': chat_id,\n            'duration': duration,\n            'caption': caption,\n            'parse_mode': parse_mode,\n            'reply_to_message_id': reply_to_message_id,\n            'reply_markup': reply_markup}\n    response = requests.post(url, data=data, files=files)\n    return response.json()['result']", "label": 0}
{"index": "gp105096", "code": "def extract(filename, doy, latitude, longitude, depth):\n    assert np.size(doy) == 1\n    assert np.size(latitude) == 1\n    assert np.size(longitude) == 1\n    assert np.size(depth) == 1\n    assert (longitude >= 0) & (longitude <= 360)\n    assert depth >= 0\n    nc = netCDF4.Dataset(filename)\n    t = 2 * np.pi * doy/366\n    Z = np.absolute(nc['depth'][:] - depth).argmin()\n    I = np.absolute(nc['lat'][:] - latitude).argmin()\n    J = np.absolute(nc['lon'][:] - longitude).argmin()\n    value = nc['mean'][:, I, J]\n    value[:64] += nc['an_cos'][Z, I, J] * np.cos(t) +            nc['an_sin'][:, I, J] * np.sin(t)\n    value[:55] += nc['sa_cos'][Z, I, J] * np.cos(2*t) +            nc['sa_sin'][:, I, J] * np.sin(2*t)\n    value = value[Z]\n    std = nc['std_dev'][Z, I, J]\n    return value, std", "contrast": "def nearest_longitude(longitude):\n    if longitude > 180:\n        longitude = -360 + longitude\n    elif longitude < -180:\n        longitude = 360 + longitude\n    return longitude", "label": 0}
{"index": "gp075293", "code": "def _load_group_permissions(self):\n        group_names = self._get_groups().get_group_names()\n        perms = Permission.objects.filter(group__name__in=group_names)\n        perms = perms.values_list('content_type__app_label', 'codename')\n        perms = perms.order_by()\n        self._group_permissions = set([\"%s.%s\" % (ct, name) for ct, name in perms])", "contrast": "def populate_group_permissions(self):\n    ldap_groups = get_ldap_group_membership() \n    for group in ldap_groups:\n        django_permissions = get_django_group_permissions(group)\n        for permission in django_permissions:\n            self._group_permissions.add(permission)", "label": 0}
{"index": "gp033421", "code": "def dot_vals(value):\n    ret = {}\n    for key, val in six.iteritems(__pillar__.get('master', {})):\n        if key.startswith('{0}.'.format(value)):\n            ret[key] = val\n    for key, val in six.iteritems(__opts__):\n        if key.startswith('{0}.'.format(value)):\n            ret[key] = val\n    return ret", "contrast": "import salt.config\ndef dot_vals(module, key=None):\n    opts = salt.config.minion_config('/etc/salt/minion')\n    config_dict = salt.config.apply_minion_config(None, opts)\n    ret = {}\n    for config_key, config_value in config_dict.items():\n        if config_key == module:\n            if key is None:\n                ret.update(config_value)\n                continue\n            if key in config_value:\n                ret[key] = config_value[key]\n    return ret", "label": 0}
{"index": "gp034590", "code": "def absent(name, auth=None):\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n    __salt__['neutronng.setup_clouds'](auth)\n    subnet = __salt__['neutronng.subnet_get'](name=name)\n    if subnet:\n        if __opts__['test'] is True:\n            ret['result'] = None\n            ret['changes'] = {'id': subnet.id}\n            ret['comment'] = 'Project will be deleted.'\n            return ret\n        __salt__['neutronng.subnet_delete'](name=subnet)\n        ret['changes']['id'] = name\n        ret['comment'] = 'Deleted subnet'\n    return ret", "contrast": "import boto3\ndef ensure_subnet_not_exists(name):\n    ec2 = boto3.client('ec2')\n    response = ec2.describe_subnets(Filters=[{'Name': 'tag:Name','Values': [name]}])\n    subnets = response['Subnets']\n    if not subnets:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp086237", "code": "def _get_regular_expression_of_symbols(self):\n        regular_expression = None\n        for symbol in self.symbols:\n            formated_symbol = self._get_formated_symbol(symbol['symbol'])\n            if regular_expression is None:\n                regular_expression = '(' + formated_symbol + ')'\n            else:\n                regular_expression = (\n                    regular_expression +\n                    '|(' +\n                    formated_symbol +\n                    ')'\n                )\n        return regular_expression", "contrast": "import re\ndef search_symbols():\n    return re.escape(r'~`!@#$%^&*()-_=+[{]}\\\\|;:\\'\",<.>/?')", "label": 0}
{"index": "gp177695", "code": "def flatten_iterable(iterable):\n    flat_list = []\n    for item in iterable:\n        if isinstance(item, str):\n            flat_list.append(item)\n        elif isinstance(item, list) or isinstance(item, tuple):\n            flat_list.extend(flatten_iterable(item))\n        else:\n            flat_list.append(item)\n    return flat_list", "contrast": "def flatten( iterables ):\n    for it in iterables:\n        if isinstance(it, str):\n            yield it\n        else:\n            for element in it:\n                yield element", "label": 1}
{"index": "gp047406", "code": "def pick_move(self):\n        if self.root.position.n >= self.temp_threshold:\n            fcoord = self.root.best_child()\n        else:\n            cdf = self.root.children_as_pi(squash=True).cumsum()\n            cdf /= cdf[-2]  \n            selection = random.random()\n            fcoord = cdf.searchsorted(selection)\n            assert self.root.child_N[fcoord] != 0\n        return coords.from_flat(fcoord)", "contrast": "def pick_move(mcts_stats, early_game_moves):\n    if len(mcts_stats) <= early_game_moves:\n        move = max(mcts_stats.items(), key=lambda x: x[1]['N']/x[1]['visits'])[0]\n    else:\n        move = max(mcts_stats.keys(), key=(lambda k: mcts_stats[k]['N']))\n    return move", "label": 0}
{"index": "gp152077", "code": "def analyze_frames(cls, workdir):\r\n        record = cls(None, workdir)\r\n        obj = {}\r\n        with open(os.path.join(workdir, 'frames', 'frames.json')) as f:\r\n            obj = json.load(f)\r\n        record.device_info = obj['device']\r\n        record.frames = obj['frames']\r\n        record.analyze_all()\r\n        record.save()", "contrast": "def generate_draft(frames):\n    draft = []\n    for frame in frames:\n        draft.append(processed_frame)\n    return draft", "label": 0}
{"index": "gp040872", "code": "def _collapseMsg(self, msg):\n        retval = {}\n        for logname in msg:\n            data = u\"\"\n            for m in msg[logname]:\n                m = bytes2unicode(m, self.builder.unicode_encoding)\n                data += m\n            if isinstance(logname, tuple) and logname[0] == 'log':\n                retval['log'] = (logname[1], data)\n            else:\n                retval[logname] = data\n        return retval", "contrast": "def concatenate_chunks(msg):\n    concatenated_string = ''\n    for key in msg.keys():\n        concatenated_string += ''.join(msg[key])\n    return concatenated_string", "label": 0}
{"index": "gp276219", "code": "def wind_direction(degrees):\n    directions = [\"N\", \"NNE\", \"NE\", \"ENE\", \"E\", \"ESE\", \"SE\", \"SSE\", \"S\", \"SSW\", \"SW\", \"WSW\", \"W\", \"WNW\", \"NW\", \"NNW\"]\n    index = round(degrees / (360/16)) % 16\n    return directions[index]", "contrast": "def winddir_text(pts):\n    global _winddir_text_array\n    if pts is None:\n        return None\n    if not isinstance(pts, int):\n        pts = int(pts + 0.5) % 16\n    if not _winddir_text_array:\n        _ = pywws.localisation.translation.ugettext\n        _winddir_text_array = (\n            _(u'N'), _(u'NNE'), _(u'NE'), _(u'ENE'),\n            _(u'E'), _(u'ESE'), _(u'SE'), _(u'SSE'),\n            _(u'S'), _(u'SSW'), _(u'SW'), _(u'WSW'),\n            _(u'W'), _(u'WNW'), _(u'NW'), _(u'NNW'),\n            )\n    return _winddir_text_array[pts]", "label": 1}
{"index": "gp220979", "code": "import ast\ndef parse_expression(expression):\n    return ast.parse(expression, mode='eval').body", "contrast": "def parse(self, text):\n        self.expr = text\n        try:\n            out = ast.parse(text)\n        except SyntaxError:\n            self.raise_exception(None, msg='Syntax Error', expr=text)\n        except:\n            self.raise_exception(None, msg='Runtime Error', expr=text)\n        return out", "label": 1}
{"index": "gp129119", "code": "def split_and_strip_without(string, exclude, separator_regexp=None):\n    result = split_and_strip(string, separator_regexp)\n    if not exclude:\n        return result\n    return [x for x in result if x not in exclude]", "contrast": "def split_and_strip_without(string, exclude):\n    items = [x.strip() for x in string.split(',')]\n    return [x for x in items if x not in exclude]", "label": 0}
{"index": "gp204422", "code": "def separate_variable(Y, variable):\n    Y1_index = Y.index('Y1')\n    variable_component = variable[Y1_index]\n    return variable_component", "contrast": "def block_sep1(self, Y):\n        Y1 = Y[..., self.cri.M:]\n        if self.cri.Cd > 1:\n            shp = list(Y1.shape)\n            shp[self.cri.axisM] = self.cri.dimN\n            shp[self.cri.axisC] = self.cri.Cd\n            Y1 = Y1.reshape(shp)\n        Y1 = np.swapaxes(Y1[..., np.newaxis], self.cri.axisM, -1)\n        return Y1", "label": 1}
{"index": "gp203755", "code": "import inspect\ndef get_source_lines(obj):\n    try:\n        lines, lineno = inspect.getsourcelines(obj)\n        return lines, lineno\n    except IOError:\n        raise", "contrast": "def getsourcelines(object):\n    lines, lnum = findsource(object)\n    if ismodule(object): return lines, 0\n    else: return getblock(lines[lnum:]), lnum + 1", "label": 1}
{"index": "gp316016", "code": "import requests\ndef search_units(query: str):\n    url = \"http://example.com/search\" \n    payload = {\n        \"view\": \"_zops_search_unit\",\n        \"query\": query\n    }\n    response = requests.post(url, json=payload)\n    if response.status_code == 200:\n        results = []\n        for item in response.json()[\"results\"]:\n            name, key = item\n            results.append((name, key))\n        return {\n            \"results\": results,\n            \"status\": \"OK\",\n            \"code\": 200\n        }\n    else:\n        return {\n            \"results\": [],\n            \"status\": \"Error\",\n            \"code\": response.status_code\n        }", "contrast": "def search_unit(current):\n    current.output = {\n        'results': [],\n        'status': 'OK',\n        'code': 201\n    }\n    for user in UnitModel(current).objects.search_on(*settings.MESSAGING_UNIT_SEARCH_FIELDS,\n                                                     contains=current.input['query']):\n        current.output['results'].append((user.name, user.key))", "label": 1}
{"index": "gp052459", "code": "def sort(self, key=None, reverse=False):\n        old_molecule = self.molecule.copy()\n        self.molecule._sites = sorted(self.molecule._sites, key=key, reverse=reverse)\n        mapping = {idx: self.molecule.index(site) for idx, site in enumerate(old_molecule)}\n        self.graph = nx.relabel_nodes(self.graph, mapping, copy=True)\n        edges_to_remove = []\n        edges_to_add = []\n        for u, v, k, d in self.graph.edges(keys=True, data=True):\n            if v < u:\n                new_v, new_u, new_d = u, v, d.copy()\n                new_d['to_jimage'] = (0, 0, 0)\n                edges_to_remove.append((u, v, k))\n                edges_to_add.append((new_u, new_v, new_d))\n        for edges_to_remove in edges_to_remove:\n            self.graph.remove_edge(*edges_to_remove)\n        for (u, v, d) in edges_to_add:\n            self.graph.add_edge(u, v, **d)", "contrast": "def remap_and_sort(molecule, key=None, reverse=False):\n    node_map = {}\n    i = 0\n    for node in molecule:\n        node_map[node] = i\n        i += 1\n    molecule.remap_nodes(node_map)\n    molecule.sort(key=key, reverse=reverse)", "label": 0}
{"index": "gp060511", "code": "def write_branch_data(self, file):\n        report = CaseReport(self.case)\n        branches = self.case.branches\n        col_width   = 8\n        col_width_2 = col_width*2+1\n        col1_width  = 7\n        sep = (\"=\" * 7 + \" \") * 3 + (\"=\" * col_width + \" \") * 6 + \"\\n\"\n        file.write(sep)\n        file.write(\"Name\".center(col1_width) + \" \")\n        file.write(\"From\".center(col1_width) + \" \")\n        file.write(\"To\".center(col1_width) + \" \")\n        file.write(\"From Bus Inj\".center(col_width_2) + \" \")\n        file.write(\"To Bus Inj\".center(col_width_2) + \" \")\n        file.write(\"Loss (I^2 * Z)\".center(col_width_2) + \" \")\n        file.write(\"\\n\")\n        file.write((\"-\"*col1_width +\" \")*3)\n        file.write((\"-\"*col_width_2 +\" \")*3 + \"\\n\")\n        file.write(\"..\".ljust(col1_width) + \" \")\n        file.write(\"Bus\".center(col1_width) + \" \")\n        file.write(\"Bus\".center(col1_width) + \" \")\n        file.write(\"P (MW)\".center(col_width) + \" \")\n        file.write(\"Q (MVAr)\".center(col_width) + \" \")\n        file.write(\"P (MW)\".center(col_width) + \" \")\n        file.write(\"Q (MVAr)\".center(col_width) + \" \")\n        file.write(\"P (MW)\".center(col_width) + \" \")\n        file.write(\"Q (MVAr)\".center(col_width) + \" \")\n        file.write(\"\\n\")\n        file.write(sep)\n        loss = report._loss()\n        for each in branches:\n            file.write(each.name[:col1_width].ljust(col1_width) + \" \")\n            file.write(each.from_bus.name[:col1_width].ljust(col1_width)+\" \")\n            file.write(each.to_bus.name[:col1_width].ljust(col1_width)+\" \")\n            file.write(\"%8.2f \" % each.p_from)\n            file.write(\"%8.2f \" % each.q_from)\n            file.write(\"%8.2f \" % each.p_to)\n            file.write(\"%8.2f \" % each.q_to)\n            file.write(\"%8.2f \" % loss.real[each._i])\n            file.write(\"%8.2f \" % loss.imag[each._i])\n            file.write(\"\\n\")\n        file.write((\"..\".ljust(col1_width) + \" \")*3)\n        file.write((\"..\".ljust(col_width) + \" \")*3)\n        file.write(\"*Total:*\".rjust(col_width) + \" \")\n        pl, ql = report.losses\n        file.write(\"%8.2f \" % pl)\n        file.write(\"%8.2f \" % ql)\n        file.write(\"\\n\")\n        file.write(sep)\n        del report", "contrast": "def write_branch_data_to_rest_table(branch_data):\n    import requests\n    api_url = \"https://example.com/api/branches/\"\n    headers = {\"Content-Type\": \"application/json\"}\n    for data in branch_data:\n        response = requests.post(api_url, headers=headers, json=data)\n        print(response.json())", "label": 0}
{"index": "gp259421", "code": "import pandas as pd\ndef from_fsi(filepath):\n    fsi = pd.read_csv(filepath, delimiter='\\t', header=None, names=['prd', 'pressure', 'temperature', 'cond', 'sal'])\n    return fsi", "contrast": "def from_fsi(fname, skiprows=9):\n    f = _read_file(fname)\n    df = pd.read_csv(\n        f,\n        header=\"infer\",\n        index_col=None,\n        skiprows=skiprows,\n        dtype=float,\n        delim_whitespace=True,\n    )\n    f.close()\n    df.set_index(\"PRES\", drop=True, inplace=True)\n    df.index.name = \"Pressure [dbar]\"\n    metadata = {\"name\": str(fname)}\n    setattr(df, \"_metadata\", metadata)\n    return df", "label": 1}
{"index": "gp225699", "code": "import pkg_resources\ndef get_entrypoints(app_name, entrypoint_name):\n    entrypoints = []\n    for entry_point in pkg_resources.iter_entry_points(entrypoint_name, app_name):\n        entrypoints.append(entry_point.load())\n    return entrypoints", "contrast": "def get_enabled(name, app):\n    plugins = app.config['PLUGINS']\n    return dict(_ep_to_kv(e) for e in iter_all(name) if e.name in plugins)", "label": 1}
{"index": "gp047814", "code": "def stop_monitoring(self) -> None:\n        self._context.optimisation_finished = True\n        self._on_iteration()\n        if self._print_summary:\n            self.print_summary()", "contrast": "def complete_optimization(monitor):\n    if monitor.print_summary:\n        monitor.print_timing_summary()\n    monitor.optimisation_completed = True\n    monitor.run_tasks()", "label": 0}
{"index": "gp288588", "code": "def create_time_series(table_data, current, pc, ts, summary):\n    for column in table_data:\n        if summary:\n            entry = {\n                'variableName': column,\n                'varMethod': {},\n                'data': [{\n                    pc: table_data[column]\n                }]\n            }\n        else:\n            entry = {\n                'variableName': column,\n                'varMethod': {},\n                'data': [{\n                    'values': table_data[column],\n                    'time': current[pc]['time']['values']\n                }]\n            }\n        ts.append(entry)\n    return ts", "contrast": "def _extract_table(table_data, current, pc, ts, tt):\n    current[\"tableType\"] = tt\n    current = _extract_table_root(table_data, current, pc)\n    current = _extract_table_model(table_data, current, tt)\n    _table_tmp = _extract_special(current, table_data)\n    try:\n        for _col_name, _col_data in table_data[\"columns\"].items():\n            _col_tmp = _extract_columns(_col_data, copy.deepcopy(_table_tmp), pc)\n            try:\n                ts.append(_col_tmp)\n            except Exception as e:\n                logger_ts.warn(\"extract_table: Unable to create ts entry, {}\".format(e))\n    except Exception as e:\n        logger_ts.error(\"extract_table: {}\".format(e))\n    return ts", "label": 1}
{"index": "gp129671", "code": "def unlock_keychain(username):\n    if 'SSH_TTY' not in os.environ:\n        return\n    if username in _unlocked:\n        return\n    _unlocked.add(username)\n    if sys.platform == 'darwin':\n        sys.stderr.write(\"You are running under SSH. Please unlock your local OS X KeyChain:\\n\")\n        subprocess.call(['security', 'unlock-keychain'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)", "contrast": "import os\ndef unlock_keychain():\n    if os.environ.get(\"SSH_CONNECTION\"):\n        os.system(\"security unlock-keychain\")", "label": 0}
{"index": "gp045464", "code": "def encode_consumer_metadata_request(cls, client_id, correlation_id, payloads):\n        message = []\n        message.append(cls._encode_message_header(client_id, correlation_id,\n                                                  KafkaProtocol.CONSUMER_METADATA_KEY))\n        message.append(struct.pack('>h%ds' % len(payloads), len(payloads), payloads))\n        msg = b''.join(message)\n        return write_int_string(msg)", "contrast": "def encode_consumer_metadata_request(client_id: str, correlation_id: int, payloads: str):\n    request_type = 10  \n    header = struct.pack('!hhi', request_type, 0, correlation_id)\n    encoded_client_id = client_id.encode('utf-8')\n    encoded_payloads = payloads.encode('utf-8')\n    body = struct.pack('!h{0}s{1}s'.format(len(encoded_client_id), len(encoded_payloads)), \n                       len(encoded_client_id), encoded_client_id, encoded_payloads)\n    message = header + body\n    return message", "label": 0}
{"index": "gp176145", "code": "import os\ndef create_symlink(source_path, target_path):\n    if not os.path.isabs(source_path) or not os.path.isabs(target_path):\n        raise ValueError(\"source_path and target_path should be absolute paths\")\n    if os.path.exists(target_path) or os.path.islink(target_path):\n        raise ValueError(\"target_path already exists or is a symlink\")\n    try:\n        os.symlink(source_path, target_path)\n    except OSError as e:\n        if e.errno == 17:\n            pass\n        elif e.errno == 2:\n            raise OSError(f\"{e.strerror}: {e.filename}\") from None\n        else:\n            raise e", "contrast": "def absolute_symlink(source_path, target_path):\n  if not os.path.isabs(source_path):\n    raise ValueError(\"Path for source : {} must be absolute\".format(source_path))\n  if not os.path.isabs(target_path):\n    raise ValueError(\"Path for link : {} must be absolute\".format(target_path))\n  if source_path == target_path:\n    raise ValueError(\"Path for link is identical to source : {}\".format(source_path))\n  try:\n    if os.path.lexists(target_path):\n      if os.path.islink(target_path) or os.path.isfile(target_path):\n        os.unlink(target_path)\n      else:\n        shutil.rmtree(target_path)\n    safe_mkdir_for(target_path)\n    os.symlink(source_path, target_path)\n  except OSError as e:\n    if not (e.errno == errno.EEXIST or e.errno == errno.ENOENT):\n      raise", "label": 1}
{"index": "gp075562", "code": "def _almost_equal(a, b):\n    threshold = 1e-9\n    diff = np.abs(a - b)\n    return (diff < threshold)", "contrast": "def almost_equal(num1, num2):\n    return abs(num1 - num2) < 0.0001", "label": 0}
{"index": "gp030213", "code": "def _make_charlist(self, tokens, tokmap, formats):\r\n        def _get_fmt(typ):\r\n            if typ in tokmap:\r\n                return tokmap[typ]\r\n            for key, val in tokmap.items():\r\n                if typ in key: \r\n                    return val\r\n            return 'normal'\r\n        charlist = []\r\n        for typ, token in tokens:\r\n            fmt = formats[_get_fmt(typ)]\r\n            for letter in token:\r\n                charlist.append((fmt, letter))\r\n        return charlist", "contrast": "import pygments.lexers as lexers\nclass Parser:\n    def __init__(self, text):\n        self._charlist = []\n        self.lexer = lexers.get_lexer_by_name('python')\n        self.parse_text(text)\n    def parse_text(self, text):\n        tokens = list(self.lexer.get_tokens(text))\n        for token in tokens:\n            token_value = token[1]\n            token_type = token[0]\n            for char in token_value:\n                self._charlist.append((char, token_type))\n    def get_charlist(self):\n        return self._charlist", "label": 0}
{"index": "gp209992", "code": "def set_risk_imtls(risk_models):\n    risk_imtls = {}\n    for taxonomy, loss_types in risk_models.items():\n        for loss_type, risk_function in loss_types.items():\n            if taxonomy not in risk_imtls:\n                risk_imtls[taxonomy] = {}\n            risk_imtls[taxonomy][loss_type] = risk_function()\n    setattr(set_risk_imtls, 'risk_imtls', risk_imtls)", "contrast": "def set_risk_imtls(self, risk_models):\n        imtls = {}\n        for taxonomy, risk_functions in risk_models.items():\n            for risk_type, rf in risk_functions.items():\n                imt = rf.imt\n                from_string(imt)  \n                imls = list(rf.imls)\n                if imt in imtls and imtls[imt] != imls:\n                    logging.debug(\n                        'Different levels for IMT %s: got %s, expected %s',\n                        imt, imls, imtls[imt])\n                    imtls[imt] = sorted(set(imls + imtls[imt]))\n                else:\n                    imtls[imt] = imls\n        self.risk_imtls = imtls\n        if self.uniform_hazard_spectra:\n            self.check_uniform_hazard_spectra()", "label": 1}
{"index": "gp030200", "code": "def make_gettext_patterns():\r\n    kwstr = 'msgid msgstr'\r\n    kw = r\"\\b\" + any(\"keyword\", kwstr.split()) + r\"\\b\"\r\n    fuzzy = any(\"builtin\", [r\"#,[^\\n]*\"])\r\n    links = any(\"normal\", [r\"#:[^\\n]*\"])\r\n    comment = any(\"comment\", [r\"#[^\\n]*\"])\r\n    number = any(\"number\",\r\n                 [r\"\\b[+-]?[0-9]+[lL]?\\b\",\r\n                  r\"\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b\",\r\n                  r\"\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?\\b\"])\r\n    sqstring = r\"(\\b[rRuU])?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*'?\"\r\n    dqstring = r'(\\b[rRuU])?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*\"?'\r\n    string = any(\"string\", [sqstring, dqstring])\r\n    return \"|\".join([kw, string, number, fuzzy, links, comment,\r\n                     any(\"SYNC\", [r\"\\n\"])])", "contrast": "def make_pat(string):\n    pat_list = []\n    for key in sorted(string.keys(), reverse=True):\n        pat_list.append(re.escape(key))\n    pattern = \"|\".join(pat_list)\n    return re.compile(\"\\\\b({})\\\\b\".format(pattern), re.IGNORECASE)", "label": 0}
{"index": "gp187937", "code": "def wait_for_queue_completion(queue):\n    queue.join()", "contrast": "def join (self, timeout=None):\n        with self.all_tasks_done:\n            if timeout is None:\n                while self.unfinished_tasks:\n                    self.all_tasks_done.wait()\n            else:\n                if timeout < 0:\n                    raise ValueError(\"'timeout' must be a positive number\")\n                endtime = _time() + timeout\n                while self.unfinished_tasks:\n                    remaining = endtime - _time()\n                    if remaining <= 0.0:\n                        raise Timeout()\n                    self.all_tasks_done.wait(remaining)", "label": 1}
{"index": "gp130936", "code": "def get_top_stories(self):\n        suburl = \"v0/topstories.json\"\n        try:\n            top_stories = self._make_request(suburl)\n        except requests.HTTPError as e:\n            hn_logger.exception('Faulted on getting top stories, with status {}'.format(e.errno))\n            raise e\n        return top_stories", "contrast": "import requests\ndef get_top_stories_items():\n    url = 'https://hacker-news.firebaseio.com/v0/topstories.json'\n    response = requests.get(url)\n    response.raise_for_status()\n    return response.json()", "label": 0}
{"index": "gp161454", "code": "def data96_send(self, type, len, data, force_mavlink1=False):\n                return self.send(self.data96_encode(type, len, data), force_mavlink1=force_mavlink1)", "contrast": "def data_packet(type: int, len: int, data: bytes) -> bytes:\n    packet = bytearray(96)\n    packet[0] = type\n    packet[1] = len\n    packet[2:2+len] = data\n    return bytes(packet)", "label": 0}
{"index": "gp189363", "code": "def reconfigure_medium_attachments(medium_attachments):\n    try:\n        machine = your_session_machine  \n        if not machine.session.state == \"Locked\":\n            raise VBoxErrorInvalidVmState\n        if not machine.session.type == 'Direct':\n            raise VBoxErrorInvalidObjectState\n        for attachment in medium_attachments:\n            attachment.reconfigure()\n    except Exception as e:\n        print(str(e))", "contrast": "def reconfigure_medium_attachments(self, attachments):\n        if not isinstance(attachments, list):\n            raise TypeError(\"attachments can only be an instance of type list\")\n        for a in attachments[:10]:\n            if not isinstance(a, IMediumAttachment):\n                raise TypeError(\n                        \"array can only contain objects of type IMediumAttachment\")\n        self._call(\"reconfigureMediumAttachments\",\n                     in_p=[attachments])", "label": 1}
{"index": "gp101033", "code": "def property(self, property_name, default=Ellipsis):\n        try:\n            return self._a_tags[property_name]\n        except KeyError:\n            if default != Ellipsis:\n                return default\n            else:\n                raise", "contrast": "def get_property_value(prop_dict, prop_name, default=None):\n    try:\n        return prop_dict[prop_name]\n    except KeyError:\n        if default is not None:\n            return default\n        else:\n            raise", "label": 0}
{"index": "gp248108", "code": "def convert_serial_to_flat(serial_data, width, height):\n    pixel_data = [[] for _ in range(height)]\n    for i in range(height):\n        for j in range(width):\n            index = (i * width + j) * 3\n            pixel_data[i].append(tuple(serial_data[index:index+3]))\n    flat_data = [pixel for row in pixel_data for pixel in row]\n    return flat_data", "contrast": "def serialtoflat(self, raw, width=None):\n        if self.bitdepth == 8:\n            return raw\n        if self.bitdepth == 16:\n            raw = bytearray_to_bytes(raw)\n            return array('H',\n              struct.unpack('!%dH' % (len(raw)  \n        assert self.bitdepth < 8\n        if width is None:\n            width = self.width\n        spb = 8  \n        out = newBarray()\n        mask = 2**self.bitdepth - 1\n        shifts = [self.bitdepth * it for it in range(spb - 1, -1, -1)]\n        l = width\n        for o in raw:\n            out.extend([(mask&(o>>s)) for s in shifts][:l])\n            l -= spb\n            if l <= 0:\n                l = width\n        return out", "label": 1}
{"index": "gp167472", "code": "def simplified_pos(pos, tagset=None):\n    if tagset == 'penn':\n        if pos.startswith('N') or pos.startswith('V'):\n            return pos[0]\n        elif pos.startswith('JJ'):\n            return 'ADJ'\n        elif pos.startswith('RB'):\n            return 'ADV'\n        else:\n            return None\n    else:   \n        if pos.startswith('N') or pos.startswith('V'):\n            return pos[0]\n        elif pos.startswith('ADJ') or pos.startswith('ADV'):\n            return pos[:3]\n        else:\n            return None", "contrast": "def simplified_pos_tag(pos, tagset='WordNet'):\n    if tagset == 'penn':\n        if pos.startswith('N'):\n            return 'N'\n        elif pos.startswith('V'):\n            return 'V'\n        elif pos.startswith('JJ'):\n            return 'ADJ'\n        elif pos.startswith('RB'):\n            return 'ADV'\n        else:\n            return None\n    else:\n        if pos.startswith('N'):\n            return 'N'\n        elif pos.startswith('V'):\n            return 'V'\n        elif pos.startswith('ADJ'):\n            return 'ADJ'\n        elif pos.startswith('ADV'):\n            return 'ADV'\n        else:\n            return None", "label": 0}
{"index": "gp148170", "code": "def _gather(self, func):\n        for ingredient, _ in self.traverse_ingredients():\n            for name, item in func(ingredient):\n                if ingredient == self:\n                    name = name[len(self.path) + 1:]\n                yield name, item", "contrast": "def remove_experiment_path(gathered_items, prefix):\n    return [item.replace(prefix+'.','') for item in gathered_items]", "label": 0}
{"index": "gp210000", "code": "def get_gmf_data():\n    return gmf_data", "contrast": "def gmf_data_dt(self):\n        return numpy.dtype(\n            [('rlzi', U16), ('sid', U32),\n             ('eid', U64), ('gmv', (F32, (len(self.imtls),)))])", "label": 1}
{"index": "gp059698", "code": "def remodel_run(self, c=None, **global_optargs):\n    if not c:\n        with remodel.connection.get_conn() as conn:\n            return run(self, conn, **global_optargs)\n    else:\n        return run(self, c, **global_optargs)", "contrast": "def run_query_from_pool(pool, query):\n    connection = pool.getconn()\n    try:\n        result = connection.cursor().execute(query)\n        connection.commit()\n        return result\n    finally:\n        pool.putconn(connection)", "label": 0}
{"index": "gp316249", "code": "def modified_NX_fast_BFS_node_generator(G, starting_node=None):\n    if starting_node is None:\n        starting_node = list(G)[0]\n    visited = set([starting_node])\n    queue = deque([(starting_node, 0)])\n    while queue:\n        node, distance = queue.popleft()\n        yield node, distance\n        neighbors = G[node]\n        for neighbor in neighbors:\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append((neighbor, distance + 1))", "contrast": "def __plain_bfs(adj, source):\n        seen = set()\n        nextlevel = {source}\n        while nextlevel:\n            thislevel = nextlevel\n            nextlevel = set()\n            for v in thislevel:\n                if v not in seen:\n                    yield v\n                    seen.add(v)\n                    nextlevel.update(adj[v])", "label": 1}
{"index": "gp063888", "code": "def _set_last_rcvd_interface(self, v, load=False):\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=last_rcvd_interface.last_rcvd_interface, is_container='container', presence=False, yang_name=\"last-rcvd-interface\", rest_name=\"last-rcvd-interface\", parent=self, choice=(u'request-type', u'get-next-request'), path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions=None, namespace='urn:brocade.com:mgmt:brocade-interface-ext', defining_module='brocade-interface-ext', yang_type='container', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': \"\"\"last_rcvd_interface must be of a type compatible with container\"\"\",\n          'defined-type': \"container\",\n          'generated-type': \"\"\"YANGDynClass(base=last_rcvd_interface.last_rcvd_interface, is_container='container', presence=False, yang_name=\"last-rcvd-interface\", rest_name=\"last-rcvd-interface\", parent=self, choice=(u'request-type', u'get-next-request'), path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions=None, namespace='urn:brocade.com:mgmt:brocade-interface-ext', defining_module='brocade-interface-ext', yang_type='container', is_config=True)\"\"\",\n        })\n    self.__last_rcvd_interface = t\n    if hasattr(self, '_set'):\n      self._set()", "contrast": "def _set_last_rcvd_interface(self, value):\n    self.last_rcvd_interface = value", "label": 0}
{"index": "gp103946", "code": "def _draw_tickgram(numbers):\n    max_number = max(filter(lambda x : type(x)==int,numbers))\n    if max_number == 0 :\n        return upticks[0]*len(numbers)\n    else:\n        normalized_numbers = [ float(x)/max_number if type(x)==int else x for x in numbers ]\n        upticks_indexes = [ int(math.ceil(x*len(upticks))) if type(x)==float else x for x in normalized_numbers ]\n        return ''.join([ ' ' if type(x)==str else upticks[x-1] if x != 0 else upticks[0] for x in upticks_indexes ])", "contrast": "def generate_ticks(nums):\n    ticks = []\n    for num in nums:\n        tick = ''\n        for i in range(num):\n            tick += '|'\n        ticks.append(tick)\n    return ticks", "label": 0}
{"index": "gp054238", "code": "def play(events, speed_factor=1.0):\n    state = stash_state()\n    last_time = None\n    for event in events:\n        if speed_factor > 0 and last_time is not None:\n            _time.sleep((event.time - last_time) / speed_factor)\n        last_time = event.time\n        key = event.scan_code or event.name\n        press(key) if event.event_type == KEY_DOWN else release(key)\n    restore_modifiers(state)", "contrast": "import time\nimport keyboard\ndef play(recorded_events, speed_factor=1):\n    keyboard.clear_all_hotkeys()\n    keyboard.unhook_all()\n    keyboard.press_and_release('ctrl+r') \n    keyboard.unhook_all()\n    keyboard.play(recorded_events, speed_factor=speed_factor)\n    keyboard.clear_all_hotkeys()\n    keyboard.unhook_all()\n    keyboard.press_and_release('ctrl+r')\n    keyboard.unhook_all()", "label": 0}
{"index": "gp245056", "code": "def find_largest_region(coords_list, labels_list):\n    largest_region = None\n    largest_region_size = 0\n    for i in range(len(coords_list)):\n        region_size = len(coords_list[i])\n        if region_size > largest_region_size:\n            largest_region = labels_list[i]\n            largest_region_size = region_size\n    label_position = (largest_region[0] + largest_region[1]) / 2\n    return largest_region, label_position", "contrast": "def label_position(self):\n        reg_sizes = [(r.size(), r) for r in self.pieces]\n        reg_sizes.sort()\n        return reg_sizes[-1][1].label_position()", "label": 1}
{"index": "gp009706", "code": "def decode_date(self, val):\n    if isinstance(val, basestring) and val.count('-') == 2 and len(val) > 9:\n      try:\n        dt = dateutil.parser.parse(val)\n        if val.endswith(('+00:00', '-00:00', 'Z')):\n          dt = dt.replace(tzinfo=None)\n        return dt\n      except (TypeError, ValueError):\n        pass\n    return val", "contrast": "from datetime import datetime\ndef decode_date_string(date_str):\n    date_formats = [\n        '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y', '%m-%d-%Y', '%m-%d-%y', '%d %b %Y', \n        '%d %B %Y', '%b %d, %Y', '%B %d, %Y', '%d %b %y', '%d %B %y', \n        '%b %d, %y', '%B %d, %y', '%Y.%m.%d', '%Y/%m/%d', '%Y %m %d'\n    ]\n    for date_format in date_formats:\n        try:\n            return datetime.strptime(date_str, date_format)\n        except ValueError:\n            pass\n    raise ValueError(\"no valid date format found for %r\" % date_str)", "label": 0}
{"index": "gp095461", "code": "def first_match(predicate, lst):\n    for item in lst:\n        val = predicate(item)\n        if val is not None:\n            return val\n    return None", "contrast": "def first_match(predicate, lst):\n    for item in lst:\n        if predicate(item) is not None:\n            return predicate(item)\n    return None", "label": 0}
{"index": "gp019698", "code": "def uniform_binning_correction(x, n_bits=8):\n  n_bins = 2**n_bits\n  batch_size, height, width, n_channels = common_layers.shape_list(x)\n  hwc = float(height * width * n_channels)\n  x = x + tf.random_uniform(\n      shape=(batch_size, height, width, n_channels),\n      minval=0.0, maxval=1.0/n_bins)\n  objective = -np.log(n_bins) * hwc * tf.ones(batch_size)\n  return x, objective", "contrast": "import tensorflow as tf\ndef replace_x_with_q(x, n_bits=None):\n    u = tfp.distributions.Uniform(low=x, high=x+1.0/256.0)\n    q = u.prob(x)\n    objective = -q*tf.math.log(q)\n    return x ~ u, objective", "label": 0}
{"index": "gp224665", "code": "from django.core.mail import send_mail\nfrom django.conf import settings\ndef send_message_to_admins(subject, message):\n    admin_email_list = getattr(settings, 'DBBACKUP_ADMINS', [])\n    if admin_email_list:\n        send_mail(subject=subject, message=message, from_email=None, recipient_list=admin_email_list)", "contrast": "def mail_admins(subject, message, fail_silently=False, connection=None,\n                html_message=None):\n    if not settings.ADMINS:\n        return\n    mail = EmailMultiAlternatives('%s%s' % (settings.EMAIL_SUBJECT_PREFIX, subject),\n                                  message, settings.SERVER_EMAIL, [a[1] for a in settings.ADMINS],\n                                  connection=connection)\n    if html_message:\n        mail.attach_alternative(html_message, 'text/html')\n    mail.send(fail_silently=fail_silently)", "label": 1}
{"index": "gp213820", "code": "def mark_main_entry(response_entries):\n    main_entry = None\n    for entry in response_entries:\n        if entry.get('response', {}).get('body', None):\n            main_entry = entry\n            break\n    if main_entry:\n        main_entry['isMain'] = True\n        for entry in response_entries:\n            if entry != main_entry:\n                entry['isMain'] = False\n    else:\n        raise Exception('No main entry found')", "contrast": "def mark_entries(self, entries):\n        for entry in entries:\n            self._set_entry_type(entry, RESOURCE_ENTRY)\n        main_entry = entries[0]\n        main_location = self._get_location(main_entry)\n        if not main_location:\n            self._set_entry_type(main_entry, MAIN_ENTRY)\n            return\n        main_url = urllib.parse.urljoin(get_url(main_entry), main_location)\n        for entry in entries[1:]:\n            url = get_url(entry)\n            if url == main_url:\n                self._set_entry_type(entry, MAIN_ENTRY)\n                break\n        else:\n            self._set_entry_type(main_entry, MAIN_ENTRY)", "label": 1}
{"index": "gp282176", "code": "import pandas as pd\nimport sqlite3\ndef create_runs_dataframe():\n    conn = sqlite3.connect('runs.db') \n    df = pd.read_sql_query(\"SELECT * from runs\", conn)\n    conn.close()\n    return df", "contrast": "def create_info_df(self):\n        logger.debug(\"running create_info_df\")\n        reader = self.reader()\n        self.info_df = make_df_from_batch(self.name, batch_col=self.batch_col,\n                                          reader=reader)\n        logger.debug(str(self.info_df.head(5)))", "label": 1}
{"index": "gp009390", "code": "def capacity_meyerhof_and_hanna_1978(sl_0, sl_1, h0, fd, gwl=1e6, verbose=0):\n    sp = sm.SoilProfile()\n    sp.add_layer(0, sl_0)\n    sp.add_layer(h0, sl_1)\n    sp.gwl = gwl\n    return capacity_sp_meyerhof_and_hanna_1978(sp, fd)", "contrast": "def meyerhof_hanna_capacity(sl_0, sl_1, h0, fd, wtl, verbose=False):\n    c = sl_1.c_u + ((sl_0.c_u - sl_1.c_u) * h0 / fd.b)\n    phi = sl_1.phi_u + ((sl_0.phi_u - sl_1.phi_u) * h0 / fd.b)\n    gamma_sat = (sl_1.gamma_sat * h0 + sl_0.gamma_sat * (fd.b - h0)) / fd.b\n    gamma_star = gamma_sat / fd.gamma\n    z = fd.depth - wtl\n    q_u1 = (c * fd.N_c + fd.q * fd.N_q + 0.5 * fd.gamma * fd.B * fd.N_gamma * gamma_star * z) * fd.gamma\n    if z < fd.d_f:\n        q_u2 = (c * (fd.N_c + 1) + fd.q * fd.N_q + 0.5 * fd.gamma * fd.B * fd.N_gamma * gamma_star *\n                z * fd.N_qc) * fd.gamma\n    else:\n        q_u2 = ((c * fd.N_c + fd.q * fd.N_q + 0.5 * fd.gamma * fd.B * fd.N_gamma *\n                 gamma_star * z) * fd.gamma) / (1 + fd.N_qc * (z / fd.d_f))\n    q_u = min(q_u1, q_u2)\n    if verbose:\n        print(f\"Meyerhof-Hanna capacity calculation: q_u = {q_u:.2f} kPa\")\n    return q_u", "label": 0}
{"index": "gp075354", "code": "def handle_message_registered(self, msg_data, host):\n        response = None\n        if msg_data[\"method\"] == \"EVENT\":\n            logger.debug(\"<%s> <euuid:%s> Event message \"\n                         \"received\" % (msg_data[\"cuuid\"], msg_data[\"euuid\"]))\n            response = self.event(msg_data[\"cuuid\"],\n                                  host,\n                                  msg_data[\"euuid\"],\n                                  msg_data[\"event_data\"],\n                                  msg_data[\"timestamp\"],\n                                  msg_data[\"priority\"])\n        elif msg_data[\"method\"] == \"OK EVENT\":\n            logger.debug(\"<%s> <euuid:%s> Event confirmation message \"\n                         \"received\" % (msg_data[\"cuuid\"], msg_data[\"euuid\"]))\n            try:\n                del self.event_uuids[msg_data[\"euuid\"]]\n            except KeyError:\n                logger.warning(\"<%s> <euuid:%s> Euuid does not exist in event \"\n                               \"buffer. Key was removed before we could process \"\n                               \"it.\" % (msg_data[\"cuuid\"], msg_data[\"euuid\"]))\n        elif msg_data[\"method\"] == \"OK NOTIFY\":\n            logger.debug(\"<%s> <euuid:%s> Ok notify \"\n                         \"received\" % (msg_data[\"cuuid\"], msg_data[\"euuid\"]))\n            try:\n                del self.event_uuids[msg_data[\"euuid\"]]\n            except KeyError:\n                logger.warning(\"<%s> <euuid:%s> Euuid does not exist in event \"\n                               \"buffer. Key was removed before we could process \"\n                               \"it.\" % (msg_data[\"cuuid\"], msg_data[\"euuid\"]))\n        return response", "contrast": "def process_message(msg: str, host: tuple) -> str:\n    return response", "label": 0}
{"index": "gp019432", "code": "def _init_from_npy2d(self, mat, missing, nthread):\n        if len(mat.shape) != 2:\n            raise ValueError('Input numpy.ndarray must be 2 dimensional')\n        data = np.array(mat.reshape(mat.size), copy=False, dtype=np.float32)\n        handle = ctypes.c_void_p()\n        missing = missing if missing is not None else np.nan\n        if nthread is None:\n            _check_call(_LIB.XGDMatrixCreateFromMat(\n                data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n                c_bst_ulong(mat.shape[0]),\n                c_bst_ulong(mat.shape[1]),\n                ctypes.c_float(missing),\n                ctypes.byref(handle)))\n        else:\n            _check_call(_LIB.XGDMatrixCreateFromMat_omp(\n                data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n                c_bst_ulong(mat.shape[0]),\n                c_bst_ulong(mat.shape[1]),\n                ctypes.c_float(missing),\n                ctypes.byref(handle),\n                nthread))\n        self.handle = handle", "contrast": "import numpy as np\ndef initialize_data(mat):\n    if mat.flags['C_CONTIGUOUS'] and mat.dtype == np.float32:\n        return mat\n    else:\n        temp_mat = np.ascontiguousarray(mat, dtype=np.float32)\n        return temp_mat", "label": 0}
{"index": "gp308633", "code": "def tagBlocks(self):\n        return [block for block in self.children if isinstance(block, AdvancedTag)]", "contrast": "def tagBlocks(self):\n        myBlocks = self.blocks\n        return [block for block in myBlocks if issubclass(block.__class__, AdvancedTag)]", "label": 1}
{"index": "gp155991", "code": "def _detect(self):\n        results = []\n        for c in self.slither.contracts_derived:\n            ret = self.detect_uninitialized(c)\n            for variable, functions in ret:\n                info = \"{}.{} ({}) is never initialized. It is used in:\\n\"\n                info = info.format(variable.contract.name,\n                                   variable.name,\n                                   variable.source_mapping_str)\n                for f in functions:\n                    info += \"\\t- {} ({})\\n\".format(f.name, f.source_mapping_str)\n                source = [variable.source_mapping]\n                source += [f.source_mapping for f in functions]\n                json = self.generate_json_result(info)\n                self.add_variable_to_json(variable, json)\n                self.add_functions_to_json(functions, json)\n                results.append(json)\n        return results", "contrast": "def detect_uninitialized_state_variables(contract):\n    uninitialized_state_variables = {}\n    for contract_name, contract_data in contract.items():\n        for state_variable_name in contract_data['abi']['networks']['debug']['address'].keys():\n            if contract_data['variables'][state_variable_name]['value'] is None:\n                if contract_name in uninitialized_state_variables:\n                    uninitialized_state_variables[contract_name].add(state_variable_name)\n                else:\n                    uninitialized_state_variables[contract_name] = {state_variable_name}\n    return uninitialized_state_variables", "label": 0}
{"index": "gp127592", "code": "def wrap(self, cause):\n        if isinstance(cause, ApplicationException):\n            return cause\n        self.with_cause(cause)\n        return self", "contrast": "class ApplicationException(Exception):\n    pass\ndef wrap_with_application_exception(cause):\n    if isinstance(cause, ApplicationException):\n        return cause\n    else:\n        return ApplicationException(str(cause)).with_traceback(cause.__traceback__)", "label": 0}
{"index": "gp276868", "code": "import numpy as np\nimport pandas as pd\ndef extract_features(data_frame, observation, skip_id=None, last_column_is_id=False):\n    if last_column_is_id:\n        features = data_frame.iloc[data_frame.index == observation, :-1].values\n    else:\n        features = data_frame.loc[observation].values[:-1]\n    if skip_id is not None:\n        features = np.delete(features, np.where(features == skip_id))\n    return features", "contrast": "def __get_features_for_observation(self, data_frame=None, observation='LA-LL',\n                                       skip_id=None, last_column_is_id=False):\n        try:\n            features = np.array([])\n            if data_frame is None:\n                data_frame = self.data_frame\n            for index, row in data_frame.iterrows():\n                if not skip_id == row['id']:\n                    features_row = np.nan_to_num(row[row.keys().str.contains(observation)].values)\n                    features_row = np.append(features_row, row['id'])\n                    features = np.vstack([features, features_row]) if features.size else features_row\n            if last_column_is_id:\n                if np.ndim(features) > 1:\n                    to_return = features[:,:-1]\n                else:\n                    to_return = features[:-1]\n            else:\n                to_return = features\n            return to_return, data_frame['id'].values\n        except:\n            logging.error(\" observation not found in data frame\")", "label": 1}
{"index": "gp329327", "code": "import numpy as np\ndef get_dtype_from_jsonschema(typespec):\n    mapping = {\n        'integer': np.int64,\n        'number': np.float64,\n        'string': np.object,\n        'boolean': np.bool_,\n        'array': np.ndarray,\n        'object': np.object\n    }\n    return mapping.get(typespec.get('type', 'object'), np.object)", "contrast": "def __get_dtype(typespec):\n    if 'type' in typespec:\n        return __TYPE_MAP__.get(typespec['type'], np.object_)\n    elif 'enum' in typespec:\n        return np.object_\n    elif 'oneOf' in typespec:\n        types = [__get_dtype(v) for v in typespec['oneOf']]\n        if all([t == types[0] for t in types]):\n            return types[0]\n    return np.object_", "label": 1}
{"index": "gp238555", "code": "import requests\ndef execute_request(url):\n    response = requests.get(url)\n    return response.content", "contrast": "def request(self,\n                method,\n                url,\n                params=None,\n                data=None,\n                files=None,\n                json=None,\n                timeout=5,\n                headers=None,\n                skip_auth=False):\n        request_url = self.base_url + url\n        floyd_logger.debug(\"Starting request to url: %s with params: %s, data: %s\", request_url, params, data)\n        request_headers = {'x-floydhub-cli-version': get_cli_version()}\n        if self.auth_header:\n            request_headers[\"Authorization\"] = self.auth_header\n        if headers:\n            request_headers.update(headers)\n        try:\n            response = requests.request(method,\n                                        request_url,\n                                        params=params,\n                                        data=data,\n                                        json=json,\n                                        headers=request_headers,\n                                        files=files,\n                                        timeout=timeout)\n        except requests.exceptions.ConnectionError as exception:\n            floyd_logger.debug(\"Exception: %s\", exception, exc_info=True)\n            sys.exit(\"Cannot connect to the Floyd server. Check your internet connection.\")\n        except requests.exceptions.Timeout as exception:\n            floyd_logger.debug(\"Exception: %s\", exception, exc_info=True)\n            sys.exit(\"Connection to FloydHub server timed out. Please retry or check your internet connection.\")\n        floyd_logger.debug(\"Response Content: %s, Headers: %s\" % (response.content, response.headers))\n        self.check_response_status(response)\n        return response", "label": 1}
{"index": "gp128169", "code": "def merge_true_table():\n    writer = pd.ExcelWriter(\"True Table.xlsx\")\n    for p in Path(__file__).parent.select_by_ext(\".csv\"):\n        df = pd.read_csv(p.abspath, index_col=0)\n        df.to_excel(writer, p.fname, index=True)\n    writer.save()", "contrast": "import pandas as pd\ndef merge_true_tables_to_excel(all_true_tables, output_file_name):\n    writer = pd.ExcelWriter(output_file_name, engine='xlsxwriter')\n    for table_name, table_data in all_true_tables.items():\n        df = pd.DataFrame(table_data)\n        df.to_excel(writer, sheet_name=table_name, index=False)\n    writer.save()", "label": 0}
{"index": "gp036238", "code": "def link_mountpoint(self, repo):\n        lcachelink = salt.utils.path.join(repo.linkdir, repo._mountpoint)\n        lcachedest = salt.utils.path.join(repo.cachedir, repo.root()).rstrip(os.sep)\n        wipe_linkdir = False\n        create_link = False\n        try:\n            with repo.gen_lock(lock_type='mountpoint', timeout=10):\n                walk_results = list(os.walk(repo.linkdir, followlinks=False))\n                if walk_results != repo.linkdir_walk:\n                    log.debug(\n                        'Results of walking %s differ from expected results',\n                        repo.linkdir\n                    )\n                    log.debug('Walk results: %s', walk_results)\n                    log.debug('Expected results: %s', repo.linkdir_walk)\n                    wipe_linkdir = True\n                else:\n                    if not all(not salt.utils.path.islink(x[0])\n                               and os.path.isdir(x[0])\n                               for x in walk_results[:-1]):\n                        log.debug(\n                            'Linkdir parents of %s are not all directories',\n                            lcachelink\n                        )\n                        wipe_linkdir = True\n                    elif not salt.utils.path.islink(lcachelink):\n                        wipe_linkdir = True\n                    else:\n                        try:\n                            ldest = salt.utils.path.readlink(lcachelink)\n                        except Exception:\n                            log.debug(\n                                'Failed to read destination of %s', lcachelink\n                            )\n                            wipe_linkdir = True\n                        else:\n                            if ldest != lcachedest:\n                                log.debug(\n                                    'Destination of %s (%s) does not match '\n                                    'the expected value (%s)',\n                                    lcachelink, ldest, lcachedest\n                                )\n                                try:\n                                    if salt.utils.platform.is_windows()                                            and not ldest.startswith('\\\\\\\\')                                            and os.path.isdir(ldest):\n                                        shutil.rmtree(lcachelink)\n                                    else:\n                                        os.remove(lcachelink)\n                                except Exception as exc:\n                                    log.exception(\n                                        'Failed to remove existing git_pillar '\n                                        'mountpoint link %s: %s',\n                                        lcachelink, exc.__str__()\n                                    )\n                                wipe_linkdir = False\n                                create_link = True\n                if wipe_linkdir:\n                    create_link = True\n                    try:\n                        shutil.rmtree(repo.linkdir)\n                    except OSError:\n                        pass\n                    try:\n                        ldirname = os.path.dirname(lcachelink)\n                        os.makedirs(ldirname)\n                        log.debug('Successfully made linkdir parent %s', ldirname)\n                    except OSError as exc:\n                        log.error(\n                            'Failed to os.makedirs() linkdir parent %s: %s',\n                            ldirname, exc.__str__()\n                        )\n                        return False\n                if create_link:\n                    try:\n                        os.symlink(lcachedest, lcachelink)\n                        log.debug(\n                            'Successfully linked %s to cachedir %s',\n                            lcachelink, lcachedest\n                        )\n                        return True\n                    except OSError as exc:\n                        log.error(\n                            'Failed to create symlink to %s at path %s: %s',\n                            lcachedest, lcachelink, exc.__str__()\n                        )\n                        return False\n        except GitLockError:\n            log.error(\n                'Timed out setting mountpoint lock for %s remote \\'%s\\'. If '\n                'this error persists, it may be because an earlier %s '\n                'checkout was interrupted. The lock can be cleared by running '\n                '\\'salt-run cache.clear_git_lock %s type=mountpoint\\', or by '\n                'manually removing %s.',\n                self.role, repo.id, self.role, self.role,\n                repo._get_lock_file(lock_type='mountpoint')\n            )\n            return False\n        return True", "contrast": "def ensure_mountpoint(mountpoint_path: str, correct_location: str, correct_path: str) -> bool:\n    if os.path.exists(mountpoint_path):\n        if os.path.realpath(mountpoint_path) == correct_path and os.path.realpath(os.path.join(mountpoint_path, '..')) == correct_location:\n            return True\n        else:\n            return False\n    else:\n        return False", "label": 0}
{"index": "gp220451", "code": "def sort_dataframe_by_column(df, column):\n    return df.sort_values(by=column)", "contrast": "def arrange(*args):\n  names = [column._name for column in args]\n  def f(df):\n    sortby_df = df >> mutate(*args)\n    index = sortby_df.sort_values([str(arg) for arg in args]).index\n    return df.loc[index]\n  return f", "label": 1}
{"index": "gp002689", "code": "def constant(duration: int, amp: complex, name: str = None) -> SamplePulse:\n    return _sampled_constant_pulse(duration, amp, name=name)", "contrast": "from qiskit.pulse import Waveform\nfrom qiskit.pulse import pulse_lib\ndef generate_sample_pulse(duration, amp, name):\n    sample_pulse = pulse_lib.SamplePulse(list(repeat(amp, duration)), name)\n    return sample_pulse.waveform()", "label": 0}
{"index": "gp005405", "code": "def build(self, pre=None, shortest=False):\n        if pre is None:\n            pre = []\n        if self.max is not None:\n            if shortest:\n                vals = [self.values[0]]\n            else:\n                vals = [self.values[0]] * rand.randint(1, self.max+1)\n        else:\n            vals = self.values\n        joins = []\n        for val in vals:\n            try:\n                v = utils.val(val, pre, shortest=shortest)\n                joins.append(v)\n            except errors.OptGram as e:\n                continue\n        return self.sep.join(joins)", "contrast": "from marshmallow import fields, pre_load\nclass Join(fields.List):\n    def __init__(self, cls_or_instance, **kwargs):\n        super().__init__(cls_or_instance, **kwargs)\n        self.data_key = 'Join'\n    @pre_load(pass_many=True)\n    def process_data(self, data, many):\n        if isinstance(data, list):\n            return data\n        return [data]", "label": 0}
{"index": "gp022164", "code": "def _multiple_callbacks(callbacks, *args, **kwargs):\n    if isinstance(callbacks, list):\n        for cb in callbacks:\n            cb(*args, **kwargs)\n        return\n    if callbacks:\n        callbacks(*args, **kwargs)", "contrast": "def send_to_callbacks(*args, **kwargs):\n    if callbacks is None:\n        return\n    elif callable(callbacks):\n        return callbacks(*args, **kwargs)\n    else:\n        results = []\n        for callback in callbacks:\n            result = callback(*args, **kwargs)\n            results.append(result)\n        return results", "label": 0}
{"index": "gp018921", "code": "def compounding(start, stop, compound):\n    def clip(value):\n        return max(value, stop) if (start > stop) else min(value, stop)\n    curr = float(start)\n    while True:\n        yield clip(curr)\n        curr *= compound", "contrast": "def compounding(initial, rate, compound_rate):\n    value = initial\n    yield initial\n    while True:\n        value *= compound_rate\n        yield value", "label": 0}
{"index": "gp236507", "code": "def get_device_by_name(name):\n    from soco import discover\n    devices = discover()\n    for device in devices:\n        if device.player_name == name:\n            return device\n    return None", "contrast": "def by_name(name):\n    devices = discover(all_households=True)\n    for device in (devices or []):\n        if device.player_name == name:\n            return device\n    return None", "label": 1}
{"index": "gp171113", "code": "def find_outer_edges(tri, triangles):\n    edges = []\n    for triangle in triangles:\n        for i in range(3):\n            edge = [triangle[i], triangle[(i+1)%3]]\n            if edge not in edges and edge[::-1] not in edges:\n                if tri.neighbors[triangle[i]] == -1 or tri.neighbors[triangle[(i+1)%3]] == -1:\n                    edges.append(edge)\n                elif not tri.is_encroached(triangle, [triangle[i], triangle[(i+1)%3]]):\n                    edges.append(edge)\n    return edges", "contrast": "def find_local_boundary(tri, triangles):\n    edges = []\n    for triangle in triangles:\n        for i in range(3):\n            pt1 = tri.simplices[triangle][i]\n            pt2 = tri.simplices[triangle][(i + 1) % 3]\n            if (pt1, pt2) in edges:\n                edges.remove((pt1, pt2))\n            elif (pt2, pt1) in edges:\n                edges.remove((pt2, pt1))\n            else:\n                edges.append((pt1, pt2))\n    return edges", "label": 1}
{"index": "gp336386", "code": "import runtimepath\ndef get_vim_runtimepath():\n    return runtimepath.RuntimePath('vim')", "contrast": "def runtimepath(self):\n        if self._runtimepath is None:\n            self._runtimepath = runtimepath.RuntimePath(self)\n        return self._runtimepath", "label": 1}
{"index": "gp003380", "code": "def user(self, login):\n        user = None\n        if login in self._users:\n            return self._users[login]\n        url_user = urijoin(self.base_url, 'users', login)\n        logging.info(\"Getting info for %s\" % (url_user))\n        r = self.fetch(url_user)\n        user = r.text\n        self._users[login] = user\n        return user", "contrast": "def update_user_cache(user_info, user_cache):\n    user_cache.update(user_info)", "label": 0}
{"index": "gp155457", "code": "def store_atlas_zonefile_data(zonefile_data, zonefile_dir, fsync=True):\n    if not os.path.exists(zonefile_dir):\n        os.makedirs(zonefile_dir, 0700 )\n    zonefile_hash = get_zonefile_data_hash( zonefile_data )\n    zonefile_path = atlas_zonefile_path( zonefile_dir, zonefile_hash )\n    zonefile_dir_path = os.path.dirname(zonefile_path)\n    if os.path.exists(zonefile_path):\n        return True\n    if not os.path.exists(zonefile_dir_path):\n        os.makedirs(zonefile_dir_path)\n    try:\n        with open( zonefile_path, \"wb\" ) as f:\n            f.write(zonefile_data)\n            f.flush()\n            if fsync:\n                os.fsync(f.fileno())\n    except Exception, e:\n        log.exception(e)\n        return False\n    return True", "contrast": "def store_zonefile(zonefile_data):\n    authenticated = True \n    if authenticated:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp087432", "code": "def _decode_sense_packet(self, version, packet):\n        data = self._sense_packet_to_data(packet)\n        offset = 4\n        i = 0\n        datalen = len(data) - offset - 6\n        temp_count = int(datalen / 2)\n        temp = []\n        for i in range(temp_count):\n            temp_index = i * 2 + offset\n            temp.append(self._decode_temp(data[temp_index], data[temp_index + 1]))\n        self._debug(PROP_LOGLEVEL_DEBUG, \"T: \" + str(temp))\n        for sensor in self._sense_sensor:\n            if (sensor.sensor_type == PROP_SENSOR_TEMPERATURE):\n                sensor.value = temp[sensor.index]\n            elif (sensor.sensor_type == PROP_SENSOR_RAW):\n                sensor.value = packet\n        self._debug(PROP_LOGLEVEL_DEBUG, str(self))", "contrast": "def decode_sense_packet(sense_packet):\n    sensors = []\n    for i in range(len(sense_packet)):\n        sensors.append(int(sense_packet[i:i+1]))\n    return sensors", "label": 0}
{"index": "gp333143", "code": "def has_dict(data):\n    if isinstance(data, dict): \n        if any(isinstance(value, dict) for value in data.values()): \n            return True\n    return False", "contrast": "def _check_for_inception(self, root_dict):\n    for key in root_dict:\n      if isinstance(root_dict[key], dict):\n          root_dict[key] = ResponseObject(root_dict[key])\n    return root_dict", "label": 1}
{"index": "gp217293", "code": "def append_transitions(transitions: Tuple, rows=None) -> Operation:\n    pass ", "contrast": "def append(self, transitions, rows=None):\n    rows = tf.range(self._capacity) if rows is None else rows\n    assert rows.shape.ndims == 1\n    assert_capacity = tf.assert_less(\n        rows, self._capacity,\n        message='capacity exceeded')\n    with tf.control_dependencies([assert_capacity]):\n      assert_max_length = tf.assert_less(\n          tf.gather(self._length, rows), self._max_length,\n          message='max length exceeded')\n    with tf.control_dependencies([assert_max_length]):\n      timestep = tf.gather(self._length, rows)\n      indices = tf.stack([rows, timestep], 1)\n      append_ops = tools.nested.map(\n          lambda var, val: tf.scatter_nd_update(var, indices, val),\n          self._buffers, transitions, flatten=True)\n    with tf.control_dependencies(append_ops):\n      episode_mask = tf.reduce_sum(tf.one_hot(\n          rows, self._capacity, dtype=tf.int32), 0)\n      return self._length.assign_add(episode_mask)", "label": 1}
{"index": "gp031149", "code": "def is_copy_only_path(path, context):\n    try:\n        for dont_render in context['cookiecutter']['_copy_without_render']:\n            if fnmatch.fnmatch(path, dont_render):\n                return True\n    except KeyError:\n        return False\n    return False", "contrast": "import fnmatch\ndef should_only_copy(path, context):\n    for pattern in context.get('copy_only', []):\n        if fnmatch.fnmatchcase(path, pattern):\n            return True\n    return False", "label": 0}
{"index": "gp061111", "code": "def variant_matches_reference_sequence(variant, ref_seq_on_transcript, strand):\n    if strand == \"-\":\n        ref_seq_on_transcript = reverse_complement_dna(ref_seq_on_transcript)\n    return ref_seq_on_transcript == variant.ref", "contrast": "def verify_reference_nucleotides(expected_ref, observed_ref):\n    return expected_ref == observed_ref", "label": 0}
{"index": "gp125099", "code": "def check_table_exists(dbcon, tablename):\n    dbcur = dbcon.cursor()\n    dbcur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='%s';\" % tablename)\n    result = dbcur.fetchone()\n    dbcur.close()\n    if result is None:\n        return False\n    else:\n        try:\n            return result[0] == tablename\n        except IndexError as e:\n            return check_table_exists(dbcon, tablename)", "contrast": "def table_exists(dbcon, tablename):\n    cur = dbcon.cursor()\n    cur.execute(\"\"\"SELECT COUNT(*)\n                   FROM information_schema.tables\n                   WHERE table_name = '{0}'\"\"\".format(tablename.replace('\\'', '\\'\\'')))\n    if cur.fetchone()[0] == 1:\n        return True\n    return False", "label": 0}
{"index": "gp131925", "code": "def initialize_request(self, request, *args, **kwargs):\n        parser_context = self.get_parser_context(request)\n        return Request(\n            request,\n            parsers=self.get_parsers(),\n            authenticators=self.get_authenticators(),\n            negotiator=self.get_content_negotiator(),\n            parser_context=parser_context\n        )", "contrast": "def get_initial_request():\n    return {}", "label": 0}
{"index": "gp009585", "code": "def _quote_to_py_ast(ctx: GeneratorContext, node: Quote) -> GeneratedPyAST:\n    assert node.op == NodeOp.QUOTE\n    return _const_node_to_py_ast(ctx, node.expr)", "contrast": "import ast\ndef quote_node(value):\n    return ast.Expr(ast.Constant(value=value))", "label": 0}
{"index": "gp069398", "code": "def normalizeXpath(xpath):\n    new_xpath = []\n    for x in range(0, len(xpath)):\n        if x > 0 and len(xpath[x-1]) == 0:\n            new_xpath.append(\"/\"+xpath[x])\n        elif len(xpath[x]) > 0:\n            new_xpath.append(xpath[x])\n    return new_xpath", "contrast": "def normalize_xpath(xpath: List[str]) -> List[str]:\n    refined_xpath = []\n    for element in xpath:\n        if element.startswith(\"[\"):\n            refined_xpath[-1] = refined_xpath[-1] + element\n        else:\n            refined_xpath.append(element)\n    return refined_xpath", "label": 0}
{"index": "gp007406", "code": "def mean_rate(self):\n        if self.counter.value == 0:\n            return 0.0\n        else:\n            elapsed = time() - self.start_time\n            return self.counter.value / elapsed", "contrast": "import time\nclass MeanRate:\n    def __init__(self):\n        self.start_time = time.time()\n        self.count = 0\n    def mark_event(self):\n        self.count += 1\n    def get_mean_rate(self):\n        current_time = time.time()\n        elapsed_time = current_time - self.start_time\n        return self.count / elapsed_time if elapsed_time > 0 else 0", "label": 0}
{"index": "gp042047", "code": "def make_tar(tfn, source_dirs, ignore_path=[], optimize_python=True):\n    def select(fn):\n        rfn = realpath(fn)\n        for p in ignore_path:\n            if p.endswith('/'):\n                p = p[:-1]\n            if rfn.startswith(p):\n                return False\n        if rfn in python_files:\n            return False\n        return not is_blacklist(fn)\n    files = []\n    for sd in source_dirs:\n        sd = realpath(sd)\n        compile_dir(sd, optimize_python=optimize_python)\n        files += [(x, relpath(realpath(x), sd)) for x in listfiles(sd)\n                  if select(x)]\n    tf = tarfile.open(tfn, 'w:gz', format=tarfile.USTAR_FORMAT)\n    dirs = []\n    for fn, afn in files:\n        dn = dirname(afn)\n        if dn not in dirs:\n            d = ''\n            for component in split(dn):\n                d = join(d, component)\n                if d.startswith('/'):\n                    d = d[1:]\n                if d == '' or d in dirs:\n                    continue\n                dirs.append(d)\n                tinfo = tarfile.TarInfo(d)\n                tinfo.type = tarfile.DIRTYPE\n                tf.addfile(tinfo)\n        tf.add(fn, afn)\n    tf.close()", "contrast": "import zipfile\nimport os\ndef make_zip_file(fn, source_dis):\n    with zipfile.ZipFile(fn, 'w') as myzip:\n        for root, dirs, files in os.walk(source_dis):\n            for file in files:\n                myzip.write(os.path.join(root, file), file)\n    return fn", "label": 0}
{"index": "gp272854", "code": "def parse_line(line: str):\n    return msg", "contrast": "def parse(self, line):\n        if not line:\n            raise KatcpSyntaxError(\"Empty message received.\")\n        type_char = line[0]\n        if type_char not in self.TYPE_SYMBOL_LOOKUP:\n            raise KatcpSyntaxError(\"Bad type character %r.\" % (type_char,))\n        mtype = self.TYPE_SYMBOL_LOOKUP[type_char]\n        parts = self.WHITESPACE_RE.split(line)\n        if not parts[-1]:\n            del parts[-1]\n        name = parts[0][1:]\n        arguments = [self._parse_arg(x) for x in parts[1:]]\n        match = self.NAME_RE.match(name)\n        if match:\n            name = match.group('name')\n            mid = match.group('id')\n        else:\n            raise KatcpSyntaxError(\"Bad message name (and possibly id) %r.\" %\n                                   (name,))\n        return Message(mtype, name, arguments, mid)", "label": 1}
{"index": "gp208935", "code": "def check_config(config):\n    required_params = ['param1', 'param2', 'param3'] \n    for param in required_params:\n        if param not in config:\n            raise ValueError('Parameter {} is missing from the config file'.format(param))\n    return config", "contrast": "def check_config(config, data):\n    if 'tolerance' not in config.keys() or not config['tolerance']:\n        config['tolerance'] = 1E-5\n    if not config.get('maximum_iterations', None):\n        config['maximum_iterations'] = 1000\n    mmin_obs = np.min(data['magnitude'])\n    if config.get('input_mmin', 0) < mmin_obs:\n        config['input_mmin'] = mmin_obs\n    if fabs(config['b-value']) < 1E-7:\n        config['b-value'] = 1E-7\n    return config", "label": 1}
{"index": "gp093408", "code": "def _build_tag_families(tagged_paired_aligns,\n                        ranked_tags,\n                        hamming_threshold,\n                        consensus_threshold,\n                        family_filter=lambda _: None):\n    tag_aligns = defaultdict(set)\n    tag_inexact_match_count = defaultdict(int)\n    for paired_align in tagged_paired_aligns:\n        (left_umt, right_umt) =  paired_align.umt\n        for best_tag in ranked_tags:\n            if paired_align.umt == best_tag:\n                tag_aligns[best_tag].add(paired_align)\n                break\n            elif left_umt == best_tag[0] or right_umt == best_tag[1]:\n                tag_aligns[best_tag].add(paired_align)\n                tag_inexact_match_count[best_tag] += 1\n                break\n            elif (_hamming_dist(left_umt, best_tag[0]) <= hamming_threshold)                or (_hamming_dist(right_umt, best_tag[1]) <= hamming_threshold):\n                tag_aligns[best_tag].add(paired_align)\n                tag_inexact_match_count[best_tag] += 1\n                break\n    tag_families = []\n    for tag in sorted(tag_aligns):\n        tag_family = TagFamily(tag,\n                               tag_aligns[tag],\n                               tag_inexact_match_count[tag],\n                               consensus_threshold,\n                               family_filter)\n        tag_families.append(tag_family)\n    return tag_families", "contrast": "def partition_aligns_to_families(aligns, tags):\n    families = {}\n    for align in aligns:\n        best_tag = None\n        best_score = -1\n        for tag in tags:\n            score = get_alignment_score(align, tag)\n            if score > best_score:\n                best_score = score\n                best_tag = tag\n        if best_tag is not None:\n            if best_tag not in families:\n                families[best_tag] = []\n            families[best_tag].append(align)\n    return families\ndef get_alignment_score(align, tag):\n    score = 0\n    for read in align:\n        if read == tag:\n            score += 1\n    return score / len(align) if align else 0.0", "label": 0}
{"index": "gp014057", "code": "def not_in(self, table):\n        idx = self.df.index - table.df.index\n        return Table(df=self.df[idx], name=self.name)", "contrast": "class Table:\n    def __init__(self, table_data):\n        self.table_data = table_data\n    def not_in(self, other_table):\n        not_in_table = [[nucleus for nucleus in row if nucleus not in other_table.table_data] for row in self.table_data]\n        return Table(not_in_table)", "label": 0}
{"index": "gp037767", "code": "def _fulfills_version_spec(versions, oper, desired_version,\n                           ignore_epoch=False):\n    cmp_func = __salt__.get('pkg.version_cmp')\n    if salt.utils.platform.is_freebsd():\n        if isinstance(versions, dict) and 'version' in versions:\n            versions = versions['version']\n    for ver in versions:\n        if (oper == '==' and fnmatch.fnmatch(ver, desired_version))                or salt.utils.versions.compare(ver1=ver,\n                                               oper=oper,\n                                               ver2=desired_version,\n                                               cmp_func=cmp_func,\n                                               ignore_epoch=ignore_epoch):\n            return True\n    return False", "contrast": "import pkg_resources\ndef check_version(specified_version):\n    installed_versions = [pkg.version for pkg in pkg_resources.working_set]\n    if specified_version in installed_versions:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp227753", "code": "def generate_paragraph(relationshiplist, picture_file_name):\n    paragraph = 'Here is an image: <img src=\"' + picture_file_name + '\">\\n'\n    paragraph += 'The updated relationshiplist is: ' + str(relationshiplist)\n    return paragraph", "contrast": "def picture(\n        relationshiplist, picname, picdescription, pixelwidth=None,\n        pixelheight=None, nochangeaspect=True, nochangearrowheads=True,\n        imagefiledict=None):\n    if imagefiledict is None:\n        warn(\n            'Using picture() without imagefiledict parameter will be depreca'\n            'ted in the future.', PendingDeprecationWarning\n        )\n    picid = '2'\n    picpath = abspath(picname)\n    if imagefiledict is not None:\n        if picpath not in imagefiledict:\n            picrelid = 'rId' + str(len(relationshiplist) + 1)\n            imagefiledict[picpath] = picrelid\n            relationshiplist.append([\n                'http: \n                'ionships/image',\n                'media/%s_%s' % (picrelid, basename(picpath))\n            ])\n        else:\n            picrelid = imagefiledict[picpath]\n    else:\n        picrelid = 'rId' + str(len(relationshiplist) + 1)\n        relationshiplist.append([\n            'http://schemas.openxmlformats.org/officeDocument/2006/relations'\n            'hips/image', 'media/' + picname\n        ])\n        media_dir = join(template_dir, 'word', 'media')\n        if not os.path.isdir(media_dir):\n            os.mkdir(media_dir)\n        shutil.copyfile(picname, join(media_dir, picname))\n    image = Image.open(picpath)\n    try:\n        exif = image._getexif()\n        exif = {} if exif is None else exif\n    except:\n        exif = {}\n    imageExif = {}\n    for tag, value in exif.items():\n        imageExif[TAGS.get(tag, tag)] = value\n    imageOrientation = imageExif.get('Orientation', 1)\n    imageAngle = {\n        1: 0, 2: 0, 3: 180, 4: 0, 5: 90, 6: 90, 7: 270, 8: 270\n    }[imageOrientation]\n    imageFlipH = 'true' if imageOrientation in (2, 5, 7) else 'false'\n    imageFlipV = 'true' if imageOrientation == 4 else 'false'\n    if not pixelwidth or not pixelheight:\n        pixelwidth, pixelheight = image.size[0:2]\n    if imageOrientation in (5, 6, 7, 8):\n        pixelwidth, pixelheight = pixelheight, pixelwidth\n    emuperpixel = 12700\n    width = str(pixelwidth * emuperpixel)\n    height = str(pixelheight * emuperpixel)\n    blipfill = makeelement('blipFill', nsprefix='pic')\n    blipfill.append(makeelement('blip', nsprefix='a', attrnsprefix='r',\n                    attributes={'embed': picrelid}))\n    stretch = makeelement('stretch', nsprefix='a')\n    stretch.append(makeelement('fillRect', nsprefix='a'))\n    blipfill.append(makeelement('srcRect', nsprefix='a'))\n    blipfill.append(stretch)\n    nvpicpr = makeelement('nvPicPr', nsprefix='pic')\n    cnvpr = makeelement(\n        'cNvPr', nsprefix='pic',\n        attributes={'id': '0', 'name': 'Picture 1', 'descr': picdescription}\n    )\n    nvpicpr.append(cnvpr)\n    cnvpicpr = makeelement('cNvPicPr', nsprefix='pic')\n    cnvpicpr.append(makeelement(\n        'picLocks', nsprefix='a',\n        attributes={'noChangeAspect': str(int(nochangeaspect)),\n                    'noChangeArrowheads': str(int(nochangearrowheads))}))\n    nvpicpr.append(cnvpicpr)\n    sppr = makeelement('spPr', nsprefix='pic', attributes={'bwMode': 'auto'})\n    xfrm = makeelement(\n        'xfrm', nsprefix='a', attributes={\n            'rot': str(imageAngle * 60000), 'flipH': imageFlipH,\n            'flipV': imageFlipV\n        }\n    )\n    xfrm.append(\n        makeelement('off', nsprefix='a', attributes={'x': '0', 'y': '0'})\n    )\n    xfrm.append(\n        makeelement(\n            'ext', nsprefix='a', attributes={'cx': width, 'cy': height}\n        )\n    )\n    prstgeom = makeelement(\n        'prstGeom', nsprefix='a', attributes={'prst': 'rect'}\n    )\n    prstgeom.append(makeelement('avLst', nsprefix='a'))\n    sppr.append(xfrm)\n    sppr.append(prstgeom)\n    pic = makeelement('pic', nsprefix='pic')\n    pic.append(nvpicpr)\n    pic.append(blipfill)\n    pic.append(sppr)\n    graphicdata = makeelement(\n        'graphicData', nsprefix='a',\n        attributes={'uri': ('http://schemas.openxmlformats.org/drawingml/200'\n                            '6/picture')})\n    graphicdata.append(pic)\n    graphic = makeelement('graphic', nsprefix='a')\n    graphic.append(graphicdata)\n    framelocks = makeelement('graphicFrameLocks', nsprefix='a',\n                             attributes={'noChangeAspect': '1'})\n    framepr = makeelement('cNvGraphicFramePr', nsprefix='wp')\n    framepr.append(framelocks)\n    docpr = makeelement('docPr', nsprefix='wp',\n                        attributes={'id': picid, 'name': 'Picture 1',\n                                    'descr': picdescription})\n    effectextent = makeelement('effectExtent', nsprefix='wp',\n                               attributes={'l': '25400', 't': '0', 'r': '0',\n                                           'b': '0'})\n    extent = makeelement('extent', nsprefix='wp',\n                         attributes={'cx': width, 'cy': height})\n    inline = makeelement('inline', attributes={'distT': \"0\", 'distB': \"0\",\n                                               'distL': \"0\", 'distR': \"0\"},\n                         nsprefix='wp')\n    inline.append(extent)\n    inline.append(effectextent)\n    inline.append(docpr)\n    inline.append(framepr)\n    inline.append(graphic)\n    drawing = makeelement('drawing')\n    drawing.append(inline)\n    run = makeelement('r')\n    run.append(drawing)\n    paragraph = makeelement('p')\n    paragraph.append(run)\n    if imagefiledict is not None:\n        return relationshiplist, paragraph, imagefiledict\n    else:\n        return relationshiplist, paragraph", "label": 1}
{"index": "gp303843", "code": "def remove_duplicates(lst):\n    return list(dict.fromkeys(lst))", "contrast": "def unique(iterable):\n    seen = set()\n    return [x for x in iterable if x not in seen and not seen.add(x)]", "label": 1}
{"index": "gp305231", "code": "class Color:\n    def __init__(self, r, g, b, a):\n        self.r = r\n        self.g = g\n        self.b = b\n        self.a = a\n    @classmethod\n    def from_rgb(cls, r, g, b, a=1.0):\n        return cls(r, g, b, a)\n    def blend(self, other):\n        return Color((1 - other.a) * self.r + other.a * other.r,\n                     (1 - other.a) * self.g + other.a * other.g,\n                     (1 - other.a) * self.b + other.a * other.b,\n                     (1 - other.a) * self.a + other.a * other.a)", "contrast": "def blend(self, other, percent=0.5):\n    dest = 1.0 - percent\n    rgb = tuple(((u * percent) + (v * dest) for u, v in zip(self.__rgb, other.__rgb)))\n    a = (self.__a * percent) + (other.__a * dest)\n    return Color(rgb, 'rgb', a, self.__wref)", "label": 1}
{"index": "gp282521", "code": "def check_networks_or_ports_exist(tenant):\n    return True  ", "contrast": "def tenant_provisioned(tenant_id):\n    session = db.get_reader_session()\n    with session.begin():\n        res = any(\n            session.query(m).filter(m.tenant_id == tenant_id).count()\n            for m in [models_v2.Network, models_v2.Port]\n        )\n    return res", "label": 1}
{"index": "gp236983", "code": "import os\ndef get_static_library_directories():\n    static_libraries = ['.a']\n    directories = []\n    for root, dirs, files in os.walk('.'):\n        for file in files:\n            if file.endswith(tuple(static_libraries)):\n                directories.append(root)\n                break\n    return directories", "contrast": "def library_directories(self):\n        libs = self.find_products('library')\n        if len(libs) > 0:\n            return [os.path.join(self.output_folder)]\n        return []", "label": 1}
{"index": "gp167625", "code": "def delegate(self, fn, *args, **kwargs):\n        return self.subexecutor.spawn(fn, *args, **kwargs)", "contrast": "import gevent\ndef get_operation_as_future(operation):\n    return gevent.spawn(operation)", "label": 0}
{"index": "gp036353", "code": "def delete_task(name, location='\\\\'):\n    if name not in list_tasks(location):\n        return '{0} not found in {1}'.format(name, location)\n    with salt.utils.winapi.Com():\n        task_service = win32com.client.Dispatch(\"Schedule.Service\")\n    task_service.Connect()\n    task_folder = task_service.GetFolder(location)\n    task_folder.DeleteTask(name, 0)\n    if name not in list_tasks(location):\n        return True\n    else:\n        return False", "contrast": "import os\ndef delete_task(name, location='\\\\'):\n    path = os.path.join(location, name)\n    if os.path.exists(path):\n        os.remove(path)\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp257449", "code": "class CachedData:\n    def __init__(self, raw_values):\n        self._data = {}\n        for key, value in raw_values.items():\n            setattr(self, key, value)\n            self._data[key] = value\n    def as_dict(self):\n        return self._data", "contrast": "def _data(self):\n        d = {}\n        for k, v in self._row_values.items():\n            attrs = k.rsplit('__', 1)\n            if len(attrs) == 2:\n                fk, fn = attrs\n                if fk not in d:\n                    d[fk] = {}\n                d[fk][fn] = v\n            else:\n                d[k] = v\n        return d", "label": 1}
{"index": "gp247578", "code": "import dask\nfrom dask.distributed import Client\ndef run_workflow_in_parallel_threads(workflow, n_threads):\n    client = Client(n_workers=n_threads)\n    output = dask.compute(workflow)\n    client.close()\n    return output", "contrast": "def run_parallel(workflow, n_threads):\n    scheduler = Scheduler()\n    threaded_worker = Queue() >> thread_pool(\n        *repeat(worker, n_threads))\n    return scheduler.run(threaded_worker, get_workflow(workflow))", "label": 1}
{"index": "gp205973", "code": "import pandas as pd\ndef write_hdf5_series(filepath_or_buffer, key, data, **kwargs):\n    with pd.HDFStore(filepath_or_buffer) as store:\n        store.put(key, data, format='table', data_columns=True, **kwargs)", "contrast": "def write_hdf5_series(series, output, path=None, attrs=None, **kwargs):\n    if attrs is None:\n        attrs = format_index_array_attrs(series)\n    return write_hdf5_array(series, output, path=path, attrs=attrs, **kwargs)", "label": 1}
{"index": "gp219346", "code": "def get_single_value_from_select_query(session, select_query):\n    return session.execute(select_query).scalar()", "contrast": "async def scalar(self, query, as_tuple=False):\n        query = self._swap_database(query)\n        return (await scalar(query, as_tuple=as_tuple))", "label": 1}
{"index": "gp031474", "code": "def _closest_ref_length(references, trans_length):\n    ref_lengths = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lengths,\n                          key=lambda ref_length: (abs(ref_length - trans_length), ref_length))\n    return closest_ref_len", "contrast": "def find_closest_ref_len(references, trans_length):\n    closest_ref_len = None\n    min_diff = float('inf')\n    for ref in references:\n        ref_len = len(ref)\n        diff = abs(ref_len - trans_length)\n        if diff < min_diff:\n            min_diff = diff\n            closest_ref_len = ref_len\n    return closest_ref_len", "label": 0}
{"index": "gp109419", "code": "def _hex_to_rgb(hex_code):\n    if len(hex_code) != 6:\n        raise ValueError(hex_code + \" is not a string of length 6, cannot convert to rgb.\")\n    return tuple(map(ord, hex_code.decode(\"hex\")))", "contrast": "def _hex_to_rgb(hex_string):\n    r = int(hex_string[0:2], 16)\n    g = int(hex_string[2:4], 16)\n    b = int(hex_string[4:6], 16)\n    return (r, g, b)", "label": 0}
{"index": "gp083332", "code": "def GetCompressedFilesInDir(fileDir, fileList, ignoreDirList, supportedFormatList = ['.rar',]):\n  goodlogging.Log.Info(\"EXTRACT\", \"Parsing file directory: {0}\".format(fileDir))\n  if os.path.isdir(fileDir) is True:\n    for globPath in glob.glob(os.path.join(fileDir, '*')):\n      if os.path.splitext(globPath)[1] in supportedFormatList:\n        fileList.append(globPath)", "contrast": "import os\ndef get_supported_files(fileDir, fileList, ignoreDirList=None, supportedFormatList=['.rar']):\n    if ignoreDirList is None:\n        ignoreDirList = []\n    for path, dirs, files in os.walk(fileDir):\n        dirs[:] = [d for d in dirs if d not in ignoreDirList]\n        for name in files:\n            if name.endswith(tuple(supportedFormatList)):\n                fileList.append(os.path.join(path, name))", "label": 0}
{"index": "gp243569", "code": "def create_raw_data(key: str, value: any) -> str:\n    return \"Success: Data written to DB\"", "contrast": "def create_raw(self, key, value):\n        data = None\n        if key is not None and value is not None:\n            data = self.db.create(key.strip(), value)\n        else:\n            self.tcex.log.warning(u'The key or value field was None.')\n        return data", "label": 1}
{"index": "gp218181", "code": "def set_diagnostics_radiation(longwave, shortwave):\n    longwave_diagnostic = longwave * 1.5\n    shortwave_diagnostic = shortwave * 2.5\n    return None", "contrast": "def do_diagnostics(self):\n        self.OLR = self.subprocess['LW'].flux_to_space\n        self.LW_down_sfc = self.subprocess['LW'].flux_to_sfc\n        self.LW_up_sfc = self.subprocess['LW'].flux_from_sfc\n        self.LW_absorbed_sfc = self.LW_down_sfc - self.LW_up_sfc\n        self.LW_absorbed_atm = self.subprocess['LW'].absorbed\n        self.LW_emission = self.subprocess['LW'].emission\n        self.ASR = (self.subprocess['SW'].flux_from_space -\n                    self.subprocess['SW'].flux_to_space)\n        self.SW_absorbed_atm = self.subprocess['SW'].absorbed\n        self.SW_down_sfc = self.subprocess['SW'].flux_to_sfc\n        self.SW_up_sfc = self.subprocess['SW'].flux_from_sfc\n        self.SW_absorbed_sfc = self.SW_down_sfc - self.SW_up_sfc\n        self.SW_up_TOA = self.subprocess['SW'].flux_to_space\n        self.SW_down_TOA = self.subprocess['SW'].flux_from_space\n        self.planetary_albedo = (self.subprocess['SW'].flux_to_space /\n                                 self.subprocess['SW'].flux_from_space)", "label": 1}
{"index": "gp316904", "code": "def distance_between_centroids(item_a, item_b, max_value):\n    centroid_a = item_a.geom.centroid\n    centroid_b = item_b.geom.centroid\n    distance = centroid_a.distance(centroid_b)\n    scaled_distance = min(distance/max_value, 1.0)\n    return scaled_distance", "contrast": "def start_centroid_distance(item_a, item_b, max_value):\n    start_a = item_a.center_of_mass(item_a.times[0])\n    start_b = item_b.center_of_mass(item_b.times[0])\n    start_distance = np.sqrt((start_a[0] - start_b[0]) ** 2 + (start_a[1] - start_b[1]) ** 2)\n    return np.minimum(start_distance, max_value) / float(max_value)", "label": 1}
{"index": "gp227446", "code": "import numpy as np\nimport matplotlib.pyplot as plt\ndef create_histogram(size, mean, sigma):\n    data = np.random.normal(mean, sigma, size)\n    n, bins, patches = plt.hist(data, bins=50, density=True, alpha=0.7)\n    return n, bins, patches", "contrast": "def normal_h1(size: int = 10000, mean: float = 0, sigma: float = 1) -> Histogram1D:\n    data = np.random.normal(mean, sigma, (size,))\n    return h1(data, name=\"normal\", axis_name=\"x\", title=\"1D normal distribution\")", "label": 1}
{"index": "gp317373", "code": "def add_key_value_pair(dictionary, key, value):\n    if key in dictionary.keys():\n        if dictionary[key] == value:\n            dictionary[key] = value\n    else:\n        dictionary[key] = value\n    return dictionary", "contrast": "def add(self, key, item):\n        mmkeys = self.mmkeys\n        if mmkeys is not None and not (key in self.data):\n            lenkey = len(key)\n            start = min(self.minkeylength,lenkey)\n            mmkeysGet = mmkeys.setdefault\n            for i in range(start,lenkey+1):\n                mmkeysGet(key[0:i],[]).append(key)\n        self.data[key] = item", "label": 1}
{"index": "gp038814", "code": "def _get_http_proxy_url():\n    http_proxy_url = ''\n    host = __salt__['config.option']('proxy_host')\n    port = __salt__['config.option']('proxy_port')\n    username = __salt__['config.option']('proxy_username')\n    password = __salt__['config.option']('proxy_password')\n    if host and port:\n        if username and password:\n            http_proxy_url = 'http://{0}:{1}@{2}:{3}'.format(\n                username,\n                password,\n                host,\n                port\n            )\n        else:\n            http_proxy_url = 'http://{0}:{1}'.format(\n                host,\n                port\n            )\n    return http_proxy_url", "contrast": "def get_http_proxy_url(proxy_username, proxy_password, proxy_host, proxy_port):\n    if proxy_username and proxy_password and proxy_host and proxy_port:\n        return f\"http://{proxy_username}:{proxy_password}@{proxy_host}:{proxy_port}\"\n    else:\n        return \"\"", "label": 0}
{"index": "gp077722", "code": "def get_user_by_token(self, token):\n        if not self.jwt_loader_implementation:\n            return self.default_token_user_loader(token)\n        try:\n            implementation = import_string(self.jwt_loader_implementation)\n        except ImportError:\n            msg = 'Failed to import custom JWT user loader implementation. '\n            msg += 'Check that configured module exists [{}]'\n            raise x.ConfigurationException(\n                msg.format(self.jwt_loader_implementation)\n            )\n        return implementation(token)", "contrast": "from typing import Optional\nfrom boiler.user.models import User\nfrom boilerplate import app\ndef get_user_by_token(token: str) -> Optional[User]:\n    user_loader = app.config.get(\"CUSTOM_USER_LOADER\", None)\n    if user_loader is not None:\n        return user_loader(token)\n    else:\n        user_id = token.get(\"user_id\", None)\n        if user_id is not None:\n            return User.query.filter_by(id=user_id).first()\n        else:\n            return None", "label": 0}
{"index": "gp301718", "code": "def calculate_median(lst):\n    sorted_lst = sorted(lst)\n    lst_len = len(lst)\n    mid = lst_len // 2\n    return (sorted_lst[mid] + sorted_lst[-mid-1]) / 2 if lst_len % 2 == 0 else sorted_lst[mid]", "contrast": "def median(lst):\n    sortedLst = sorted(lst)\n    lstLen = len(lst)\n    index = (lstLen - 1)  \n    if (lstLen % 2):\n        return sortedLst[index]\n    else:\n        return (sortedLst[index] + sortedLst[index + 1])/2.0", "label": 1}
{"index": "gp064801", "code": "def cd_ctx(directory):\n    prevdir = os.path.abspath(os.curdir)\n    if os.path.isdir(directory):\n        os.chdir(directory)\n    yield\n    os.chdir(prevdir)", "contrast": "import os\nclass ChangeDir:\n    def __init__(self, directory):\n        self.old_dir = os.getcwd()\n        self.new_dir = directory\n    def __enter__(self):\n        os.chdir(self.new_dir)\n    def __exit__(self, exc_type, exc_value, traceback):\n        os.chdir(self.old_dir)", "label": 0}
{"index": "gp135431", "code": "def keep_on_one_line():\n    class CondensedStream:\n        def __init__(self):\n            self.sys_stdout = sys.stdout\n        def write(self, string):\n            with swap_streams(self.sys_stdout):\n                string = string.replace('\\n', ' ')\n                string = truncate_to_fit_terminal(string)\n                if string.strip():\n                    update(string)\n        def flush(self):\n            with swap_streams(self.sys_stdout):\n                flush()\n    with swap_streams(CondensedStream()):\n        yield", "contrast": "import sys\ndef oneline_output(func):\n    def wrapper(*args, **kwargs):\n        sys.stdout.write(\"\\x1b[2K\\r\")   \n        sys.stdout.flush()\n        with func(*args, **kwargs) as output:\n            for line in output.splitlines():\n                print(line, end=\"\\r\")\n    return wrapper", "label": 0}
{"index": "gp166702", "code": "def I_minus_R(self,singular_value):\n        if self.__I_R is not None and singular_value == self.__I_R_sv:\n            return self.__I_R\n        else:\n            if singular_value > self.jco.ncol:\n                return self.parcov.zero\n            else:\n                v2 = self.xtqx.v[:, singular_value:]\n                self.__I_R = v2 * v2.T\n                self.__I_R_sv = singular_value\n                return self.__I_R", "contrast": "def get_I_R_at_singular(singular_value):\n    return I - R(singular_value)", "label": 0}
{"index": "gp172802", "code": "from ibis.expr import joins\ndef perform_join(left, right, predicates, how = 'inner'):\n    return left.join(right, predicates, how=how)", "contrast": "def join(left, right, predicates=(), how='inner'):\n    klass = _join_classes[how.lower()]\n    if isinstance(predicates, Expr):\n        predicates = _L.flatten_predicate(predicates)\n    op = klass(left, right, predicates)\n    return op.to_expr()", "label": 1}
{"index": "gp313614", "code": "import urllib\ndef get_page_content(url):\n    with urllib.request.urlopen(url) as response:\n        content = response.read().decode('utf-8')\n    return content", "contrast": "def get_content(self):\n        url = self.build_url()\n        try:\n            self.content_page = requests.get(url)\n            if not(self.content_page.status_code == requests.codes.ok):\n                self.content_page.raise_for_status()\n        except requests.exceptions.RequestException as ex:\n            logging.info('A requests exception has ocurred: ' + str(ex))\n            logging.error(traceback.format_exc())\n            sys.exit(0)", "label": 1}
{"index": "gp071138", "code": "def lookups(self, request, model_admin):\n        output = []\n        for i in models.Model.objects.values('information__type').distinct():\n            ct = i['information__type']\n            if ct is not None:\n                ct = ContentType.objects.get(pk=ct)\n                output.append([ct.pk, ct.app_label + '.' + ct.model])\n        return output", "contrast": "def get_options_list():\n    options = [\n        (1, \"Option 1\"),\n        (2, \"Option 2\"),\n        (3, \"Option 3\"),\n    ]\n    return options", "label": 0}
{"index": "gp133006", "code": "def clean(self, value):\n        super_value = super(RichTextField, self).clean(value)\n        if super_value in fields.EMPTY_VALUES:\n            if self.instance:\n                obj_id = self.get_instance_id(self.instance)\n                if not obj_id:\n                    SourceText.objects.filter(content_type=self.ct, object_id=obj_id, field=self.field_name, processor=self.processor).delete()\n                else:\n                    SourceText.objects.filter(content_type=self.ct, object_id=obj_id, field=self.field_name).delete()\n            self.validate_rendered('')\n            return ''\n        text = smart_unicode(value)\n        if self.instance:\n            obj_id = self.get_instance_id(self.instance)\n            try:\n                if not obj_id:\n                    src_text = SourceText(content_type=self.ct, object_id=obj_id, field=self.field_name, processor=self.processor)\n                else:\n                    src_text = SourceText.objects.get(content_type=self.ct, object_id=obj_id, field=self.field_name)\n                assert src_text.processor == self.processor\n            except SourceText.DoesNotExist:\n                src_text = SourceText(content_type=self.ct, object_id=obj_id, field=self.field_name, processor=self.processor)\n            src_text.content = text\n            try:\n                rendered = src_text.render()\n            except ProcessorError, e:\n                raise ValidationError(self.error_messages['syntax_error'])\n        else:\n            self.instance = src_text = SourceText(\n                content_type=self.ct,\n                field=self.field_name,\n                content=text,\n                processor=self.processor\n            )\n            try:\n                rendered = src_text.render()\n            except Exception, err:\n                raise ValidationError(self.error_messages['syntax_error'])\n        self.validate_rendered(rendered)\n        if not hasattr(self.model, RICH_FIELDS_SET):\n            setattr(self.model, RICH_FIELDS_SET, set())\n        getattr(self.model, RICH_FIELDS_SET).add(self.field_name)\n        signals.post_save.connect(receiver=self.post_save_listener, sender=self.model)\n        rendered = UnicodeWrapper(rendered)\n        setattr(rendered, self.src_text_attr, src_text)\n        return rendered", "contrast": "from django.core.exceptions import ValidationError\nfrom your_app.models import SourceText\ndef clean_field_and_store(field_value):\n    try:\n        cleaned_value = my_cleaning_function(field_value)\n        source_text = SourceText.objects.create(original_value=field_value)\n        return cleaned_value\n    except Exception as e:\n        raise ValidationError(str(e))", "label": 0}
{"index": "gp252317", "code": "def get_file_contents(path):\n    with open(path, 'r') as f:\n        contents = f.read().splitlines()\n    return contents", "contrast": "def file_to_list(path):\n    if not os.path.exists(path):\n        ui.error(c.MESSAGES[\"path_missing\"], path)\n        sys.exit(1)\n    with codecs.open(path, \"r\", \"UTF-8\") as contents:\n        lines = contents.read().splitlines()\n    return lines", "label": 1}
{"index": "gp215981", "code": "import dis\ndef simple_opcode_disassembly(opcode):\n    dis.dis(compile(opcode, '', 'exec'))", "contrast": "def main():\n    test_targets = (    \n        [ARCH_I386, MACH_I386_I386_INTEL_SYNTAX, ENDIAN_MONO, \"\\x55\\x89\\xe5\\xE8\\xB8\\xFF\\xFF\\xFF\", 0x1000],\n        [ARCH_I386, MACH_X86_64_INTEL_SYNTAX, ENDIAN_MONO, \"\\x55\\x48\\x89\\xe5\\xE8\\xA3\\xFF\\xFF\\xFF\", 0x1000],\n        [ARCH_ARM, MACH_ARM_2, ENDIAN_LITTLE, \"\\x04\\xe0\\x2d\\xe5\\xED\\xFF\\xFF\\xEB\", 0x1000],\n        [ARCH_MIPS, MACH_MIPSISA32, ENDIAN_BIG, \"\\x0C\\x10\\x00\\x97\\x00\\x00\\x00\\x00\", 0x1000],\n        [ARCH_POWERPC, MACH_PPC, ENDIAN_BIG, \"\\x94\\x21\\xFF\\xE8\\x7C\\x08\\x02\\xA6\", 0x1000],\n        )\n    for target_arch, target_mach, target_endian, binary, address in test_targets:\n        opcodes = Opcodes(target_arch, target_mach, target_endian)\n        print \"\\n[+] Architecture %s - Machine %d\" %            (opcodes.architecture_name, opcodes.machine)\n        print \"[+] Disassembly:\"\n        for vma, size, disasm in opcodes.disassemble(binary, address):\n            print \"0x%X (size=%d)\\t %s\" % (vma, size, disasm)", "label": 1}
{"index": "gp274542", "code": "def remove_access_list_items(load_balancer, *item_ids):\n    for item_id in item_ids:\n        load_balancer.access_list.remove(item_id)", "contrast": "def delete_access_list_items(self, loadbalancer, item_ids):\n        if not isinstance(item_ids, (list, tuple)):\n            item_ids = [item_ids]\n        valid_ids = [itm[\"id\"] for itm in self.get_access_list(loadbalancer)]\n        bad_ids = [str(itm) for itm in item_ids if itm not in valid_ids]\n        if bad_ids:\n            raise exc.AccessListIDNotFound(\"The following ID(s) are not valid \"\n                    \"Access List items: %s\" % \", \".join(bad_ids))\n        items = \"&\".join([\"id=%s\" % item_id for item_id in item_ids])\n        uri = \"/loadbalancers/%s/accesslist?%s\" % (\n                utils.get_id(loadbalancer), items)\n        resp, body = self.api.method_delete(uri)\n        return body", "label": 1}
{"index": "gp100691", "code": "def add_path(self, path):\n        if not os.path.exists(path):\n            raise RuntimeError('Path does not exists: %s.' % path)\n        self.paths.append(path)", "contrast": "import json\ndef load_translations(path):\n    with open(path, 'r') as f:\n        return json.load(f)", "label": 0}
{"index": "gp183572", "code": "import subprocess\ndef git_command_generator(command):\n  process = subprocess.Popen([\"git\", command], stdout=subprocess.PIPE)\n  while True:\n    output = process.stdout.readline()\n    if output == '' and process.poll() is not None:\n        break\n    if output:\n        yield output.strip()", "contrast": "def git_lines(*args, git=maybeloggit, **kwargs):\n    err = io.StringIO()\n    try:\n        for line in git('--no-pager', _err=err, *args, _decode_errors='replace', _iter=True, _bg_exc=False, **kwargs):\n            yield line[:-1]  \n    except sh.ErrorReturnCode as e:\n        status('exit_code=%s' % e.exit_code)\n    errlines = err.getvalue().splitlines()\n    if len(errlines) < 3:\n        for line in errlines:\n            status(line)\n    else:\n        vd().push(TextSheet('git ' + ' '.join(args), errlines))", "label": 1}
{"index": "gp282306", "code": "import discord\nfrom ui_embed import UI\ndef create_error_embed(channel: discord.Channel, max_warnings: int) -> UI:\n    ui = UI(channel)\n    ui.set_title('Error')\n    ui.set_description(f'Maximum warnings now set to {max_warnings}')\n    ui.set_color(0xff0000)\n    return ui", "contrast": "def warning_max_changed(channel, max_warnings):\n    gui = ui_embed.UI(\n        channel,\n        \"Maximum Warnings Changed\",\n        \"Users must now have {} warnings to be banned \"\n        \"(this won't ban existing users with warnings)\".format(max_warnings),\n        modulename=modulename\n    )\n    return gui", "label": 1}
{"index": "gp318185", "code": "def filter_resources(resources, filters):\n    filtered_resources = []\n    for i, resource in enumerate(resources):\n        filtered_resource = [r for r in resource if filters[i](r)]\n        filtered_resources.append(filtered_resource)\n    return filtered_resources", "contrast": "def filter(self, request, queryset, view):\n        summary_queryset = queryset\n        filtered_querysets = []\n        for queryset in summary_queryset.querysets:\n            filter_class = self._get_filter(queryset)\n            queryset = filter_class(request.query_params, queryset=queryset).qs\n            filtered_querysets.append(queryset)\n        summary_queryset.querysets = filtered_querysets\n        return summary_queryset", "label": 1}
{"index": "gp067611", "code": "def check_cousins(self, individual_1_id, individual_2_id):\n        self.logger.debug(\"Checking if {0} and {1} are cousins\".format(\n            individual_1_id, individual_2_id\n        ))\n        pass", "contrast": "def are_cousins(individual_1_id: str, individual_2_id: str) -> bool:\n    grandparents_1 = set([parent for parent in get_parents(individual_1_id) if parent])\n    grandparents_2 = set([parent for parent in get_parents(individual_2_id) if parent])\n    cousins = []\n    for grandparent in grandparents_1:\n        if grandparent in grandparents_2:\n            cousins.append(grandparent)\n    if len(cousins) > 0:\n        return True\n    else:\n        return False\ndef get_parents(individual_id: str) -> list:\n    pass  ", "label": 0}
{"index": "gp029517", "code": "def run_selection(self):\r\n        text = self.get_current_editor().get_selection_as_executable_code()\r\n        if text:\r\n            self.exec_in_extconsole.emit(text.rstrip(), self.focus_to_editor)\r\n            return\r\n        editor = self.get_current_editor()\r\n        line = editor.get_current_line()\r\n        text = line.lstrip()\r\n        if text:\r\n            self.exec_in_extconsole.emit(text, self.focus_to_editor)\r\n        if editor.is_cursor_on_last_line() and text:\r\n            editor.append(editor.get_line_separator())\r\n        editor.move_cursor_to_next('line', 'down')", "contrast": "import sublime\nimport sublime_plugin\nclass RunTextCommand(sublime_plugin.TextCommand):\n  def run(self, edit):\n    sels = self.view.sel()\n    if not sels:\n      return\n    for sel in sels:\n      if sel.empty():\n        line = self.view.line(sel)\n        if line.empty():\n          self.view.run_command(\"insert\", {\"characters\": \"\\n\"})\n          self.view.show(self.view.size())\n          return\n        sel = line\n      code = self.view.substr(sel)\n      self.view.window().run_command(\"exec\", {\"cmd\": [code], \"shell\": True})", "label": 0}
{"index": "gp324358", "code": "def get_direct_members():\n    member_names = self.get_member_names()\n    members = []\n    for member in member_names:\n        members.append(self.get_member(member))\n    return members", "contrast": "def get_member_list(self):\n        if not self.is_collection:\n            raise NotImplementedError\n        memberList = []\n        for name in self.get_member_names():\n            member = self.get_member(name)\n            assert member is not None\n            memberList.append(member)\n        return memberList", "label": 1}
{"index": "gp239263", "code": "async def add_alternative_data_id(data_id):\n    if data_id > 0 and data_id < 256:\n        return data_id\n    else:\n        return None", "contrast": "async def add_alternative(self, alt, timeout=OTGW_DEFAULT_TIMEOUT):\n        cmd = OTGW_CMD_ADD_ALT\n        alt = int(alt)\n        if alt < 1 or alt > 255:\n            return None\n        ret = await self._wait_for_cmd(cmd, alt, timeout)\n        if ret is not None:\n            return int(ret)", "label": 1}
{"index": "gp032347", "code": "def nodes(**kwargs):\n    cfg = _setup_conn(**kwargs)\n    try:\n        api_instance = kubernetes.client.CoreV1Api()\n        api_response = api_instance.list_node()\n        return [k8s_node['metadata']['name'] for k8s_node in api_response.to_dict().get('items')]\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception('Exception when calling CoreV1Api->list_node')\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "contrast": "import subprocess\ndef kubernetes_nodes(kubeconfig=None, context=None):\n    command = 'kubectl get nodes -o jsonpath=\"{.items[*].metadata.name}\"'\n    if context:\n        command += f' --context={context}'\n    if kubeconfig:\n        command += f' --kubeconfig={kubeconfig}'\n    result = subprocess.run(command, stdout=subprocess.PIPE, shell=True)\n    return result.stdout.decode().strip().split()", "label": 0}
{"index": "gp100782", "code": "def has_comic(name):\n    names = [\n        (\"Creators/%s\" % name).lower(),\n        (\"GoComics/%s\" % name).lower(),\n    ]\n    for scraperclass in get_scraperclasses():\n        lname = scraperclass.getName().lower()\n        if lname in names:\n            return True\n    return False", "contrast": "def check_comic_name(name, existing_names):\n    if name in existing_names:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp103180", "code": "def DosDateTimeToTimeTuple(dosDateTime):\n    dos_date = dosDateTime >> 16\n    dos_time = dosDateTime & 0xffff\n    day = dos_date & 0x1f\n    month = (dos_date >> 5) & 0xf\n    year = 1980 + (dos_date >> 9)\n    second = 2 * (dos_time & 0x1f)\n    minute = (dos_time >> 5) & 0x3f\n    hour = dos_time >> 11\n    return time.localtime(\n        time.mktime((year, month, day, hour, minute, second, 0, 1, -1)))", "contrast": "import datetime\ndef msdos_datetime_to_time_tuple(msdos_datetime):\n    dos_date = (msdos_datetime >> 16) & 0xFFFF\n    dos_time = msdos_datetime & 0xFFFF\n    year = ((dos_date >> 9) & 0x7F) + 1980\n    month = (dos_date >> 5) & 0xF\n    day = dos_date & 0x1F\n    hour = (dos_time >> 11) & 0x1F\n    minute = (dos_time >> 5) & 0x3F\n    second = (dos_time & 0x1F) * 2\n    return datetime.datetime(year, month, day, hour, minute, second).timetuple()", "label": 0}
{"index": "gp287602", "code": "from pathlib import Path\nfrom glob import glob\ndef default_mapper(module_name: str) -> list:\n    path = Path(__file__).parent.resolve()\n    latest_path = max(path.glob(f'*/{module_name}'), key=lambda p: p.stat().st_mtime)\n    js_files = glob(str(latest_path) + '/*.js')\n    js_modules = [f'./{latest_path.name}/{p.stem}' for p in js_files]\n    return js_modules", "contrast": "def mapper_python(module, entry_point, globber='root', fext=JS_EXT):\n    return mapper(\n        module, entry_point=entry_point, modpath='pkg_resources',\n        globber=globber, modname='python', fext=fext)", "label": 1}
{"index": "gp333237", "code": "import threading\ndef stop_timer(timer: threading.Timer):\n    if not timer.is_alive():\n        return\n    timer.cancel()", "contrast": "def stop_timer(self, func):\n        if func in self._timer_callbacks:\n            t = self._timer_callbacks[func]\n            t.cancel()\n            del self._timer_callbacks[func]", "label": 1}
{"index": "gp187201", "code": "from prompt_toolkit.key_binding.key_processor import KeyPress\nfrom pymux.keys import keys\nfrom typing import List\ndef pymux_to_prompt_toolkit_key(pymux_key: str) -> List[KeyPress]:\n    try:\n        return keys[pymux_key]\n    except KeyError:\n        raise ValueError(\"Unknown key: {}\".format(pymux_key))", "contrast": "def pymux_key_to_prompt_toolkit_key_sequence(key):\n    if key.lower().startswith('m-c-'):\n        key = 'M-C-' + key[4:]\n    elif key.lower().startswith('c-'):\n        key = 'C-' + key[2:]\n    elif key.lower().startswith('m-'):\n        key = 'M-' + key[2:]\n    try:\n        return PYMUX_TO_PROMPT_TOOLKIT_KEYS[key]\n    except KeyError:\n        if len(key) == 1:\n            return (key, )\n        else:\n            raise ValueError('Unknown key: %r' % (key, ))", "label": 1}
{"index": "gp311585", "code": "def format_feature(feature):\n    return f\"{feature.capitalize()}: {bed}\"", "contrast": "def bed(self, *attrs, **kwargs):\n        exclude = (\"chrom\", \"start\", \"end\", \"txStart\", \"txEnd\", \"chromStart\",\n                \"chromEnd\")\n        if self.is_gene_pred:\n            return self.bed12(**kwargs)\n        return \"\\t\".join(map(str, (\n                 [self.chrom, self.start, self.end] +\n                 [getattr(self, attr) for attr in attrs if not attr in exclude]\n                         )))", "label": 1}
{"index": "gp311487", "code": "import hashlib\nimport os\ndef calculate_peep_hash(files):\n    try:\n        peep_hash = \"\"\n        for file in files:\n            with open(file, \"rb\") as f:\n                bytes_data = f.read()\n            hash_object = hashlib.sha256(bytes_data)\n            peep_hash += hash_object.hexdigest()\n        if not peep_hash:\n            return 1\n        else:\n            print(peep_hash)\n            return 0\n    except Exception:\n        raise PipException(\"Failed to calculate peep hash\")", "contrast": "def peep_hash(argv):\n    parser = OptionParser(\n        usage='usage: %prog hash file [file ...]',\n        description='Print a peep hash line for one or more files: for '\n                    'example, \"# sha256: '\n                    'oz42dZy6Gowxw8AelDtO4gRgTW_xPdooH484k7I5EOY\".')\n    _, paths = parser.parse_args(args=argv)\n    if paths:\n        for path in paths:\n            print('# sha256:', hash_of_file(path))\n        return ITS_FINE_ITS_FINE\n    else:\n        parser.print_usage()\n        return COMMAND_LINE_ERROR", "label": 1}
{"index": "gp215146", "code": "def transform_dict(data):\n    result = {}\n    for d in data:\n        source_col = d['source column']\n        transform = d['transform']\n        if source_col in result:\n            result[source_col].add(transform)\n        else:\n            result[source_col] = {transform}\n        d.pop('transform')\n    return result", "contrast": "def invert_features(features):\n  inverted_features = collections.defaultdict(list)\n  for transform in six.itervalues(features):\n    source_column = transform['source_column']\n    inverted_features[source_column].append(transform)\n  return dict(inverted_features)", "label": 1}
{"index": "gp125387", "code": "def tar_runner(self):\n        script_bytes = self.RUNNER.read_bytes()\n        tarstream = BytesIO()\n        tar = tarfile.TarFile(fileobj=tarstream, mode='w')\n        tarinfo = tarfile.TarInfo(name=\"runner.py\")\n        tarinfo.size = len(script_bytes)\n        tarinfo.mtime = int(time.time())\n        tar.addfile(tarinfo, BytesIO(script_bytes))\n        tar.close()\n        return tarstream.getvalue()", "contrast": "import tarfile\ndef get_runner_script_tar():\n    with tarfile.open(\"runner_script.tar.gz\", \"w:gz\") as tar:\n        tar.add(\"path/to/runner/script\")\n    return tar", "label": 0}
{"index": "gp066693", "code": "def _check(self):\n        for k,ix in six.iteritems(self._indices):\n            assert k is not None, 'null key'\n            assert ix, 'Key does not map to any indices'\n            assert ix == sorted(ix), \"Key's indices are not in order\"\n            for i in ix:\n                assert i in self._lines, 'Key index does not map to line'\n                assert self._lines[i].key is not None, 'Key maps to comment'\n                assert self._lines[i].key == k, 'Key does not map to itself'\n                assert self._lines[i].value is not None, 'Key has null value'\n        prev = None\n        for i, line in six.iteritems(self._lines):\n            assert prev is None or prev < i, 'Line indices out of order'\n            prev = i\n            if line.key is None:\n                assert line.value is None, 'Comment/blank has value'\n                assert line.source is not None, 'Comment source not stored'\n                assert loads(line.source) == {}, 'Comment source is not comment'\n            else:\n                assert line.value is not None, 'Key has null value'\n                if line.source is not None:\n                    assert loads(line.source) == {line.key: line.value},                        'Key source does not deserialize to itself'\n                assert line.key in self._indices, 'Key is missing from map'\n                assert i in self._indices[line.key],                    'Key does not map to itself'", "contrast": "def assert_consistency(self):\n    assert len(self.data_structure_1) == len(self.data_structure_2), \"Data structures have different lengths\"\n    assert set(self.data_structure_1.keys()) == set(self.data_structure_2.keys()), \"Keys do not match\"\n    assert all(isinstance(value, int) for value in self.data_structure_1.values()), \"Values in data_structure_1 must be integers\"\n    assert all(isinstance(value, str) for value in self.data_structure_2.values()), \"Values in data_structure_2 must be strings\"", "label": 0}
{"index": "gp310318", "code": "def inform_montblanc(dimensions: tuple[int, int, int]) -> None:\n    print(f\"Dimensions (in mm): {dimensions[0]} x {dimensions[1]} x {dimensions[2]}\")", "contrast": "def updated_dimensions(self):\n        return [(\"ntime\", args.ntime),      \n                (\"nchan\", args.nchan),      \n                (\"na\", args.na),            \n                (\"npsrc\", len(lm_coords))]", "label": 1}
{"index": "gp018310", "code": "def show_some(items:Collection, n_max:int=5, sep:str=','):\n    if items is None or len(items) == 0: return ''\n    res = sep.join([f'{o}' for o in items[:n_max]])\n    if len(items) > n_max: res += '...'\n    return res", "contrast": "def get_first_n_items(items, n_max):\n    return items[:n_max]", "label": 0}
{"index": "gp224317", "code": "def compare_dicts(old_cmp_dict: dict, new_cmp_dict: dict, id_col: str) -> list:\n    result = []\n    for k, v in old_cmp_dict.items():\n        if k in new_cmp_dict:\n            if v != new_cmp_dict[k]:\n                diff = {id_col: k}\n                for nested_k, nested_v in v.items():\n                    if nested_v != new_cmp_dict[k][nested_k]:\n                        diff[nested_k] = (nested_v, new_cmp_dict[k][nested_k])\n                if len(diff) > 1:\n                    result.append(diff)\n    return result", "contrast": "def changes(new_cmp_dict, old_cmp_dict, id_column, columns):\n    update_ldict = []\n    same_keys = set(new_cmp_dict).intersection(set(old_cmp_dict))\n    for same_key in same_keys:\n        old_dict = old_cmp_dict[same_key]\n        new_dict = new_cmp_dict[same_key]\n        dict_keys = set(old_dict).intersection(set(new_dict))\n        update_dict = {}\n        for dict_key in columns:\n            old_val = old_dict.get(dict_key, 'NaN')\n            new_val = new_dict.get(dict_key, 'NaN')\n            if old_val != new_val and new_val != 'NaN':\n                if id_column!=None:\n                    try:\n                        update_dict[id_column] = old_dict[id_column]\n                    except KeyError:\n                        print(\"Input Dictionary 'old_cmp_dict' must have ID column\")\n                update_dict[dict_key] = new_val\n        if update_dict:\n            update_ldict.append(update_dict)\n    return update_ldict", "label": 1}
{"index": "gp238983", "code": "import datetime\ndef go_to_time_forward(hours=0,minutes=0,seconds=0):\n    current_time = datetime.datetime.now()\n    delta = datetime.timedelta(hours=hours,minutes=minutes,seconds=seconds)\n    target_time = current_time + delta\n    while datetime.datetime.now() < target_time:\n        pass", "contrast": "def add_time(self, extra_time):\n        window_start = self.parent.value('window_start') + extra_time\n        self.parent.overview.update_position(window_start)", "label": 1}
{"index": "gp299225", "code": "import numpy as np\ndef create_poly1d_objects(a, r, t):\n    p1 = np.poly1d(a)\n    p2 = np.poly1d([1, -r])\n    p3 = np.poly1d([1, r * np.exp(1j * t)])\n    return p1, p2, p3", "contrast": "def create_polynoms():\n    fname = pr.resource_filename('pyciss', 'data/soliton_prediction_parameters.csv')\n    res_df = pd.read_csv(fname)\n    polys = {}\n    for resorder, row in zip('65 54 43 21'.split(),\n                             range(4)):\n        p = poly1d([res_df.loc[row, 'Slope (km/yr)'], res_df.loc[row, 'Intercept (km)']])\n        polys['janus ' + ':'.join(resorder)] = p\n    return polys", "label": 1}
{"index": "gp212569", "code": "def unpack_data(data):\n    raw_data = RawDataWrapper(data)\n    return raw_data", "contrast": "def load_data(handle, reader=None):\n    if not reader:\n        reader = os.path.splitext(handle)[1][1:].lower()\n    if reader not in _READERS:\n        raise NeuroMError('Do not have a loader for \"%s\" extension' % reader)\n    filename = _get_file(handle)\n    try:\n        return _READERS[reader](filename)\n    except Exception as e:\n        L.exception('Error reading file %s, using \"%s\" loader', filename, reader)\n        raise RawDataError('Error reading file %s:\\n%s' % (filename, str(e)))", "label": 1}
{"index": "gp272288", "code": "def filter_artifacts(user_permissions, artifacts):\r\n    allowed_artifacts = []\r\n    for artifact in artifacts:\r\n        if all(permission in user_permissions for permission in artifact['permissions']):\r\n            allowed_artifacts.append(artifact)\r\n    return allowed_artifacts\r", "contrast": "def filter_queryset(self, request, queryset, view):\n        if request.user.is_superuser:\n            return queryset\n        return queryset.filter(status__user=request.user)", "label": 1}
{"index": "gp054632", "code": "def onKeyPressInCanvas(self, event):\n        char_map = { 'w':'move 1', 'a':'strafe -1', 's':'move -1', 'd':'strafe 1', ' ':'jump 1' }\n        keysym_map = { 'continuous': { 'Left':'turn -1', 'Right':'turn 1', 'Up':'pitch -1', 'Down':'pitch 1', 'Shift_L':'crouch 1',\n                                       'Shift_R':'crouch 1', \n                                       '1':'hotbar.1 1', '2':'hotbar.2 1', '3':'hotbar.3 1', '4':'hotbar.4 1', '5':'hotbar.5 1',\n                                       '6':'hotbar.6 1', '7':'hotbar.7 1', '8':'hotbar.8 1', '9':'hotbar.9 1' },\n                       'discrete':   { 'Left':'turn -1', 'Right':'turn 1', 'Up':'move 1', 'Down':'move -1', \n                                       '1':'hotbar.1 1', '2':'hotbar.2 1', '3':'hotbar.3 1', '4':'hotbar.4 1', '5':'hotbar.5 1',\n                                       '6':'hotbar.6 1', '7':'hotbar.7 1', '8':'hotbar.8 1', '9':'hotbar.9 1' } }\n        if event.char == '/':\n            self.command_entry.focus_set() \n        elif event.char.lower() in char_map:\n            self.agent_host.sendCommand( char_map[ event.char.lower() ] )\n        elif event.keysym in keysym_map[self.action_space]:\n            self.agent_host.sendCommand( keysym_map[self.action_space][ event.keysym ] )", "contrast": "def key_pressed(event):\n    print(f\"A key was pressed: {event.char}\")", "label": 0}
{"index": "gp226146", "code": "from datetime import datetime, timedelta\ndef morph_log_data_by_time_period(log_line, time_period):\n    time_stamp = datetime.strptime(log_line.split()[0], '%H:%M:%S')\n    delta = timedelta(hours=int(time_period[:2]), minutes=int(time_period[3:5]), seconds=int(time_period[6:]))\n    new_time_stamp = (time_stamp - delta).strftime('%H:%M:%S')\n    return log_line.replace(log_line.split()[0], new_time_stamp, 1)", "contrast": "def filter_data(self, data, value=None, args=None):\n        if args:\n            if not args.last:\n                return data\n        if not value: value = args.last\n        lastunit = value[-1]\n        lastnum = value[:-1]\n        if lastunit == 's':\n            starttime = datetime.utcnow() -                    timedelta(seconds=int(lastnum))\n        if lastunit == 'm':\n            starttime = datetime.utcnow() -                    timedelta(minutes=int(lastnum))\n        if lastunit == 'h':\n            starttime = datetime.utcnow() -                    timedelta(hours=int(lastnum))\n        if lastunit == 'd':\n            starttime = datetime.utcnow() -                    timedelta(days=int(lastnum))\n        ourstart = int(starttime.strftime('%Y%m%d%H%M%S'))\n        newdata = {}\n        if 'parser' in data.keys():\n            newdata['parser'] = data['parser']\n            newdata['source_path'] = data['source_path']\n            newdata['source_file'] = data['source_file']\n            newdata['source_file_mtime'] = data['source_file_mtime']\n            newdata['source_file_year'] = data['source_file_year']\n        newdata['entries'] = []\n        for entry in data['entries']:\n            if 'numeric_date_stamp_utc' in entry.keys():\n                if '.' in entry['numeric_date_stamp_utc']:\n                    dstamp = int(entry['numeric_date_stamp_utc'].split('.')[0])\n                else:\n                    dstamp = int(entry['numeric_date_stamp_utc'])\n                if dstamp >= ourstart: \n                    newdata['entries'].append(entry)\n        return newdata", "label": 1}
{"index": "gp304011", "code": "def get_classID_classes(module):\n    return [cls for name, cls in module.__dict__.items() if isinstance(cls, type) and hasattr(cls, 'classID')] ", "contrast": "def obj_classes_from_module(module):\n    for name in dir(module):\n        if not name.startswith('_'):\n            cls = getattr(module, name)\n            if getattr(cls, 'classID', None):\n                yield (name, cls)", "label": 1}
{"index": "gp257103", "code": "def insert_asm_instruction(mnemonic, params, memory_bytes):\n    memory_bytes.append(opcode(mnemonic, params))\n    return memory_bytes", "contrast": "def add_instruction(self, instr):\n        if gl.has_errors:\n            return\n        __DEBUG__('%04Xh [%04Xh] ASM: %s' % (self.org, self.org - self.ORG, instr.asm))\n        self.set_memory_slot()\n        self.orgs[self.org] += (instr,)\n        for byte in instr.bytes():\n            self.__set_byte(byte, instr.lineno)", "label": 1}
{"index": "gp147209", "code": "def rnaseq_variant_calling(samples, run_parallel):\n    samples = run_parallel(\"run_rnaseq_variant_calling\", samples)\n    variantcaller = dd.get_variantcaller(to_single_data(samples[0]))\n    if variantcaller and (\"gatk-haplotype\" in variantcaller):\n        out = []\n        for d in joint.square_off(samples, run_parallel):\n            out.extend([[to_single_data(xs)] for xs in multi.split_variants_by_sample(to_single_data(d))])\n        samples = out\n    if variantcaller:\n        samples = run_parallel(\"run_rnaseq_ann_filter\", samples)\n    if variantcaller and (\"gatk-haplotype\" in variantcaller):\n        out = []\n        for data in (to_single_data(xs) for xs in samples):\n            if \"variants\" not in data:\n                data[\"variants\"] = []\n            data[\"variants\"].append({\"variantcaller\": \"gatk-haplotype\", \"vcf\": data[\"vrn_file_orig\"],\n                                     \"population\": {\"vcf\": data[\"vrn_file\"]}})\n            data[\"vrn_file\"] = data.pop(\"vrn_file_orig\")\n            out.append([data])\n        samples = out\n    return samples", "contrast": "import subprocess\ndef run_gatk_rnaseq(input_file, reference_genome, output_file):\n    gatk_cmd = \"gatk HaplotypeCaller -R {0} -I {1} -O {2}\".format(reference_genome, input_file, output_file)\n    subprocess.call(gatk_cmd, shell=True)", "label": 0}
{"index": "gp140401", "code": "def has_param(param):\n    def has_param_closure(element):\n        if element.params.get(param, \"\").strip():\n            return True\n        return False\n    return has_param_closure", "contrast": "def is_param_in_element(param, element):\n    return param in element.text", "label": 0}
{"index": "gp009416", "code": "def get_term_by_sis_id(self, sis_term_id):\n        for term in self.get_all_terms():\n            if term.sis_term_id == sis_term_id:\n                return term", "contrast": "def get_term_resource(sis_id):\n    return term_resource", "label": 0}
{"index": "gp181202", "code": "def get_psd_single_ifo(cli):\n    psd_data = cli.get_single_detector_psd()\n    return psd_data[0]", "contrast": "def from_cli_single_ifo(opt, length, delta_f, low_frequency_cutoff, ifo,\n             **kwargs):\n    single_det_opt = copy_opts_for_single_ifo(opt, ifo)\n    return from_cli(single_det_opt, length, delta_f, low_frequency_cutoff,\n                    **kwargs)", "label": 1}
{"index": "gp142847", "code": "def get_alarms(zone=None):\n    if zone is None:\n        zone = discovery.any_soco()\n    response = zone.alarmClock.ListAlarms()\n    alarm_list = response['CurrentAlarmList']\n    tree = XML.fromstring(alarm_list.encode('utf-8'))\n    alarms = tree.findall('Alarm')\n    result = set()\n    for alarm in alarms:\n        values = alarm.attrib\n        alarm_id = values['ID']\n        if Alarm._all_alarms.get(alarm_id):\n            instance = Alarm._all_alarms.get(alarm_id)\n        else:\n            instance = Alarm(None)\n            instance._alarm_id = alarm_id\n            Alarm._all_alarms[instance._alarm_id] = instance\n        instance.start_time = datetime.strptime(\n            values['StartTime'], \"%H:%M:%S\").time()  \n        instance.duration = None if values['Duration'] == '' else            datetime.strptime(values['Duration'], \"%H:%M:%S\").time()\n        instance.recurrence = values['Recurrence']\n        instance.enabled = values['Enabled'] == '1'\n        instance.zone = next((z for z in zone.all_zones\n                              if z.uid == values['RoomUUID']), None)\n        if instance.zone is None:\n            continue\n        instance.program_uri = None if values['ProgramURI'] ==            \"x-rincon-buzzer:0\" else values['ProgramURI']\n        instance.program_metadata = values['ProgramMetaData']\n        instance.play_mode = values['PlayMode']\n        instance.volume = values['Volume']\n        instance.include_linked_zones = values['IncludeLinkedZones'] == '1'\n        result.add(instance)\n    return result", "contrast": "from soco import SoCo\ndef get_all_alarms(zone=None):\n    if zone is None:\n        zone = SoCo.any_soco()\n    response = zone.avTransport.GetZoneAttributes([\n        ('InstanceID', 0),\n        ('DesiredAttributes', {'AlarmRunSequence': '', 'IncludeLinkedZones': 0})\n    ])\n    alarms = set()\n    for attr in response['CurrentZoneAttributes']['AlarmRunSequence']:\n        alarms.add(Alarm.from_xml(attr))\n    return alarms", "label": 0}
{"index": "gp299787", "code": "def delete_document(id):\n    collection.delete_one({'_id': id})", "contrast": "def delete(self, request, _id):\n        _id = deserialize(_id)\n        to_delete = self.collection.get({'_id': _id})\n        if to_delete:\n            deleted = to_delete.delete()\n            return Response(\n                response=serialize(deleted),\n                status=(\n                    200 if not all(\n                        key in deleted for key in [\n                            'error_code', 'error_type', 'error_message'\n                        ]\n                    ) else 400\n                )\n            )\n        else:\n            return Response(\n                response=serialize(\n                    DocumentNotFoundError(self.collection.__name__, _id)\n                ),\n                status=404\n            )", "label": 1}
{"index": "gp165685", "code": "def remove_annotations(self, remove_sequence):\n        return self._apply_to_annotations(lambda alist: tuple(oa for oa in alist if oa not in remove_sequence))", "contrast": "def remove_annotations(ast, remove_sequence):\n    new_ast = ast.clone()\n    for node in ast.walk():\n        annotations = getattr(node, 'annotations', [])\n        annotations = [a for a in annotations if a not in remove_sequence]\n        setattr(node, 'annotations', annotations)\n    return new_ast", "label": 0}
{"index": "gp171172", "code": "import numpy as np\ndef next_non_masked(a, idx):\n    masked = np.ma.masked_array(a)\n    if masked.mask[idx]:\n        for i in range(idx+1, len(a)):\n            if not masked.mask[i]:\n                return i, masked[i]\n    else:\n        return idx, masked[idx]\n    return None, None ", "contrast": "def _next_non_masked_element(a, idx):\n    try:\n        next_idx = idx + a[idx:].mask.argmin()\n        if ma.is_masked(a[next_idx]):\n            return None, None\n        else:\n            return next_idx, a[next_idx]\n    except (AttributeError, TypeError, IndexError):\n        return idx, a[idx]", "label": 1}
{"index": "gp218201", "code": "def get_latitude_bounds():\n    try:\n        lat_bounds = \n        return lat_bounds\n    except:\n        raise ValueError(\"No 'lat' axis can be found.\")", "contrast": "def lat_bounds(self):\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thislat = dom.axes['lat'].bounds\n                except:\n                    pass\n            return thislat\n        except:\n            raise ValueError('Can\\'t resolve a lat axis.')", "label": 1}
{"index": "gp251244", "code": "import numpy as np\nimport SimpleITK as sitk\ndef write_numpy_to_disk(array: np.ndarray, id: int) -> str:\n    itk_image = sitk.GetImageFromArray(array)\n    image_size = itk_image.GetSize()\n    image_spacing = itk_image.GetSpacing()\n    image_origin = itk_image.GetOrigin()\n    image_direction = itk_image.GetDirection()\n    image_pixeltype = sitk.sitkFloat32\n    mhd_path = f'image_{id}.mhd'\n    sitk.WriteImage(itk_image, mhd_path)\n    raw_path = f'image_{id}.raw'\n    with open(raw_path, 'wb') as file:\n        file.write(array.tobytes())\n    return mhd_path", "contrast": "def _write_image_data(im, id):\n    im = im* (1.0/3000)\n    lines = [   \"ObjectType = Image\",\n                \"NDims = <ndim>\",\n                \"BinaryData = True\",\n                \"BinaryDataByteOrderMSB = False\",\n                \"CompressedData = False\",\n                \"Offset = <origin>\",\n                \"CenterOfRotation = <centrot>\",\n                \"ElementSpacing = <sampling>\",\n                \"DimSize = <shape>\",\n                \"ElementType = <dtype>\",\n                \"ElementDataFile = <fname>\",\n                \"\" ]\n    text = '\\n'.join(lines)\n    tempdir = get_tempdir()\n    fname_raw_ = 'im%i.raw' % id\n    fname_raw = os.path.join(tempdir, fname_raw_)\n    fname_mhd = os.path.join(tempdir, 'im%i.mhd' % id)\n    shape = im.shape\n    if hasattr(im, 'sampling'): sampling = im.sampling\n    else: sampling = [1 for s in im.shape]\n    if hasattr(im, 'origin'): origin = im.origin\n    else: origin = [0 for s in im.shape]\n    shape = ' '.join([str(s) for s in reversed(shape)])\n    sampling = ' '.join([str(s) for s in reversed(sampling)])\n    origin = ' '.join([str(s) for s in reversed(origin)])\n    dtype_itk = DTYPE_NP2ITK.get(im.dtype.name, None)\n    if dtype_itk is None:\n        raise ValueError('Cannot convert data of this type: '+ str(im.dtype))\n    text = text.replace('<fname>', fname_raw_)\n    text = text.replace('<ndim>', str(im.ndim))\n    text = text.replace('<shape>', shape)\n    text = text.replace('<sampling>', sampling)\n    text = text.replace('<origin>', origin)\n    text = text.replace('<dtype>', dtype_itk)\n    text = text.replace('<centrot>', ' '.join(['0' for s in im.shape]))\n    if im.ndim==2:\n        text = text.replace('<transmatrix>', '1 0 0 1')\n    elif im.ndim==3:\n        text = text.replace('<transmatrix>', '1 0 0 0 1 0 0 0 1')\n    elif im.ndim==4:\n        pass \n    f = open(fname_raw, 'wb')\n    try:\n        f.write(im.data)\n    finally:\n        f.close()\n    f = open(fname_mhd, 'wb')\n    try:\n        f.write(text.encode('utf-8'))\n    finally:\n        f.close()\n    return fname_mhd", "label": 1}
{"index": "gp155627", "code": "def get_all_names( self, offset=None, count=None, include_expired=False ):\n        if offset is not None and offset < 0:\n            offset = None\n        if count is not None and count < 0:\n            count = None \n        cur = self.db.cursor()\n        names = namedb_get_all_names( cur, self.lastblock, offset=offset, count=count, include_expired=include_expired )\n        return names", "contrast": "def get_registered_names(page=None, page_size=None):\n    registered_names = ['John', 'Doe', 'Anna', 'James', 'Mary', 'William', 'Emma', 'Oliver']\n    if page is not None and page_size is not None:\n        start_index = (page - 1) * page_size\n        end_index = start_index + page_size\n        registered_names = registered_names[start_index:end_index]\n    return registered_names", "label": 0}
{"index": "gp230958", "code": "def add_reduce_slice(start: int, end: int, options: dict):\n    options['reduce'] = {\n        'language': 'javascript',\n        'source': 'Riak.reduceSlice',\n        'keep': False,\n        'arg': [start, end]\n    }", "contrast": "def reduce_slice(self, start, end, options=None):\n        if options is None:\n            options = dict()\n        options['arg'] = [start, end]\n        return self.reduce(\"Riak.reduceSlice\", options=options)", "label": 1}
{"index": "gp124671", "code": "def show_data(self, item):\n        child, cookie = self.mainview_tree.GetFirstChild(item)\n        child_list = []\n        while child.IsOk():\n            child_list.append(child)\n            child, cookie = self.mainview_tree.GetNextChild(item, cookie)\n        lc = self.nodeview_lc\n        lc.DeleteAllItems()\n        for i, child in enumerate(child_list):\n            text = self.mainview_tree.GetItemText(child)\n            try:\n                k, v = [s.strip() for s in text.split(':')]\n            except ValueError:\n                k, v = text, '...'\n            idx = lc.InsertItem(MAXNROW, v)\n            lc.SetItem(idx, 1, k)", "contrast": "import wx\ndef show_data_in_list_ctrl_for_tree_item(tree_item, data):\n    list_ctrl = wx.ListCtrl(tree_item.GetTopLevelParent(), style=wx.LC_REPORT)\n    list_ctrl.InsertColumn(0, 'Key')\n    list_ctrl.InsertColumn(1, 'Value')\n    for key, value in data.items():\n        index = list_ctrl.InsertItem(list_ctrl.GetItemCount(), key)\n        list_ctrl.SetItem(index, 1, str(value))\n    tree_item.SetData(list_ctrl)", "label": 0}
{"index": "gp194784", "code": "def parameter_decorator(func):\n    def wrapper(param):\n        return func(f\"--{param}\")\n    return wrapper", "contrast": "def store_parameter(parameter):\n    def decorator(fun):\n        KeywordArgumentParser._parameter_dict[parameter] = fun\n        return fun\n    return decorator", "label": 1}
{"index": "gp078434", "code": "def scan_django_settings(values, imports):\n    if isinstance(values, (str, bytes)):\n        if utils.is_import_str(values):\n            imports.add(values)\n    elif isinstance(values, dict):\n        for k, v in values.items():\n            scan_django_settings(k, imports)\n            scan_django_settings(v, imports)\n    elif hasattr(values, '__file__') and getattr(values, '__file__'):\n        imp, _ = utils.import_path_from_file(getattr(values, '__file__'))\n        imports.add(imp)\n    elif hasattr(values, '__iter__'):\n        for item in values:\n            scan_django_settings(item, imports)", "contrast": "import inspect\nfrom django.conf import settings\ndef scan_settings_for_imported_modules(django_settings=settings, modules=None):\n    if modules is None:\n        modules = set()\n    for key, value in django_settings.__dict__.items():\n        if isinstance(value, str) and value.startswith(('django.', 'app.', 'local.')):\n            try:\n                module = __import__(value, fromlist=['dummy'])\n                modules.add(module)\n            except ImportError:\n                pass\n        elif inspect.ismodule(value) and value.__name__.startswith(('django.', 'app.', 'local.')):\n            modules.add(value)\n            scan_settings_for_imported_modules(value, modules)\n    return modules", "label": 0}
{"index": "gp144387", "code": "def resample_sig(x, fs, fs_target):\n    t = np.arange(x.shape[0]).astype('float64')\n    if fs == fs_target:\n        return x, t\n    new_length = int(x.shape[0]*fs_target/fs)\n    resampled_x, resampled_t = signal.resample(x, num=new_length, t=t)\n    assert resampled_x.shape == resampled_t.shape and resampled_x.shape[0] == new_length\n    assert np.all(np.diff(resampled_t) > 0)\n    return resampled_x, resampled_t", "contrast": "import numpy as np\nfrom scipy.interpolate import interp1d\ndef resample_signal(x, fs, fs_target):\n    t = np.arange(0, len(x)/fs, 1/fs)\n    f = interp1d(t, x)\n    t_new = np.arange(0, len(x)/fs, 1/fs_target)\n    return f(t_new), t_new", "label": 0}
{"index": "gp257723", "code": "def find_closing_tag_pos(tag_options, tokens, current_pos):\n    stack = []\n    consume = False\n    for i in range(current_pos, len(tokens)):\n        token = tokens[i]\n        if tag_options.get(token):\n            stack.append(token)\n        elif token == '\\n':\n            if not stack:\n                return (i, True)\n        elif tag_options.get('~'+token):\n            if not stack:\n                return (i, False)\n            elif tag_options['~'+token] == stack[-1]:\n                stack.pop()\n    return (len(tokens), consume)", "contrast": "def _find_closing_token(self, tag, tokens, pos):\n        embed_count = 0\n        block_count = 0\n        lt = len(tokens)\n        while pos < lt:\n            token_type, tag_name, tag_opts, token_text = tokens[pos]\n            if token_type == self.TOKEN_DATA:\n                pos += 1\n                continue\n            if tag.newline_closes and token_type in (self.TOKEN_TAG_START, self.TOKEN_TAG_END):\n                inner_tag = self.recognized_tags[tag_name][1]\n                if not inner_tag.transform_newlines:\n                    if token_type == self.TOKEN_TAG_START:\n                        block_count += 1\n                    else:\n                        block_count -= 1\n            if token_type == self.TOKEN_NEWLINE and tag.newline_closes and block_count == 0:\n                return pos, True\n            elif token_type == self.TOKEN_TAG_START and tag_name == tag.tag_name:\n                if tag.same_tag_closes:\n                    return pos, False\n                if tag.render_embedded:\n                    embed_count += 1\n            elif token_type == self.TOKEN_TAG_END and tag_name == tag.tag_name:\n                if embed_count > 0:\n                    embed_count -= 1\n                else:\n                    return pos, True\n            pos += 1\n        return pos, True", "label": 1}
{"index": "gp145694", "code": "def send_metrics_to_cloudwatch(self, rule, metric, dimensions):\n        timestamp = datetime.datetime.utcfromtimestamp(metric.timestamp)\n        self.log.debug(\n            \"CloudWatch: Attempting to publish metric: %s to %s \"\n            \"with value (%s) for dimensions %s @%s\",\n            rule['name'],\n            rule['namespace'],\n            str(metric.value),\n            str(dimensions),\n            str(metric.timestamp)\n        )\n        try:\n            self.connection.put_metric_data(\n                str(rule['namespace']),\n                str(rule['name']),\n                str(metric.value),\n                timestamp, str(rule['unit']),\n                dimensions)\n            self.log.debug(\n                \"CloudWatch: Successfully published metric: %s to\"\n                \" %s with value (%s) for dimensions %s\",\n                rule['name'],\n                rule['namespace'],\n                str(metric.value),\n                str(dimensions))\n        except AttributeError as e:\n            self.log.error(\n                \"CloudWatch: Failed publishing - %s \", str(e))\n        except Exception as e:  \n            self.log.error(\n                \"CloudWatch: Failed publishing - %s\\n%s \",\n                str(e),\n                str(sys.exc_info()[0]))\n            self._bind()", "contrast": "import boto3\ndef send_metrics_to_cloudwatch(dimensions, metric_name, value, namespace):\n    cloudwatch = boto3.client('cloudwatch')\n    cloudwatch.put_metric_data(\n        Namespace=namespace,\n        MetricData=[\n            {\n                'MetricName': metric_name,\n                'Dimensions': dimensions,\n                'Value': value\n            }\n        ]\n    )", "label": 0}
{"index": "gp230041", "code": "import hashlib\ndef verify_checksum(file_path, checksum):\n    with open(file_path, 'rb') as f:\n        file_checksum = hashlib.md5(f.read()).hexdigest()\n    if file_checksum == checksum:\n        return True\n    else:\n        return False", "contrast": "def compare_checksums(self):\n        is_match = True\n        reference_checksums = self.parse_checksum_file(\n            self.files['checksum']['file'])\n        for md5, file in reference_checksums.items():\n            if os.path.isfile('/'.join((self.rawdir, file))):\n                if self.get_file_md5(self.rawdir, file) != md5:\n                    is_match = False\n                    LOG.warning('%s was not downloaded completely', file)\n                    return is_match\n        return is_match", "label": 1}
{"index": "gp156704", "code": "def get_route(self, name):\n        for route in self.routes:\n            if route.name == name:\n                return route\n        for child in self.routes:\n            route = child.get_route(name)\n            if route:\n                return route", "contrast": "def get_child_router_by_name(name):\n    if self.name == name:\n        return self\n    for child in self.children:\n        router = child.get_child_router_by_name(name)\n        if router is not None:\n            return router\n    return None", "label": 0}
{"index": "gp057301", "code": "def get_contents(self, path):\n        try:\n            if not os.path.exists(path):\n                raise ConfigurationError('specified path does not exist %s' % path)\n            with open(path) as f:\n                data = f.read()\n            return data\n        except (IOError, OSError) as exc:\n            raise ConfigurationError('error trying to load file contents: %s' % exc)", "contrast": "from typing import Union\nclass ConfigurationError(Exception):\n    pass\ndef load_file_contents(path: str) -> str:\n    try:\n        with open(path, \"r\") as file:\n            contents = file.read()\n            return contents\n    except FileNotFoundError:\n        raise ConfigurationError(f\"File {path} not found\")", "label": 0}
{"index": "gp187352", "code": "def remove_top_container(container_stack):\n    container_stack.pop()\n    return container_stack", "contrast": "def pop(self):\n        if not self._containers:\n            raise KittyException('no container to pop')\n        self._containers.pop()\n        if self._container():\n            self._container().pop()", "label": 1}
{"index": "gp036936", "code": "def delete_key_pair(name, profile, **libcloud_kwargs):\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    key = conn.get_key_pair(name)\n    return conn.delete_key_pair(key, **libcloud_kwargs)", "contrast": "def delete_key_pair(name, profile=None, libcloud_kwargs=None):\n    driver = get_driver(Provider.EC2, profile=profile)\n    return driver.delete_key_pair(name, **(libcloud_kwargs or {}))", "label": 0}
{"index": "gp059253", "code": "def set_attitude(roll_angle = 0.0, pitch_angle = 0.0,\n                 yaw_angle = None, yaw_rate = 0.0, use_yaw_rate = False,\n                 thrust = 0.5, duration = 0):\n    send_attitude_target(roll_angle, pitch_angle,\n                         yaw_angle, yaw_rate, False,\n                         thrust)\n    start = time.time()\n    while time.time() - start < duration:\n        send_attitude_target(roll_angle, pitch_angle,\n                             yaw_angle, yaw_rate, False,\n                             thrust)\n        time.sleep(0.1)\n    send_attitude_target(0, 0,\n                         0, 0, True,\n                         thrust)", "contrast": "def send_attitude_target(time_boot_ms, target_system, target_component, \n                          type_mask, q, body_roll_rate, body_pitch_rate, body_yaw_rate, \n                          thrust, target_id):\n    for i in range(5):\n        q_bytes = struct.pack('<4f', q[0], q[1], q[2], q[3])\n        message = struct.pack('<Q3BIB3fI', time_boot_ms, target_system, target_component,\n                              type_mask, q_bytes, body_roll_rate, body_pitch_rate,\n                              body_yaw_rate, thrust, target_id)\n        send_message(message, target_system, target_component)\n        time.sleep(0.05)", "label": 0}
{"index": "gp321555", "code": "import tensorflow as tf\nclass MyClass:\n    def __init__(self):\n        self.scaffold = None\n    def create_scaffold(self):\n        self.scaffold = tf.train.Scaffold()", "contrast": "def setup_scaffold(self):\n        if self.execution_type == \"single\":\n            global_variables = self.get_variables(include_submodules=True, include_nontrainable=True)\n            init_op = tf.variables_initializer(var_list=global_variables)\n            if self.summarizer_init_op is not None:\n                init_op = tf.group(init_op, self.summarizer_init_op)\n            if self.graph_summary is None:\n                ready_op = tf.report_uninitialized_variables(var_list=global_variables)\n                ready_for_local_init_op = None\n                local_init_op = None\n            else:\n                ready_op = None\n                ready_for_local_init_op = tf.report_uninitialized_variables(var_list=global_variables)\n                local_init_op = self.graph_summary\n        else:\n            global_variables = self.global_model.get_variables(include_submodules=True, include_nontrainable=True)\n            local_variables = self.get_variables(include_submodules=True, include_nontrainable=True)\n            init_op = tf.variables_initializer(var_list=global_variables)\n            if self.summarizer_init_op is not None:\n                init_op = tf.group(init_op, self.summarizer_init_op)\n            ready_op = tf.report_uninitialized_variables(var_list=(global_variables + local_variables))\n            ready_for_local_init_op = tf.report_uninitialized_variables(var_list=global_variables)\n            if self.graph_summary is None:\n                local_init_op = tf.group(\n                    tf.variables_initializer(var_list=local_variables),\n                    *(tf.assign(ref=local_var, value=global_var) for local_var, global_var in zip(\n                        self.get_variables(include_submodules=True),\n                        self.global_model.get_variables(include_submodules=True)\n                    ))\n                )\n            else:\n                local_init_op = tf.group(\n                    tf.variables_initializer(var_list=local_variables),\n                    self.graph_summary,\n                    *(tf.assign(ref=local_var, value=global_var) for local_var, global_var in zip(\n                        self.get_variables(include_submodules=True),\n                        self.global_model.get_variables(include_submodules=True)\n                    ))\n                )\n        def init_fn(scaffold, session):\n            if self.saver_spec is not None and self.saver_spec.get('load', True):\n                directory = self.saver_spec['directory']\n                file = self.saver_spec.get('file')\n                if file is None:\n                    file = tf.train.latest_checkpoint(\n                        checkpoint_dir=directory,\n                        latest_filename=None  \n                    )\n                elif not os.path.isfile(file):\n                    file = os.path.join(directory, file)\n                if file is not None:\n                    try:\n                        scaffold.saver.restore(sess=session, save_path=file)\n                        session.run(fetches=self.list_buffer_index_reset_op)\n                    except tf.errors.NotFoundError:\n                        raise TensorForceError(\"Error: Existing checkpoint could not be loaded! Set \\\"load\\\" to false in saver_spec.\")\n        self.scaffold = tf.train.Scaffold(\n            init_op=init_op,\n            init_feed_dict=None,\n            init_fn=init_fn,\n            ready_op=ready_op,\n            ready_for_local_init_op=ready_for_local_init_op,\n            local_init_op=local_init_op,\n            summary_op=None,\n            saver=self.saver,\n            copy_from_scaffold=None\n        )", "label": 1}
{"index": "gp049394", "code": "def ExtendAnomalies(self, other):\n    for o in other:\n      if o is not None:\n        self.anomaly.Extend(list(o.anomaly))", "contrast": "def merge_anomalies(check_result, anomalies):\n    check_result.anomalies += anomalies", "label": 0}
{"index": "gp253057", "code": "def declared(annotationtype, set):\n    return annotationtype.declared(set)", "contrast": "def declared(self, annotationtype, set):\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\n        return ( (annotationtype,set) in self.annotations) or (set in self.alias_set and self.alias_set[set] and (annotationtype, self.alias_set[set]) in self.annotations )", "label": 1}
{"index": "gp318027", "code": "def update_properties(appstruct, exclude_keys=None, include_keys=None):\n    model_keys = set(dir(Model))\n    if exclude_keys:\n        exclude_keys = set(exclude_keys)\n        model_keys = model_keys - exclude_keys\n    if include_keys:\n        include_keys = set(include_keys)\n        model_keys = model_keys & include_keys\n    for key in appstruct.keys():\n        if key in model_keys:\n            setattr(self, key, appstruct[key])", "contrast": "def populate_obj(self, appstruct, exclude_keys=None, include_keys=None):\n        exclude_keys_list = exclude_keys or []\n        include_keys_list = include_keys or []\n        for k in self._get_keys():\n            if (\n                k in appstruct\n                and k not in exclude_keys_list\n                and (k in include_keys_list or not include_keys)\n            ):\n                setattr(self, k, appstruct[k])", "label": 1}
{"index": "gp239526", "code": "def store_feature(self, k, v):\n    if k not in self.feats:\n        self.feats[k] = v\n    else:\n        if isinstance(self.feats[k], list):\n            self.feats[k].append(v)\n        else:\n            self.feats[k] = [self.feats[k], v]", "contrast": "def feat(self,k,v):\n  if (not hasattr(self,'feats')):\n\t\t\tself.feats = {}\n   if (not hasattr(self,'featpaths')):\n\t\t\t\tself.featpaths={}\n  if (not k in self.feats):\n\t\t\tself.feats[k] = v\n  else:\n\t\t\tif type(self.feats[k])==type([]):\n\t\t\t\tself.feats[k].append(v)\n   else:\n\t\t\t\tobj=self.feats[k]\n    self.feats[k]=[obj,v]", "label": 1}
{"index": "gp086391", "code": "def formatBodyNode(root,path):\n    body        = root\n    body.name   = \"body\"\n    body.weight = calcFnWeight(body)\n    body.path   = path\n    body.pclass = None\n    return body", "contrast": "def format_root_node(root_node):\n    return root_node", "label": 0}
{"index": "gp128873", "code": "def _get_report(self, with_line_nums=True):\n  templ = '{} \u2190 {}' if with_line_nums else '{}'\n  return '\\n'.join([\n   templ.format(error.string, ','.join(map(str, sorted(set(lines)))))\n   for error, lines in self.errors.items()])", "contrast": "def get_error_report(errors, flag=True):\n    error_report = []\n    unique_errors = set(errors)\n    for error in unique_errors:\n        error_lines = [i+1 for i, x in enumerate(errors) if x == error]\n        if flag:\n            error_report.append((error, error_lines))\n        else:\n            error_report.append(error)\n    return error_report", "label": 0}
{"index": "gp241439", "code": "def merge_identities(lists):\n    merged = []\n    for lst in lists:\n        for identity in lst:\n            if identity not in merged:\n                merged.append(identity)\n    return merged", "contrast": "def __merge(self, matched, interactive):\n        for m in matched:\n            identities = m['identities']\n            uuid = identities[0]\n            try:\n                for c in identities[1:]:\n                    if self.__merge_unique_identities(c, uuid, interactive):\n                        self.matched += 1\n                        if interactive:\n                            uuid = api.unique_identities(self.db, uuid=uuid)[0]\n            except Exception as e:\n                if self.recovery:\n                    self.recovery_file.save_matches(matched)\n                raise e\n            m['processed'] = True", "label": 1}
{"index": "gp135071", "code": "def get_dump_names(self, names, dumps=None):\n        if dumps is None:\n            dumps = set([])\n        for item in names:\n            if item not in self:\n                if not self.silent_key_error:\n                    raise KeyError(\"Dump name '{0}' is unknowed\".format(item))\n                else:\n                    continue\n            dumps.add(item)\n            deps = self.__getitem__(item).get('dependancies', [])\n            dumps.update(deps)\n        if names == dumps:\n            return dumps\n        return self.get_dump_names(dumps.copy(), dumps)", "contrast": "from collections import OrderedDict\ndef find_required_dump_names(dump_names_list, dump_name_dict):\n    required_dump_names = set()\n    for dump_name in dump_names_list:\n        if dump_name in dump_name_dict:\n            required_dump_names.update(dump_name_dict[dump_name])\n    return list(required_dump_names)", "label": 0}
{"index": "gp217046", "code": "import importlib.util\ndef run_arbitrary_code_as_module(code):\n    spec = importlib.util.spec_from_loader(\"custom_module\", loader=None)\n    custom_module = importlib.util.module_from_spec(spec)\n    exec(code, custom_module.__dict__)\n    return custom_module", "contrast": "def import_string_code_as_module(code):\n    sha256 = hashlib.sha256(code.encode('UTF-8')).hexdigest()\n    module = imp.new_module(sha256)\n    try:\n        exec_(code, module.__dict__)\n    except Exception as e:\n        raise exceptions.UserError('User code exception', exception_message=str(e))\n    sys.modules[sha256] = module\n    return module", "label": 1}
{"index": "gp332666", "code": "import json\nimport requests\ndef send_json_request(url, json_data):\n    headers = {'Content-type': 'application/json'}\n    response = requests.post(url, data=json.dumps(json_data), headers=headers)\n    if response.ok:\n        return True\n    else:\n        return response.status_code", "contrast": "def _pybossa_req(method, domain, id=None, payload=None, params={},\n                 headers={'content-type': 'application/json'},\n                 files=None):\n    url = _opts['endpoint'] + '/api/' + domain\n    if id is not None:\n        url += '/' + str(id)\n    if 'api_key' in _opts:\n        params['api_key'] = _opts['api_key']\n    if method == 'get':\n        r = requests.get(url, params=params)\n    elif method == 'post':\n        if files is None and headers['content-type'] == 'application/json':\n            r = requests.post(url, params=params, headers=headers,\n                              data=json.dumps(payload))\n        else:\n            r = requests.post(url, params=params, files=files, data=payload)\n    elif method == 'put':\n        r = requests.put(url, params=params, headers=headers,\n                         data=json.dumps(payload))\n    elif method == 'delete':\n        r = requests.delete(url, params=params, headers=headers,\n                            data=json.dumps(payload))\n    if r.status_code  \n        if r.text and r.text != '\"\"':\n            return json.loads(r.text)\n        else:\n            return True\n    else:\n        return json.loads(r.text)", "label": 1}
{"index": "gp141750", "code": "def Client(api_version, *args, **kwargs):\n    neutron_client = utils.get_client_class(\n        API_NAME,\n        api_version,\n        API_VERSIONS,\n    )\n    return neutron_client(*args, **kwargs)", "contrast": "from neutronclient.v2_0 import client as neutron_client\ndef get_neutron_client(api_version='2.0'):\n    return neutron_client.Client(api_version=api_version)", "label": 0}
{"index": "gp224039", "code": "import requests\ndef add_attachment(oid, file_path):\n    url = f'https://myfeaturelayer/0/addAttachment?&objectid={oid}&rollbackOnFailure=true'\n    files = {'file': open(file_path, 'rb')}\n    response = requests.post(url, files=files)\n    return response.json()", "contrast": "def addAttachment(self, oid, file_path):\n        if self.hasAttachments == True:\n            attachURL = self._url + \"/%s/addAttachment\" % oid\n            params = {'f':'json'}\n            parsed = urlparse.urlparse(attachURL)\n            files = {'attachment': file_path}\n            res = self._post(url=attachURL,\n                             param_dict=params,\n                             files=files,\n                             securityHandler=self._securityHandler,\n                             proxy_port=self._proxy_port,\n                             proxy_url=self._proxy_url)\n            return self._unicode_convert(res)\n        else:\n            return \"Attachments are not supported for this feature service.\"", "label": 1}
{"index": "gp215690", "code": "def parse_infobel():\n    return i3visio_list", "contrast": "def extractFieldsFromResult(data):\n    entities = []\n    fieldsRegExp = {}\n    fieldsRegExp[\"i3visio.fullname\"] = \"<span class=\\\"fn\\\">([^<]*)</span>\"\n    fieldsRegExp[\"i3visio.name\"] = \"    por <strong>[^ ]* ([^<]*)</strong>\"        \n    fieldsRegExp[\"i3visio.surname\"] = \"    por <strong>([^ ]*) \"     \n    fieldsRegExp[\"i3visio.location.address\"] = \"itemprop=\\\"streetAddress\\\">([^<]*)</span>\"\n    fieldsRegExp[\"i3visio.location.city\"] = \"addressLocality\\\">([^<]*)</span>\"\n    fieldsRegExp[\"i3visio.location.postalcode\"] = \"postalCode\\\">([^<]*)</span>\"    \n    fieldsRegExp[\"i3visio.phone\"] = \"document.write\\('([0-9]+)'\"        \n    for field in fieldsRegExp.keys():\n        listRecovered = re.findall(fieldsRegExp[field], data)\n        if len(listRecovered) >0:\n            aux = {}\n            aux[\"type\"]= field\n            aux[\"value\"] = listRecovered[0].replace('\\xa0', ' ')\n            aux[\"attributes\"] = []\n            entities.append(aux)\n    return entities", "label": 1}
{"index": "gp245282", "code": "def add_channel():\n    return \"https://example.com/uploadedchannel\"", "contrast": "def add_channel(self):\n        config.LOGGER.info(\"   Creating channel {0}\".format(self.channel.title))\n        payload = {\n            \"channel_data\":self.channel.to_dict(),\n        }\n        response = config.SESSION.post(config.create_channel_url(), data=json.dumps(payload))\n        response.raise_for_status()\n        new_channel = json.loads(response._content.decode(\"utf-8\"))\n        return new_channel['root'], new_channel['channel_id']", "label": 1}
{"index": "gp295790", "code": "from queue import Queue\ndef read_and_put(lines):\n    queue = Queue()\n    for line in lines:\n        queue.put(line)\n    return queue", "contrast": "def run(self) -> None:\n        fd = self._fd\n        encoding = self._encoding\n        line_terminators = self._line_terminators\n        queue = self._queue\n        buf = \"\"\n        while True:\n            try:\n                c = fd.read(1).decode(encoding)\n            except UnicodeDecodeError as e:\n                log.warning(\"Decoding error from {!r}: {!r}\", self._cmdargs, e)\n                if self._suppress_decoding_errors:\n                    continue\n                else:\n                    raise\n            if not c:\n                return\n            buf += c\n            for t in line_terminators:\n                try:\n                    t_idx = buf.index(t) + len(t)  \n                    fragment = buf[:t_idx]\n                    buf = buf[t_idx:]\n                    queue.put(fragment)\n                except ValueError:\n                    pass", "label": 1}
{"index": "gp088117", "code": "def feature_extraction(self, algorithms):\n        assert type(algorithms) is list\n        features = []\n        for algorithm in algorithms:\n            new_features = algorithm(self)\n            assert len(new_features) == algorithm.get_dimension(),                \"Expected %i features from algorithm %s, got %i features\" %                (algorithm.get_dimension(), str(algorithm), len(new_features))\n            features += new_features\n        return features", "contrast": "def get_features():\n    features = ['feature1', 'feature2', 'feature3']\n    return features", "label": 0}
{"index": "gp287423", "code": "from dataclasses import dataclass\n@dataclass\nclass CharSetCodeField:\n    name: str\ndef create_char_set_code_field(name: str) -> CharSetCodeField:\n    return CharSetCodeField(name)", "contrast": "def char_code(columns, name=None):\n    if name is None:\n        name = 'Char Code Field (' + str(columns) + ' columns)'\n    if columns <= 0:\n        raise BaseException()\n    char_sets = None\n    for char_set in _tables.get_data('character_set'):\n        regex = '[ ]{' + str(15 - len(char_set)) + '}' + char_set\n        if char_sets is None:\n            char_sets = regex\n        else:\n            char_sets += '|' + regex\n    _character_sets = pp.Regex(char_sets)\n    _unicode_1_16b = pp.Regex('U\\+0[0-8,A-F]{3}[ ]{' + str(columns - 6) + '}')\n    _unicode_2_21b = pp.Regex('U\\+0[0-8,A-F]{4}[ ]{' + str(columns - 7) + '}')\n    char_code_field = (_character_sets | _unicode_1_16b | _unicode_2_21b)\n    char_code_field = char_code_field.setParseAction(lambda s: s[0].strip())\n    char_code_field.setName(name)\n    return char_code_field", "label": 1}
{"index": "gp313450", "code": "def copy_collection(redis_conn, collection, key=None):\n    if key:\n        redis_conn.rename(collection, key)\n        collection = key\n    redis_conn.duplicate(collection)\n    return redis_conn", "contrast": "def copy(self, key=None):\n        other = self.__class__(\n            self.__iter__(),\n            self.maxlen,\n            redis=self.redis,\n            key=key,\n            writeback=self.writeback,\n        )\n        return other", "label": 1}
{"index": "gp258756", "code": "def setup_visualization():\n    plt.rcParams['figure.figsize'] = (10, 6)\n    plt.rcParams['font.size'] = 12\n    plt.rcParams['axes.labelsize'] = 14\n    plt.rcParams['axes.titlesize'] = 16\n    plt.rcParams['xtick.labelsize'] = 12\n    plt.rcParams['ytick.labelsize'] = 12\n    plt.rcParams['legend.fontsize'] = 12", "contrast": "def standard_settings(self):\n        cmd.set('bg_rgb', [1.0, 1.0, 1.0])  \n        cmd.set('depth_cue', 0)  \n        cmd.set('cartoon_side_chain_helper', 1)  \n        cmd.set('cartoon_fancy_helices', 1)  \n        cmd.set('transparency_mode', 1)  \n        cmd.set('dash_radius', 0.05)\n        self.set_custom_colorset()", "label": 1}
{"index": "gp306823", "code": "from pathlib import Path\ndef fix_path(path: str) -> Path:\n    path_obj = Path(path)\n    if path_obj.is_absolute() or path_obj.drive:\n        return path_obj\n    else:\n        return Path(\".\" + str(path))", "contrast": "def stringify_with_dot_if_path(x):\n    if isinstance(x, PurePath):\n        return os.path.join('.', str(x))\n    return x", "label": 1}
{"index": "gp154181", "code": "def global_data_dir():\n    prefix = False\n    if VIRTUALENV_VAR in os.environ:\n        prefix = os.environ[VIRTUALENV_VAR]\n    elif CONDA_VAR in os.environ:\n        prefix = sys.prefix\n    elif which('conda'):\n        prefix = conda_prefix()\n    if prefix:\n        return make_path_posix(os.path.join(prefix, 'share', 'intake'))\n    else:\n        return appdirs.site_data_dir(appname='intake', appauthor='intake')", "contrast": "import os\ndef get_intake_catalog_dir():\n    return os.environ.get('INTAKE_CATALOG_DIR', '')", "label": 0}
{"index": "gp300250", "code": "def extract_file_info(file_info):\n    try:\n        file_obj = file_info[\"file_obj\"]\n    except KeyError:\n        file_obj = None\n    try:\n        filename = file_info[\"filename\"]\n    except KeyError:\n        filename = None\n    try:\n        content_type = file_info[\"content_type\"]\n    except KeyError:\n        content_type = None\n    return file_obj, filename, content_type", "contrast": "def fileinfo(fileobj, filename=None, content_type=None, existing=None):\n        return _FileInfo(fileobj, filename, content_type).get_info(existing)", "label": 1}
{"index": "gp003571", "code": "def items(self):\n        return ((k, v) for k, v in zip(self.keys(), self.values()))", "contrast": "def get_items() -> Iterable[(Descriptor, value)]:\n    pass", "label": 0}
{"index": "gp284036", "code": "def add_command_with_delay(command, delay, sequence):\n    new_sequence = sequence.copy()\n    new_sequence.append((command, delay))\n    return new_sequence", "contrast": "def append (self, cmd, delay=0.000, attrs=None):\n    self.lines.append( SeqCmd(cmd, delay, attrs) )", "label": 1}
{"index": "gp279138", "code": "from datetime import datetime\nimport pytz\ndef set_audit_timezone(tz: str, audit_record: dict) -> dict:\n    audit_time_str = audit_record['audit_time']\n    audit_time = datetime.strptime(audit_time_str, '%Y-%m-%d %H:%M:%S.%f')\n    if tz in pytz.all_timezones:\n        timezone = pytz.timezone(tz)\n    else:\n        try:\n            timezone = pytz.timezone('Etc/GMT+' + str(int(tz)))\n        except ValueError:\n            raise ValueError('Invalid timezone format')\n    audit_time = timezone.localize(audit_time)\n    audit_record['audit_time'] = audit_time.strftime('%Y-%m-%d %H:%M:%S.%f %Z%z')\n    return audit_record", "contrast": "def timezone(self, tz):\n        self.data['resolving'].update(\n            timezone=tz,\n            time_show_zone=True)", "label": 1}
{"index": "gp097571", "code": "def get_pr_info(requester, reponame, number):\n    resp = requester.get(\n        'https://api.github.com/repos/%s/pulls/%s' % (reponame, number))\n    return PRInfo(resp.json())", "contrast": "def get_pull_request_info(pull_request):\n    pr_info = PRInfo(\n        pr_num=pull_request.number,\n        title=pull_request.title,\n        author_login=pull_request.user.login,\n        created_at=pull_request.created_at,\n        updated_at=pull_request.updated_at,\n        merged_at=pull_request.merged_at,\n        base_branch=pull_request.base.ref,\n        head_branch=pull_request.head.ref,\n        state=pull_request.state,\n        additions=pull_request.additions,\n        deletions=pull_request.deletions,\n        changed_files=pull_request.changed_files\n    )\n    return pr_info", "label": 0}
{"index": "gp027662", "code": "def get_runtime_config():\n    from .._connect import main as _glconnect\n    unity = _glconnect.get_unity()\n    return unity.list_globals(True)", "contrast": "import turicreate as tc\ndef get_runtime_config_options():\n    return tc.config.get_runtime_config_options()", "label": 0}
{"index": "gp153835", "code": "def _fetch_features(self):\n        if self.feature_offset is None:\n            return\n        main_url = '{}{}/{}?'.format(self.base_url, ServiceType.WFS.value, self.instance_id)\n        params = {'SERVICE': ServiceType.WFS.value,\n                  'REQUEST': 'GetFeature',\n                  'TYPENAMES': DataSource.get_wfs_typename(self.data_source),\n                  'BBOX': str(self.bbox.reverse()) if self.bbox.crs is CRS.WGS84 else str(self.bbox),\n                  'OUTPUTFORMAT': MimeType.get_string(MimeType.JSON),\n                  'SRSNAME': CRS.ogc_string(self.bbox.crs),\n                  'TIME': '{}/{}'.format(self.time_interval[0], self.time_interval[1]),\n                  'MAXCC': 100.0 * self.maxcc,\n                  'MAXFEATURES': SHConfig().max_wfs_records_per_query,\n                  'FEATURE_OFFSET': self.feature_offset}\n        url = main_url + urlencode(params)\n        LOGGER.debug(\"URL=%s\", url)\n        response = get_json(url)\n        is_sentinel1 = self.data_source.is_sentinel1()\n        for tile_info in response[\"features\"]:\n            if not is_sentinel1 or self._sentinel1_product_check(tile_info['properties']['id'], self.data_source):\n                self.tile_list.append(tile_info)\n        if len(response[\"features\"]) < SHConfig().max_wfs_records_per_query:\n            self.feature_offset = None\n        else:\n            self.feature_offset += SHConfig().max_wfs_records_per_query", "contrast": "def collect_data_from_wfs_service():\n    return {}", "label": 0}
{"index": "gp298365", "code": "def compute_activations(X, weights, bias):\n    z = np.dot(X, weights) + bias\n    activations = np.maximum(z, 0)\n    return activations", "contrast": "def _compute_hidden_activations(self, X):\n        self._compute_input_activations(X)\n        acts = self.input_activations_\n        if (callable(self.activation_func)):\n            args_dict = self.activation_args if (self.activation_args) else {}\n            X_new = self.activation_func(acts, **args_dict)\n        else:\n            func_name = self.activation_func\n            func = self._internal_activation_funcs[func_name]\n            X_new = func(acts, **self._extra_args)\n        return X_new", "label": 1}
{"index": "gp330911", "code": "def add_namespace_to_graph(manager_namespace, graph):\n    graph.add_namespace(manager_namespace)\n    return graph", "contrast": "def add_namespace_to_graph(self, graph: BELGraph) -> Namespace:\n        namespace = self.upload_bel_namespace()\n        graph.namespace_url[namespace.keyword] = namespace.url\n        self._add_annotation_to_graph(graph)\n        return namespace", "label": 1}
{"index": "gp333914", "code": "import xml.etree.ElementTree as ET\ndef read_block_as_xml(block):\n    root = ET.fromstring(block)\n    return root", "contrast": "def _readxml(self):\n        block = re.sub(r'<(/?)s>', r'&lt;\\1s&gt;', self._readblock())\n        try:\n            xml = XML(block)\n        except ParseError:\n            xml = None\n        return xml", "label": 1}
{"index": "gp162611", "code": "def launch_help(self, helpname, filename):\n        position = config[\"help_window_position\"]\n        size = config[\"help_window_size\"]\n        self.help_window = wx.Frame(self.main_window, -1,\n                                    helpname, position, size)\n        self.help_htmlwindow = wx.html.HtmlWindow(self.help_window, -1,\n                                                  (0, 0), size)\n        self.help_window.Bind(wx.EVT_MOVE, self.OnHelpMove)\n        self.help_window.Bind(wx.EVT_SIZE, self.OnHelpSize)\n        self.help_htmlwindow.Bind(wx.EVT_RIGHT_DOWN, self.OnHelpBack)\n        self.help_htmlwindow.Bind(wx.html.EVT_HTML_LINK_CLICKED,\n                                  lambda e: self.open_external_links(e))\n        self.help_htmlwindow.Bind(wx.EVT_MOUSEWHEEL,\n                                  lambda e: self.zoom_html(e))\n        current_path = os.getcwd()\n        os.chdir(get_help_path())\n        try:\n            if os.path.isfile(filename):\n                self.help_htmlwindow.LoadFile(filename)\n            else:\n                self.help_htmlwindow.LoadPage(filename)\n        except IOError:\n            self.help_htmlwindow.LoadPage(filename)\n        self.help_window.Show()\n        os.chdir(current_path)", "contrast": "import webbrowser\ndef launch_help(filename):\n    webbrowser.open(filename, new=2)", "label": 0}
{"index": "gp069561", "code": "def interleave_skip(iterables, limit=None):\n    iterators = map(iter, iterables)\n    while iterators:\n        for i, it in enumerate(iterators):\n            try:\n                yield next(it)\n            except StopIteration:\n                del iterators[i]", "contrast": "from itertools import chain, izip_longest\ndef interleave_skip(iterables, obj=None):\n    return chain.from_iterable(izip_longest(*iterables, fillvalue=obj))", "label": 0}
{"index": "gp209720", "code": "def create_filename(filename: str, filetype: str, resolution: int) -> str:\n    return f\"{filename}_{resolution}dpi.{filetype}\"", "contrast": "def build_filename(filename, filetype='png', resolution=300):\n    filevals = os.path.splitext(filename)\n    if filevals[1]:\n        filetype = filevals[1][1:]\n    if not filetype:\n        filetype = 'png'\n    filename = filevals[0] + '.' + filetype\n    if not resolution:\n        resolution = 300\n    return filename, filetype, resolution", "label": 1}
{"index": "gp179681", "code": "def show_view(view):\n    from com.dtmilano.android.viewclient import View\n    import time\n    class_name = view.getClass()\n    unique_id = view.getUniqueId()\n    content_description = view.getContentDescription()\n    text = view.getText()\n    id_value = view.getId()\n    if text:\n        view_info = f\"{class_name} | id: {id_value} | text: '{text}' | unique_id: {unique_id}\"\n    else:\n        view_info = f\"{class_name} | id: {id_value} | unique_id: {unique_id}\"\n    print(view_info)\n    screenshot_path = f\"view_screenshot_{unique_id}.png\"\n    view.writeImageToFile(screenshot_path, \"PNG\")\n    if content_description:\n        return f\"{view_info} | content description: '{content_description}'\"\n    else:\n        return view_info", "contrast": "def traverseShowClassIdTextContentDescriptionAndScreenshot(view):\n        return ViewClient.traverseShowClassIdAndText(view, View.getContentDescription, 'NAF', extraAction=ViewClient.writeViewImageToFileInDir)", "label": 1}
{"index": "gp177970", "code": "def create_new_token_instance(ttl):\n    return client.tokens.create(ttl=ttl)", "contrast": "def create(self, ttl=values.unset):\n        data = values.of({'Ttl': ttl, })\n        payload = self._version.create(\n            'POST',\n            self._uri,\n            data=data,\n        )\n        return TokenInstance(self._version, payload, account_sid=self._solution['account_sid'], )", "label": 1}
{"index": "gp310941", "code": "import ipaddress\ndef cidr2block(cidr: str) -> tuple:\n    try:\n        network = ipaddress.ip_network(cidr)\n        return str(network.network_address), str(network.broadcast_address)\n    except ValueError:\n        return None", "contrast": "def cidr2block(cidr):\n    if not validate_cidr(cidr):\n        return None\n    ip, prefix = cidr.split('/')\n    prefix = int(prefix)\n    ip = ip2long(ip)\n    shift = 128 - prefix\n    block_start = ip >> shift << shift\n    mask = (1 << shift) - 1\n    block_end = block_start | mask\n    return (long2ip(block_start), long2ip(block_end))", "label": 1}
{"index": "gp130970", "code": "def register_command(self, name, command):\n        if name in self.commands:\n            raise RuntimeError('%s is already defined' % name)\n        self.commands[name] = command", "contrast": "command_registry = {}\ndef register_command(name, command):\n    if name in command_registry:\n        raise RuntimeError(\"A command with this name has already been registered.\")\n    command_registry[name] = command", "label": 0}
{"index": "gp248806", "code": "from typing import Dict, Union\nimport numpy as np\ndef check_output_types(transition: callable) -> bool:\n    state = 0  \n    time = 0.0  \n    output = transition(state, time)\n    if isinstance(output, dict):\n        for key in output:\n            if not isinstance(key, (int, tuple)) or not isinstance(output[key], float):\n                return False\n        return True\n    elif isinstance(output, tuple) and len(output) == 2:\n        if isinstance(output[0], np.ndarray) and isinstance(output[1], np.ndarray):\n            if output[0].dtype == np.int and output[1].dtype == np.float64:\n                return True\n    return False", "contrast": "def checkTransitionType(self,state):\n        test = self.transition(state)\n        assert isinstance(test,(dict,tuple)), \"Transition function does not return a dict or tuple\"\n        if isinstance(test,dict):\n            assert all(isinstance(states, (int,tuple)) for states in test.keys()), \"Transition function returns a dict, but states are not represented as tuples or integers\"\n            assert all(isinstance(rates, float) for rates in test.values()), \"Transition function returns a dict, but the rates should be floats.\"\n            usesNumpy=False\n        if isinstance(test,tuple):\n            assert len(test)==2, \"The transition function should return two variables: states and rates.\"\n            states,rates = test\n            assert isinstance(states, np.ndarray) and states.ndim==2 and issubclass(states.dtype.type, np.integer), \"The states returned by the transition function need to be an integer 2d numpy array: %r\" %states\n            assert isinstance(rates, np.ndarray) and rates.ndim==1, \"The rates returned by the transition function need to be a 1d numpy array: %r\" % rates\n            usesNumpy = True\n        return usesNumpy", "label": 1}
{"index": "gp123391", "code": "def attrs(self):\n        ret = dict(self.__dict__) \n        del ret[\"_matches\"] \n        if self.type != c.COMPUTER: \n            del ret[\"difficulty\"]\n        return ret", "contrast": "def get_player_attributes(player):\n    return {\n        'name': player.name,\n        'age': player.age,\n        'height': player.height,\n        'weight': player.weight,\n        'team': player.team,\n        'position': player.position,\n        'number': player.number,\n        'stats': player.stats,\n    }", "label": 0}
{"index": "gp018331", "code": "def link_markdown_cells(cells, modules):\n    for i, cell in enumerate(cells):\n        if cell['cell_type'] == 'markdown':\n            cell['source'] = link_docstring(modules, cell['source'])", "contrast": "def create_doc_links(md_text):\n    import re\n    links_pattern = r'`([a-zA-Z0-9._/\\-]+)`'\n    md_text = re.sub(links_pattern, r'[`\\1`](https://docs.example.com/\\1)', md_text)\n    return md_text", "label": 0}
{"index": "gp261355", "code": "def subset_train_data(train, idx, keep_other=True):\n    x, y = train[0], train[1]\n    if isinstance(train[0], list) or isinstance(train[0], np.ndarray):\n        x_sub = x[idx]\n        y_sub = y[idx]\n    elif isinstance(train[0], tuple) or isinstance(train[0], np.ndarray):\n        x_sub = x[idx, :]\n        y_sub = y[idx]\n    elif isinstance(train[0], dict):\n        x_sub = {k: x[k][idx] for k in x.keys()}\n        y_sub = y[idx]\n    if keep_other:\n        return (x_sub, y_sub) + train[2:]\n    else:\n        return x_sub, y_sub", "contrast": "def subset(train, idx, keep_other=True):\n    test_len(train)\n    y = train[1][idx]\n    if isinstance(train[0], (list, tuple)):\n        x = [x[idx] for x in train[0]]\n    elif isinstance(train[0], dict):\n        x = {k: v[idx] for k, v in train[0].items()}\n    elif isinstance(train[0], np.ndarray):\n        x = train[0][idx]\n    else:\n        raise ValueError(\"Input can only be of type: list, dict or np.ndarray\")\n    if keep_other:\n        return (x, y) + train[2:]\n    else:\n        return (x, y)", "label": 1}
{"index": "gp233779", "code": "def set_condition(value: str):\n    condition = value\n    return condition", "contrast": "def set_condition(self, value):\n        if value is None or not isinstance(value, str):\n            raise TypeError(\"Condition is required and must be set to a String\")\n        else:\n            self.__condition = value", "label": 1}
{"index": "gp083206", "code": "async def create_connection(\n    host,\n    port,\n    *,\n    loop=None,\n    secure=True,\n    ssl_context=None,\n    **kwargs,\n):\n    loop = loop or asyncio.get_event_loop()\n    secure = True if port == 443 else secure\n    connection = HTTP2ClientConnection(host, loop=loop, secure=secure)\n    if not isinstance(ssl_context, SSLContext):\n        ssl_context = default_ssl_context()\n    await loop.create_connection(\n        lambda: connection,\n        host=host,\n        port=port,\n        ssl=ssl_context,\n    )\n    return connection", "contrast": "import http2\nfrom http.client import HTTPConnection\ndef open_http2_connection(host, port):\n    conn = HTTPConnection(host, port)\n    conn.sock = http2.HTTP2Connection(host, port)\n    return conn", "label": 0}
{"index": "gp032593", "code": "def get_linode(kwargs=None, call=None):\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The get_linode function must be called with -f or --function.'\n        )\n    if kwargs is None:\n        kwargs = {}\n    name = kwargs.get('name', None)\n    linode_id = kwargs.get('linode_id', None)\n    if name is None and linode_id is None:\n        raise SaltCloudSystemExit(\n            'The get_linode function requires either a \\'name\\' or a \\'linode_id\\'.'\n        )\n    if linode_id is None:\n        linode_id = get_linode_id_from_name(name)\n    result = _query('linode', 'list', args={'LinodeID': linode_id})\n    return result['DATA'][0]", "contrast": "def get_linode(name=None, linode_id=None):\n    import requests\n    base_url = 'https://api.linode.com/v4'\n    headers = {\n        'Authorization': 'Bearer <api_token>',\n        'Content-Type': 'application/json'\n    }\n    if name is not None:\n        url = base_url + '/linode/instances?label={}'.format(name)\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            linode_id = response.json()['data'][0]['id']\n        else:\n            print('Error: {}'.format(response.json()['errors'][0]['reason']))\n            return None\n    url = base_url + '/linode/instances/{}'.format(linode_id)\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print('Error: {}'.format(response.json()['errors'][0]['reason']))\n        return None", "label": 0}
{"index": "gp017027", "code": "def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        return self.sort_values(return_indexer=True, ascending=ascending)", "contrast": "def sort_index(ascending=True, level=None, sort_remaining=True):\n    return Index(sorted(self.values, reverse=(not ascending)))", "label": 0}
{"index": "gp208157", "code": "import sentry_sdk\ndef send_message_to_sentry(message):\n    sentry_sdk.capture_message(message)", "contrast": "def send_message(message, params, site, logger):\n    client.capture(\n        'Message',\n        message=message,\n        params=tuple(params),\n        data={\n            'site': site,\n            'logger': logger,\n        },\n    )", "label": 1}
{"index": "gp199602", "code": "def add_rule(value: str, tag: str) -> dict:\n    response = {\n        \"status\": \"success\",\n        \"message\": f\"Rule '{value}' with tag '{tag}' added successfully\"\n    }\n    return response", "contrast": "def add_rule(self, value, tag):\n        resp = requests.post(url=self.REQUEST_URL.format(**self._params),\n                             json={'rule': {'value': value, 'tag': tag}})\n        return resp.json()", "label": 1}
{"index": "gp262310", "code": "def describe_region(language_tag):\n    import pycountry\n    region_code = language_tag.split('-')[1].upper()\n    try:\n        region_name = pycountry.countries.get(alpha_2=region_code).name\n    except:\n        region_name = ''\n    return region_name", "contrast": "def region_name(self, language=DEFAULT_LANGUAGE, min_score: int=75) -> str:\n        return self._get_name('region', language, min_score)", "label": 1}
{"index": "gp134494", "code": "def stream_array(self, generator):\n        def chunkify(generator):\n            log.debug('Data Stream STARTED')\n            yield '['.encode()\n            try:\n                yield jsonify(next(generator)).encode()\n            except StopIteration:\n                pass\n            while True:\n                try:\n                    bit = next(generator)\n                except StopIteration:\n                    yield ']'.encode()\n                    break\n                else:\n                    yield ',\\n'.encode()\n                    yield jsonify(bit).encode()\n            log.debug('Data Stream ENDED')\n        return chunkify(generator)", "contrast": "import json\ndef stream_to_json_array(stream):\n    array = []\n    for data in stream:\n        json_data = json.loads(data)\n        array.append(json_data)\n    return array", "label": 0}
{"index": "gp005039", "code": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        local_stream = utils.BytearrayStream()\n        if self._unique_identifier is not None:\n            self._unique_identifier.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        else:\n            raise ValueError(\n                \"The Rekey response payload is missing the unique identifier.\"\n            )\n        if self._template_attribute is not None:\n            self._template_attribute.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        self.length = local_stream.length()\n        super(RekeyResponsePayload, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)", "contrast": "def encode_rekey_request_payload(output_stream, kmip_version=KMIPVersion.KMIP_1_0):\n    if not output_stream:\n        raise ValueError(\"Invalid output stream\")", "label": 0}
{"index": "gp107461", "code": "def backup_db(release=None, limit=5):\n    assert \"mysql_user\" in env, \"Missing mysqL_user in env\"\n    assert \"mysql_password\" in env, \"Missing mysql_password in env\"\n    assert \"mysql_host\" in env, \"Missing mysql_host in env\"\n    assert \"mysql_db\" in env, \"Missing mysql_db in env\"\n    if not release:\n        release = paths.get_current_release_name()\n    max_versions = limit+1\n    if not release:\n        return\n    env.run(\"mkdir -p %s\" % paths.get_backup_path(\"mysql\"))\n    backup_file = \"mysql/%s.sql.gz\" % release\n    backup_path = paths.get_backup_path(backup_file)\n    env.run(\"mysqldump -u %s -p%s -h %s %s | gzip -c > %s\" %\n            (env.mysql_user, env.mysql_password, env.mysql_host, env.mysql_db,\n             backup_path))\n    env.run(\"ls -dt %s/* | tail -n +%s | xargs rm -rf\" % (\n        paths.get_backup_path(\"mysql\"),\n        max_versions)\n    )", "contrast": "def backup_database_release(database, release):\n    backup_filename = f\"{database}_{release}.bak\"\n    print(f\"Successfully backed up {database} database and associated it with release {release}.\")", "label": 0}
{"index": "gp150686", "code": "def _find_inaccessible_hdd_files(self):\n        hdds = []\n        try:\n            properties = yield from self.execute(\"list\", [\"hdds\"])\n        except VirtualBoxError:\n            return hdds\n        flag_inaccessible = False\n        for prop in properties:\n            try:\n                name, value = prop.split(':', 1)\n            except ValueError:\n                continue\n            if name.strip() == \"State\" and value.strip() == \"inaccessible\":\n                flag_inaccessible = True\n            if flag_inaccessible and name.strip() == \"Location\":\n                hdds.append(value.strip())\n                flag_inaccessible = False\n        return reversed(hdds)", "contrast": "import os\ndef find_inaccessible_files(dir_path):\n    inaccessible_files = []\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                with open(file_path) as f:\n                    pass\n            except (IOError, OSError):\n                inaccessible_files.append(file_path)\n    return inaccessible_files", "label": 0}
{"index": "gp196620", "code": "import pulseaudio\ndef retrieve_sink_and_request_update():\n    with pulseaudio.Pulse('mypulseapp') as pulse:\n        sink = pulse.default_sink\n        pulse.request_update(sink)", "contrast": "def server_info_cb(self, context, server_info_p, userdata):\n        server_info = server_info_p.contents\n        self.request_update(context)", "label": 1}
{"index": "gp280184", "code": "import mapnik\ndef renderMapnik(grid):\n    mapRenderer = mapnik.Renderer(\"image.png\")\n    mapStylesheet = mapnik.Style()\n    mapLayer = mapnik.Layer(\"grid\")\n    lineSymbolizer = mapnik.LineSymbolizer()\n    lineSymbolizer.stroke = mapnik.Stroke(mapnik.Color(\"#000000\"), grid.strokewidth)\n    mapStyleRule = mapnik.Rule()\n    mapStyleRule.symbols.append(lineSymbolizer)\n    mapStylesheet.rules.append(mapStyleRule)\n    mapLayer.datasource = mapnik.GeoJSON(file=grid.geojsonfile)\n    mapLayer.styles.append(\"grid\")\n    mapnikMap = mapnik.Map(grid.imgwidth, grid.imgheight)\n    mapnikMap.append_style(\"grid\", mapStylesheet)\n    mapnikMap.layers.append(mapLayer)\n    mapnikMap.zoom_all()\n    mapnik.render_to_file(mapnikMap, \"image.png\", \"png\")", "contrast": "def render_grid(self, bbox, grid_fields, layer, width=None, height=None):\n        width = width or self.tilesize\n        height = height or self.tilesize\n        self._prepare_rendering(bbox, width=width, height=height)\n        grid = mapnik.Grid(width, height)\n        mapnik.render_layer(self._mapnik, grid, layer=layer, fields=grid_fields)\n        grid = grid.encode()\n        return json.dumps(grid)", "label": 1}
{"index": "gp084854", "code": "def inter_spin_hamiltonian(self, u_int, J_coup):\n        J_coup *= u_int\n        h_int  = (u_int - 2*J_coup)/2.*self.oper['sumSz2']\n        h_int += J_coup*self.oper['sumSz-sp2']\n        h_int -= J_coup/2.*self.oper['sumSz-or2']\n        h_int -= J_coup*self.oper['Sfliphop']\n        return h_int", "contrast": "def interaction_hamiltonian(coulomb_interaction, hund_coupling):\n    return coulomb_interaction * hund_coupling", "label": 0}
{"index": "gp102035", "code": "def get_details(cls, node, as_model=False):\n        rest_job = RestJob(\n            process_name=node.process_name,\n            timeperiod=node.timeperiod,\n            time_qualifier=node.time_qualifier,\n            number_of_children=len(node.children),\n            number_of_failures='NA' if not node.job_record else node.job_record.number_of_failures,\n            state='NA' if not node.job_record else node.job_record.state,\n            event_log=[] if not node.job_record else node.job_record.event_log)\n        if as_model:\n            return rest_job\n        else:\n            return rest_job.document", "contrast": "def get_rest_job_or_document(as_model: bool, id: str):\n    if as_model:\n        return RestJob(id=id, ...other_params)\n    else:\n        return document", "label": 0}
{"index": "gp190053", "code": "def bsh_formula(a: int, b: int, imm: int) -> str:\n    return f\"BSH R{a}, R{b}, #{imm}\"", "contrast": "def _translate_bsh(self, oprnd1, oprnd2, oprnd3):\n        assert oprnd1.size and oprnd2.size and oprnd3.size\n        assert oprnd1.size == oprnd2.size\n        op1_var = self._translate_src_oprnd(oprnd1)\n        op2_var = self._translate_src_oprnd(oprnd2)\n        op3_var, op3_var_constrs = self._translate_dst_oprnd(oprnd3)\n        if oprnd3.size > oprnd1.size:\n            op1_var_zx = smtfunction.zero_extend(op1_var, oprnd3.size)\n            op2_var_zx = smtfunction.zero_extend(op2_var, oprnd3.size)\n            op2_var_neg_sx = smtfunction.sign_extend(-op2_var, oprnd3.size)\n            shr = smtfunction.extract(op1_var_zx >> op2_var_neg_sx, 0, op3_var.size)\n            shl = smtfunction.extract(op1_var_zx << op2_var_zx, 0, op3_var.size)\n        elif oprnd3.size < oprnd1.size:\n            shr = smtfunction.extract(op1_var >> -op2_var, 0, op3_var.size)\n            shl = smtfunction.extract(op1_var << op2_var, 0, op3_var.size)\n        else:\n            shr = op1_var >> -op2_var\n            shl = op1_var << op2_var\n        result = smtfunction.ite(oprnd3.size, op2_var >= 0, shl, shr)\n        return [op3_var == result] + op3_var_constrs", "label": 1}
{"index": "gp016722", "code": "def last(col, ignorenulls=False):\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.last(_to_java_column(col), ignorenulls)\n    return Column(jc)", "contrast": "from typing import List, Any\ndef last_value(group: List[Any], ignore_nulls: bool = False) -> Any:\n    values = [val for val in group if val is not None]\n    if values:\n        return values[-1]\n    elif ignore_nulls:\n        return None\n    else:\n        return group[-1]", "label": 0}
{"index": "gp131640", "code": "def get_arg_value_as_type(self, key, default=None, convert_int=False):\n        val = self.get_query_argument(key, default)\n        if isinstance(val, int):\n            return val\n        if val.lower() in ['true', 'yes']:\n            return True\n        if val.lower() in ['false', 'no']:\n            return False\n        return val", "contrast": "def convert_truthy(val):\n    if val.lower() in ['true', 'yes']:\n        return True\n    elif val.lower() in ['false', 'no']:\n        return False\n    else:\n        return val", "label": 0}
{"index": "gp196149", "code": "import os\ndef joinpath(base, path):\n    if os.path.isabs(path):\n        return path\n    return os.path.join(base or '', path)", "contrast": "def _badpath(path, base):\n    return not _resolved(os.path.join(base, path)).startswith(base)", "label": 1}
{"index": "gp145237", "code": "def _format_map_output(self,\n                                  result_format, success,\n                                  element_count, nonnull_count,\n                                  unexpected_count,\n                                  unexpected_list, unexpected_index_list\n                                  ):\n        result_format = parse_result_format(result_format)\n        return_obj = {\n            'success': success\n        }\n        if result_format['result_format'] == 'BOOLEAN_ONLY':\n            return return_obj\n        missing_count = element_count - nonnull_count\n        if element_count > 0:\n            unexpected_percent = unexpected_count / element_count\n            missing_percent = missing_count / element_count\n            if nonnull_count > 0:\n                unexpected_percent_nonmissing = unexpected_count / nonnull_count\n            else:\n                unexpected_percent_nonmissing = None\n        else:\n            missing_percent = None\n            unexpected_percent = None\n            unexpected_percent_nonmissing = None\n        return_obj['result'] = {\n            'element_count': element_count,\n            'missing_count': missing_count,\n            'missing_percent': missing_percent,\n            'unexpected_count': unexpected_count,\n            'unexpected_percent': unexpected_percent,\n            'unexpected_percent_nonmissing': unexpected_percent_nonmissing,\n            'partial_unexpected_list': unexpected_list[:result_format['partial_unexpected_count']]\n        }\n        if result_format['result_format'] == 'BASIC':\n            return return_obj\n        if 0 < result_format.get('partial_unexpected_count'):\n            try:\n                partial_unexpected_counts = [\n                {'value': key, 'count': value}\n                for key, value\n                in sorted(\n                    Counter(unexpected_list).most_common(result_format['partial_unexpected_count']),\n                    key=lambda x: (-x[1], x[0]))\n                ]\n            except TypeError:\n                partial_unexpected_counts = [\n                    'partial_exception_counts requires a hashable type']\n            finally:\n                return_obj['result'].update(\n                    {\n                        'partial_unexpected_index_list': unexpected_index_list[:result_format[\n                            'partial_unexpected_count']] if unexpected_index_list is not None else None,\n                        'partial_unexpected_counts': partial_unexpected_counts\n                    }\n                )\n        if result_format['result_format'] == 'SUMMARY':\n            return return_obj\n        return_obj['result'].update(\n            {\n                'unexpected_list': unexpected_list,\n                'unexpected_index_list': unexpected_index_list\n            }\n        )\n        if result_format['result_format'] == 'COMPLETE':\n            return return_obj\n        raise ValueError(\"Unknown result_format %s.\" %\n                         (result_format['result_format'],))", "contrast": "from typing import Dict, Any\nfrom great_expectations.execution_engine import ExecutionEngine\ndef map_expectation_over_column(\n    execution_engine: ExecutionEngine,\n    func: Any,\n    column: str,\n    result_format: str,\n    *args,\n    **kwargs,\n) -> Dict[str, Any]:\n    expectation_args = {\n        \"column\": column,\n        \"result_format\": result_format,\n    }\n    return func(execution_engine, *args, **expectation_args, **kwargs)", "label": 0}
{"index": "gp279206", "code": "from smc.elements.servers import ManagementServer\ndef get_contact_addresses():\n    mgt_server = ManagementServer.objects.first()\n    return mgt_server.contact_addresses", "contrast": "def contact_addresses(self):\n        return MultiContactAddress(\n            href=self.get_relation('contact_addresses'),\n            type=self.typeof,\n            name=self.name)", "label": 1}
{"index": "gp179533", "code": "def register_artifact_definition(artifact_definition):\n    if artifact_definition.name.lower() in artifact_definitions:\n        raise KeyError(\"Artifact definition for '{}' is already set.\".format(artifact_definition.name.lower()))\n    artifact_definitions[artifact_definition.name.lower()] = artifact_definition", "contrast": "def RegisterDefinition(self, artifact_definition):\n    artifact_definition_name = artifact_definition.name.lower()\n    if artifact_definition_name in self._artifact_definitions:\n      raise KeyError(\n          'Artifact definition already set for name: {0:s}.'.format(\n              artifact_definition.name))\n    self._artifact_definitions[artifact_definition_name] = artifact_definition\n    self._defined_artifact_names.add(artifact_definition.name)\n    for source in artifact_definition.sources:\n      if source.type_indicator == definitions.TYPE_INDICATOR_ARTIFACT_GROUP:\n        self._artifact_name_references.update(source.names)", "label": 1}
{"index": "gp273319", "code": "def dispatch_put_to_server(mri: str, attribute_name: str, value) -> None:\n    pass ", "contrast": "def send_put(self, mri, attribute_name, value):\n        path = attribute_name + \".value\"\n        typ, value = convert_to_type_tuple_value(serialize_object(value))\n        if isinstance(typ, tuple):\n            _, typeid, fields = typ\n            value = Value(Type(fields, typeid), value)\n        try:\n            self._ctxt.put(mri, {path: value}, path)\n        except RemoteError:\n            if attribute_name == \"exports\":\n                self._queues[mri].get(timeout=DEFAULT_TIMEOUT)\n            else:\n                raise", "label": 1}
{"index": "gp220263", "code": "def get_post_form_kwargs(**kwargs):\n    return kwargs", "contrast": "def get_post_form_kwargs(self):\n        kwargs = {\n            'user': self.request.user,\n            'forum': self.get_forum(),\n            'topic': self.get_topic(),\n        }\n        post = self.get_post()\n        if post:\n            kwargs.update({'instance': post})\n        if self.request.method in ('POST', 'PUT'):\n            kwargs.update({\n                'data': self.request.POST,\n                'files': self.request.FILES,\n            })\n        return kwargs", "label": 1}
{"index": "gp274295", "code": "import subprocess\ndef is_root_user_enabled():\n    result = subprocess.run(['sudo', '-n', 'true'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    return result.returncode == 0", "contrast": "def root_user_status(self):\n        uri = \"/instances/%s/root\" % self.id\n        resp, body = self.manager.api.method_get(uri)\n        return body[\"rootEnabled\"]", "label": 1}
{"index": "gp295961", "code": "from typing import Optional\nfrom sqlalchemy import Table, MetaData, Column, sql\nfrom sqlalchemy.engine.base import Engine\nfrom sqlalchemy.engine.reflection import Inspector\nclass SqlaColumnInspectionInfo:\n    def __init__(self, name: str, type_: str, nullable: bool):\n        self.name = name\n        self.type = type_\n        self.nullable = nullable\ndef get_column_info(engine: Engine, table_name: str, column_name: str) -> Optional[SqlaColumnInspectionInfo]:\n    metadata = MetaData()\n    metadata.bind = engine\n    table = Table(table_name, metadata, autoload=True, autoload_with=engine)\n    inspector = Inspector.from_engine(engine)\n    if column_name in inspector.get_columns(table_name):\n        column = Column(column_name, type_=sql.NullType)\n        column_info = table.c[column_name]\n        nullable = column_info.nullable\n        type_ = column_info.type.__repr__().split(\".\")[-1].split(\"(\")[0]\n        return SqlaColumnInspectionInfo(column_name, type_, nullable)\n    else:\n        return None", "contrast": "def get_column_info(engine: Engine, tablename: str,\n                    columnname: str) -> Optional[SqlaColumnInspectionInfo]:\n    for info in gen_columns_info(engine, tablename):\n        if info.name == columnname:\n            return info\n    return None", "label": 1}
{"index": "gp022920", "code": "def get_short_help_str(self, limit=45):\n        return self.short_help or self.help and make_default_short_help(self.help, limit) or ''", "contrast": "def get_short_help(help_string: str) -> str:\n    short_help = ''\n    words = help_string.split()\n    for word in words:\n        if len(short_help + word) > 20:\n            short_help += '...'\n            break\n        short_help += word + ' '\n    return short_help.strip()", "label": 0}
{"index": "gp302476", "code": "from functools import wraps\nfrom pyramid.httpexceptions import HTTPConflict, HTTPInternalServerError\ndef prevent_uri_resource_in_use(func):\n    @wraps(func)\n    def wrapper(request, *args, **kwargs):\n        registry = request.registry\n        referencer = registry.queryUtility(IReferencer)\n        try:\n            if referencer.is_uri_resource_in_use():\n                raise HTTPConflict()\n            else:\n                return func(request, *args, **kwargs)\n        except Exception:\n            raise HTTPInternalServerError()\n    return wrapper", "contrast": "def protected_operation_with_request(fn):\n    @functools.wraps(fn)\n    def wrapped(request, *args, **kwargs):\n        response = _advice(request)\n        if response is not None:\n            return response\n        else:\n            return fn(request, *args, **kwargs)\n    return wrapped", "label": 1}
{"index": "gp324848", "code": "def gas_viscosity(T, P, method):\n    mu = 0.0 \n    return mu", "contrast": "def calculate_P(self, T, P, method):\n        if method == COOLPROP:\n            mu = PropsSI('V', 'T', T, 'P', P, self.CASRN)\n        elif method in self.tabular_data:\n            mu = self.interpolate_P(T, P, method)\n        return mu", "label": 1}
{"index": "gp151773", "code": "def setup_logging(level, monchrome=False, log_file=None):\n    if log_file:\n        logging.basicConfig(filename=log_file, filemode='w',\n                            level=logging.DEBUG)\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = ColoredFormatter(\"%(levelname)s: %(message)s\", monchrome)\n    ch.setFormatter(formatter)\n    packages = ('__main__', 'fusesoc',)\n    for package in packages:\n        logger = logging.getLogger(package)\n        logger.addHandler(ch)\n        logger.setLevel(level)\n    warning_only_packages = []\n    for package in warning_only_packages:\n        logger = logging.getLogger(package)\n        logger.addHandler(ch)\n        logger.setLevel(logging.WARNING)\n    logger.debug('Setup logging at level {}.'.format(level))", "contrast": "import logging\ndef setup_logging(log_file):\n    logging.basicConfig(filename=log_file,\n                        level=logging.INFO,\n                        format='%(asctime)s [%(levelname)s] %(message)s',\n                        datefmt='%Y-%m-%d %H:%M:%S')", "label": 0}
{"index": "gp112753", "code": "def _open(self):\n        try:\n            self.db = connect(\n                host=self.settings.host,\n                port=self.settings.port,\n                user=coalesce(self.settings.username, self.settings.user),\n                passwd=coalesce(self.settings.password, self.settings.passwd),\n                db=coalesce(self.settings.schema, self.settings.db),\n                read_timeout=coalesce(self.settings.read_timeout, (EXECUTE_TIMEOUT / 1000) - 10 if EXECUTE_TIMEOUT else None, 5*60),\n                charset=u\"utf8\",\n                use_unicode=True,\n                ssl=coalesce(self.settings.ssl, None),\n                cursorclass=cursors.SSCursor\n            )\n        except Exception as e:\n            if self.settings.host.find(\"://\") == -1:\n                Log.error(\n                    u\"Failure to connect to {{host}}:{{port}}\",\n                    host=self.settings.host,\n                    port=self.settings.port,\n                    cause=e\n                )\n            else:\n                Log.error(u\"Failure to connect.  PROTOCOL PREFIX IS PROBABLY BAD\", e)\n        self.cursor = None\n        self.partial_rollback = False\n        self.transaction_level = 0\n        self.backlog = []  \n        if self.readonly:\n            self.begin()", "contrast": "def do_not_use():\n    raise Exception(\"DO NOT USE THIS UNLESS YOU close() FIRST\")", "label": 0}
{"index": "gp192065", "code": "def evaluate_bezier_curve(coordinates, t):\n    p0, p1, p2, p3 = coordinates\n    x = (1-t)**3*p0[0] + 3*t*(1-t)**2*p1[0] + 3*t**2*(1-t)*p2[0] + t**3*p3[0]\n    y = (1-t)**3*p0[1] + 3*t*(1-t)**2*p1[1] + 3*t**2*(1-t)*p2[1] + t**3*p3[1]\n    return (x, y)", "contrast": "def point(self, t):\n        distance = self.end - self.start\n        return self.start + distance*t", "label": 1}
{"index": "gp070347", "code": "def capability(self, cap_name):\n  if cap_name in self.__class_capabilities__:\n\t\t\tfunction_name = self.__class_capabilities__[cap_name]\n   return getattr(self, function_name)", "contrast": "def get_capability(cap_name):\n    return getattr(self, cap_name, None)", "label": 0}
{"index": "gp130308", "code": "def create_logger(self, args={}):\n        logger = logging.getLogger(\"SmartFileSorter\")\n        logger.level = logging.INFO\n        if '--debug' in args and args['--debug'] is True:\n            logger.setLevel(logging.DEBUG)\n        file_log_formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)s %(message)s',\n                                               '%Y-%m-%d %H:%M:%S')\n        console_log_formatter = logging.Formatter('%(message)s')\n        stdout_stream = logging.StreamHandler(stream=sys.stdout)\n        stdout_stream.setFormatter(console_log_formatter)\n        logger.addHandler(stdout_stream)\n        if '--log' in args and args['--log'] is not None:\n            logfile = open(args['--log'], 'w')\n            logfile_stream = logging.StreamHandler(stream=logfile)\n            logfile_stream.setFormatter(file_log_formatter)\n            logger.addHandler(logfile_stream)\n        if '--dry-run' in args and args['--dry-run'] is True:\n            logger.info('Running with --dry-run parameter. Actions will not be performed.')\n        self.logger = logger", "contrast": "import logging\ndef configure_logger(args):\n    logger = logging.getLogger()\n    if args.debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)", "label": 0}
{"index": "gp266480", "code": "def guess_column_ends(column):\n    diff = abs(np.diff(column))\n    thresh = 1.5 * np.median(diff)\n    peaks, _ = find_peaks(diff, height=thresh)\n    return peaks", "contrast": "def _get_column_ends(self):\n        ends = collections.Counter()\n        for line in self.text.splitlines():\n            for matchobj in re.finditer('\\s{2,}', line.lstrip()):\n                ends[matchobj.end()] += 1\n        return ends", "label": 1}
{"index": "gp190709", "code": "def print_errors(test_errors, noise_errors):\n    for i, (test_error, noise_error) in enumerate(zip(test_errors, noise_errors)):\n        print(f\"Iteration {i}: Test Error: {test_error}, Noise Error: {noise_error}\")", "contrast": "def learningCurve(expPath, suite):\n  print(\"\\nLEARNING CURVE ================\",expPath,\"=====================\")\n  try:\n    headers=[\"testerror\",\"totalCorrect\",\"elapsedTime\",\"entropy\"]\n    result = suite.get_value(expPath, 0, headers, \"all\")\n    info = []\n    for i,v in enumerate(zip(result[\"testerror\"],result[\"totalCorrect\"],\n                             result[\"elapsedTime\"],result[\"entropy\"])):\n      info.append([i, v[0], v[1], int(v[2]), v[3]])\n    headers.insert(0,\"iteration\")\n    print(tabulate(info, headers=headers, tablefmt=\"grid\"))\n  except:\n    print(\"Couldn't load experiment\",expPath)", "label": 1}
{"index": "gp158396", "code": "def dict_to_attributes_code(dict_):\n    lines = []\n    for key, value in dict_.iteritems():\n        if isinstance(value, dict):\n            txt = dict_to_attributes_code(value)\n            lines_ = txt.split('\\n')\n            for line in lines_:\n                if not line.startswith(' '):\n                    line = \"%s.%s\" % (key, line)\n                lines.append(line)\n        else:\n            value_txt = pformat(value)\n            if '\\n' in value_txt:\n                lines.append(\"%s = \\\\\" % key)\n                value_txt = indent(value_txt)\n                lines.extend(value_txt.split('\\n'))\n            else:\n                line = \"%s = %s\" % (key, value_txt)\n                lines.append(line)\n    return '\\n'.join(lines)", "contrast": "def dict_to_attributes_code(d):\n    code = \"\"\n    for key in d:\n        if isinstance(d[key], dict):\n            for sub_key in d[key]:\n                code += key + \".\" + sub_key + \" = \" + str(d[key][sub_key]) + \"\\n\"\n        else:\n            code += key + \" = \" + str(d[key]) + \"\\n\"\n    return code", "label": 0}
{"index": "gp218455", "code": "def create_soql_tree(soql_query):\n    tree_structure = {}\n    select_fields = []\n    where_fields = []\n    split_query = soql_query.split()\n    for i, word in enumerate(split_query):\n        if word == \"SELECT\":\n            select_fields = split_query[i+1].split(\",\")\n        elif word == \"FROM\":\n            object_name = split_query[i+1]\n            if \"(\" in object_name:\n                relationship_name = object_name.split(\"(\")[1].split(\")\")[0]\n                object_name = object_name.split(\"(\")[0]\n                tree_structure[relationship_name] = {}\n                tree_structure[relationship_name][object_name] = {}\n            else:\n                tree_structure[object_name] = {}\n        elif word == \"WHERE\":\n            where_fields = split_query[i+1:]\n    for field in select_fields:\n        if \".\" in field:\n            relationship_name = field.split(\".\")[0]\n            field_name = field.split(\".\")[1]\n            tree_structure[relationship_name][object_name][field_name] = {}\n    for field in where_fields:\n        if \".\" in field:\n            relationship_name = field.split(\".\")[0]\n            field_name = field.split(\".\")[1]\n            tree_structure[relationship_name][object_name][field_name] = {}\n    return tree_structure", "contrast": "def _from_sql(self, soql):\n        assert not self.soql, \"Don't use _from_sql method directly\"\n        self.soql = soql\n        soql, self.subqueries = split_subquery(soql)\n        match_parse = re.match(r'SELECT (.*) FROM (\\w+)\\b(.*)$', soql, re.I)\n        if not match_parse:\n            raise ProgrammingError('Invalid SQL: %s' % self.soql)\n        fields_sql, self.root_table, self.extra_soql = match_parse.groups()\n        fields = [x.strip() for x in fields_sql.split(',')]\n        self.is_aggregation = bool(pattern_groupby.search(self.extra_soql) or\n                                   pattern_aggregation.search(fields[0]))\n        self.is_plain_count = fields[0].upper() == 'COUNT()'\n        consumed_subqueries = 0\n        expr_alias_counter = 0\n        if not self.is_plain_count:\n            for field in fields:\n                if self.is_aggregation:\n                    match = re.search(r'\\b\\w+$', field)\n                    if match:\n                        alias = match.group()\n                        assert alias not in RESERVED_WORDS, \"invalid alias name\"\n                        if match.start() > 0 and field[match.start() - 1] == ' ':\n                            field = field[match.start() - 1]\n                    else:\n                        alias = 'expr{}'.format(expr_alias_counter)\n                        expr_alias_counter += 1\n                    assert '&' not in field, \"Subquery not expected as field in aggregation query\"\n                elif '&' in field:\n                    assert field == '(&)'  \n                    subquery = QQuery(self.subqueries[consumed_subqueries][0])\n                    consumed_subqueries += 1\n                    self.has_child_rel_field = True\n                    field = subquery\n                    alias = subquery.root_table\n                else:\n                    alias = field\n                    if '.' in alias:\n                        if alias.split('.', 1)[0].lower() == self.root_table.lower():\n                            alias = alias.split('.', 1)[1]\n                        if '.' in alias:\n                            subroots = self.subroots\n                            root_crumbs = alias.lower().split('.')[:-1]\n                            for scrumb in root_crumbs:\n                                subroots.setdefault(scrumb, {})\n                                subroots = subroots[scrumb]\n                self.aliases.append(alias)\n                self.fields.append(field)", "label": 1}
{"index": "gp024331", "code": "def _get_history_daily_window(self,\n                                  assets,\n                                  end_dt,\n                                  bar_count,\n                                  field_to_use,\n                                  data_frequency):\n        session = self.trading_calendar.minute_to_session_label(end_dt)\n        days_for_window = self._get_days_for_window(session, bar_count)\n        if len(assets) == 0:\n            return pd.DataFrame(None,\n                                index=days_for_window,\n                                columns=None)\n        data = self._get_history_daily_window_data(\n            assets, days_for_window, end_dt, field_to_use, data_frequency\n        )\n        return pd.DataFrame(\n            data,\n            index=days_for_window,\n            columns=assets\n        )", "contrast": "import pandas as pd\ndef get_history_bars(sids):\n    return pd.DataFrame() ", "label": 0}
{"index": "gp152565", "code": "def to_df(self, method: str = 'MEMORY', **kwargs) -> 'pd.DataFrame':\n        ll = self._is_valid()\n        if ll:\n            print(ll['LOG'])\n            return None\n        else:\n            return self.sas.sasdata2dataframe(self.table, self.libref, self.dsopts, method, **kwargs)", "contrast": "import pandas as pd\nimport saspy\ndef export_sas_to_pandas(dataset, method=\"MEMORY\", **kwargs):\n    if method == \"MEMORY\":\n        sas_dataset = saspy.SASdata(dataset)\n        return sas_dataset.to_df()\n    elif method == \"CSV\":\n        csv_file = dataset + \".csv\"\n        sas_dataset = saspy.SASdata(dataset)\n        sas_dataset.to_csv(csv_file)\n        return pd.read_csv(csv_file, **kwargs)\n    else:\n        raise ValueError(\"Invalid method. Choose either MEMORY or CSV.\")", "label": 0}
{"index": "gp071882", "code": "def calculate_normalized_ratios(ratios, ch_medians, channels):\n    outratios = []\n    for quant in ratios:\n        quant.update({ch: str(quant[ch] / ch_medians[ch])\n                      if quant[ch] != 'NA' else 'NA' for ch in channels})\n        outratios.append(quant)\n    return outratios", "contrast": "from typing import List\nimport numpy as np\ndef calculate_ratios(intensities: List[List[float]], min_intensity: float) -> np.ndarray:\n    intensities = np.array(intensities)\n    intensities[intensities < min_intensity] = np.nan\n    med_intensities = np.nanmedian(intensities, axis=1)\n    ratios = np.divide(intensities.T, med_intensities).T\n    return ratios", "label": 0}
{"index": "gp280112", "code": "def get_cluster_agents():\n    agents = []\n    return agents", "contrast": "def get_agents():\n    agent_list = []\n    agents = __get_all_agents()\n    for agent in agents:\n        agent_list.append(agent[\"hostname\"])\n    return agent_list", "label": 1}
{"index": "gp070999", "code": "def enable(self, folder):\n\t\thost_name = self.base.options.ip\n  email = self.base.options.user\n  isHost(host_name)\n  self.__userExists(email)\n  data = self.getHostData(host_name)\n  website_dir = data['website_dir']\n  full_path = '%s/%s' % (website_dir, folder)\n  full_path_git = '%s/.git' % full_path\n  repository = '%s/%s/%s.git' % (self.data[email]['dir'], host_name, folder)\n  real_repository = '%s/%s.git' % (self.base.git['repositories'], host_name)\n  user_hook = '%s/hooks/post-receive' % repository\n  repo_hooks = '%s/.git/hooks' % website_dir\n  origin = md5(email)\n  if not fileExists(full_path):\n\t\t\terror_message('Folder %s not exists!' % full_path)\n  if not host_name in self.data[email]['projects']:\n\t\t\tself.data[email]['projects'][host_name] = []\n  if folder in self.data[email]['projects'][host_name]:\n\t\t\terror_message('Repository already exists for this user!')\n  self.data[email]['projects'][host_name].append(folder)\n  self.__makeDir(repository)\n  os.system('cd %s && git init --bare 1> /dev/null' % repository)\n  putFile(\n   '%s/.gitignore' % full_path,\n   getTemplate('git-jail-gitignore-default')\n  )\n  os.system(\n   'cd %(full_path)s && git init 1> /dev/null && '\n   'git remote add %(origin)s %(repository)s && '\n   'git add . && '\n   'git commit -am \"Initial commit\" 1> /dev/null && '\n   'git push %(origin)s master 1> /dev/null' % locals()\n  )\n  os.system('chown git:git -R %s' % full_path)\n  os.system('chown git:git -R %s' % repository)\n  os.system('chown git:git -R %s' % real_repository)\n  os.system('chown git:git -R %s/.git' % website_dir)\n  key = hash('%(email)s-%(host_name)s-%(folder)s' % locals())\n  templates = {\n   user_hook: 'git-jail-post-receive-user-repo',\n   '%s/post-commit' % repo_hooks: 'git-jail-post-commit-repo',\n   '%s/post-receive' % repo_hooks: 'git-jail-post-receive-repo',\n   '%s/hooks/post-receive' % real_repository: 'git-jail-post-receive-real-repo',\n  }\n  putFile(\n   '%s/hooks/post-receive.db' % real_repository,\n   '%(website_dir)s;%(full_path)s;.;%(key)s;%(origin)s;' % locals(),\n   'a'\n  )\n  putFile(\n   '%s.db' % user_hook,\n   '%(full_path)s;%(website_dir)s;./%(folder)s/*;%(key)s;%(origin)s;' % locals(),\n   'a'\n  )\n  putFile(\n   '%s/post-receive.db' % repo_hooks,\n   '%(full_path)s;%(real_repository)s;%(key)s;%(origin)s;' % locals(),\n   'a'\n  )\n  for f,t in templates.items():\n\t\t\tputFile(f, getTemplate(t) % locals())\n   os.system('chmod +x %s' % f)\n  info_message('Successful!')", "contrast": "import os\ndef remove_and_add_files(website_dir, folder):\n    os.system('cd %s; git rm --cached %s; git add ./%s/*' % (website_dir, folder, folder))", "label": 0}
{"index": "gp050815", "code": "def style_factory(self, style_name):\n        try:\n            style = get_style_by_name(style_name)\n        except ClassNotFound:\n            style = get_style_by_name('vim')\n        styles = {}\n        styles.update(style.styles)\n        styles.update(default_style_extensions)\n        t = Token\n        styles.update({\n            t.Menu.Completions.Completion.Current: 'bg:#00aaaa #000000',\n            t.Menu.Completions.Completion: 'bg:#008888 #ffffff',\n            t.Menu.Completions.Meta.Current: 'bg:#00aaaa #000000',\n            t.Menu.Completions.Meta: 'bg:#00aaaa #ffffff',\n            t.Scrollbar.Button: 'bg:#003333',\n            t.Scrollbar: 'bg:#00aaaa',\n            t.Toolbar: 'bg:#222222 #cccccc',\n            t.Toolbar.Off: 'bg:#222222 #696969',\n            t.Toolbar.On: 'bg:#222222 #ffffff',\n            t.Toolbar.Search: 'noinherit bold',\n            t.Toolbar.Search.Text: 'nobold',\n            t.Toolbar.System: 'noinherit bold',\n            t.Toolbar.Arg: 'noinherit bold',\n            t.Toolbar.Arg.Text: 'nobold'\n        })\n        return style_from_dict(styles)", "contrast": "import pygments.styles as styles\ndef get_pygments_style(style_name):\n    try:\n        return styles.get_style_by_name(style_name)\n    except Exception:\n        return styles.get_style_by_name('vim')", "label": 0}
{"index": "gp299482", "code": "def match_results(models, results, relation):\n    matched_results = []\n    for model in models:\n        parent_id = model.id\n        related_results = []\n        for result in results:\n            if getattr(result, relation+'_id') == parent_id:\n                related_results.append(result)\n        setattr(model, relation, related_results)\n        matched_results.extend(related_results)\n    unmatched_results = set(results) - set(matched_results)\n    return unmatched_results", "contrast": "def match_one(self, models, results, relation):\n        return self._match_one_or_many(models, results, relation, 'one')", "label": 1}
{"index": "gp232809", "code": "import pandas_datareader.data as web\ndef get_series_info(series_id):\n    info = web.DataReader(series_id, 'fred').describe().iloc[:,0]\n    return info", "contrast": "def get_series_info(self, series_id):\n        url = \"%s/series?series_id=%s\" % (self.root_url, series_id)\n        root = self.__fetch_data(url)\n        if root is None or not len(root):\n            raise ValueError('No info exists for series id: ' + series_id)\n        info = pd.Series(root.getchildren()[0].attrib)\n        return info", "label": 1}
{"index": "gp105210", "code": "def usearch_fasta_sort_from_filepath(\n        fasta_filepath,\n        output_filepath=None,\n        log_name=\"sortlen.log\",\n        HALT_EXEC=False,\n        save_intermediate_files=False,\n        remove_usearch_logs=False,\n        working_dir=None):\n    if not output_filepath:\n        _, output_filepath = mkstemp(prefix='usearch_fasta_sort',\n                                     suffix='.fasta')\n    log_filepath = join(working_dir, log_name)\n    params = {}\n    app = Usearch(params, WorkingDir=working_dir, HALT_EXEC=HALT_EXEC)\n    data = {'--mergesort': fasta_filepath,\n            '--output': output_filepath,\n            }\n    if not remove_usearch_logs:\n        data['--log'] = log_filepath\n    app_result = app(data)\n    return app_result, output_filepath", "contrast": "def generate_sorted_fasta(fasta_filepath, output_filepath, log_name, HALT_EXEC=False, save_intermediate_files=False):\n    import subprocess\n    command = f'usearch --mergesort {fasta_filepath} --output {output_filepath} --log {log_name}'\n    if HALT_EXEC:\n        command += ' --halt_exit'\n    if save_intermediate_files:\n        command += ' --save_intermediate'\n    subprocess.run(command, shell=True)", "label": 0}
{"index": "gp259274", "code": "def close_broker_clients(clients):\n    for client in clients:\n        client.close()", "contrast": "def _close_brokerclients(self, clients):\n        def _log_close_failure(failure, brokerclient):\n            log.debug(\n                'BrokerClient: %s close result: %s: %s', brokerclient,\n                failure.type.__name__, failure.getErrorMessage())\n        def _clean_close_dlist(result, close_dlist):\n            if close_dlist == self.close_dlist:\n                self.close_dlist = None\n        if not self.close_dlist:\n            dList = []\n        else:\n            log.debug(\"%r: _close_brokerclients has nested deferredlist: %r\",\n                      self, self.close_dlist)\n            dList = [self.close_dlist]\n        for brokerClient in clients:\n            log.debug(\"Calling close on: %r\", brokerClient)\n            d = brokerClient.close().addErrback(_log_close_failure, brokerClient)\n            dList.append(d)\n        self.close_dlist = DeferredList(dList)\n        self.close_dlist.addBoth(_clean_close_dlist, self.close_dlist)", "label": 1}
{"index": "gp099196", "code": "def compare_parts(list1, list2):\n    for i, item in enumerate(list1):\n        if item != list2[i]:\n            return 0\n    if len(list2) > len(list1):\n        return ISDIR\n    else:\n        return ISFILE", "contrast": "def check_lists(list1, list2):\n    if not list2.startswith(list1):\n        return 0", "label": 0}
{"index": "gp257344", "code": "def _add_window_title(title):\n    if not title:\n        return False\n    else:\n        return True", "contrast": "def _set_pyqtgraph_title(layout):\n    if 'title_size' in pytplot.tplot_opt_glob:\n        size = pytplot.tplot_opt_glob['title_size']\n    if 'title_text' in pytplot.tplot_opt_glob:\n        if pytplot.tplot_opt_glob['title_text'] != '':\n            layout.addItem(LabelItem(pytplot.tplot_opt_glob['title_text'], size=size, color='k'), row=0, col=0)\n            return True\n    return False", "label": 1}
{"index": "gp174411", "code": "from typing import List, Dict\nimport mxnet as mx\ndef get_optimizer_config(args: argparse.Namespace, source_vocab_sizes: List[int], extra_initializers: Dict[str, str]) -> Dict[str, any]:\n    num_update_steps = args.num_buckets * args.batch_size_update / args.batch_size\n    lr_scheduler = mx.lr_scheduler.FactorScheduler(step=num_update_steps, factor=args.learning_rate_decay, stop_factor_lr=args.stop_factor_lr)\n    optimizer_params = {\n        'wd': args.weight_decay,\n        'learning_rate': args.learning_rate,\n        'lr_scheduler': lr_scheduler\n    }\n    if args.clip_gradient:\n        optimizer_params['clip_gradient'] = args.clip_gradient\n    kvstore = mx.kvstore.create(args.kvstore) if args.kvstore else None\n    optimizer_type = args.optimizer.lower()\n    if optimizer_type == 'adam':\n        optimizer_params['beta1'] = args.adam_beta1\n    elif optimizer_type == 'nag':\n        optimizer_params['momentum'] = args.momentum\n    optimizer_params.update({k: float(v) for k, v in args.optimizer_params.items()})\n    if args.hybridize:\n        optimizer_params['multi_precision'] = True\n    if args.update_begin_of_epoch:\n        optimizer_params[\"rescale_grad\"] = 1.0 / float(args.batch_size_update)\n    if args.profile_gradient:\n        optimizer_params['profile'] = True\n    if args.bucketing is not None:\n        l2_reg = L2Regularization(params=args.weight_decay)\n        optimizer_params[BOPT_DEFAULTS] = BucketingOptions(default_bucket_key=args.bucketing,\n                                                           regularization=l2_reg,\n                                                           gradient_compression_params=args.gradient_compression_params)\n    return {'type': optimizer_type, 'params': optimizer_params, 'kvstore': kvstore}", "contrast": "def create_optimizer_config(args: argparse.Namespace, source_vocab_sizes: List[int],\n                            extra_initializers: List[Tuple[str, mx.initializer.Initializer]] = None) -> OptimizerConfig:\n    optimizer_params = {'wd': args.weight_decay,\n                        \"learning_rate\": args.initial_learning_rate}\n    gradient_clipping_threshold = none_if_negative(args.gradient_clipping_threshold)\n    if gradient_clipping_threshold is None:\n        logger.info(\"Gradient clipping threshold set to negative value. Will not perform gradient clipping.\")\n        gradient_clipping_type = C.GRADIENT_CLIPPING_TYPE_NONE\n    else:\n        gradient_clipping_type = args.gradient_clipping_type\n    effective_batch_size = args.batch_size * args.update_interval\n    if gradient_clipping_threshold is not None and gradient_clipping_type == C.GRADIENT_CLIPPING_TYPE_ABS:\n        optimizer_params[\"clip_gradient\"] = gradient_clipping_threshold\n    if args.momentum is not None:\n        optimizer_params[\"momentum\"] = args.momentum\n    if args.loss_normalization_type == C.LOSS_NORM_VALID:\n        optimizer_params[\"rescale_grad\"] = 1.0 / args.update_interval\n    elif args.loss_normalization_type == C.LOSS_NORM_BATCH:\n        optimizer_params[\"rescale_grad\"] = 1.0 / effective_batch_size\n    if args.optimizer_params:\n        optimizer_params.update(args.optimizer_params)\n    weight_init = initializer.get_initializer(default_init_type=args.weight_init,\n                                              default_init_scale=args.weight_init_scale,\n                                              default_init_xavier_rand_type=args.weight_init_xavier_rand_type,\n                                              default_init_xavier_factor_type=args.weight_init_xavier_factor_type,\n                                              embed_init_type=args.embed_weight_init,\n                                              embed_init_sigma=source_vocab_sizes[0] ** -0.5,\n                                              rnn_init_type=args.rnn_h2h_init,\n                                              extra_initializers=extra_initializers)\n    lr_sched = lr_scheduler.get_lr_scheduler(args.learning_rate_scheduler_type,\n                                             args.checkpoint_interval,\n                                             none_if_negative(args.learning_rate_half_life),\n                                             args.learning_rate_reduce_factor,\n                                             args.learning_rate_reduce_num_not_improved,\n                                             args.learning_rate_schedule,\n                                             args.learning_rate_warmup)\n    config = OptimizerConfig(name=args.optimizer,\n                             params=optimizer_params,\n                             kvstore=args.kvstore,\n                             initializer=weight_init,\n                             gradient_clipping_type=gradient_clipping_type,\n                             gradient_clipping_threshold=gradient_clipping_threshold,\n                             update_interval=args.update_interval)\n    config.set_lr_scheduler(lr_sched)\n    logger.info(\"Optimizer: %s\", config)\n    logger.info(\"Gradient Compression: %s\", gradient_compression_params(args))\n    if args.update_interval > 1:\n        logger.info(\"Gradient accumulation over %d batches. Effective batch size: %d\",\n                    args.update_interval, effective_batch_size)\n    return config", "label": 1}
{"index": "gp317606", "code": "def simple_search(keywords):\n    for password in password_list:\n        if all(keyword.lower() in password.lower() for keyword in keywords):\n            yield password", "contrast": "def simple_search(self, *keywords):\n        matches = []\n        keywords = [kw.lower() for kw in keywords]\n        logger.verbose(\n            \"Performing simple search on %s (%s) ..\",\n            pluralize(len(keywords), \"keyword\"),\n            concatenate(map(repr, keywords)),\n        )\n        for entry in self.filtered_entries:\n            normalized = entry.name.lower()\n            if all(kw in normalized for kw in keywords):\n                matches.append(entry)\n        logger.log(\n            logging.INFO if matches else logging.VERBOSE,\n            \"Matched %s using simple search.\",\n            pluralize(len(matches), \"password\"),\n        )\n        return matches", "label": 1}
{"index": "gp137092", "code": "def close(self, commit=True):\n    self.data_access.close(commit=commit)\n    return self", "contrast": "def close():\n    data_access.close()", "label": 0}
{"index": "gp121902", "code": "async def get_scene(self, scene_id, from_cache=True) -> Scene:\n        if not from_cache:\n            await self.get_scenes()\n        for _scene in self.scenes:\n            if _scene.id == scene_id:\n                return _scene\n        raise ResourceNotFoundException(\"Scene not found scene_id: {}\".format(scene_id))", "contrast": "def get_scene_resource():\n    raise ResourceNotFoundException(\"No scene found\")\n    raise PvApiError(\"Something is wrong with the hub\")", "label": 0}
{"index": "gp231438", "code": "def check_checksum(key, clientside_checksum, file_checksums):\n    if key in file_checksums:\n        return clientside_checksum == file_checksums[key]\n    else:\n        return False", "contrast": "def _item_exists_in_bucket(self, bucket, key, checksums):\n        try:\n            obj = self.target_s3.meta.client.head_object(Bucket=bucket, Key=key)\n            if obj and obj.containsKey('Metadata'):\n                if obj['Metadata'] == checksums:\n                    return True\n        except ClientError:\n            return False", "label": 1}
{"index": "gp030896", "code": "def display(port=None, height=None):\n  _display(port=port, height=height, print_message=True, display_handle=None)", "contrast": "import webbrowser\nimport requests\ndef display_tensorboard(port=None, height=None):\n    if port is None:\n        url = 'http://localhost:' + requests.get(\"http://localhost:6006/data/plugin/scalars/tags\").url.split(\":\")[-1]\n    else:\n        url = 'http://localhost:' + str(port)\n    if height is None:\n        height = 800\n    webbrowser.open_new_tab(url + '#height=' + str(height) + ',waterfall')", "label": 0}
{"index": "gp047083", "code": "def metrics(self, name):\n        return [\n            MetricStub(\n                ensure_unicode(stub.name),\n                stub.type,\n                stub.value,\n                normalize_tags(stub.tags),\n                ensure_unicode(stub.hostname),\n            )\n            for stub in self._metrics.get(to_string(name), [])\n        ]", "contrast": "def get_metrics_by_name(name:str, metrics:dict) -> dict:\n    return metrics.get(name, {})", "label": 0}
{"index": "gp201765", "code": "class DateTimeValues(object):\n    def __init__(self, modification_time=None):\n        self._modification_time = modification_time\n    @property\n    def modification_time(self):\n        return self._modification_time", "contrast": "def modification_time(self):\n    if self._stat_info is None:\n      return None\n    timestamp = int(self._stat_info.st_mtime)\n    return dfdatetime_posix_time.PosixTime(timestamp=timestamp)", "label": 1}
{"index": "gp077408", "code": "def update_unit(self, unit_id, unit_dict):\n        return self._create_put_request(resource=UNITS, billomat_id=unit_id, send_data=unit_dict)", "contrast": "def update_unit(unit_id, unit_dict):\n    units[unit_id].update(unit_dict)\n    return units[unit_id]", "label": 0}
{"index": "gp271235", "code": "from typing import List\nfrom sevenbridges.models import ExportBulkRecord\nfrom sevenbridges.errors import SbgError\ndef create_bulk_exports(exports: List[str], copy_only: bool, api) -> List[ExportBulkRecord]:\n    try:\n        bulk_export = api.exports.bulk_create(exports, copy_only=copy_only)\n        return bulk_export\n    except SbgError as error:\n        raise ValueError(f'Error creating bulk exports: {error}')", "contrast": "def bulk_submit(cls, exports, copy_only=False, api=None):\n        if not exports:\n            raise SbgError('Exports are required')\n        api = api or cls._API\n        items = []\n        for export in exports:\n            file_ = Transform.to_file(export.get('file'))\n            volume = Transform.to_volume(export.get('volume'))\n            location = Transform.to_location(export.get('location'))\n            properties = export.get('properties', {})\n            overwrite = export.get('overwrite', False)\n            item = {\n                'source': {\n                    'file': file_\n                },\n                'destination': {\n                    'volume': volume,\n                    'location': location\n                },\n                'properties': properties,\n                'overwrite': overwrite\n            }\n            items.append(item)\n        data = {'items': items}\n        params = {'copy_only': copy_only}\n        response = api.post(\n            url=cls._URL['bulk_create'], params=params, data=data\n        )\n        return ExportBulkRecord.parse_records(response=response, api=api)", "label": 1}
{"index": "gp072428", "code": "def parse_lines(log_parsers, fileinp):\n    while 1:\n        logentry = fileinp.readline()\n        if not logentry:\n            break\n        elif not logentry.rstrip():\n            continue  \n        processed = False\n        for lp in log_parsers:\n            if lp.grok(logentry):\n                processed = True\n        if not processed:\n            logger = logging.getLogger('logparser')\n            logger.warning(\n                'Could not parse line >>>%s<<<', logentry.rstrip())\n            print('Could not parse line >>>%s<<<' % logentry.rstrip())", "contrast": "import fileinput\ndef parse_lines_to_log_parsers(log_parsers):\n    for line in fileinput.input():\n        for parser in log_parsers:\n            parser.parse(line)", "label": 0}
{"index": "gp113389", "code": "def enumerate_zones(self):\n        zones = []\n        for controller in range(1, 8):\n            for zone in range(1, 17):\n                zone_id = ZoneID(zone, controller)\n                try:\n                    name = yield from self.get_zone_variable(zone_id, 'name')\n                    if name:\n                        zones.append((zone_id, name))\n                except CommandException:\n                    break\n        return zones", "contrast": "def get_zones():\n    zones = [(1, \"Zone A\"), (2, \"Zone B\"), (3, \"Zone C\"), (4, \"Zone D\")]\n    return zones", "label": 0}
{"index": "gp170453", "code": "def export_mesh_to_obj(mesh):\n    export = mesh.export(file_type=\"obj\")\n    return export", "contrast": "def export_wavefront(mesh,\n                     include_normals=True,\n                     include_texture=True):\n    face_formats = {('v',): '{}',\n                    ('v', 'vn'): '{}//{}',\n                    ('v', 'vt'): '{}/{}',\n                    ('v', 'vn', 'vt'): '{}/{}/{}'}\n    face_type = ['v']\n    export = 'v '\n    export += util.array_to_string(mesh.vertices,\n                                   col_delim=' ',\n                                   row_delim='\\nv ',\n                                   digits=8) + '\\n'\n    if include_normals and 'vertex_normals' in mesh._cache:\n        face_type.append('vn')\n        export += 'vn '\n        export += util.array_to_string(mesh.vertex_normals,\n                                       col_delim=' ',\n                                       row_delim='\\nvn ',\n                                       digits=8) + '\\n'\n    if (include_texture and\n        'vertex_texture' in mesh.metadata and\n            len(mesh.metadata['vertex_texture']) == len(mesh.vertices)):\n        face_type.append('vt')\n        export += 'vt '\n        export += util.array_to_string(mesh.metadata['vertex_texture'],\n                                       col_delim=' ',\n                                       row_delim='\\nvt ',\n                                       digits=8) + '\\n'\n    face_format = face_formats[tuple(face_type)]\n    faces = 'f ' + util.array_to_string(mesh.faces + 1,\n                                        col_delim=' ',\n                                        row_delim='\\nf ',\n                                        value_format=face_format)\n    export += faces\n    return export", "label": 1}
{"index": "gp278877", "code": "def render_template(service_name: str, environment: str) -> str:\n    return rendered_template", "contrast": "def render_local_template(service_name, environment, repo_root, template_file):\n    cmd = 'cd {} && ef-cf {} {} --devel --verbose'.format(repo_root, template_file, environment)\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if p.returncode != 0:\n        stderr = indentify('\\n{}'.format(stderr))\n        stdout = indentify('\\n{}'.format(stdout))\n        raise Exception('Service: `{}`, Env: `{}`, Msg: `{}{}`'\n                        .format(service_name, environment, stderr, stdout))\n    logger.debug('Rendered template for `%s` in `%s`', template_file, environment)\n    r = re.match(r\".*(^{.*^})$\", stdout, re.MULTILINE | re.DOTALL)\n    return jsonify(json.loads(r.group(1)))", "label": 1}
{"index": "gp046551", "code": "def _update_container_metrics(self, instance, subcontainer, kube_labels):\n        tags = list(instance.get('tags', []))  \n        if len(subcontainer.get('aliases', [])) >= 1:\n            container_name = subcontainer['aliases'][0]\n        else:\n            self.log.debug(\"Subcontainer doesn't have a name, skipping.\")\n            return\n        tags.append('container_name:%s' % container_name)\n        container_image = self.kubeutil.image_name_resolver(subcontainer['spec'].get('image'))\n        if container_image:\n            tags.append('container_image:%s' % container_image)\n            split = container_image.split(\":\")\n            if len(split) > 2:\n                split = [':'.join(split[:-1]), split[-1]]\n            tags.append('image_name:%s' % split[0])\n            if len(split) == 2:\n                tags.append('image_tag:%s' % split[1])\n        try:\n            cont_labels = subcontainer['spec']['labels']\n        except KeyError:\n            self.log.debug(\"Subcontainer, doesn't have any labels\")\n            cont_labels = {}\n        if KubeUtil.NAMESPACE_LABEL in cont_labels and KubeUtil.POD_NAME_LABEL in cont_labels:\n            tags += self._get_post_1_2_tags(cont_labels, subcontainer, kube_labels)\n        elif KubeUtil.POD_NAME_LABEL in cont_labels:\n            tags += self._get_pre_1_2_tags(cont_labels, subcontainer, kube_labels)\n        else:\n            tags.append(\"pod_name:no_pod\")\n        is_filtered = self.kubeutil.are_tags_filtered(tags)\n        if is_filtered:\n            self._filtered_containers.add(subcontainer['id'])\n            return tags\n        stats = subcontainer['stats'][-1]  \n        self._publish_raw_metrics(NAMESPACE, stats, tags)\n        if subcontainer.get(\"spec\", {}).get(\"has_filesystem\") and stats.get('filesystem', []) != []:\n            fs = stats['filesystem'][-1]\n            if fs['capacity'] > 0:\n                fs_utilization = float(fs['usage'])/float(fs['capacity'])\n                self.publish_gauge(self, NAMESPACE + '.filesystem.usage_pct', fs_utilization, tags)\n            else:\n                self.log.debug(\"Filesystem capacity is 0: cannot report usage metrics.\")\n        if subcontainer.get(\"spec\", {}).get(\"has_network\"):\n            net = stats['network']\n            self.publish_rate(self, NAMESPACE + '.network_errors',\n                              sum(float(net[x]) for x in NET_ERRORS),\n                              tags)\n        return tags", "contrast": "def publish_subcontainer_metrics(subcontainer_name: str, metrics: dict, tags: list = []) -> bool:\n    return True", "label": 0}
{"index": "gp222367", "code": "def generate_changelog_markdown(version:str, changelog:dict, header:bool) -> str:\n    markdown = ''\n    if header:\n        markdown += f'# Changelog for version {version}\\n\\n'\n    for section in changelog:\n        markdown += f'## {section.capitalize()}\\n\\n'\n        for change in changelog[section]:\n            markdown += f'{change[\"type\"].capitalize()}: {change[\"description\"]}\\n'\n            if 'issues' in change:\n                markdown += f'  - Resolves #{\", #\".join(map(str, change[\"issues\"]))}\\n'\n            markdown += '\\n'\n    return markdown", "contrast": "def markdown_changelog(version: str, changelog: dict, header: bool = False) -> str:\n    debug('markdown_changelog(version=\"{}\", header={}, changelog=...)'.format(version, header))\n    output = ''\n    if header:\n        output += '## v{0}\\n'.format(version)\n    for section in CHANGELOG_SECTIONS:\n        if not changelog[section]:\n            continue\n        output += '\\n### {0}\\n'.format(section.capitalize())\n        for item in changelog[section]:\n            output += '* {0} ({1})\\n'.format(item[1], item[0])\n    return output", "label": 1}
{"index": "gp004443", "code": "def _get_obj_count_difference(objs1, objs2):\n    clean_obj_list1 = _process_in_memory_objects(objs1)\n    clean_obj_list2 = _process_in_memory_objects(objs2)\n    obj_count_1 = _get_object_count_by_type(clean_obj_list1)\n    obj_count_2 = _get_object_count_by_type(clean_obj_list2)\n    return obj_count_1 - obj_count_2", "contrast": "def collection_difference(collection1, collection2):\n    count1 = len(collection1)\n    count2 = len(collection2)\n    count_diff = abs(count1 - count2)\n    return count_diff", "label": 0}
{"index": "gp314321", "code": "def get_hypermap_endpoint(layer: str, original_endpoint: str, mapproxy_endpoint: str) -> str:\n    if layer == \"WM\":\n        return original_endpoint\n    else:\n        return mapproxy_endpoint", "contrast": "def get_url_endpoint(self):\n        endpoint = self.url\n        if self.type not in ('Hypermap:WorldMap',):\n            endpoint = 'registry/%s/layer/%s/map/wmts/1.0.0/WMTSCapabilities.xml' % (\n                self.catalog.slug,\n                self.id\n            )\n        return endpoint", "label": 1}
{"index": "gp123586", "code": "def rinseElement(self, ele):\n        if '*' in ele:\n            tmplist = ''.join(ele.split()).split('*')\n            tmplist_num = tmplist[[x.isdigit() for x in tmplist].index(True)]\n            tmplist_ele = tmplist[[x.isdigit() for x in tmplist].index(False)]\n            return dict(zip(('num', 'name'), (int(tmplist_num), tmplist_ele)))\n        else:\n            return dict(zip(('num', 'name'), (1, ele)))", "contrast": "def rinseElement(ele):\n    num, name = ele.split('*')\n    return {'num': num, 'name': name}", "label": 0}
{"index": "gp070173", "code": "def set_errors(self, result):\n        errors = result.get_messages()\n        for property_name in errors:\n            if not hasattr(self, property_name):\n                continue \n            prop_errors = errors[property_name]\n            if type(prop_errors) is not list:\n                prop_errors = ['<Nested schema result following...>']\n            if property_name in self.errors:\n                self.errors[property_name].extend(prop_errors)\n            else:\n                self.errors[property_name] = prop_errors", "contrast": "def populate_field_errors(field_errors, schema_errors):\n    for error in schema_errors:\n        field_errors[error['field']] = error['message']\n    return field_errors", "label": 0}
{"index": "gp106385", "code": "def create_selfsim(oracle, method='rsfx'):\n    len_oracle = oracle.n_states - 1\n    mat = np.zeros((len_oracle, len_oracle))\n    if method == 'com':\n        if not oracle.code:\n            print(\"Codes not generated. Generating codes with encode().\")\n            oracle.encode()\n        ind = 0  \n        for l, p in oracle.code:  \n            if l == 0:\n                inc = 1\n            else:\n                inc = l\n            mat[range(ind, ind + inc), range(p - 1, p - 1 + inc)] = 1\n            mat[range(p - 1, p - 1 + inc), range(ind, ind + inc)] = 1\n            ind = ind + l\n    elif method == 'sfx':\n        for i, s in enumerate(oracle.sfx[1:]):\n            if s != 0:\n                mat[i][s - 1] = 1\n                mat[s - 1][i] = 1\n    elif method == 'rsfx':\n        for cluster in oracle.latent:\n            p = itertools.product(cluster, repeat=2)\n            for _p in p:\n                mat[_p[0] - 1][_p[1] - 1] = 1\n    elif method == 'lrs':\n        for i, l in enumerate(oracle.lrs[1:]):\n            if l != 0:\n                s = oracle.sfx[i + 1]\n                mat[range((s - l) + 1, s + 1), range(i - l + 1, i + 1)] = 1\n                mat[range(i - l + 1, i + 1), range((s - l) + 1, s + 1)] = 1\n    elif method == 'seg':\n        seg = oracle.segment\n        ind = 0\n        for l, p in seg:  \n            if l == 0:\n                inc = 1\n            else:\n                inc = l\n            mat[range(ind, ind + inc), range(p - 1, p - 1 + inc)] = 1\n            mat[range(p - 1, p - 1 + inc), range(ind, ind + inc)] = 1\n            ind = ind + l\n    return mat", "contrast": "import numpy as np\ndef create_self_similarity_matrix(oracle, method):\n    if method == \"comp\":\n        matrix = np.zeros((len(oracle.comp_code), len(oracle.comp_code)))\n        for i in range(len(oracle.comp_code)):\n            for j in range(i, len(oracle.comp_code)):\n                matrix[i, j] = matrix[j, i] = oracle.comp_code[i].similarity(oracle.comp_code[j])\n    elif method == \"sfx\":\n        matrix = np.zeros((len(oracle.sfx_link), len(oracle.sfx_link)))\n        for i in range(len(oracle.sfx_link)):\n            for j in range(i, len(oracle.sfx_link)):\n                matrix[i, j] = matrix[j, i] = oracle.sfx_link[i].similarity(oracle.sfx_link[j])\n    elif method == \"rsfx\":\n        matrix = np.zeros((len(oracle.rsfx_link), len(oracle.rsfx_link)))\n        for i in range(len(oracle.rsfx_link)):\n            for j in range(i, len(oracle.rsfx_link)):\n                matrix[i, j] = matrix[j, i] = oracle.rsfx_link[i].similarity(oracle.rsfx_link[j])\n    elif method == \"lrs\":\n        matrix = np.zeros((len(oracle.lrs_val), len(oracle.lrs_val)))\n        for i in range(len(oracle.lrs_val)):\n            for j in range(i, len(oracle.lrs_val)):\n                matrix[i, j] = matrix[j, i] = oracle.lrs_val[i].similarity(oracle.lrs_val[j])\n    elif method == \"seg\":\n        matrix = np.zeros((len(oracle.patterns), len(oracle.patterns)))\n        for i in range(len(oracle.patterns)):\n            for j in range(i, len(oracle.patterns)):\n                matrix[i, j] = matrix[j, i] = oracle.patterns[i].similarity(oracle.patterns[j])\n    else:\n        raise ValueError(\"Invalid method\")\n    return matrix", "label": 0}
{"index": "gp140830", "code": "def python_2_nonzero_compatible(klass):\n    if six.PY2:\n        if '__bool__' not in klass.__dict__:\n            raise ValueError(\n                '@python_2_nonzero_compatible cannot be applied to {0} because '\n                'it doesn\\'t define __bool__().'.format(klass.__name__))\n        klass.__nonzero__ = klass.__bool__\n    return klass", "contrast": "def add_nonzero_method(klass):\n    if not hasattr(klass, '__bool__'):\n        raise TypeError(\"Given class does not define __bool__ method.\")\n    if not hasattr(klass, '__nonzero__'):\n        klass.__nonzero__ = klass.__bool__\n    return klass", "label": 0}
{"index": "gp055437", "code": "def tdot_blas(mat, out=None):\n    if (mat.dtype != 'float64') or (len(mat.shape) != 2):\n        return np.dot(mat, mat.T)\n    nn = mat.shape[0]\n    if out is None:\n        out = np.zeros((nn, nn))\n    else:\n        assert(out.dtype == 'float64')\n        assert(out.shape == (nn, nn))\n        assert(8 in out.strides)\n        out[:] = 0.0\n    mat = np.asfortranarray(mat)\n    out = blas.dsyrk(alpha=1.0, a=mat, beta=0.0, c=out, overwrite_c=1,\n                     trans=0, lower=0)\n    symmetrify(out, upper=True)\n    return np.ascontiguousarray(out)", "contrast": "import numpy as np\ndef fast_dot(mat):\n    return np.einsum('ij,jk->ik', mat, mat)", "label": 0}
{"index": "gp233975", "code": "def send_data(socket, data):\n    socket.sendall(data.encode())", "contrast": "def send(self, s):\n        self._print_header('======== Sending ({0}) ========'.format(len(s)))\n        self._log_send(s)\n        out = len(s)\n        while s:\n            s = s[self._send(s):]\n        return out", "label": 1}
{"index": "gp125593", "code": "def get_sort_function(order):\n    stable = tuple((d['key'], -1 if d['reverse'] else 1) for d in order)\n    def sort_function(a, b):\n        for name, direction in stable:\n            v = cmp(getattr(a, name) if a else a, getattr(b, name) if b else b)\n            if v != 0:\n                return v * direction\n        return 0\n    return sort_function", "contrast": "def make_comparator(keys):\n    def comparator(a, b):\n        for key in keys:\n            attr_a = getattr(a, key['key'])\n            attr_b = getattr(b, key['key'])\n            if attr_a != attr_b:\n                if key.get('reverse', False):\n                    return -1 if attr_a > attr_b else 1\n                else:\n                    return -1 if attr_a < attr_b else 1\n        return 0\n    return comparator", "label": 0}
{"index": "gp331925", "code": "def cast_to_type(text, _type):\n    try:\n        return _type(text)\n    except (ValueError, TypeError):\n        return None", "contrast": "def value(self,ascode=None):\n        if ascode is None:\n            ascode = self.code\n        return self.cast[ascode](self.text)", "label": 1}
{"index": "gp148782", "code": "def GetDateRange(self):\n    (minvalue, maxvalue, minorigin, maxorigin) = self.GetDateRangeWithOrigins()\n    return (minvalue, maxvalue)", "contrast": "from datetime import datetime\ndef service_dates(schedule):\n    start_date = datetime.max.date()\n    end_date = datetime.min.date()\n    for start, end in schedule:\n        if start < start_date:\n            start_date = start\n        if end > end_date:\n            end_date = end\n    return (start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\"))", "label": 0}
{"index": "gp314837", "code": "def add_final_message(profiler, message_list, final_message):\n    message_list.append(final_message)\n    if profiler and profiler.parent:\n        profiler.parent.add_message(message_list)\n    else:\n        message_list.clear()", "contrast": "def finish(self, msg=None):\n        if self._finished or self.disable:\n            return        \n        self._finished = True\n        if msg is not None:\n            self(msg)\n        self._new_msg(\"< Exiting %s, total time: %0.4f ms\", \n                      self._name, (ptime.time() - self._firstTime) * 1000)\n        type(self)._depth -= 1\n        if self._depth < 1:\n            self.flush()", "label": 1}
{"index": "gp006655", "code": "def child_value(self, name=None):\n        if name is None:\n            res = lib.lsl_child_value(self.e)\n        else:\n            res = lib.lsl_child_value_n(self.e, str.encode(name))\n        return res.decode('utf-8')", "contrast": "def get_child_value(node, name=None):\n    if name:\n        return node.findtext(name)\n    else:\n        for child in node:\n            if child.text is not None:\n                return child.text\n    return None", "label": 0}
{"index": "gp021405", "code": "def fetcher_loop_v1(data_queue, data_buffer, pin_memory=False,\n                    pin_device_id=0, data_buffer_lock=None):\n    while True:\n        idx, batch = data_queue.get()\n        if idx is None:\n            break\n        if pin_memory:\n            batch = _as_in_context(batch, context.cpu_pinned(pin_device_id))\n        else:\n            batch = _as_in_context(batch, context.cpu())\n        if data_buffer_lock is not None:\n            with data_buffer_lock:\n                data_buffer[idx] = batch\n        else:\n            data_buffer[idx] = batch", "contrast": "def fetcher_loop(queue, reorder_dict):\n    while queue:\n        data = queue.pop(0)\n        key = data['key']\n        value = data['value']\n        if key in reorder_dict:\n            reorder_dict[key].append(value)\n        else:\n            reorder_dict[key] = [value]", "label": 0}
{"index": "gp273081", "code": "def refine_dataset(dataset, **kwargs):\n    def sanitize(s):\n        return ''.join(c if c.isalnum() or c in '._' else '_' for c in s)\n    result = []\n    for item in dataset:\n        valid = True\n        for dim, condition in kwargs.items():\n            if callable(condition):\n                val = sanitize(str(item.get(dim, '')))\n                if not condition(val):\n                    valid = False\n                    break\n            else:\n                val = sanitize(str(item.get(dim, '')))\n                cond_val = sanitize(str(condition))\n                if val != cond_val:\n                    valid = False\n                    break\n        if valid:\n            result.append(item)\n    return result", "contrast": "def where(self, **kwargs):\n        clauses = copy(self.clauses)\n        for dimension, condition in kwargs.items():\n            if dimension in self.clauses:\n                raise Exception('There should be only one clause for {}'.format(dimension))\n            if dimension not in self.schema:\n                raise Exception('The dimension {} doesn\\'t exist'.format(dimension))\n            if isfunction(condition) or isinstance(condition, functools.partial):\n                clauses[dimension] = condition\n            else:\n                clauses[dimension] = functools.partial((lambda x, y: x == y), self._sanitize_dimension(str(condition)))\n        return self._copy(clauses=clauses)", "label": 1}
{"index": "gp310254", "code": "def debug_output(condition: str) -> str:\n    return condition.replace('[\"', '.').replace('\"][', '.').replace('\"]', '').replace('[', '.').replace(']', '')", "contrast": "def printable_name(column, path=None):\n    pieces = [column.name]\n    path = path or path_of(column)\n    for segment in path:\n        if isinstance(segment, str):\n            pieces.append(segment)\n        else:\n            pieces[-1] += \"[{}]\".format(segment)\n    return \".\".join(pieces)", "label": 1}
{"index": "gp158250", "code": "def prepend_string_list(self, key, value, max_length_key):\n        max_len = self.get(max_length_key)\n        strings = self.get_string_list(key)\n        strings = [value] + [x for x in strings if x != value]\n        strings = strings[:max_len]\n        self.beginWriteArray(key)\n        for i in range(len(strings)):\n            self.setArrayIndex(i)\n            self.setValue(\"entry\", strings[i])\n        self.endArray()", "contrast": "def prepend_string(string_list: List[str], new_string: str, max_length: int) -> List[str]:\n    if new_string in string_list:\n        string_list.remove(new_string)\n    string_list.insert(0, new_string)\n    return string_list[:max_length]", "label": 0}
{"index": "gp115149", "code": "def set_data(self, data):\n        if data is None:\n            raise errors.NullArgument('data cannot be None')\n        if not isinstance(data, DataInputStream):\n            raise errors.InvalidArgument('data must be instance of DataInputStream')\n        dbase = JSONClientValidated('repository',\n                                    runtime=self._runtime).raw()\n        filesys = gridfs.GridFS(dbase)\n        self._my_map['data'] = filesys.put(data._my_data)\n        data._my_data.seek(0)\n        self._my_map['base64'] = base64.b64encode(data._my_data.read())", "contrast": "def set_content_data(self, data):\n    if data is None:\n        raise NullArgument()\n    if not isinstance(data, osid.transport.DataInputStream):\n        raise InvalidArgument('data argument is not an instance of osid.transport.DataInputStream')\n    if Metadata.isReadOnly() is True:\n        raise NoAccess()", "label": 0}
{"index": "gp148070", "code": "def prep_rg_names(item, config, fc_name, fc_date):\n    if fc_name and fc_date:\n        lane_name = \"%s_%s_%s\" % (item[\"lane\"], fc_date, fc_name)\n    else:\n        lane_name = item[\"description\"]\n    return {\"rg\": item[\"description\"],\n            \"sample\": item[\"description\"],\n            \"lane\": lane_name,\n            \"pl\": (tz.get_in([\"algorithm\", \"platform\"], item)\n                   or tz.get_in([\"algorithm\", \"platform\"], item, \"illumina\")).lower(),\n            \"lb\": tz.get_in([\"metadata\", \"library\"], item),\n            \"pu\": tz.get_in([\"metadata\", \"platform_unit\"], item) or lane_name}", "contrast": "def generate_read_groups(*items):\n    read_groups = []\n    for item in items:\n        if isinstance(item, str):\n            item = [item]\n        for i in range(len(item)):\n            read_groups.extend([f\"{it}_R{j}\" for j in range(1, 3)])\n    return read_groups", "label": 0}
{"index": "gp017627", "code": "def _addsub_int_array(self, other, op):\n        assert not is_period_dtype(self)\n        assert op in [operator.add, operator.sub]\n        if self.freq is None:\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n        elif isinstance(self.freq, Tick):\n            td = Timedelta(self.freq)\n            return op(self, td * other)\n        assert not is_timedelta64_dtype(self)\n        return op(self, np.array(other) * self.freq)", "contrast": "import operator\nimport numpy as np\nfrom pandas import Index, ExtensionArray\ndef time_shift_array(other: (Index, ExtensionArray, np.ndarray), op: callable):\n    return other.__array_ufunc__(op, asarray=True)", "label": 0}
{"index": "gp249050", "code": "def create_form_base_new(request, form_class, template_name=None, success_url=None, extra_context=None):\n    if template_name is None:\n        template_name = 'form_base_new.html'\n    if success_url is None:\n        success_url = reverse_lazy('home')\n    if request.method == 'POST':\n        form = form_class(request.POST)\n        if form.is_valid():\n            form.save()\n            return redirect(success_url)\n    else:\n        form = form_class()\n    context = {\n        'form': form,\n    }\n    if extra_context is not None:\n        context.update(extra_context)\n    return render(request, template_name, context)", "contrast": "def create(self):\n        self.add_handlers({'^T': self.quit, '^Q': self.quit})\n        self.services_tft = self.add(npyscreen.TitleFixedText,\n                                     name='No services running.',\n                                     value='')\n        services = Services(self.core, external=self.external)\n        if services:\n            self.services_tft.hidden = True\n            for service in services:\n                value = ''\n                for val in service[1]:\n                    value += val+', '\n                self.add(npyscreen.TitleFixedText,\n                         name=service[0],\n                         value=value[:-2])", "label": 1}
{"index": "gp016700", "code": "def build(self):\n        keys = self._param_grid.keys()\n        grid_values = self._param_grid.values()\n        def to_key_value_pairs(keys, values):\n            return [(key, key.typeConverter(value)) for key, value in zip(keys, values)]\n        return [dict(to_key_value_pairs(keys, prod)) for prod in itertools.product(*grid_values)]", "contrast": "from itertools import product\ndef build_all_combinations(param_grid):\n    keys, values = zip(*param_grid.items())\n    combinations = product(*values)\n    for combination in combinations:\n        yield dict(zip(keys, combination))", "label": 0}
{"index": "gp141342", "code": "def results(self, limit=100):\n        limited = True if self.high_mark is not None else False\n        rmax = self.high_mark - self.low_mark if limited else None\n        rnum = 0\n        params = self.get_params()\n        params[\"offset\"] = self.low_mark\n        params[\"limit\"] = limit\n        while not limited and rmax is None or rnum < rmax:\n            if limited or rmax is not None:\n                rleft = rmax - rnum\n                params[\"limit\"] = rleft if rleft < limit else limit\n            r = self.resource._meta.api.http_resource(\"GET\", self.resource._meta.resource_name, params=params)\n            data = self.resource._meta.api.resource_deserialize(r.text)\n            if not limited:\n                rmax = data[\"meta\"][\"total_count\"]\n            if data[\"meta\"][\"total_count\"] < rmax:\n                rmax = data[\"meta\"][\"total_count\"]\n            params[\"offset\"] = data[\"meta\"][\"offset\"] + data[\"meta\"][\"limit\"]\n            for item in data[\"objects\"]:\n                rnum += 1\n                yield item", "contrast": "import requests\ndef api_results(url, params):\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    results = response.json()['results']\n    yield results\n    while response.json()['next']:\n        response = requests.get(response.json()['next'])\n        response.raise_for_status()\n        results = response.json()['results']\n        yield results", "label": 0}
{"index": "gp130502", "code": "def put_stream(self, rel_path, metadata=None, cb=None):\n        import Queue\n        import time\n        import threading\n        md5 = metadata.get('md5', None) if metadata else None\n        acl = ('public-read' if metadata.get('public',\n                                             False) else metadata.get('acl',\n                                                                      'public-read')) if metadata else 'public-read'\n        path = self._prefix(self._rename(rel_path))\n        class ThreadUploader(threading.Thread):\n            def __init__(self, n, queue):\n                threading.Thread.__init__(self)\n                self.n = n\n                self.queue = queue\n            def run(self):\n                while True:\n                    mp, part_number, buf = self.queue.get()\n                    if mp is None:  \n                        logger.debug(\n                            \"put_stream: Thread {} exiting\".format(\n                                self.n))\n                        self.queue.task_done()\n                        return\n                    logger.debug(\n                        \"put_stream: Thread {}: processing part: {}\".format(\n                            self.n,\n                            part_number))\n                    t1 = time.time()\n                    try:\n                        mp.upload_part_from_file(buf, part_number)\n                    finally:\n                        self.queue.task_done()\n                        t2 = time.time()\n                        logger.debug(\"put_stream: Thread {}, part {}. time = {} rate =  {} b/s\" .format(\n                            self.n, part_number, round(t2 - t1, 3), round((float(buf.tell()) / (t2 - t1)), 2)))\n        if metadata is None:\n            metadata = {}\n        if md5:\n            metadata['md5'] = md5  \n        for k, v in metadata.items():\n            if not v:\n                del metadata[k]\n        this = self\n        buffer_size = 50 * 1024 * 1024  \n        num_threads = 4\n        thread_upload_queue = Queue.Queue(maxsize=100)\n        for i in range(num_threads):\n            t = ThreadUploader(i, thread_upload_queue)\n            t.setDaemon(True)\n            t.start()\n        class flo:\n            def __init__(self, rel_path):\n                import io\n                self.mp = this.bucket.initiate_multipart_upload(\n                    path,\n                    metadata=metadata)\n                self.part_number = 1\n                self.buffer = io.BytesIO()\n                self.total_size = 0\n                self.rel_path = rel_path\n            def _send_buffer(self):\n                logger.debug(\n                    \"_send_buffer: sending part {} to thread pool size: {}, total_size = {}\" .format(\n                        self.part_number,\n                        self.buffer.tell(),\n                        self.total_size))\n                self.buffer.seek(0)\n                thread_upload_queue.put(\n                    (self.mp, self.part_number, self.buffer))\n            def write(self, d):\n                import io\n                self.buffer.write(d)  \n                self.total_size += len(d)\n                if self.buffer.tell() > buffer_size:\n                    self._send_buffer()\n                    self.part_number += 1\n                    self.buffer = io.BytesIO()\n            def writelines(self, lines):\n                raise NotImplemented()\n            def close(self):\n                if self.buffer.tell() > 0:\n                    self._send_buffer()\n                thread_upload_queue.join()\n                for i in range(num_threads):\n                    thread_upload_queue.put(\n                        (None, None, None))  \n                thread_upload_queue.join()\n                if self.total_size > 0:\n                    self.mp.complete_upload()\n                    this.bucket.set_acl(acl, path)\n                this.put_metadata(self.rel_path, metadata)\n            def __enter__(self):\n                return self\n            def __exit__(self, type_, value, traceback):\n                if type_:\n                    return False\n                self.close()\n        return flo(rel_path)", "contrast": "import boto3\ndef get_s3_flo(bucket, key):\n    s3_client = boto3.client('s3')\n    multipart_upload = s3_client.create_multipart_upload(Bucket=bucket, Key=key)\n    upload_id = multipart_upload['UploadId']\n    def write(data):\n        nonlocal upload_id\n        nonlocal s3_client\n        s3_client.upload_part(Body=data, Bucket=bucket, Key=key,\n                              PartNumber=len(parts) + 1, UploadId=upload_id)\n    parts = []\n    return write", "label": 0}
{"index": "gp062250", "code": "def clear_surroundings(self):\n        cells_to_clear = self.grd.eight_neighbors(self.current_y, self.current_x)\n        for cell in cells_to_clear:\n            self.grd.set_tile(cell[0], cell[1], ' ')", "contrast": "def clear_surrounding_cells(grid, agent_position):\n    row, col = agent_position\n    for i in range(row-1, row+2):\n        for j in range(col-1, col+2):\n            if i in range(len(grid)) and j in range(len(grid[0])):\n                grid[i][j] = 0", "label": 0}
{"index": "gp266256", "code": "def reset_atom_value(newval):\n    return newval", "contrast": "def reset(self, newval):\n        oldval = self._state.get()\n        self._state.set(newval)\n        self.notify_watches(oldval, newval)\n        return newval", "label": 1}
{"index": "gp190504", "code": "def generate_sequences(iteration_index):\n    if iteration_index < switchover_iterations:\n        seq_label = \"Sequence A\"\n        seq_list = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n    else:\n        seq_label = \"Sequence B\"\n        seq_list = [(10, 11, 12), (13, 14, 15), (16, 17, 18)]\n    return seq_list, seq_label", "contrast": "def getHighOrderSequenceChunk(it, switchover=1000, w=40, n=2048):\n  if it%10==3:\n    s = numpy.random.randint(5)\n    if it <= switchover:\n      if s==0:\n        label=\"XABCDE\"\n      elif s==1:\n        label=\"YCBEAF\"\n      elif s==2:\n        label=\"GHIJKL\"\n      elif s==3:\n        label=\"WABCMN\"\n      else:\n        label=\"ZDBCAE\"\n    else:\n      if s==0:\n        label=\"XCBEAF\"\n      elif s==1:\n        label=\"YABCDE\"\n      elif s==2:\n        label=\"GABCMN\"\n      elif s==3:\n        label=\"WHIJKL\"\n      else:\n        label=\"ZDHICF\"\n    vecs = letterSequence(label)\n  else:\n    vecs= [getRandomVector(w, n)]\n    label=\".\"\n  return vecs,label", "label": 1}
{"index": "gp309503", "code": "def find_link_html(attrs: dict, for_editor: bool) -> str:\n    link = \"https://example.com\"\n    html = \"<a href='{}'\".format(link)\n    for key, value in attrs.items():\n        html += \" {}='{}'\".format(key, value)\n    html += \">Link Text</a>\"\n    return html if not for_editor else f\"<span>{html}</span>\"", "contrast": "def expand_db_attributes(attrs, for_editor):\n        try:\n            editor_attrs    = ''\n            link            = Link.objects.get(id=attrs['id'])\n            if for_editor:\n                editor_attrs = 'data-linktype=\"link\" data-id=\"{0}\" '.format(\n                    link.id\n                )\n            return '<a {0}href=\"{1}\" title=\"{2}\">'.format(\n                editor_attrs,\n                escape(link.get_absolute_url()),\n                link.title\n            )\n        except Link.DoesNotExist:\n            return '<a>'", "label": 1}
{"index": "gp295775", "code": "def serialize(obj):\n    if isinstance(obj, (list, tuple)):\n        return [serialize(item) for item in obj]\n    elif isinstance(obj, dict):\n        return {key: serialize(val) for key, val in obj.items()}\n    elif isinstance(obj, str):\n        return obj\n    else:\n        return str(obj)", "contrast": "def _serialize_iterable(obj):\n    if isinstance(obj, (tuple, set)):\n        obj = list(obj)\n    for item in obj:\n        obj[obj.index(item)] = serialize_obj(item)\n    return obj", "label": 1}
{"index": "gp125092", "code": "def insert_row(dbconn, tablename, *args):\n    cur = dbconn.cursor()\n    cur.execute(\"INSERT INTO '{name}' VALUES{args}\".format(name=tablename, args=args))\n    dbconn.commit()", "contrast": "def insert_row(dbconn, table_name, *args):\n    sql_query = \"INSERT INTO \" + table_name + \" VALUES \" + str(args)\n    cursor = dbconn.cursor()\n    cursor.execute(sql_query)\n    dbconn.commit() \n    cursor.close()", "label": 0}
{"index": "gp290584", "code": "def get_sync_status(package: str) -> str:\n    owner, repo, pkg = package.split('/')\n    return 'synced'  ", "contrast": "def status(ctx, opts, owner_repo_package):\n    owner, repo, slug = owner_repo_package\n    click.echo(\n        \"Getting status of %(package)s in %(owner)s/%(repo)s ... \"\n        % {\n            \"owner\": click.style(owner, bold=True),\n            \"repo\": click.style(repo, bold=True),\n            \"package\": click.style(slug, bold=True),\n        },\n        nl=False,\n    )\n    context_msg = \"Failed to get status of package!\"\n    with handle_api_exceptions(ctx, opts=opts, context_msg=context_msg):\n        with maybe_spinner(opts):\n            res = get_package_status(owner, repo, slug)\n            ok, failed, _, status_str, stage_str, reason = res\n    click.secho(\"OK\", fg=\"green\")\n    if not stage_str:\n        package_status = status_str\n    else:\n        package_status = \"%(status)s / %(stage)s\" % {\n            \"status\": status_str,\n            \"stage\": stage_str,\n        }\n    if ok:\n        status_colour = \"green\"\n    elif failed:\n        status_colour = \"red\"\n    else:\n        status_colour = \"magenta\"\n    click.secho(\n        \"The package status is: %(status)s\"\n        % {\"status\": click.style(package_status, fg=status_colour)}\n    )\n    if reason:\n        click.secho(\n            \"Reason given: %(reason)s\" % {\"reason\": click.style(reason, fg=\"yellow\")},\n            fg=status_colour,\n        )", "label": 1}
{"index": "gp245226", "code": "def send_message():\n    while True:\n        try:\n            break\n        except WebsocketClosedError:\n            run() ", "contrast": "def send(self, data):\n        while not self.stopped():\n            try:\n                self.ws.send(data)\n                return\n            except websocket.WebSocketConnectionClosedException:\n                time.sleep(0.1)", "label": 1}
{"index": "gp046082", "code": "def overpass_request(data, pause_duration=None, timeout=180, error_pause_duration=None):\n    url = 'http://overpass-api.de/api/interpreter'\n    prepared_url = requests.Request('GET', url, params=data).prepare().url\n    cached_response_json = get_from_cache(prepared_url)\n    if cached_response_json is not None:\n        return cached_response_json\n    else:\n        if pause_duration is None:\n            this_pause_duration = get_pause_duration()\n        log('Pausing {:,.2f} seconds before making API POST request'.format(this_pause_duration))\n        time.sleep(this_pause_duration)\n        start_time = time.time()\n        log('Posting to {} with timeout={}, \"{}\"'.format(url, timeout, data))\n        response = requests.post(url, data=data, timeout=timeout, headers=get_http_headers())\n        size_kb = len(response.content) / 1000.\n        domain = re.findall(r'//(?s)(.*?)/', url)[0]\n        log('Downloaded {:,.1f}KB from {} in {:,.2f} seconds'.format(size_kb, domain, time.time()-start_time))\n        try:\n            response_json = response.json()\n            if 'remark' in response_json:\n                log('Server remark: \"{}\"'.format(response_json['remark'], level=lg.WARNING))\n            save_to_cache(prepared_url, response_json)\n        except Exception:\n            if response.status_code in [429, 504]:\n                if error_pause_duration is None:\n                    error_pause_duration = get_pause_duration()\n                log('Server at {} returned status code {} and no JSON data. Re-trying request in {:.2f} seconds.'.format(domain,\n                                                                                                                         response.status_code,\n                                                                                                                         error_pause_duration),\n                                                                                                                         level=lg.WARNING)\n                time.sleep(error_pause_duration)\n                response_json = overpass_request(data=data, pause_duration=pause_duration, timeout=timeout)\n            else:\n                log('Server at {} returned status code {} and no JSON data'.format(domain, response.status_code), level=lg.ERROR)\n                raise Exception('Server returned no JSON data.\\n{} {}\\n{}'.format(response, response.reason, response.text))\n        return response_json", "contrast": "import requests\nimport time\ndef send_request_to_overpass_api(data, pause_duration=None, timeout=180, error_pause_duration=30):\n    if pause_duration is not None:\n        time.sleep(pause_duration)\n    url = \"http://overpass-api.de/api/interpreter\"\n    try:\n        response = requests.post(url, data=data, timeout=timeout)\n        result = response.json()\n        return result\n    except requests.exceptions.RequestException as e:\n        print(\"Error: \", e)\n        time.sleep(error_pause_duration)\n        send_request_to_overpass_api(data=data, pause_duration=None, timeout=timeout, error_pause_duration=error_pause_duration)", "label": 0}
{"index": "gp321047", "code": "import logging\ndef log_info(msg, *args, **kwargs):\n    logging.basicConfig(level=logging.INFO)\n    logging.info(msg, *args, **kwargs)", "contrast": "def info(self, msg, *args, **kwargs):\n    self._baseLogger.info(self, self.getExtendedMsg(msg), *args, **kwargs)", "label": 1}
{"index": "gp167895", "code": "def main(argv=None):\n    if argv is None:\n        argv = sys.argv\n    parser = U.OptionParser(version=\"%prog version: $Id$\",\n                            usage=usage,\n                            description=globals()[\"__doc__\"])\n    group = U.OptionGroup(parser, \"count-specific options\")\n    parser.add_option(\"--wide-format-cell-counts\", dest=\"wide_format_cell_counts\",\n                      action=\"store_true\",\n                      default=False,\n                      help=(\"output the cell counts in a wide format \"\n                            \"(rows=genes, columns=cells)\"))\n    parser.add_option_group(group)\n    (options, args) = U.Start(parser, argv=argv, add_group_dedup_options=False)\n    options.per_gene = True  \n    U.validateSamOptions(options, group=False)\n    if options.random_seed:\n        np.random.seed(options.random_seed)\n    if options.stdin != sys.stdin:\n        in_name = options.stdin.name\n        options.stdin.close()\n    else:\n        raise ValueError(\"Input on standard in not currently supported\")\n    if options.in_sam:\n        in_mode = \"r\"\n    else:\n        in_mode = \"rb\"\n    infile = pysam.Samfile(in_name, in_mode)\n    tmpfilename = U.getTempFilename(dir=options.tmpdir)\n    tmpfile = U.openFile(tmpfilename, mode=\"w\")\n    nInput, nOutput, input_reads = 0, 0, 0\n    gene_tag = options.gene_tag\n    metacontig2contig = None\n    if options.chrom:\n        inreads = infile.fetch(reference=options.chrom)\n    else:\n        if options.gene_transcript_map:\n            metacontig2contig = sam_methods.getMetaContig2contig(\n                infile, options.gene_transcript_map)\n            metatag = \"MC\"\n            inreads = sam_methods.metafetcher(infile, metacontig2contig, metatag)\n            gene_tag = metatag\n        else:\n            inreads = infile.fetch()\n    bundle_iterator = sam_methods.get_bundles(\n        options,\n        only_count_reads=True,\n        metacontig_contig=metacontig2contig)\n    processor = network.UMIClusterer(options.method)\n    for bundle, key, status in bundle_iterator(inreads):\n        if status == \"single_read\":\n            continue\n        gene, cell = key\n        umis = bundle.keys()\n        counts = {umi: bundle[umi][\"count\"] for umi in umis}\n        nInput += sum(counts.values())\n        while nInput >= input_reads + 1000000:\n            input_reads += 1000000\n            U.info(\"Parsed %i input reads\" % input_reads)\n        groups = processor(\n            counts,\n            threshold=options.threshold)\n        gene_count = len(groups)\n        if options.per_cell:\n            tmpfile.write(\"%s\\n\" % \"\\t\".join((gene, cell.decode(), str(gene_count))))\n        else:\n            tmpfile.write(\"%s\\n\" % \"\\t\".join((gene, str(gene_count))))\n        nOutput += gene_count\n    tmpfile.close()\n    if options.per_cell:\n        gene_counts_dict = {}\n        with U.openFile(tmpfilename, mode=\"r\") as inf:\n            genes = set()\n            cells = set()\n            for line in inf:\n                gene, cell, gene_count = line.strip().split(\"\\t\")\n                genes.add(gene)\n                cells.add(cell)\n                if gene not in gene_counts_dict:\n                    gene_counts_dict[gene] = {}\n                gene_counts_dict[gene][cell] = gene_count\n        if options.wide_format_cell_counts:  \n            options.stdout.write(\n                \"%s\\t%s\\n\" % (\"gene\", \"\\t\".join(sorted(cells))))\n            for gene in sorted(genes):\n                counts = []\n                for cell in sorted(cells):\n                    if cell in gene_counts_dict[gene]:\n                        counts.append(gene_counts_dict[gene][cell])\n                    else:\n                        counts.append(0)\n                options.stdout.write(\n                    \"%s\\t%s\\n\" % (gene, \"\\t\".join(map(str, counts))))\n        else:  \n            options.stdout.write(\"%s\\t%s\\t%s\\n\" % (\"gene\", \"cell\", \"count\"))\n            for gene in sorted(genes):\n                for cell in sorted(list(gene_counts_dict[gene].keys())):\n                    options.stdout.write(\"%s\\t%s\\t%s\\n\" % (\n                        gene, cell, gene_counts_dict[gene][cell]))\n    else:\n        options.stdout.write(\"%s\\t%s\\n\" % (\"gene\", \"count\"))\n        with U.openFile(tmpfilename, mode=\"r\") as inf:\n            for line in inf:\n                options.stdout.write(line)\n    os.unlink(tmpfilename)\n    for event in bundle_iterator.read_events.most_common():\n        U.info(\"%s: %s\" % (event[0], event[1]))\n    U.info(\"Number of (post deduplication) reads counted: %i\" % nOutput)\n    U.Stop()", "contrast": "import argparse\nimport sys\ndef script_main(argv=None):\n    if argv is None:\n        argv = sys.argv[1:]\n    parser = argparse.ArgumentParser(description='Script main')\n    args = parser.parse_args(argv)", "label": 0}
{"index": "gp137732", "code": "def get_local_annotations(\n            cls, target, exclude=None, ctx=None, select=lambda *p: True\n    ):\n        result = []\n        exclude = () if exclude is None else exclude\n        try:\n            local_annotations = get_local_property(\n                target, Annotation.__ANNOTATIONS_KEY__, result, ctx=ctx\n            )\n            if not local_annotations:\n                if ismethod(target):\n                    func = get_method_function(target)\n                    local_annotations = get_local_property(\n                        func, Annotation.__ANNOTATIONS_KEY__,\n                        result, ctx=ctx\n                    )\n                    if not local_annotations:\n                        local_annotations = get_local_property(\n                            func, Annotation.__ANNOTATIONS_KEY__,\n                            result\n                        )\n                elif isfunction(target):\n                    local_annotations = get_local_property(\n                        target, Annotation.__ANNOTATIONS_KEY__,\n                        result\n                    )\n        except TypeError:\n            raise TypeError('target {0} must be hashable'.format(target))\n        for local_annotation in local_annotations:\n            inherited = isinstance(local_annotation, cls)\n            not_excluded = not isinstance(local_annotation, exclude)\n            selected = select(target, ctx, local_annotation)\n            if inherited and not_excluded and selected:\n                result.append(local_annotation)\n        return result", "contrast": "def get_local_annotations(cls, target, exclude=(type(None),), ctx=None, select=lambda target, ctx, ann: True):\n    annotations = []\n    if target is None:\n        return annotations\n    if ctx is None:\n        ctx = target\n    for name in dir(target):\n        try:\n            value = getattr(target, name)\n            if isinstance(value, cls):\n                for cls_exclude in exclude:\n                    if isinstance(value, cls_exclude):\n                        break\n                else:\n                    if select(target, ctx, value):\n                        annotations.append(value)\n        except AttributeError:\n            pass\n    return annotations", "label": 0}
{"index": "gp093510", "code": "def get_last_live_chat(self):\n        now = datetime.now()\n        lcqs = self.get_query_set()\n        lcqs = lcqs.filter(\n            chat_ends_at__lte=now,\n            ).order_by('-chat_ends_at')\n        for itm in lcqs:\n            if itm.chat_ends_at + timedelta(days=3) > now:\n                return itm\n        return None", "contrast": "import datetime\ndef get_recent_live_chat():\n    recent_chats = LiveChat.objects.filter(end_date__gte=datetime.datetime.today()-datetime.timedelta(days=3))\n    if recent_chats.exists():\n        return recent_chats.latest('end_date')\n    else:\n        return None", "label": 0}
{"index": "gp230468", "code": "def dissociate_values_or_ranges(vlan_id_range):\n        vlans = []\n        for value_or_range in vlan_id_range.split(\",\"):\n            if \"-\" in value_or_range:\n                start, end = value_or_range.split(\"-\")\n                vlans.extend(range(int(start), int(end)+1))\n            else:\n                vlans.append(int(value_or_range))\n        return vlans\nenet = []\nenet = dissociate_values_or_ranges('1-2,5')\nprint(enet) ", "contrast": "def dissociate_values_or_ranges(self, vlan_id_range):\n        values_or_ranges = vlan_id_range.split(',')\n        vlan_ids = []\n        if len(values_or_ranges) == 1 and '-' not in values_or_ranges[0]:\n            vlan_ids = list(range(1, int(values_or_ranges[0]) + 1))\n        else:\n            for value_or_range in values_or_ranges:\n                value_or_range.strip()\n                if '-' not in value_or_range:\n                    vlan_ids.append(int(value_or_range))\n                else:\n                    start, end = value_or_range.split('-')\n                    range_ids = range(int(start), int(end) + 1)\n                    vlan_ids.extend(range_ids)\n        return vlan_ids", "label": 1}
{"index": "gp010285", "code": "def argmax(self, C):\n        ind = np.absolute(C).argmax()\n        return np.unravel_index(ind, C.shape)", "contrast": "import numpy as np\ndef get_ArgMax(C):\n    indices = np.unravel_index(C.argmax(), C.shape)\n    return indices", "label": 0}
{"index": "gp280105", "code": "def authenticate(oauth_token):\n    if oauth_token == 'VALID_TOKEN':\n        return 'ACS_TOKEN'\n    else:\n        return None", "contrast": "def authenticate_oauth(oauth_token):\n    url = _gen_url('acs/api/v1/auth/login')\n    creds = {\n        'token': oauth_token\n    }\n    response = dcos.http.request('post', url, json=creds)\n    if response.status_code == 200:\n        return response.json()['token']\n    else:\n        return None", "label": 1}
{"index": "gp233221", "code": "from Bio import Entrez\ndef recover_entrez_data(record, expected, batchsize, *fnargs, **fnkwargs):\n    retstart = 0\n    results = []\n    while retstart < expected:\n        handle = Entrez.efetch(db=record['db'], webhistory=record['query_key'], query_key=record['query_key'], retstart=retstart, retmax=batchsize, *fnargs, **fnkwargs)\n        batch = handle.read()\n        results.append(batch)\n        retstart += batchsize\n    return results", "contrast": "def entrez_batch_webhistory(record, expected, batchsize, *fnargs, **fnkwargs):\n    results = []\n    for start in range(0, expected, batchsize):\n        batch_handle = entrez_retry(\n            Entrez.efetch,\n            retstart=start,\n            retmax=batchsize,\n            webenv=record[\"WebEnv\"],\n            query_key=record[\"QueryKey\"],\n            *fnargs,\n            **fnkwargs)\n        batch_record = Entrez.read(batch_handle, validate=False)\n        results.extend(batch_record)\n    return results", "label": 1}
{"index": "gp067437", "code": "def replace(self, uid, ical, filename=None):\n        return self.replace_vobject(uid, readOne(ical), filename)", "contrast": "def update_remind_command(uid, icalendar_file):\n    with open('remind_file', 'r+') as file:\n        remind_commands = file.readlines()\n        for i, remind_command in enumerate(remind_commands):\n            if uid in remind_command:\n                remind_commands[i] = f'REM {uid} MSG {icalendar_file}\\n'\n                break\n        file.seek(0)\n        file.writelines(remind_commands)\n        file.truncate()", "label": 0}
{"index": "gp127610", "code": "def removeChild(self, child, end_tag_too=True):\n        if _is_iterable(child):\n            for x in child:\n                self.removeChild(child=x, end_tag_too=end_tag_too)\n            return\n        if not self.childs:\n            return\n        end_tag = None\n        if end_tag_too:\n            end_tag = child.endtag\n        for e in self.childs:\n            if e != child:\n                e.removeChild(child, end_tag_too)\n                continue\n            if end_tag_too and end_tag in self.childs:\n                self.childs.remove(end_tag)\n            self.childs.remove(e)", "contrast": "def remove_child(child, end_tag_too=True):\n    parent = child.parentNode\n    parent.removeChild(child)\n    if end_tag_too:\n        end_tag = parent.ownerDocument.createTextNode(child.outerHTML.rsplit(child.innerHTML, 1)[-1])\n        parent.insertBefore(end_tag, child.nextSibling)", "label": 0}
{"index": "gp255994", "code": "def generate_dic(items, sep):\n    dic = {}\n    for item in items:\n        key, value = item\n        dic.setdefault(key, []).append(value)\n    for key, value in dic.items():\n        yield key, sep.join(value)", "contrast": "def _parsed_items(items, sep=_SEP, **options):\n    parse = _parse if options.get(\"ac_parse_value\") else anyconfig.utils.noop\n    for key, val in items:\n        yield (key, parse(val, sep))", "label": 1}
{"index": "gp270470", "code": "def submit_completion(service_user, course):\n    return (BlockCompletion(), True)", "contrast": "def submit_completion(self, block_key, completion):\n        return BlockCompletion.objects.submit_completion(\n            user=self._user,\n            course_key=self._course_key,\n            block_key=block_key,\n            completion=completion\n        )", "label": 1}
{"index": "gp270592", "code": "import os\ndef get_metadata(file_path):\n    return os.stat(file_path)", "contrast": "def get_file_metadata(self, secure_data_path, version=None):\n        if not version:\n            version = \"CURRENT\"\n        payload = {'versionId': str(version)}\n        secret_resp = head_with_retry(str.join('', [self.cerberus_url, '/v1/secure-file/', secure_data_path]),\n                                    params=payload, headers=self.HEADERS)\n        throw_if_bad_response(secret_resp)\n        return secret_resp.headers", "label": 1}
{"index": "gp090805", "code": "def _parse_message(self, data):\n        try:\n            _, values = data.split(':')\n            self.serial_number, self.value = values.split(',')\n            self.value = int(self.value, 16)\n            is_bit_set = lambda b: self.value & (1 << (b - 1)) > 0\n            self.battery = is_bit_set(2)\n            self.supervision = is_bit_set(3)\n            self.loop[2] = is_bit_set(5)\n            self.loop[1] = is_bit_set(6)\n            self.loop[3] = is_bit_set(7)\n            self.loop[0] = is_bit_set(8)\n        except ValueError:\n            raise InvalidMessageError('Received invalid message: {0}'.format(data))", "contrast": "def parse_message(data):\n    try:\n    except:\n        raise InvalidMessageError('Invalid message data')", "label": 0}
{"index": "gp334067", "code": "def add_weight_to_learner(learner):\n    def weighted_learner(x, y, w):\n        return learner(x, y)\n    return weighted_learner", "contrast": "def WeightedLearner(unweighted_learner):\n    def train(dataset, weights):\n        return unweighted_learner(replicated_dataset(dataset, weights))\n    return train", "label": 1}
{"index": "gp095108", "code": "def read_json_document(title):\n    if not title.endswith('.json'):\n        juicer.utils.Log.log_warn(\"File name (%s) does not end with '.json', appending it automatically.\" % title)\n        title += '.json'\n    if not os.path.exists(title):\n        raise IOError(\"Could not find file: '%s'\" % title)\n    f = open(title, 'r')\n    doc = f.read()\n    f.close()\n    return load_json_str(doc)", "contrast": "import json\ndef read_json_file(file_path):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    return data", "label": 0}
{"index": "gp008084", "code": "def clean_year_month(year, month, month_orig):\n    error = False\n    error_msg = \"The date given was invalid.\"\n    if month_orig not in xrange(1, 13) and month_orig is not None:\n        month = now.month\n        error = error_msg\n    while month > 12:\n        month -= 12\n        year += 1\n    while month < 1:\n        month += 12\n        year -= 1\n    year, month, error = _check_year(year, month, error, error_msg)\n    return year, month, error", "contrast": "import datetime\ndef validate_month(month_orig, year):\n    current_year = datetime.datetime.now().year\n    current_month = datetime.datetime.now().month\n    if abs(current_year - year) > 50:\n        return \"Year out of range.\"\n    if month_orig not in range(1, 13):\n        return \"Invalid month.\"\n    if abs(current_year - year) == 50 and month_orig > current_month:\n        return \"Month out of range.\"\n    if month_orig != month_orig % 12:\n        year += (month_orig-1) // 12\n        month_orig = ((month_orig-1) % 12)+1\n    return month_orig, year", "label": 0}
{"index": "gp047497", "code": "def fill_and_wait_models(bufsize=EXAMPLES_PER_GENERATION,\n                         write_dir=None,\n                         threads=8,\n                         model_window=100,\n                         skip_first_rsync=False):\n    write_dir = write_dir or fsdb.golden_chunk_dir()\n    buf = ExampleBuffer(bufsize)\n    models = fsdb.get_models()[-model_window:]\n    if not skip_first_rsync:\n        with timer(\"Rsync\"):\n            smart_rsync(models[-1][0] - 6)\n    files = tqdm(map(files_for_model, models), total=len(models))\n    buf.parallel_fill(list(itertools.chain(*files)), threads=threads)\n    print(\"Filled buffer, watching for new games\")\n    while fsdb.get_latest_model()[0] == models[-1][0]:\n        with timer(\"Rsync\"):\n            smart_rsync(models[-1][0] - 2)\n        new_files = tqdm(map(files_for_model, models[-2:]), total=len(models))\n        buf.update(list(itertools.chain(*new_files)))\n        time.sleep(60)\n    latest = fsdb.get_latest_model()\n    print(\"New model!\", latest[1], \"!=\", models[-1][1])\n    print(buf)\n    buf.flush(os.path.join(write_dir, str(latest[0] + 1) + '.tfrecord.zz'))", "contrast": "def update_ringbuffer():\n    ringbuffer = initialize_ringbuffer()\n    while True:\n        ringbuffer = rsync_and_update_ringbuffer(ringbuffer)\n        if new_model_detected():\n            dump_ringbuffer_for_training(ringbuffer)\n            break", "label": 0}
{"index": "gp084211", "code": "def create_element_tree(elem_or_name=None, text=None, **attribute_kwargs):\n    if elem_or_name is None:\n        return ElementTree()\n    is_elem = isinstance(elem_or_name, ElementType)\n    element = elem_or_name if is_elem else Element(elem_or_name)\n    if text is not None:\n        element.text = text\n    element.attrib.update(attribute_kwargs)\n    return ElementTree(element)", "contrast": "from xml.etree import ElementTree as ET\ndef create_element_tree(elem_or_name=None, text=None, **attribute_kwargs):\n    root = None\n    if elem_or_name is None:\n        root = ET.Element('')\n    elif isinstance(elem_or_name, ET.Element):\n        root = elem_or_name\n    else:\n        root = ET.Element(elem_or_name)\n    if text:\n        root.text = text\n    for attribute_name, attribute_value in attribute_kwargs.items():\n        root.set(attribute_name, attribute_value)\n    return ET.ElementTree(root)", "label": 0}
{"index": "gp158334", "code": "def del_hyperedge(self, hyperedge):\n        if (hyperedge in self.hyperedges()):\n            for n in self.edge_links[hyperedge]:\n                self.node_links[n].remove(hyperedge)\n            del(self.edge_links[hyperedge])\n            self.del_edge_labeling(hyperedge)\n            self.graph.del_node((hyperedge,'h'))", "contrast": "def delete_hyperedge(hyperedge):\n    del hyperedge", "label": 0}
{"index": "gp261519", "code": "def count_instances(objects):\n    count = {}\n    for obj in objects:\n        class_name = obj.__class__.__name__\n        count[class_name] = count.get(class_name, 0) + 1\n    return count", "contrast": "def gcstats():\n    all = gc.get_objects()\n    _stats = {}\n    for obj in all:\n        K = type(obj)\n        if K is StatsDelta:\n            continue  \n        elif K is InstanceType:  \n            K = getattr(obj, '__class__', K)\n        K = str(K)\n        try:\n            _stats[K] += 1\n        except KeyError:\n            _stats[K] = 1\n    del all\n    return _stats", "label": 1}
{"index": "gp236307", "code": "def delete_template(template_id):\n    return status_code", "contrast": "def delete_template(self, template_id):\n        url = self.TEMPLATE_DELETE_URL\n        request = self._get_request()\n        response = request.post(url + template_id, get_json=False)\n        return response", "label": 1}
{"index": "gp102898", "code": "def list_build_configurations_for_product_version(product_id, version_id, page_size=200, page_index=0, sort=\"\", q=\"\"):\n    data = list_build_configurations_for_project_raw(product_id, version_id, page_size, page_index, sort, q)\n    if data:\n        return utils.format_json_list(data)", "contrast": "def get_build_configurations(product_version):\n    build_configs = []\n    return build_configs", "label": 0}
{"index": "gp131053", "code": "def load(self, cause=None, do_message=True):\n        if cause is not None:\n            self.cause = cause\n            if do_message:\n                self.message = error_message(self, self.base_message, cause)\n        self.exc_type, self.exc_value, self.exc_traceback = sys.exc_info()\n        self.traceback_formatted = traceback.format_exc()\n        self.traceback = traceback.extract_tb(self.exc_traceback)", "contrast": "def load_exception_data():\n    return sys.exc_info()", "label": 0}
{"index": "gp227654", "code": "allowed_types = []\ndef add_type(association_type, session_type):\n    allowed_types.append((association_type, session_type))", "contrast": "def addAllowedType(self, assoc_type, session_type=None):\n        if self.allowed_types is None:\n            self.allowed_types = []\n        if session_type is None:\n            available = getSessionTypes(assoc_type)\n            if not available:\n                raise ValueError('No session available for association type %r'\n                                 % (assoc_type,))\n            for session_type in getSessionTypes(assoc_type):\n                self.addAllowedType(assoc_type, session_type)\n        else:\n            checkSessionType(assoc_type, session_type)\n            self.allowed_types.append((assoc_type, session_type))", "label": 1}
{"index": "gp041929", "code": "def remove_rules(self, description):\n        rm = []\n        description = description.lower()\n        for i in range(0, len(self.extract_rules)):\n            if self.extract_rules[i]['regex'].search(description):\n                rm.append(i)\n        for i in rm:\n            self.extract_rules.pop(i)\n        return len(rm)", "contrast": "def remove_rules(description):\n    count = 0\n    for rule in rules_list:\n        if rule.description == description:\n            rules_list.remove(rule)\n            count += 1\n    return count", "label": 0}
{"index": "gp041953", "code": "def get_my_contacts(self):\n        my_contacts = self.wapi_functions.getMyContacts()\n        return [Contact(contact, self) for contact in my_contacts]", "contrast": "def fetch_added_contacts() -> list:\n    contacts = [\n        {'name': 'Alice', 'phone': '123456789'},\n        {'name': 'Bob', 'phone': '987654321'},\n        {'name': 'Charlie', 'phone': '456123789'}\n    ]\n    return contacts", "label": 0}
{"index": "gp037643", "code": "def _gather_buffer_space():\n    if HAS_PSUTIL and psutil.version_info >= (0, 6, 0):\n        total_mem = psutil.virtual_memory().total\n    else:\n        import platform\n        import salt.grains.core\n        os_data = {'kernel': platform.system()}\n        grains = salt.grains.core._memdata(os_data)\n        total_mem = grains['mem_total'] * 1024 * 1024\n    return max([total_mem * 0.05, 10 << 20])", "contrast": "import psutil\ndef calculate_buffer_space():\n    system_cpu_usage = psutil.cpu_percent()\n    system_memory_usage = psutil.virtual_memory().percent\n    total_disk_space = psutil.disk_usage('/').total\n    free_disk_space = psutil.disk_usage('/').free\n    used_disk_space = total_disk_space - free_disk_space\n    buffer_space = used_disk_space * (system_cpu_usage + system_memory_usage)\n    return buffer_space", "label": 0}
{"index": "gp275798", "code": "def compose_src_dict(name:str, paramsonly:bool=False, reoptimize:bool=False, npts:int=10) -> dict:\n    src_dict = {\n        'name': name,\n        'paramsonly': paramsonly,\n        'reoptimize': reoptimize,\n        'npts': npts\n    }\n    return src_dict", "contrast": "def get_src_model(self, name, paramsonly=False, reoptimize=False,\n                      npts=None, **kwargs):\n        self.logger.debug('Generating source dict for ' + name)\n        optimizer = kwargs.get('optimizer', self.config['optimizer'])\n        if npts is None:\n            npts = self.config['gtlike']['llscan_npts']\n        name = self.get_source_name(name)\n        source = self.like[name].src\n        spectrum = source.spectrum()\n        normPar = self.like.normPar(name)\n        src_dict = defaults.make_default_dict(defaults.source_flux_output)\n        src_dict.update({'name': name,\n                         'pivot_energy': 1000.,\n                         'ts': np.nan,\n                         'loglike': np.nan,\n                         'npred': 0.0,\n                         'npred_wt': 0.0,\n                         'loglike_scan': np.nan * np.ones(npts),\n                         'dloglike_scan': np.nan * np.ones(npts),\n                         'eflux_scan': np.nan * np.ones(npts),\n                         'flux_scan': np.nan * np.ones(npts),\n                         'norm_scan': np.nan * np.ones(npts),\n                         })\n        src_dict.update(gtutils.gtlike_spectrum_to_vectors(spectrum))\n        src_dict['spectral_pars'] = gtutils.get_function_pars_dict(spectrum)\n        src_dict['model_counts'] = self.model_counts_spectrum(\n            name, summed=True)\n        src_dict['model_counts_wt'] = self.model_counts_spectrum(\n            name, summed=True, weighted=True)\n        src_dict['npred'] = self.like.NpredValue(str(name))\n        try:\n            src_dict['npred_wt'] = self.like.NpredValue(str(name), True)\n        except (TypeError, NotImplementedError):\n            src_dict['npred_wt'] = src_dict['npred']\n        try:\n            thesrc = self.like[name]\n            src_dict['flux'] = self.like.flux(name, self.energies[0],\n                                              self.energies[-1])\n            src_dict['flux100'] = self.like.flux(name, 100., 10 ** 5.5)\n            src_dict['flux1000'] = self.like.flux(name, 1000., 10 ** 5.5)\n            src_dict['flux10000'] = self.like.flux(name, 10000., 10 ** 5.5)\n            src_dict['eflux'] = self.like.energyFlux(name,\n                                                     self.energies[0],\n                                                     self.energies[-1])\n            src_dict['eflux100'] = self.like.energyFlux(name, 100.,\n                                                        10 ** 5.5)\n            src_dict['eflux1000'] = self.like.energyFlux(name, 1000.,\n                                                         10 ** 5.5)\n            src_dict['eflux10000'] = self.like.energyFlux(name, 10000.,\n                                                          10 ** 5.5)\n            src_dict['dnde'] = self.like[name].spectrum()(\n                pyLike.dArg(src_dict['pivot_energy']))\n            src_dict['dnde100'] = self.like[name].spectrum()(\n                pyLike.dArg(100.))\n            src_dict['dnde1000'] = self.like[name].spectrum()(\n                pyLike.dArg(1000.))\n            src_dict['dnde10000'] = self.like[name].spectrum()(\n                pyLike.dArg(10000.))\n            if normPar.getValue() == 0:\n                normPar.setValue(1.0)\n                dnde_index = -get_spectral_index(self.like[name],\n                                                 src_dict['pivot_energy'])\n                dnde100_index = -get_spectral_index(self.like[name],\n                                                    100.)\n                dnde1000_index = -get_spectral_index(self.like[name],\n                                                     1000.)\n                dnde10000_index = -get_spectral_index(self.like[name],\n                                                      10000.)\n                normPar.setValue(0.0)\n            else:\n                dnde_index = -get_spectral_index(self.like[name],\n                                                 src_dict['pivot_energy'])\n                dnde100_index = -get_spectral_index(self.like[name],\n                                                    100.)\n                dnde1000_index = -get_spectral_index(self.like[name],\n                                                     1000.)\n                dnde10000_index = -get_spectral_index(self.like[name],\n                                                      10000.)\n            src_dict['dnde_index'] = dnde_index\n            src_dict['dnde100_index'] = dnde100_index\n            src_dict['dnde1000_index'] = dnde1000_index\n            src_dict['dnde10000_index'] = dnde10000_index\n        except Exception:\n            self.logger.error('Failed to update source parameters.',\n                              exc_info=True)\n        if not self.get_free_source_params(name) or paramsonly:\n            return src_dict\n        emax = 10 ** 5.5\n        try:\n            src_dict['flux_err'] = self.like.fluxError(name,\n                                                       self.energies[0],\n                                                       self.energies[-1])\n            src_dict['flux100_err'] = self.like.fluxError(name, 100., emax)\n            src_dict['flux1000_err'] = self.like.fluxError(name, 1000., emax)\n            src_dict['flux10000_err'] = self.like.fluxError(name, 10000., emax)\n            src_dict['eflux_err'] =                self.like.energyFluxError(name, self.energies[0],\n                                          self.energies[-1])\n            src_dict['eflux100_err'] = self.like.energyFluxError(name, 100.,\n                                                                 emax)\n            src_dict['eflux1000_err'] = self.like.energyFluxError(name, 1000.,\n                                                                  emax)\n            src_dict['eflux10000_err'] = self.like.energyFluxError(name, 10000.,\n                                                                   emax)\n        except Exception:\n            pass\n        lnlp = self.profile_norm(name, savestate=True,\n                                 reoptimize=reoptimize, npts=npts,\n                                 optimizer=optimizer)\n        src_dict['loglike_scan'] = lnlp['loglike']\n        src_dict['dloglike_scan'] = lnlp['dloglike']\n        src_dict['eflux_scan'] = lnlp['eflux']\n        src_dict['flux_scan'] = lnlp['flux']\n        src_dict['norm_scan'] = lnlp['xvals']\n        src_dict['loglike'] = np.max(lnlp['loglike'])\n        flux_ul_data = utils.get_parameter_limits(\n            lnlp['flux'], lnlp['dloglike'])\n        eflux_ul_data = utils.get_parameter_limits(\n            lnlp['eflux'], lnlp['dloglike'])\n        if normPar.getValue() == 0:\n            normPar.setValue(1.0)\n            flux = self.like.flux(name, self.energies[0], self.energies[-1])\n            flux100 = self.like.flux(name, 100., emax)\n            flux1000 = self.like.flux(name, 1000., emax)\n            flux10000 = self.like.flux(name, 10000., emax)\n            eflux = self.like.energyFlux(name, self.energies[0],\n                                         self.energies[-1])\n            eflux100 = self.like.energyFlux(name, 100., emax)\n            eflux1000 = self.like.energyFlux(name, 1000., emax)\n            eflux10000 = self.like.energyFlux(name, 10000., emax)\n            flux100_ratio = flux100 / flux\n            flux1000_ratio = flux1000 / flux\n            flux10000_ratio = flux10000 / flux\n            eflux100_ratio = eflux100 / eflux\n            eflux1000_ratio = eflux1000 / eflux\n            eflux10000_ratio = eflux10000 / eflux\n            normPar.setValue(0.0)\n        else:\n            flux100_ratio = src_dict['flux100'] / src_dict['flux']\n            flux1000_ratio = src_dict['flux1000'] / src_dict['flux']\n            flux10000_ratio = src_dict['flux10000'] / src_dict['flux']\n            eflux100_ratio = src_dict['eflux100'] / src_dict['eflux']\n            eflux1000_ratio = src_dict['eflux1000'] / src_dict['eflux']\n            eflux10000_ratio = src_dict['eflux10000'] / src_dict['eflux']\n        src_dict['flux_ul95'] = flux_ul_data['ul']\n        src_dict['flux100_ul95'] = flux_ul_data['ul'] * flux100_ratio\n        src_dict['flux1000_ul95'] = flux_ul_data['ul'] * flux1000_ratio\n        src_dict['flux10000_ul95'] = flux_ul_data['ul'] * flux10000_ratio\n        src_dict['eflux_ul95'] = eflux_ul_data['ul']\n        src_dict['eflux100_ul95'] = eflux_ul_data['ul'] * eflux100_ratio\n        src_dict['eflux1000_ul95'] = eflux_ul_data['ul'] * eflux1000_ratio\n        src_dict['eflux10000_ul95'] = eflux_ul_data['ul'] * eflux10000_ratio\n        fd = None\n        try:\n            fd = FluxDensity.FluxDensity(self.like, name)\n            src_dict['covar'] = fd.covar\n        except RuntimeError:\n            pass\n        if fd and len(src_dict['covar']) and src_dict['covar'].ndim >= 1:\n            loge = np.linspace(self.log_energies[0],\n                               self.log_energies[-1], 50)\n            src_dict['model_flux'] = self.bowtie(name, fd=fd, loge=loge)\n            src_dict['dnde100_err'] = fd.error(100.)\n            src_dict['dnde1000_err'] = fd.error(1000.)\n            src_dict['dnde10000_err'] = fd.error(10000.)\n            src_dict['pivot_energy'] = src_dict['model_flux']['pivot_energy']\n            e0 = src_dict['pivot_energy']\n            src_dict['dnde'] = self.like[name].spectrum()(pyLike.dArg(e0))\n            src_dict['dnde_err'] = fd.error(e0)\n        if not reoptimize:\n            src_dict['ts'] = self.like.Ts2(name, reoptimize=reoptimize)\n        else:\n            src_dict['ts'] = -2.0 * lnlp['dloglike'][0]\n        return src_dict", "label": 1}
{"index": "gp075699", "code": "def normalize_genotypes(genotypes):\n    genotypes = genotypes.genotypes\n    return (genotypes - np.nanmean(genotypes)) / np.nanstd(genotypes)", "contrast": "def normalize_genotypes(genotypes):\n    normalized_genotypes = []\n    for genotype in genotypes:\n        normalized_genotype = genotype/sum(genotype)\n        normalized_genotypes.append(normalized_genotype)\n    return numpy.array(normalized_genotypes)", "label": 0}
{"index": "gp230317", "code": "from flask_cors import CORS\ndef enable_cors(routing_entity, config):\n    return CORS(routing_entity, **config)", "contrast": "def add(self,\n            routing_entity,\n            config: _ConfigType = None,\n            webview: bool=False):\n        if webview:\n            warnings.warn('webview argument is deprecated, '\n                          'views are handled authomatically without '\n                          'extra settings',\n                          DeprecationWarning,\n                          stacklevel=2)\n        return self._cors_impl.add(routing_entity, config)", "label": 1}
{"index": "gp138789", "code": "def p_case(self,t):\n    if len(t)==4: t[0] = CaseX(t[2],None)\n    elif len(t)==6: t[0] = CaseX(t[2],t[4])\n    else: raise NotImplementedError('unk_len',len(t)) ", "contrast": "def expression(case, whenlist, else_case=None):\n    if else_case is not None:\n        return case.get(whenlist[-1], else_case)\n    else:\n        return case.get(whenlist[-1])", "label": 0}
{"index": "gp152088", "code": "def adb_cmd(self, command, **kwargs):\n        kwargs['timeout'] = kwargs.get('timeout', self._adb_shell_timeout)\n        if isinstance(command, list) or isinstance(command, tuple):\n            return self.adb_device.run_cmd(*list(command), **kwargs)\n        return self.adb_device.run_cmd(command, **kwargs)", "contrast": "import subprocess\ndef adb(command):\n    if isinstance(command, str):\n        command = command.split()\n    adb_cmd = ['adb'] + command\n    output = subprocess.check_output(adb_cmd)\n    return output.decode('utf-8')", "label": 0}
{"index": "gp106177", "code": "def mergeidngram(output_file, input_files, n=3, ascii_input=False, ascii_output=False):\n    cmd = ['mergeidngram']\n    if n:\n        cmd.extend(['-n', n])\n    if ascii_input:\n        cmd.append('-ascii_input')\n    if ascii_output:\n        cmd.append('-ascii_output')\n    if len(input_file) > 1:\n        raise MergeError(\"mergeidngram needs at least 1 input file\")\n    cmd.extend(input_files)\n    cmd = [str(x) for x in cmd]\n    with open(output_file,'w+') as output_f:\n        with  output_to_debuglogger() as err_f:\n            exitcode = subprocess.call(cmd, stdout=output_f, stderr=err_f)\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))", "contrast": "import struct\ndef merge_id_ngram_files(files, is_ascii=False):\n    ngram_set = set()\n    mode = 'rb' if not is_ascii else 'r'\n    for file in files:\n        with open(file, mode) as f:\n            if not is_ascii:\n                size_bytes = f.read(4)\n                while size_bytes:\n                    size = struct.unpack('i', size_bytes)[0]\n                    ngram = f.read(size).decode('utf-8')\n                    ngram_set.add(ngram)\n                    size_bytes = f.read(4)\n            else:\n                for line in f:\n                    ngram_set.add(line.strip())\n    return ngram_set", "label": 0}
{"index": "gp016388", "code": "def find_gui_and_backend(gui=None):\n    import matplotlib\n    if gui and gui != 'auto':\n        backend = backends[gui]\n    else:\n        backend = matplotlib.rcParams['backend']\n        gui = backend2gui.get(backend, None)\n    return gui, backend", "contrast": "def get_gui_backend(gui: str) -> Tuple[str, str]:\n    backends_dict = {\n        'tk': 'TkAgg',\n        'gtk': 'GTKAgg',\n        'wx': 'WXAgg',\n        'qt': 'Qt4Agg',\n        'qt4': 'Qt4Agg',\n        'inline': 'module://IPython.zmq.pylab.backend_inline'\n    }\n    return gui, backends_dict[gui.lower()]", "label": 0}
{"index": "gp296281", "code": "def rename_and_alter_column(table_name: str, old_col_name: str, new_col_name: str, new_col_definition: str):\n    query = f\"ALTER TABLE {table_name} RENAME COLUMN {old_col_name} TO {new_col_name};\"\n    query += f\"ALTER TABLE {table_name} ALTER COLUMN {new_col_name} {new_col_definition};\"\n    execute_query(query)", "contrast": "def change_column_if_table_exists(self,\n                                      tablename: str,\n                                      oldfieldname: str,\n                                      newfieldname: str,\n                                      newdef: str) -> Optional[int]:\n        if not self.table_exists(tablename):\n            return None\n        if not self.column_exists(tablename, oldfieldname):\n            return None\n        sql = \"ALTER TABLE {t} CHANGE COLUMN {old} {new} {newdef}\".format(\n            t=tablename,\n            old=oldfieldname,\n            new=newfieldname,\n            newdef=newdef,\n        )\n        log.info(sql)\n        return self.db_exec_literal(sql)", "label": 1}
{"index": "gp252254", "code": "from decimal import Decimal\ndef create_decimal_field(default=0, required=False, repr=True, cmp=True, key=None):\n    return {'type': Decimal, 'default': default, 'required': required, 'repr': repr, 'cmp': cmp, 'key': key}", "contrast": "def DecimalField(default=NOTHING, required=True, repr=True, cmp=True,\n                 key=None):\n    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, Decimal)\n    return attrib(default=default, converter=lambda x: Decimal(x),\n                  validator=validator, repr=repr, cmp=cmp,\n                  metadata=dict(key=key))", "label": 1}
{"index": "gp221865", "code": "def deserialize(attr, dict_type):\n    if isinstance(attr, list):\n        return dict(attr)\n    elif isinstance(attr, dict):\n        return {dict_type(k): v for k, v in attr.items()}\n    else:\n        raise TypeError(\"Parameter 'attr' must be a dictionary or list of key-value pairs.\")", "contrast": "def deserialize_dict(self, attr, dict_type):\n        if isinstance(attr, list):\n            return {x['key']: self.deserialize_data(x['value'], dict_type) for x in attr}\n        if isinstance(attr, ET.Element):\n            attr = {el.tag: el.text for el in attr}\n        return {k: self.deserialize_data(v, dict_type) for k, v in attr.items()}", "label": 1}
{"index": "gp048979", "code": "def DownloadDir(aff4_path, output_dir, bufsize=8192, preserve_path=True):\n  if not os.path.isdir(output_dir):\n    os.makedirs(output_dir)\n  fd = aff4.FACTORY.Open(aff4_path)\n  for child in fd.OpenChildren():\n    if preserve_path:\n      full_dir = utils.JoinPath(output_dir, child.urn.Path())\n      full_dir = os.path.dirname(full_dir)\n      if not os.path.isdir(full_dir):\n        os.makedirs(full_dir)\n      outfile = os.path.join(full_dir, child.urn.Basename())\n    else:\n      outfile = os.path.join(output_dir, child.urn.Basename())\n    logging.info(u\"Downloading %s to %s\", child.urn, outfile)\n    with open(outfile, \"wb\") as out_fd:\n      try:\n        buf = child.Read(bufsize)\n        while buf:\n          out_fd.write(buf)\n          buf = child.Read(bufsize)\n      except IOError as e:\n        logging.error(\"Failed to read %s. Err: %s\", child.urn, e)", "contrast": "import os\nimport stat\nfrom typing import List\ndef download_aff4(aff4_path: str, output_dir: str, bufsize: int = 8192, preserve_path: bool = False) -> List[str]:\n    from pyaff4 import data_store\n    from pyaff4 import aff4_image\n    os.makedirs(output_dir, exist_ok=True)\n    aff4_files = []\n    with aff4_image.ImageFactoryOpen(aff4_path) as aff4:\n        for child in aff4:\n            if isinstance(child, aff4_image.AFF4Volume):\n                if not preserve_path:\n                    continue\n                prefix = output_dir if output_dir.endswith(os.sep) else f\"{output_dir}{os.sep}\"\n                path = f\"{prefix}{child.urn.path}\"\n                os.makedirs(path, exist_ok=True)\n                for stream in child:\n                    stream_path = os.path.join(path, os.path.basename(stream.urn.value))\n                    with open(stream_path, \"wb+\") as f:\n                        for chunk in stream.Read(len(stream)):\n                            f.write(chunk)\n                            aff4_files.append(stream_path)                       \n            else:\n                prefix = output_dir if output_dir.endswith(os.sep) else f\"{output_dir}{os.sep}\"\n                file_path = f\"{prefix}{child.urn.path}\"\n                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n                with open(file_path, \"wb+\") as f:\n                    for chunk in child.Read(len(child)):\n                        f.write(chunk)\n                        aff4_files.append(file_path)\n                os.chmod(file_path, stat.S_IRUSR | stat.S_IWUSR)\n    return aff4_files", "label": 0}
{"index": "gp034582", "code": "def getObjectTypeBit(self, t):\n        if isinstance(t, string_types):\n            t = t.upper()\n            try:\n                return self.objectType[t]\n            except KeyError:\n                raise CommandExecutionError((\n                    'Invalid object type \"{0}\".  It should be one of the following:  {1}'\n                    ).format(t, ', '.join(self.objectType)))\n        else:\n            return t", "contrast": "def get_bit_value(string_obj):\n    return ''.join(format(ord(c), '08b') for c in string_obj)", "label": 0}
{"index": "gp266476", "code": "def import_scraper(scraper_name):\n    from scraper_registry import registry\n    if scraper_name not in registry:\n        raise ValueError('Scraper \"{}\" not found in registry.'.format(scraper_name))\n    scraper_module = registry[scraper_name]\n    scraper = scraper_module.Scraper()\n    return scraper", "contrast": "def get_scraper(mod_path, scraper_type):\n    try:\n        module = importlib.import_module(mod_path)\n    except ImportError as e:\n        raise ScrapeError(\"could not import %s\" % mod_path, e)\n    ScraperClass = None\n    for k, v in module.__dict__.items():\n        if k.startswith('_'):\n            continue\n        if getattr(v, 'scraper_type', None) == scraper_type:\n            if ScraperClass:\n                raise ScrapeError(\"two %s scrapers found in module %s: %s %s\" %\n                                  (scraper_type, mod_path, ScraperClass, k))\n            ScraperClass = v\n    if not ScraperClass:\n        raise ScrapeError(\"no %s scraper found in module %s\" % (\n            scraper_type, mod_path))\n    return ScraperClass", "label": 1}
{"index": "gp153713", "code": "def newPage(doc, pno=-1, width=595, height=842):\n    doc._newPage(pno, width=width, height=height)\n    return doc[pno]", "contrast": "def create_page():\n    return Page()", "label": 0}
{"index": "gp320039", "code": "def parse_complex_list(input_list):\n    parsed_dict = {}\n    for complex_struct in input_list:\n        for key, value in complex_struct.items():\n            if key in parsed_dict:\n                parsed_dict[key] += str(value)\n            else:\n                parsed_dict[key] = str(value)\n    return parsed_dict", "contrast": "def _parse_complex_list(self, prop):\n        xpath_root = self._get_xroot_for(prop)\n        xpath_map = self._data_structures[prop]\n        return parse_complex_list(self._xml_tree, xpath_root, xpath_map, prop)", "label": 1}
{"index": "gp040768", "code": "def formatResults(self, op, results):\n        formatted_results = \"\"\n        if op == 'add':\n            formatted_results += \"user(s) added:\\n\"\n            for user in results:\n                if isinstance(user, str):\n                    formatted_results += \"identifier: %s\\n\" % user\n                else:\n                    formatted_results += \"uid: %d\\n\\n\" % user\n        elif op == 'remove':\n            formatted_results += \"user(s) removed:\\n\"\n            for user in results:\n                if user:\n                    formatted_results += \"identifier: %s\\n\" % (user)\n        elif op == 'update':\n            formatted_results += \"user(s) updated:\\n\"\n            for user in results:\n                if user:\n                    formatted_results += \"identifier: %s\\n\" % (user)\n        elif op == 'get':\n            formatted_results += \"user(s) found:\\n\"\n            for user in results:\n                if user:\n                    for key in sorted(user.keys()):\n                        if key != 'bb_password':\n                            formatted_results += \"%s: %s\\n\" % (key, user[key])\n                    formatted_results += \"\\n\"\n                else:\n                    formatted_results += \"no match found\\n\"\n        return formatted_results", "contrast": "def format_db_results(op, results):\n    if op == 'add':\n        return f\"{results[0]} added successfully with ID {results[1]}.\"\n    elif op == 'remove':\n        return f\"{results[0]} with ID {results[1]} removed successfully.\"\n    elif op == 'update':\n        return f\"{results[0]} with ID {results[1]} updated successfully.\"\n    elif op == 'get':\n        if len(results) == 0:\n            return \"No results found.\"\n        else:\n            return \"\\n\".join([f\"{i[0]} - {i[1]}\" for i in results])", "label": 0}
{"index": "gp132108", "code": "def get_box_files(self, box_key):\n  uri = '/'.join([self.api_uri,\n      self.boxes_suffix,\n      box_key,\n      self.files_suffix\n      ])\n  return self._req('get', uri)", "contrast": "def get_file_infos(box_key):\n    file_infos = [{'name': 'file1.txt', 'size': '10KB'}, {'name': 'file2.png', 'size': '2MB'}, {'name': 'file3.mp3', 'size': '5MB'}]\n    return 200, file_infos", "label": 0}
{"index": "gp311257", "code": "def apply_config_overrides(graph, overrides):\n    for key, value in overrides.items():\n        parts = key.split('.')\n        obj = graph\n        for part in parts[:-1]:\n            obj = getattr(obj, part)\n        setattr(obj, parts[-1], value)", "contrast": "def handle_overrides(graph, overrides):\n    for key in overrides:\n        levels = key.split('.')\n        part = graph\n        for lvl in levels[:-1]:\n            try:\n                part = part[lvl]\n            except KeyError:\n                raise KeyError(\"'%s' override failed at '%s'\", (key, lvl))\n        try:\n            part[levels[-1]] = overrides[key]\n        except KeyError:\n            raise KeyError(\"'%s' override failed at '%s'\", (key, levels[-1]))", "label": 1}
{"index": "gp025048", "code": "async def on_error(self, event_method, *args, **kwargs):\n        print('Ignoring exception in {}'.format(event_method), file=sys.stderr)\n        traceback.print_exc()", "contrast": "async def default_error_handler(self, event, *args, **kwargs):\n    print(f\"Ignoring exception in {event}\", file=sys.stderr)\n    traceback.print_exc()", "label": 0}
{"index": "gp092755", "code": "def get_blocks(self):\n        self.process_triple_quoted_doc_string()\n        num_indented_blocks = 0\n        for indentation, block in self._blocks:\n            if indentation > 0:\n                num_indented_blocks += 1\n            yield indentation, block\n        if self.expects_body_or_pass and num_indented_blocks == 0:\n            yield 1, \"pass\"\n            return", "contrast": "class MyClass:\n    def my_func(self):\n        self.process_triple_quoted_doc_string()\n        pass", "label": 0}
{"index": "gp277188", "code": "def update_dict(d1, d2):\n    for key, value in d2.items():\n        if key in d1:\n            if isinstance(value, dict) and isinstance(d1[key], dict):\n                update_dict(d1[key], value)\n            elif '__delete__' in value:\n                del d1[key]\n            else:\n                d1[key] = value\n        else:\n            d1[key] = value\n    return d1", "contrast": "def update(d1, d2):\n    NoneType = type(None)\n    if d2.get(\"__delete__\", False):\n        return {}\n    for k, v in d2.items():\n        if isinstance(v, dict):\n            if v.get(\"__delete__\", False):\n                del d1[k]\n            else:\n                d1[k] = update(d1.get(k, {}), v)\n        elif isinstance(v, (tuple, list)) and all(isinstance(li, (NoneType, dict)) for li in v):\n            orig_list = d1.get(k, [])\n            new_list = []\n            pairs = list(zip_longest(orig_list, v, fillvalue=None))\n            for orig_item, new_item in pairs:\n                if orig_item is None:\n                    orig_item = {}  \n                if new_item is None:\n                    new_item = {}\n                if new_item.get(\"__delete__\", False):\n                    d = None  \n                else:\n                    d = update(orig_item, new_item)\n                if d is not None:\n                    new_list.append(d)\n            d1[k] = new_list\n        else:\n            if k in d1 and v == \"__delete__\":\n                del d1[k]\n            else:\n                d1[k] = v\n    return d1", "label": 1}
{"index": "gp172381", "code": "def resolve_forward_references(routes, resolved_routes=None):\n    if resolved_routes is None:\n        resolved_routes = {}\n    for route, dependencies in routes.items():\n        resolved_dependencies = []\n        for dependency in dependencies:\n            if dependency in routes:\n                resolved_dependency = resolve_forward_references({dependency: routes[dependency]}, resolved_routes)[dependency]\n            else:\n                resolved_dependency = dependency\n            resolved_dependencies.append(resolved_dependency)\n        resolved_routes[route] = resolved_dependencies\n    return resolved_routes", "contrast": "def _populate_route_attributes(self):\n        route_schema = self._validate_stone_cfg()\n        self.api.add_route_schema(route_schema)\n        for namespace in self.api.namespaces.values():\n            env = self._get_or_create_env(namespace.name)\n            for route in namespace.routes:\n                self._populate_route_attributes_helper(env, route, route_schema)", "label": 1}
{"index": "gp148687", "code": "def _validate_label(self, label):\n        letter_pattern = compile(\"^[a-z]{1}$\")\n        number_pattern = compile(\"^[0]{1}$|^[1-9]{1,2}$\")\n        icon_pattern = compile(\"^[a-zA-Z ]{1,}$\")\n        if not match(letter_pattern, label)            and not match(number_pattern, label)                and not match(icon_pattern, label):\n                    raise InvalidLabelError(\n                        \"{} is not a valid label\".format(label)\n                    )\n        return label", "contrast": "def validate_label(label):\n    if not isinstance(label, str):\n        raise TypeError(\"Label must be a string.\")\n    if not label.isalnum():\n        raise ValueError(\"Label must be alphanumeric.\")", "label": 0}
{"index": "gp011125", "code": "def wrapHeart(service):\n    master = taservice.MultiService()\n    service.setServiceParent(master)\n    maybeAddHeart(master)\n    return master", "contrast": "from twisted.application.service import MultiService\nfrom twisted.application.internet import TimerService, TCPServer\ndef wrap_service_with_heart(service, interval_seconds):\n    multi_service = MultiService()\n    timer_service = TimerService(interval_seconds, service.checkHeart)\n    tcp_service = TCPServer(0, service, interface=\"127.0.0.1\")\n    multi_service.addService(timer_service)\n    multi_service.addService(tcp_service)\n    return multi_service", "label": 0}
{"index": "gp139574", "code": "def save(self, update_site=False, *args, **kwargs):\n        if update_site or (self.id is None and self.site_id is None):\n            self.site_id = current_site_id()\n        super(SiteRelated, self).save(*args, **kwargs)", "contrast": "def set_site(record, current_site, update_site=False):\n    if update_site or not record.site:\n        record.site = current_site", "label": 0}
{"index": "gp132995", "code": "def childgroup(self, field):\n        grid = getattr(self, \"grid\", None)\n        named_grid = getattr(self, \"named_grid\", None)\n        if grid is not None:\n            childgroup = self._childgroup(field.children, grid)\n        elif named_grid is not None:\n            childgroup = self._childgroup_by_name(field.children, named_grid)\n        else:\n            raise AttributeError(u\"Missing the grid or named_grid argument\")\n        return childgroup", "contrast": "def get_row_fields(field):\n    row_fields = []\n    return row_fields", "label": 0}
{"index": "gp251994", "code": "def parse_team_abbreviation(stats):\n    team_name_tag = stats(\"div.sidearm-team-name>span.longname\")\n    team_abbreviation = team_name_tag.attr(\"class\").split(\"-\")[-1].upper()\n    return team_abbreviation", "contrast": "def _parse_team_abbreviation(self, stats):\n        team_tag = stats(PLAYER_SCHEME['team_abbreviation'])\n        team = re.sub(r'.*/cbb/schools/', '', str(team_tag('a')))\n        team = re.sub(r'/.*', '', team)\n        return team", "label": 1}
{"index": "gp293650", "code": "import base64\ndef authorize_to_registry(username: str, password: str, token_scope: str):\n    auth_string = f\"{username}:{password}\"\n    auth_enc = base64.b64encode(auth_string.encode('utf-8')).decode('utf-8')\n    headers = {\"Authorization\": f\"Basic {auth_enc}\"}\n    params = {\"scope\": token_scope}\n    return {\"headers\": headers, \"params\": params}", "contrast": "async def login(\n        sess: aiohttp.ClientSession,\n        registry_url: yarl.URL,\n        credentials: dict,\n        scope: str) -> dict:\n    if credentials.get('username') and credentials.get('password'):\n        basic_auth = aiohttp.BasicAuth(\n            credentials['username'], credentials['password'],\n        )\n    else:\n        basic_auth = None\n    realm = registry_url / 'token'  \n    service = 'registry'            \n    async with sess.get(registry_url / 'v2/', auth=basic_auth) as resp:\n        ping_status = resp.status\n        www_auth_header = resp.headers.get('WWW-Authenticate')\n        if www_auth_header:\n            match = re.search(r'realm=\"([^\"]+)\"', www_auth_header)\n            if match:\n                realm = match.group(1)\n            match = re.search(r'service=\"([^\"]+)\"', www_auth_header)\n            if match:\n                service = match.group(1)\n    if ping_status == 200:\n        log.debug('docker-registry: {0} -> basic-auth', registry_url)\n        return {'auth': basic_auth, 'headers': {}}\n    elif ping_status == 404:\n        raise RuntimeError(f'Unsupported docker registry: {registry_url}! '\n                           '(API v2 not implemented)')\n    elif ping_status == 401:\n        params = {\n            'scope': scope,\n            'offline_token': 'true',\n            'client_id': 'docker',\n            'service': service,\n        }\n        async with sess.get(realm, params=params, auth=basic_auth) as resp:\n            log.debug('docker-registry: {0} -> {1}', registry_url, realm)\n            if resp.status == 200:\n                data = json.loads(await resp.read())\n                token = data.get('token', None)\n                return {'auth': None, 'headers': {\n                    'Authorization': f'Bearer {token}'\n                }}\n    raise RuntimeError('authentication for docker registry '\n                       'f{registry_url} failed')", "label": 1}
{"index": "gp198416", "code": "def store_job_info(job_info_artifact, job_details):\n    job_details['job_info'] = job_info_artifact", "contrast": "def store_job_info_artifact(job, job_info_artifact):\n    job_details = json.loads(job_info_artifact['blob'])['job_details']\n    for job_detail in job_details:\n        job_detail_dict = {\n            'title': job_detail.get('title'),\n            'value': job_detail['value'],\n            'url': job_detail.get('url')\n        }\n        for (k, v) in job_detail_dict.items():\n            max_field_length = JobDetail._meta.get_field(k).max_length\n            if v is not None and len(v) > max_field_length:\n                logger.warning(\"Job detail '%s' for job_guid %s too long, truncating\",\n                               v[:max_field_length], job.guid)\n                job_detail_dict[k] = v[:max_field_length]\n        job_detail_dict['defaults'] = {'url': job_detail_dict['url']}\n        del job_detail_dict['url']\n        JobDetail.objects.update_or_create(\n            job=job,\n            **job_detail_dict)", "label": 1}
{"index": "gp043967", "code": "def get_identifier(identifier, module_globals, module_name):\n    if isinstance(identifier, six.string_types):\n        fn = module_globals.get(identifier)\n        if fn is None:\n            raise ValueError('Unknown {}: {}'.format(module_name, identifier))\n        return fn\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret identifier')", "contrast": "def get_callable(identifier, module_globals, module_name):\n    if isinstance(identifier, str):\n        return module_globals[identifier]\n    elif callable(identifier):\n        return identifier\n    else:\n        raise TypeError(f\"{identifier} is not a string or callable\")", "label": 0}
{"index": "gp268721", "code": "from typing import List, Dict\ndef list_users(limit: int, order: str, after: str, filters: Dict[str, str]) -> List[User]:\n    pass", "contrast": "def list_users(self, **kwargs):\n        kwargs = self._verify_sort_options(kwargs)\n        kwargs = self._verify_filters(kwargs, User)\n        api = self._get_api(iam.AccountAdminApi)\n        return PaginatedResponse(api.get_all_users, lwrap_type=User, **kwargs)", "label": 1}
{"index": "gp183947", "code": "def process_sphinx_gallery_config(config):\n    processed_config = {}\n    if 'reference_url' in config:\n        processed_config['reference_url'] = config['reference_url']\n    if 'gallery_dirs' in config:\n        processed_config['gallery_dirs'] = config['gallery_dirs']\n    if 'examples_dirs' in config:\n        processed_config['examples_dirs'] = config['examples_dirs']\n    if 'filename_pattern' in config:\n        processed_config['filename_pattern'] = config['filename_pattern']\n    if 'line_numbers' in config:\n        processed_config['line_numbers'] = config['line_numbers']\n    if 'backreferences_dir' in config:\n        processed_config['backreferences_dir'] = config['backreferences_dir']\n    if 'doc_module' in config:\n        processed_config['doc_module'] = config['doc_module']\n    if 'subsection_order' in config:\n        processed_config['subsection_order'] = config['subsection_order']\n    if 'within_subsection_order' in config:\n        processed_config['within_subsection_order'] = config['within_subsection_order']\n    if 'gallery_menu' in config:\n        processed_config['gallery_menu'] = config['gallery_menu']\n    if 'sphinx_gallery_conf' in config:\n        processed_config['sphinx_gallery_conf'] = config['sphinx_gallery_conf']\n    return processed_config", "contrast": "def parse_config(app):\n    try:\n        plot_gallery = eval(app.builder.config.plot_gallery)\n    except TypeError:\n        plot_gallery = bool(app.builder.config.plot_gallery)\n    src_dir = app.builder.srcdir\n    abort_on_example_error = app.builder.config.abort_on_example_error\n    lang = app.builder.config.highlight_language\n    gallery_conf = _complete_gallery_conf(\n        app.config.sphinx_gallery_conf, src_dir, plot_gallery,\n        abort_on_example_error, lang, app.builder.name)\n    app.config.sphinx_gallery_conf = gallery_conf\n    app.config.html_static_path.append(glr_path_static())\n    return gallery_conf", "label": 1}
{"index": "gp038670", "code": "def _load_policy_definitions(path='c:\\\\Windows\\\\PolicyDefinitions',\n                             language='en-US'):\n    display_language_fallback = INSTALL_LANGUAGE\n    t_policy_definitions = lxml.etree.Element('policyDefinitions')\n    t_policy_definitions.append(lxml.etree.Element('categories'))\n    t_policy_definitions.append(lxml.etree.Element('policies'))\n    t_policy_definitions.append(lxml.etree.Element('policyNamespaces'))\n    t_policy_definition_resources = lxml.etree.Element(\n            'policyDefinitionResources')\n    policydefs_policies_xpath = etree.XPath('/policyDefinitions/policies')\n    policydefs_categories_xpath = etree.XPath('/policyDefinitions/categories')\n    policydefs_policyns_xpath = etree.XPath(\n            '/policyDefinitions/policyNamespaces')\n    policydefs_resources_localname_xpath = etree.XPath(\n            '//*[local-name() = \"policyDefinitionResources\"]/*')\n    policydef_resources_xpath = etree.XPath('/policyDefinitionResources')\n    for root, dirs, files in salt.utils.path.os_walk(path):\n        if root == path:\n            for t_admfile in files:\n                admfile = os.path.join(root, t_admfile)\n                parser = lxml.etree.XMLParser(remove_comments=True)\n                try:\n                    xmltree = lxml.etree.parse(admfile, parser=parser)\n                except lxml.etree.XMLSyntaxError:\n                    try:\n                        xmltree = _remove_unicode_encoding(admfile)\n                    except Exception:\n                        log.exception('Handle this explicitly')\n                        log.error('A error was found while processing admx '\n                                  'file %s, all policies from this file will '\n                                  'be unavailable via this module', admfile)\n                        continue\n                namespaces = xmltree.getroot().nsmap\n                namespace_string = ''\n                if None in namespaces:\n                    namespaces['None'] = namespaces[None]\n                    namespaces.pop(None)\n                    namespace_string = 'None:'\n                this_prefix = xmltree.xpath(\n                        '/{0}policyDefinitions/{0}policyNamespaces/{0}target/@prefix'.format(namespace_string),\n                        namespaces=namespaces)[0]\n                this_namespace = xmltree.xpath(\n                        '/{0}policyDefinitions/{0}policyNamespaces/{0}target/@namespace'.format(namespace_string),\n                        namespaces=namespaces)[0]\n                categories = xmltree.xpath(\n                        '/{0}policyDefinitions/{0}categories/{0}category'.format(namespace_string),\n                        namespaces=namespaces)\n                for category in categories:\n                    temp_cat = category\n                    temp_cat = _updateNamespace(temp_cat, this_namespace)\n                    policydefs_categories_xpath(t_policy_definitions)[0].append(temp_cat)\n                policies = xmltree.xpath('/{0}policyDefinitions/{0}policies/{0}policy'.format(namespace_string),\n                                         namespaces=namespaces)\n                for policy in policies:\n                    temp_pol = policy\n                    temp_pol = _updateNamespace(temp_pol, this_namespace)\n                    if 'key' in temp_pol.attrib:\n                        temp_pol = _updatePolicyElements(temp_pol, temp_pol.attrib['key'])\n                    policydefs_policies_xpath(t_policy_definitions)[0].append(temp_pol)\n                policy_namespaces = xmltree.xpath(\n                        '/{0}policyDefinitions/{0}policyNamespaces/{0}*'.format(namespace_string),\n                        namespaces=namespaces)\n                for policy_ns in policy_namespaces:\n                    temp_ns = policy_ns\n                    temp_ns = _updateNamespace(temp_ns, this_namespace)\n                    policydefs_policyns_xpath(t_policy_definitions)[0].append(temp_ns)\n                adml_file = os.path.join(\n                    root,\n                    language,\n                    os.path.splitext(t_admfile)[0] + '.adml')\n                if not __salt__['file.file_exists'](adml_file):\n                    log.info('An ADML file in the specified ADML language '\n                             '\"%s\" does not exist for the ADMX \"%s\", the '\n                             'the abbreviated language code will be tried.',\n                             language, t_admfile)\n                    adml_file = os.path.join(\n                        root,\n                        language.split('-')[0],\n                        os.path.splitext(t_admfile)[0] + '.adml')\n                    if not __salt__['file.file_exists'](adml_file):\n                        log.info('An ADML file in the specified ADML language '\n                                 'code %s does not exist for the ADMX \"%s\", '\n                                 'the fallback language will be tried.',\n                                 language[:2], t_admfile)\n                        adml_file = os.path.join(\n                            root,\n                            display_language_fallback,\n                            os.path.splitext(t_admfile)[0] + '.adml')\n                        if not __salt__['file.file_exists'](adml_file):\n                            log.info('An ADML file in the specified ADML '\n                                     'fallback language \"%s\" '\n                                     'does not exist for the ADMX \"%s\" '\n                                     'the abbreviated fallback language code '\n                                     'will be tried.',\n                                     display_language_fallback, t_admfile)\n                            adml_file = os.path.join(\n                                root,\n                                display_language_fallback.split('-')[0],\n                                os.path.splitext(t_admfile)[0] + '.adml')\n                            if not __salt__['file.file_exists'](adml_file):\n                                msg = ('An ADML file in the specified ADML language '\n                                       '\"{0}\" and the fallback language \"{1}\" do not '\n                                       'exist for the ADMX \"{2}\".')\n                                raise SaltInvocationError(msg.format(language,\n                                                                     display_language_fallback,\n                                                                     t_admfile))\n                try:\n                    xmltree = lxml.etree.parse(adml_file)\n                except lxml.etree.XMLSyntaxError:\n                    try:\n                        xmltree = _remove_unicode_encoding(adml_file)\n                    except Exception:\n                        log.exception('Handle this explicitly')\n                        log.error('An error was found while processing '\n                                  'adml file %s, all policy '\n                                  'language data from this file will be '\n                                  'unavailable via this module',\n                                  adml_file)\n                        continue\n                if None in namespaces:\n                    namespaces['None'] = namespaces[None]\n                    namespaces.pop(None)\n                policydefs_resources = policydefs_resources_localname_xpath(xmltree)\n                for policydefs_resource in policydefs_resources:\n                    t_poldef = policydefs_resource\n                    t_poldef = _updateNamespace(t_poldef, this_namespace)\n                    policydef_resources_xpath(t_policy_definition_resources)[0].append(t_poldef)\n    __context__['lgpo.policy_definitions'] = t_policy_definitions\n    __context__['lgpo.policy_resources'] = t_policy_definition_resources", "contrast": "import os\nfrom xml.etree.ElementTree import ElementTree, Element, SubElement\ndef process_ADMX_files(policy_def_path):\n    root = Element('policies')\n    for filename in os.listdir(policy_def_path):\n        if filename.endswith('.admx'):\n            filepath = os.path.join(policy_def_path, filename)\n            with open(filepath, 'r') as f:\n                xml = ElementTree(file=f).getroot()\n            for category in xml.findall('category'):\n                root.append(category)\n    return ElementTree(root)", "label": 0}
{"index": "gp207520", "code": "def allow_if_not_authenticated(user):\n    if not user.is_authenticated:\n        return True\n    else:\n        return False", "contrast": "def anonymous_required(view, redirect_to=None):\n    if redirect_to is None:\n        redirect_to = settings.LOGIN_REDIRECT_URL\n    @wraps(view)\n    def wrapper(request, *a, **k):\n        if request.user and request.user.is_authenticated():\n            return HttpResponseRedirect(redirect_to)\n        return view(request, *a, **k)\n    return wrapper", "label": 1}
{"index": "gp000891", "code": "def from_config(cls, config):\n    return cls(**{\n        'initializers': [tf.compat.v2.initializers.deserialize(init)\n                         for init in config.get('initializers', [])],\n        'sizes': config.get('sizes', []),\n        'validate_args': config.get('validate_args', False),\n    })", "contrast": "def initialize_from_config(config_dict):\n    initializer_type = config_dict['initializer_type']\n    initializer_params = config_dict['initializer_params']\n    if initializer_type == 'normal':\n        return tf.initializers.RandomNormal(**initializer_params)\n    elif initializer_type == 'uniform':\n        return tf.initializers.RandomUniform(**initializer_params)\n    elif initializer_type == 'truncated_normal':\n        return tf.initializers.TruncatedNormal(**initializer_params)\n    else:\n        raise ValueError('Initializer type not recognized.')", "label": 0}
{"index": "gp213769", "code": "import os\ndef which(cmd):\n    for path in os.environ[\"PATH\"].split(os.pathsep):\n        full_path = os.path.join(path, cmd)\n        if os.path.exists(full_path) and os.access(full_path, os.X_OK):\n            return full_path\n    return None", "contrast": "def which(cmd):\n    def is_exe(fp):\n        return os.path.isfile(fp) and os.access(fp, os.X_OK)\n    fpath, fname = os.path.split(cmd)\n    if fpath:\n        if is_exe(cmd):\n            return cmd\n    else:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            exe_file = os.path.join(path, cmd)\n            if is_exe(exe_file):\n                return exe_file\n    return None", "label": 1}
{"index": "gp268583", "code": "import subprocess\ndef start_ffmpeg_process(input_file):\n    cmd = ['ffmpeg', '-i', input_file, '-']\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    while True:\n        output = process.stdout.readline().decode('utf-8')\n        if output == '' and process.poll() is not None:\n            break\n        if output:\n            print(output.strip())\n    rc = process.poll()\n    return rc", "contrast": "async def start_worker(\n        self,\n        cmd: List[str],\n        input_source: str,\n        output: Optional[str] = None,\n        extra_cmd: Optional[str] = None,\n        pattern: Optional[str] = None,\n        reading: str = FFMPEG_STDERR,\n    ) -> None:\n        if self.is_running:\n            _LOGGER.warning(\"Can't start worker. It is allready running!\")\n            return\n        if reading == FFMPEG_STDERR:\n            stdout = False\n            stderr = True\n        else:\n            stdout = True\n            stderr = False\n        await self.open(\n            cmd=cmd,\n            input_source=input_source,\n            output=output,\n            extra_cmd=extra_cmd,\n            stdout_pipe=stdout,\n            stderr_pipe=stderr,\n        )\n        self._input = await self.get_reader(reading)\n        self._read_task = self._loop.create_task(self._process_lines(pattern))\n        self._loop.create_task(self._worker_process())", "label": 1}
{"index": "gp053356", "code": "def get_layer(self, name: str=None):\n        if name is None:\n            name = self.__last_layer_name\n        return self.__layers[name]", "contrast": "def retrieve_layer_by_name(name=None):\n    if name is None:\n        return network.layers[-1].output\n    else:\n        for layer in network.layers:\n            if layer.name == name:\n                return layer.output\n    return None", "label": 0}
{"index": "gp187452", "code": "def make_login_config_consistent(config):\n    config['username'] = config['username'].lower()\n    import hashlib\n    PASSWORD_SALT = 'my-secret-salt'  \n    password = config['password']\n    hashed_password = hashlib.sha256((password + PASSWORD_SALT).encode()).hexdigest()\n    config['password'] = hashed_password\n    return config", "contrast": "def sanitize_loginurl (self):\n        url = self[\"loginurl\"]\n        disable = False\n        if not self[\"loginpasswordfield\"]:\n            log.warn(LOG_CHECK,\n            _(\"no CGI password fieldname given for login URL.\"))\n            disable = True\n        if not self[\"loginuserfield\"]:\n            log.warn(LOG_CHECK,\n            _(\"no CGI user fieldname given for login URL.\"))\n            disable = True\n        if self.get_user_password(url) == (None, None):\n            log.warn(LOG_CHECK,\n            _(\"no user/password authentication data found for login URL.\"))\n            disable = True\n        if not url.lower().startswith((\"http:\", \"https:\")):\n            log.warn(LOG_CHECK, _(\"login URL is not a HTTP URL.\"))\n            disable = True\n        urlparts = urlparse.urlsplit(url)\n        if not urlparts[0] or not urlparts[1] or not urlparts[2]:\n            log.warn(LOG_CHECK, _(\"login URL is incomplete.\"))\n            disable = True\n        if disable:\n            log.warn(LOG_CHECK,\n              _(\"disabling login URL %(url)s.\") % {\"url\": url})\n            self[\"loginurl\"] = None", "label": 1}
{"index": "gp008705", "code": "def get_overrides_filename(variable):\n    filename = os.environ.get(variable)\n    if filename is None:\n        msg = 'Please set the {} environment variable.'.format(variable)\n        raise EnvironmentError(msg)\n    return filename", "contrast": "import os\ndef get_config_override_file():\n    return os.environ.get('CONFIG_OVERRIDE_FILE', None)", "label": 0}
{"index": "gp031501", "code": "def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n        while True:\n            total_length = len(tokens_a) + len(tokens_b)\n            if total_length <= max_length:\n                break\n            if len(tokens_a) > len(tokens_b):\n                tokens_a.pop()\n            else:\n                tokens_b.pop()", "contrast": "def truncate_to_max_length(sequence_pair, max_length):\n    sequence_pair[0] = sequence_pair[0][:max_length]\n    sequence_pair[1] = sequence_pair[1][:max_length]", "label": 0}
{"index": "gp006531", "code": "def multiple_l2_norm(tensors):\n    flattened = [T.as_tensor_variable(t).flatten() for t in tensors]\n    flattened = [(t if t.ndim > 0 else t.dimshuffle('x'))\n                 for t in flattened]\n    joined = T.join(0, *flattened)\n    return T.sqrt(T.sqr(joined).sum())", "contrast": "import numpy as np\ndef l2_norm(tensors):\n    tensors = np.asarray(tensors)\n    return np.sqrt(np.sum(np.square(tensors), axis=-1))", "label": 0}
{"index": "gp298500", "code": "def wrap_format_call(func, **kwargs):\n    return func(format(*kwargs.values()), kwargs.keys())", "contrast": "def _format(self, _, result):\n        return self._fmt.format(six.text_type(result))", "label": 1}
{"index": "gp275742", "code": "def extract_paths(path_list):\n    files = []\n    for file in path_list:\n        if isinstance(file, list):\n            for f in file:\n                files.append(f)\n        else:\n            files.append(file)\n    return files", "contrast": "def get_files(files, extnames=['.root']):\n    files_out = []\n    for f in files:\n        mime = mimetypes.guess_type(f)\n        if os.path.splitext(f)[1] in extnames:\n            files_out += [f]\n        elif mime[0] == 'text/plain':\n            files_out += list(np.loadtxt(f, unpack=True, dtype='str'))\n        else:\n            raise Exception('Unrecognized input type.')\n    return files_out", "label": 1}
{"index": "gp115587", "code": "def get_gradebook_admin_session(self, proxy):\n        if not self.supports_gradebook_admin():\n            raise errors.Unimplemented()\n        return sessions.GradebookAdminSession(proxy=proxy, runtime=self._runtime)", "contrast": "def get_gradebook_admin_session(proxy):\n    if proxy is None:\n        raise NullArgument()\n    elif not supports_gradebook_admin():\n        raise Unimplemented()\n    else:\n        try:\n            gradebook_admin_session = osid.grading.GradebookAdminSession(proxy=proxy)\n        except:\n            raise OperationFailed()\n        return gradebook_admin_session", "label": 0}
{"index": "gp219232", "code": "import configparser\ndef write_config_to_ini(config, space_around_delimiters=True):\n    config_string = configparser.ConfigParser(delimiters=(' = ' if space_around_delimiters else '='))\n    config_string.read_dict(config)\n    return config_string.write_string()", "contrast": "def write(self, fp, space_around_delimiters=True):\n        if space_around_delimiters:\n            d = \" {0} \".format(self._delimiters[0])\n        else:\n            d = self._delimiters[0]\n        if self._defaults:\n            self._write_section(fp, self.default_section,\n                                    self._defaults.items(), d)\n        for section in self._sections:\n            self._write_section(fp, section,\n                                self._sections[section].items(), d)", "label": 1}
{"index": "gp247950", "code": "def default_conflict_solver(conflicting_match, match):\n    if len(match) > len(conflicting_match):\n        return conflicting_match\n    return match", "contrast": "def _default_conflict_solver(match, conflicting_match):\n    if len(conflicting_match.initiator) < len(match.initiator):\n        return conflicting_match\n    if len(match.initiator) < len(conflicting_match.initiator):\n        return match\n    return None", "label": 1}
{"index": "gp255344", "code": "def circularize(sequence):\n    first_char = sequence[0]\n    remaining_seq = sequence[1:]\n    return remaining_seq + first_char", "contrast": "def circularize(self):\n        if self.top[-1].seq == '-' and self.bottom[0].seq == '-':\n            raise ValueError('Cannot circularize - termini disconnected.')\n        if self.bottom[-1].seq == '-' and self.top[0].seq == '-':\n            raise ValueError('Cannot circularize - termini disconnected.')\n        copy = self.copy()\n        copy.circular = True\n        copy.top.circular = True\n        copy.bottom.circular = True\n        return copy", "label": 1}
{"index": "gp198801", "code": "def check_changes(source: str, changed_source: str) -> str:\n    if source != changed_source:\n        return changed_source\n    else:\n        return None", "contrast": "def rename_in_module(occurrences_finder, new_name, resource=None,\n                     pymodule=None, replace_primary=False, region=None,\n                     reads=True, writes=True):\n    if resource is not None:\n        source_code = resource.read()\n    else:\n        source_code = pymodule.source_code\n    change_collector = codeanalyze.ChangeCollector(source_code)\n    for occurrence in occurrences_finder.find_occurrences(resource, pymodule):\n        if replace_primary and occurrence.is_a_fixed_primary():\n            continue\n        if replace_primary:\n            start, end = occurrence.get_primary_range()\n        else:\n            start, end = occurrence.get_word_range()\n        if (not reads and not occurrence.is_written()) or           (not writes and occurrence.is_written()):\n            continue\n        if region is None or region[0] <= start < region[1]:\n            change_collector.add_change(start, end, new_name)\n    return change_collector.get_changed()", "label": 1}
{"index": "gp075148", "code": "def _send_request(self, job_type, params={}):\n        params[\"wsid\"] = params.get(\"wsid\", self.userid)\n        params[\"pw\"]   = params.get(\"pw\", self.password)\n        path = os.path.join(self.base_url, job_type)\n        if self.request_type == 'GET':\n            r = requests.get(path, params=params)\n        elif self.request_type == 'POST':\n            r = requests.post(path, data=params)\n        else:\n            raise ValueError('`resest_type` is invalid!')\n        code = r.status_code\n        if code != 200:\n            raise Exception(\"%s failed with status: %d\"%(job_type, code))\n        return r", "contrast": "import requests\ndef submit_request(job_type, **params):\n    url = 'http://example.com/submit_job'\n    headers = {'Content-Type': 'application/json'}\n    data = {'job_type': job_type, **params}\n    r = requests.post(url, headers=headers, json=data)\n    return r", "label": 0}
{"index": "gp030152", "code": "def is_cursor_on_first_line(self):\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.StartOfBlock)\r\n        return cursor.atStart()", "contrast": "def is_cursor_on_first_line(cursor_position):\n    return cursor_position == 0", "label": 0}
{"index": "gp086646", "code": "def prior_sediment_rate(*args, **kwargs):\n        acc_mean = kwargs['acc_mean']\n        acc_shape = kwargs['acc_shape']\n        x = np.linspace(0, 6 * np.max(acc_mean), 100)\n        y = stats.gamma.pdf(x, a=acc_shape,\n                            scale=1 / (acc_shape/acc_mean))\n        return y, x", "contrast": "import numpy as np\nfrom scipy.stats import norm\ndef get_prior_density(sediment_accumulation_values, prior_mean, prior_stddev):\n    x = np.linspace(0, max(sediment_accumulation_values), 1000)\n    y = norm.pdf(x, loc=prior_mean, scale=prior_stddev)\n    return y, x", "label": 0}
{"index": "gp302523", "code": "import re\ndef split_vv_sequence(word):\n    regex = r'(?<=[aeiou])(?=[^aeiouy]{0,2}(?<![aeiou]{2})$)'\n    return re.sub(regex, '.', word)", "contrast": "def T2(word, rules):\n    WORD = word\n    offset = 0\n    for vv in vv_sequences(WORD):\n        seq = vv.group(1)\n        if not phon.is_diphthong(seq) and not phon.is_long(seq):\n            i = vv.start(1) + 1 + offset\n            WORD = WORD[:i] + '.' + WORD[i:]\n            offset += 1\n    rules += ' T2' if word != WORD else ''\n    return WORD, rules", "label": 1}
{"index": "gp304114", "code": "def get_favorited_images(user_id):\n    return favorited_images", "contrast": "def get_favorites(self):\n        url = self._imgur._base_url + \"/3/account/{0}/favorites\".format(self.name)\n        resp = self._imgur._send_request(url, needs_auth=True)\n        return [_get_album_or_image(thing, self._imgur) for thing in resp]", "label": 1}
{"index": "gp197070", "code": "def moment_matrix(input_matrix):\n    rows, cols = input_matrix.shape\n    moment_matrix = np.zeros((cols, cols))\n    for i in range(cols):\n        if np.array_equal(input_matrix[:, i], np.zeros(rows)):\n            moment_matrix[i, i] = 0\n        else:\n            moment_matrix[i, i] = np.dot(input_matrix[:, i], input_matrix[:, i])\n            for j in range(i+1, cols):\n                if np.array_equal(input_matrix[:, j], np.zeros(rows)):\n                    moment_matrix[i, j] = 0\n                    moment_matrix[j, i] = 0\n                else:\n                    moment_matrix[i, j] = moment_matrix[j, i] = np.dot(input_matrix[:, i], input_matrix[:, j])\n    return moment_matrix", "contrast": "def _M2_sparse(Xvar, mask_X, Yvar, mask_Y, weights=None):\n    C = np.zeros((len(mask_X), len(mask_Y)))\n    C[np.ix_(mask_X, mask_Y)] = _M2_dense(Xvar, Yvar, weights=weights)\n    return C", "label": 1}
{"index": "gp086724", "code": "def main(cls, args=None):\n        if args is None:\n            args = sys.argv[1:]\n        try:\n            o = cls()\n            o.parseOptions(args)\n        except usage.UsageError as e:\n            print(o.getSynopsis())\n            print(o.getUsage())\n            print(str(e))\n            return 1\n        except CLIError as ce:\n            print(str(ce))\n            return ce.returnCode\n        return 0", "contrast": "import argparse\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(description='Fill in command-line arguments from argv')\n    parser.add_argument('-f', '--file', help='path to input file')\n    parser.add_argument('-o', '--output', help='path to output file')\n    parser.add_argument('-v', '--verbose', action='store_true', help='verbose mode')\n    return parser.parse_args(argv)", "label": 0}
{"index": "gp297798", "code": "def apply_template_values(manifest_file_path, values):\n    with open(manifest_file_path, 'r') as file:\n        manifest_content = file.read()\n    for key, value in values.items():\n        manifest_content = manifest_content.replace(\"{{\"+key+\"}}\", value)\n    return manifest_content", "contrast": "def manifest(self, values, *paths, filename: str = None) -> Dict:\n        filename = filename or self.filename(*paths)\n        with open(filename, 'r') as fp:\n            template = Template(fp.read())\n        return yaml.load(template.render(values))", "label": 1}
{"index": "gp053129", "code": "def get_element_spd_dos(self, el):\n        el = get_el_sp(el)\n        el_dos = {}\n        for site, atom_dos in self.pdos.items():\n            if site.specie == el:\n                for orb, pdos in atom_dos.items():\n                    orbital_type = _get_orb_type_lobster(orb)\n                    if orbital_type not in el_dos:\n                        el_dos[orbital_type] = pdos\n                    else:\n                        el_dos[orbital_type] =                            add_densities(el_dos[orbital_type], pdos)\n        return {orb: Dos(self.efermi, self.energies, densities)\n                for orb, densities in el_dos.items()}", "contrast": "def get_element_spd_dos(el):\n    spd_densities = {\"S\": [], \"P\": [], \"D\": []}\n    if el not in LobsterCompleteDos.structure.composition.elements:\n        raise ValueError(\"Element not found in structure composition\")\n    for site, site_dos in LobsterCompleteDos.get_site_dos().items():\n        if site.specie == el:\n            el_dos = site_dos.get_element_dos()\n            for orbital, dos in el_dos.items():\n                if \"s\" in orbital.lower():\n                    spd_densities[\"S\"].append(dos)\n                elif \"p\" in orbital.lower():\n                    spd_densities[\"P\"].append(dos)\n                elif \"d\" in orbital.lower():\n                    spd_densities[\"D\"].append(dos)\n    return {el: spd_densities}", "label": 0}
{"index": "gp050517", "code": "def _to_dict(self):\n        _dict = {}\n        if hasattr(self, 'text') and self.text is not None:\n            _dict['text'] = self.text\n        if hasattr(self, 'created') and self.created is not None:\n            _dict['created'] = datetime_to_string(self.created)\n        if hasattr(self, 'updated') and self.updated is not None:\n            _dict['updated'] = datetime_to_string(self.updated)\n        return _dict", "contrast": "def to_json(model):\n    return model.to_dict() ", "label": 0}
{"index": "gp088403", "code": "def get_string(stream):\n    ba = bytearray()\n    for c in stream:\n        if not c:\n            break\n        ba.append(c)\n    return bytes(ba)", "contrast": "def grab_string(stream: bytearray) -> bytes:\n    index = 0\n    for byte in stream:\n        if byte == 0:\n            break\n        index += 1\n    return bytes(stream[:index])", "label": 0}
{"index": "gp058034", "code": "def settings_get(self, block_id, settings):\n        block = self._get_blocks([block_id.hex()])[0]\n        block_header = BlockHeader()\n        block_header.ParseFromString(block.header)\n        try:\n            settings_view = self._settings_view_factory.create_settings_view(\n                block_header.state_root_hash)\n        except KeyError:\n            LOGGER.error(\n                'Settings from block %s requested, but root hash %s was '\n                'missing. Returning no setting values.',\n                block_id.hex(),\n                block_header.state_root_hash)\n            return []\n        result = []\n        for setting in settings:\n            try:\n                value = settings_view.get_setting(setting)\n            except KeyError:\n                continue\n            result.append((setting, value))\n        return result", "contrast": "def get_key_value_pairs() -> List[Tuple[str, str]]:\n    key_value_pairs = [('key1', 'value1'), ('key2', 'value2'), ('key3', 'value3')]\n    return key_value_pairs", "label": 0}
{"index": "gp250854", "code": "import os\nimport documentcloud\ndef upload_directory(directory_path):\n    client = documentcloud.DocumentCloud()\n    documents = []\n    for file_name in os.listdir(directory_path):\n        if file_name.endswith('.pdf'):\n            with open(os.path.join(directory_path, file_name), 'rb') as pdf_file:\n                title, ext = os.path.splitext(file_name)\n                document = client.documents.upload(\n                    pdf_file, title=title, source=os.path.abspath(file_name),\n                )\n                documents.append(document)\n    return documents", "contrast": "def upload_directory(\n        self, path, source=None, description=None,\n        related_article=None, published_url=None, access='private',\n        project=None, data=None, secure=False, force_ocr=False\n    ):\n        path_list = []\n        for (dirpath, dirname, filenames) in os.walk(path):\n            path_list.extend([\n                os.path.join(dirpath, i) for i in filenames\n                if i.lower().endswith(\".pdf\")\n            ])\n        obj_list = []\n        for pdf_path in path_list:\n            obj = self.upload(\n                pdf_path, source=source, description=description,\n                related_article=related_article, published_url=published_url,\n                access=access, project=project, data=data, secure=secure,\n                force_ocr=force_ocr\n            )\n            obj_list.append(obj)\n        return obj_list", "label": 1}
{"index": "gp028502", "code": "def get_event_name(lambda_name, name):\n        return '{prefix:.{width}}-{postfix}'.format(prefix=lambda_name, width=max(0, 63 - len(name)), postfix=name)[:64]", "contrast": "def get_lambda_event_name(event_name):\n    return event_name.replace(\" \", \"-\").lower()", "label": 0}
{"index": "gp173862", "code": "def parse_dsv_file(parser_mediator, file_object):\n    try:\n        delimiter = \"\\t\"\n        data = []\n        for line in file_object:\n            line = line.strip()\n            values = line.split(delimiter)\n            data.append(values)\n        return data\n    except Exception as exception:\n        parser_mediator.ProduceExtractionError(\n            \"unable to parse DSV file with error: {0!s}\".format(exception))\n        raise errors.UnableToParseFile(\n            \"unable to parse DSV file with error: {0!s}\".format(exception))", "contrast": "def ParseFileObject(self, parser_mediator, file_object):\n    if not self._encoding:\n      self._encoding = parser_mediator.codepage\n    try:\n      if not self._HasExpectedLineLength(file_object):\n        display_name = parser_mediator.GetDisplayName()\n        raise errors.UnableToParseFile((\n            '[{0:s}] Unable to parse DSV file: {1:s} with error: '\n            'unexpected line length.').format(self.NAME, display_name))\n    except UnicodeDecodeError as exception:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile(\n          '[{0:s}] Unable to parse DSV file: {1:s} with error: {2!s}.'.format(\n              self.NAME, display_name, exception))\n    try:\n      line_reader = self._CreateLineReader(file_object)\n      reader = self._CreateDictReader(line_reader)\n      row_offset = line_reader.tell()\n      row = next(reader)\n    except (StopIteration, csv.Error, UnicodeDecodeError) as exception:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile(\n          '[{0:s}] Unable to parse DSV file: {1:s} with error: {2!s}.'.format(\n              self.NAME, display_name, exception))\n    number_of_columns = len(self.COLUMNS)\n    number_of_records = len(row)\n    if number_of_records != number_of_columns:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile((\n          '[{0:s}] Unable to parse DSV file: {1:s}. Wrong number of '\n          'records (expected: {2:d}, got: {3:d})').format(\n              self.NAME, display_name, number_of_columns,\n              number_of_records))\n    for key, value in row.items():\n      if self._MAGIC_TEST_STRING in (key, value):\n        display_name = parser_mediator.GetDisplayName()\n        raise errors.UnableToParseFile((\n            '[{0:s}] Unable to parse DSV file: {1:s}. Signature '\n            'mismatch.').format(self.NAME, display_name))\n    row = self._ConvertRowToUnicode(parser_mediator, row)\n    if not self.VerifyRow(parser_mediator, row):\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile((\n          '[{0:s}] Unable to parse DSV file: {1:s}. Verification '\n          'failed.').format(self.NAME, display_name))\n    self.ParseRow(parser_mediator, row_offset, row)\n    row_offset = line_reader.tell()\n    for row in reader:\n      if parser_mediator.abort:\n        break\n      row = self._ConvertRowToUnicode(parser_mediator, row)\n      self.ParseRow(parser_mediator, row_offset, row)\n      row_offset = line_reader.tell()", "label": 1}
{"index": "gp005584", "code": "def maxs(self, value):\n        self.x_max, self.y_max, self.z_max = value", "contrast": "import numpy as np\ndef set_max_values(x: float, y: float, z: float) -> np.ndarray:\n    max_values = np.array([x, y, z])\n    max_values = np.max(max_values)\n    return max_values", "label": 0}
{"index": "gp336287", "code": "def load_experiment_resource(path: str, root: str) -> str:\n    import os\n    from pathlib import Path\n    resource_path = Path(root) / path\n    if os.path.isfile(resource_path):\n        with open(resource_path, 'r') as f:\n            resource_content = f.read()\n            return resource_content\n    else:\n        return \"\"", "contrast": "def load_description(name, root=''):\n        desc = ''\n        try:\n            desc = Component.load_resource(name, root=root)\n        except (IOError, ImportError):\n            pass\n        return desc", "label": 1}
{"index": "gp241450", "code": "def check_overlapped_periods(enrollments):\n    for i in range(len(enrollments)):\n        for j in range(i+1, len(enrollments)):\n            if enrollments[i][1] > enrollments[j][0] and enrollments[i][0] < enrollments[j][1]:\n                return True\n    return False", "contrast": "def __validate_enrollment_periods(self, enrollments):\n        for a, b in itertools.combinations(enrollments, 2):\n            max_start = max(a.start, b.start)\n            min_end = min(a.end, b.end)\n            if max_start < min_end:\n                msg = \"invalid GrimoireLab enrollment dates. \"                      \"Organization dates overlap.\"\n                raise InvalidFormatError(cause=msg)\n        return enrollments", "label": 1}
{"index": "gp079187", "code": "def filter_traceback(error, tb, ignore_pkg=CURRENT_PACKAGE):\n    if not isinstance(tb, types.TracebackType):\n        return tb\n    def in_namespace(n):\n        return n and (n.startswith(ignore_pkg + '.') or n == ignore_pkg)\n    while tb and in_namespace(tb.tb_frame.f_globals['__package__']):\n        tb = tb.tb_next\n    starting_tb = tb\n    limit = 0\n    while tb and not in_namespace(tb.tb_frame.f_globals['__package__']):\n        tb = tb.tb_next\n        limit += 1\n    return ''.join(traceback.format_exception(error.__class__, error, starting_tb, limit))", "contrast": "import traceback\ndef filter_stacktrace(parent_trace, variable_name):\n    child_trace = traceback.extract_stack()\n    trace_idx = len(parent_trace) - 1\n    while trace_idx >= 0 and not parent_trace[trace_idx][0].startswith(\"<module>\"):\n        if variable_name in parent_trace[trace_idx][0].f_globals:\n            break\n        trace_idx -= 1\n    parent_trace = parent_trace[trace_idx:]\n    return parent_trace + child_trace", "label": 0}
{"index": "gp258634", "code": "def print_in_color(color):\n    if color:\n        print(\"\\033[1;31;48mHello, This is printed in color.\")\n    else:\n        print(\"Hello, this is printed in black and white.\")", "contrast": "def set_theme(color=True):\n        if color:\n            Console.theme = Console.theme_color\n        else:\n            Console.theme = Console.theme_bw\n        Console.color = color", "label": 1}
{"index": "gp181879", "code": "import nacl.signing\ndef generate_random_signing_key():\n    return nacl.signing.SigningKey.generate()", "contrast": "def generate(cls):\n        return cls(\n            random(nacl.bindings.crypto_sign_SEEDBYTES),\n            encoder=encoding.RawEncoder,\n        )", "label": 1}
{"index": "gp329042", "code": "import random\ndef resolve_iupac_hetero_codes(seq):\n    iupac_hetero = {'R': ('A', 'G'), 'Y': ('C', 'T'), 'S': ('G', 'C'),\n                    'W': ('A', 'T'), 'K': ('G', 'T'), 'M': ('A', 'C')}\n    new_seq = ''\n    for base in seq:\n        if base in iupac_hetero:\n            new_seq += random.choice(iupac_hetero[base])\n        else:\n            new_seq += base\n    return new_seq", "contrast": "def _resolveambig(subseq):\n    N = []\n    for col in subseq:\n        rand = np.random.binomial(1, 0.5)\n        N.append([_AMBIGS[i][rand] for i in col])\n    return np.array(N)", "label": 1}
{"index": "gp293761", "code": "import re\ndef match_records(map_records, regexText, parameter1, parameter2):\n    regex = re.compile(regexText)\n    result = []\n    for record in map_records:\n        if parameter1 in record and parameter2 in record:\n            if regex.match(record[parameter1]) is not None:\n                result.append(record)\n    return result", "contrast": "def filterMapNames(regexText,  records=getIndex(), excludeRegex=False, closestMatch=True):\n    bestScr = 99999 \n    regex = re.compile(regexText, flags=re.IGNORECASE)\n    ret = []\n    if excludeRegex: \n        if regexText and closestMatch: \n            for m in list(records):\n                if re.search(regex, m.name): continue \n                score = len(m.name) \n                if score == bestScr:\n                    bestScr = score\n                    ret.append(m)\n                elif score <  bestScr: \n                    bestScr = score\n                    ret = [m]\n        else: \n            for m in list(records):\n                if re.search(regex, m.name): continue \n                ret.append(m) \n    else: \n        if regexText and closestMatch: \n            for m in records:\n                if not re.search(regex, m.name): continue \n                score = len(m.name) \n                if score == bestScr:\n                    bestScr = score\n                    ret.append(m)\n                elif score <  bestScr: \n                    bestScr = score\n                    ret = [m]\n        else: \n            for m in records:\n                if not re.search(regex, m.name): continue \n                ret.append(m) \n    return ret", "label": 1}
{"index": "gp147644", "code": "def get_parallel_regions_block(batch):\n    samples = [utils.to_single_data(d) for d in batch]\n    regions = _get_parallel_regions(samples[0])\n    out = []\n    n = 10\n    for region_block in tz.partition_all(n, regions):\n        out.append({\"region_block\": [\"%s:%s-%s\" % (c, s, e) for c, s, e in region_block]})\n    return out", "contrast": "def retrieve_block_group(cwl_target, num_cores):\n    blocks = []\n    num_blocks = num_cores * 2\n    count = 0\n    for i in range(num_blocks):\n        block = []\n        for j in range(count, len(cwl_target), num_blocks):\n            block.append(cwl_target[j])\n        count += 1\n        blocks.append(block)\n    return blocks", "label": 0}
{"index": "gp182990", "code": "import cache\nfrom datetime import datetime, timedelta\nMAX_ATTEMPTS = 5\nLOCKOUT_TIME = 10 \ndef login(username, password):\n    user_attempts = cache.get(username, 0)\n    user_attempts += 1\n    cache.set(username, user_attempts, LOCKOUT_TIME * 60)\n    if user_attempts >= MAX_ATTEMPTS:\n        raise AxesSignalPermissionDenied(\"USER LOCKED OUT\")\n    if username == \"valid_user\" and password == \"valid_password\":\n        return True\n    else:\n        return False", "contrast": "def user_login_failed(\n            self,\n            sender,\n            credentials: dict,\n            request: AxesHttpRequest = None,\n            **kwargs\n    ):  \n        if request is None:\n            log.error('AXES: AxesCacheHandler.user_login_failed does not function without a request.')\n            return\n        username = get_client_username(request, credentials)\n        client_str = get_client_str(username, request.axes_ip_address, request.axes_user_agent, request.axes_path_info)\n        if self.is_whitelisted(request, credentials):\n            log.info('AXES: Login failed from whitelisted client %s.', client_str)\n            return\n        failures_since_start = 1 + self.get_failures(request, credentials)\n        if failures_since_start > 1:\n            log.warning(\n                'AXES: Repeated login failure by %s. Count = %d of %d. Updating existing record in the cache.',\n                client_str,\n                failures_since_start,\n                settings.AXES_FAILURE_LIMIT,\n            )\n        else:\n            log.warning(\n                'AXES: New login failure by %s. Creating new record in the cache.',\n                client_str,\n            )\n        cache_key = get_client_cache_key(request, credentials)\n        self.cache.set(cache_key, failures_since_start, self.cache_timeout)\n        if failures_since_start >= settings.AXES_FAILURE_LIMIT:\n            log.warning('AXES: Locking out %s after repeated login failures.', client_str)\n            user_locked_out.send(\n                'axes',\n                request=request,\n                username=username,\n                ip_address=request.axes_ip_address,\n            )\n            raise AxesSignalPermissionDenied('Locked out due to repeated login failures.')", "label": 1}
{"index": "gp047195", "code": "def fit(self, X, y=None, **kwargs):\n        if is_dataframe(X):\n            self.X = X.values\n            if self.features_ is None:\n                self.features_ = X.columns\n        else:\n            self.X = X\n        self.y = y\n        super(MissingDataVisualizer, self).fit(X, y, **kwargs)", "contrast": "def fit(self, X, y, **kwargs):\n    return self", "label": 0}
{"index": "gp117028", "code": "def match_deadline(self, start, end, match):\n        self._match_minimum_date_time('deadline', start, match)\n        self._match_maximum_date_time('deadline', end, match)", "contrast": "def match_assessments(start, end, match):\n    if start is None or end is None:\n        raise NullArgument(\"Start or end is null\")\n    if end < start:\n        raise InvalidArgument(\"End is less than start\")\n    assessments = get_all_assessments() \n    matching_assessments = []\n    for assessment in assessments:\n        if match == True:\n            if assessment.end_time >= start and assessment.end_time <= end:\n                matching_assessments.append(assessment)\n        else:\n            if assessment.end_time < start or assessment.end_time > end:\n                matching_assessments.append(assessment)\n    return matching_assessments", "label": 0}
{"index": "gp210433", "code": "import json\nfrom pypot.primitive.move import Move\ndef load_move_from_json(json_string):\n    json_dict = json.loads(json_string)\n    return Move.from_dict(json_dict)", "contrast": "def loads(cls, str):\n        d = json.loads(str)\n        return cls.create(d)", "label": 1}
{"index": "gp170863", "code": "import numpy as np\ndef generate_basis(z):\n    z = z / np.linalg.norm(z)\n    x = np.array([1, 0, 0])\n    if np.allclose(z, x):\n        x = np.array([0, 1, 0])\n    y = np.cross(z, x)\n    y /= np.linalg.norm(y)\n    x = np.cross(y, z)\n    return x, y, z", "contrast": "def generate_basis(z):\n    z = np.array(z, dtype=np.float64, copy=True)\n    if z.shape != (3,):\n        raise ValueError('z must be (3,) float!')\n    z /= np.linalg.norm(z)\n    x = np.array([-z[1], z[0], 0.0])\n    if np.isclose(np.linalg.norm(x), 0.0):\n        x = np.array([1.0, 0.0, 0.0])\n    else:\n        x /= np.linalg.norm(x)\n    y = np.cross(z, x)\n    result = np.array([x, y, z], dtype=np.float64)\n    return result", "label": 1}
{"index": "gp031077", "code": "def text_list_to_colors_simple(names):\n    uNames = list(set(names))\n    uNames.sort()\n    textToColor = [ uNames.index(n) for n in names ]\n    textToColor = np.array(textToColor)\n    textToColor = 255 * (textToColor - textToColor.min()) /                  (textToColor.max() - textToColor.min())\n    textmaps = generateColorMap();\n    colors = [textmaps[int(c)] for c in textToColor]\n    return colors", "contrast": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport hashlib\nimport random\ndef generate_colors(names):\n    processed_names = [''.join([char.lower() for char in name if char.isalpha()]) for name in names]\n    vectorizer = CountVectorizer()\n    bow = vectorizer.fit_transform(processed_names)\n    lda = LatentDirichletAllocation(n_components=3, random_state=42)\n    topic_dist = lda.fit_transform(bow)\n    colors = []\n    for topic_prob in topic_dist:\n        md5 = hashlib.md5(str(topic_prob).encode('utf-8')).hexdigest()\n        color = '#' + md5[:6]\n        colors.append(color)\n    return colors", "label": 0}
{"index": "gp191227", "code": "def move_cursor_to_line(line_number):\n    vim.current.window.cursor = (line_number, 0)", "contrast": "def _go_to_line(editor, line):\n    b = editor.application.current_buffer\n    b.cursor_position = b.document.translate_row_col_to_index(max(0, int(line) - 1), 0)", "label": 1}
{"index": "gp143973", "code": "def ones(self, name, **kwargs):\n        return self._write_op(self._ones_nosync, name, **kwargs)", "contrast": "import zarr\ndef create_array(**kwargs):\n    return zarr.creation.ones(**kwargs)", "label": 0}
{"index": "gp230838", "code": "def compute_total_remote_file_size(self, options):\n    remote_file_size = 0\n    return remote_file_size", "contrast": "def _compute_remote_size(self, options):\n        size = self.local_path.size\n        if (self._ase.mode == blobxfer.models.azure.StorageModes.Page and\n                self.local_path.use_stdin):\n            if options.stdin_as_page_blob_size == 0:\n                allocatesize = _MAX_PAGE_BLOB_SIZE\n                self._needs_resize = True\n            else:\n                allocatesize = options.stdin_as_page_blob_size\n        elif size > 0:\n            if self._ase.is_encrypted:\n                allocatesize = (size  \n                    self._AES_BLOCKSIZE\n            else:\n                allocatesize = size\n        else:\n            allocatesize = 0\n        self._ase.size = allocatesize\n        if blobxfer.util.is_not_empty(self._ase.replica_targets):\n            for rt in self._ase.replica_targets:\n                rt.size = allocatesize\n        if self._verbose:\n            logger.debug('remote size for {} is {} bytes'.format(\n                self._ase.path, self._ase.size))", "label": 1}
{"index": "gp105946", "code": "def loader_for_type(self, ctype):\n        for loadee, mimes in Mimer.TYPES.iteritems():\n            for mime in mimes:\n                if ctype.startswith(mime):\n                    return loadee", "contrast": "def get_deserialize_function(mimetype):\n    deserialize_functions = {\n        'application/json': json.loads,\n        'application/xml': xmltodict.parse,\n        'text/csv': csv.reader\n    }\n    return deserialize_functions.get(mimetype, None)", "label": 0}
{"index": "gp177296", "code": "import email\ndef create_message_structure(file_path, headersonly=False):\n    with open(file_path, 'rb') as f:\n        data = f.read()\n    return email.message_from_bytes(data, headersonly=headersonly)", "contrast": "def parse(self, fp, headersonly=False):\n        feedparser = FeedParser(self._class, policy=self.policy)\n        if headersonly:\n            feedparser._set_headersonly()\n        while True:\n            data = fp.read(8192)\n            if not data:\n                break\n            feedparser.feed(data)\n        return feedparser.close()", "label": 1}
{"index": "gp069830", "code": "def qteMakeWindowActive(self, windowObj: QtmacsWindow):\n        if windowObj in self._qteWindowList:\n            windowObj.activateWindow()\n        else:\n            self.qteLogger.warning('Window to activate does not exist')", "contrast": "def activate_window(windowObj):\n    if not isinstance(windowObj, QtmacsWindow):\n        raise QtmacsArgumentError(\"Invalid argument type\")\n    applets = windowObj.applets()\n    if len(applets) > 0:\n        applets[0].setFocus()\n    windowObj.activateWindow()", "label": 0}
{"index": "gp318405", "code": "import zlib\ndef merge_blocks(blocks):\n    compressed_data = b\"\".join(blocks)\n    compressed_data = zlib.compress(compressed_data)\n    return list(compressed_data)", "contrast": "def merge(blocks):\n    current_block = blocks[sorted(blocks.keys())[0]]\n    compressed_data = []\n    eof = False\n    while not eof:\n        data_size_to_append = None\n        next_block = None\n        i = 0\n        while i < len(current_block.data) - 1:\n            current_byte = current_block.data[i]\n            next_byte = current_block.data[i + 1]\n            if current_byte == RLE_BYTE:\n                if next_byte == RLE_BYTE:\n                    i += 2\n                else:\n                    i += 3\n            elif current_byte == SPECIAL_BYTE:\n                if next_byte in SPECIAL_DEFAULTS:\n                    i += 3\n                elif next_byte == SPECIAL_BYTE:\n                    i += 2\n                else:\n                    data_size_to_append = i\n                    if next_byte == EOF_BYTE:\n                        eof = True\n                    else:\n                        next_block = blocks[next_byte]\n                    break\n            else:\n                i += 1\n        assert data_size_to_append is not None, \"Ran off the end of a \"            \"block without encountering a block switch or EOF\"\n        compressed_data.extend(current_block.data[0:data_size_to_append])\n        if not eof:\n            assert next_block is not None, \"Switched blocks, but did \"                \"not provide the next block to switch to\"\n            current_block = next_block\n    return compressed_data", "label": 1}
{"index": "gp283697", "code": "import subprocess\ndef run_pngout(file_path):\n    subprocess.call([\"pngout\", file_path])", "contrast": "def pngout(ext_args):\n    args = _PNGOUT_ARGS + [ext_args.old_filename, ext_args.new_filename]\n    extern.run_ext(args)\n    return _PNG_FORMAT", "label": 1}
{"index": "gp282937", "code": "import matplotlib.pyplot as plt\ndef abundance_plots(p, xlm, abus='All', show=False, xaxis='Lagrangian'):\n    fig, ax = plt.subplots(2, 2, figsize=(12, 8)) \n    ax = ax.ravel()\n    if abus == 'All':\n        abus = p.abu_fields\n    for k, specie in enumerate(abus):\n        if k == 4:\n            break\n        ax[k].plot(p.get(xaxis), p.abundances[specie])\n        ax[k].set_title('{0}'.format(specie))\n        ax[k].set_xlim(xlm)\n    if show:\n        plt.show()", "contrast": "def abu_profiles(p,ifig=1,xlm=xlm,ylm=(-8,0),show=False,abunds='All',xaxis=xaxis_type, figsize1=(8,8)):\n    matplotlib.rc('figure',facecolor='white',figsize=figsize1)\n    f, ([ax1,ax2],[ax3,ax4]) = pl.subplots(2, 2, sharex=False, sharey=True, figsize=figsize1)\n    all_isos=[['h1','he3','he4','li6','c12','c13','n13','n14','n15','o16','o17','o18','f19'],['ne20','ne21','ne22','na22','na23','mg24','mg25','mg26','al26','al27','si28','si29','si30'], ['p31', 's32','s33', 's34','s36','cl35','cl37','ar36', 'ar38','ar40', 'k39', 'k40','k41'],\n['ca40','ca42','ca48','sc45','ti46','ti48','ti50','v50','v51','cr52','cr54','mn55','fe56']]\n    if abunds == 'All':\n        abus=[[],[],[],[]]\n        j=0\n        for i, row in enumerate(all_isos):\n            for iso in row:\n                if iso in p.cols:\n                    abus[i].append(iso)\n                    j+=1  \n        abus1=[]\n        abus2 =[[],[],[],[]]\n        for l in range(len(abus)):\n            for k in range(len(abus[l])):\n                abus1.append(abus[l][k])\n        is_small_isos = False                \n        for i in range(len(abus)):\n            if len(abus[i]) < 5:\n                is_small_isos = True\n                print(\"Missing isotopes from the default list. Distributing the ones you have over the panels.\")\n        if is_small_isos:\n            n=4\n            quo, rem = divmod(len(abus1), n)\n            for i in range(len(abus2)):\n                for k in range(i*quo,(i+1)*quo+rem):\n                    abus2[i].append(abus1[k])\n                    abus = abus2\n    else:\n        abus = abus    \n    ax = [ax1,ax2,ax3,ax4]\n    xxx = p.get('radius') if xaxis is \"Eulerian\" else p.get('mass') \n    mass = p.get('mass')                      \n    radius = p.get('radius')*ast.rsun_cm/1.e8  \n    if xaxis is \"Eulerian\":\n        xxx = radius\n        if xlm[0] == 0 and xlm[1] == 0:\n            indtop = 0\n            indbot = len(mass)-1\n        else: \n            indbot = np.where(radius>=xlm[0])[0][-1]\n            indtop = np.where(radius<xlm[1])[0][0]\n        xll = (radius[indbot],radius[indtop])\n        xxlabel = \"Radius (Mm)\"\n    elif xaxis is \"Lagrangian\": \n        xxx = mass\n        xll = xlm\n        xxlabel = \"$M / \\mathrm{M_{sun}}$\"\n    else:\n        print(\"Error: don't understand xaxis choice, must be Lagrangian or Eulerian\")\n    for i in range(4):\n        for thing in abus[i]:\n            ind = abus[i].index(thing)\n            ax[i].plot(xxx, np.log10(p.get(thing)), ls=u.linestylecb(ind,a,b)[0],            marker=u.linestylecb(ind,a,b)[1], color=u.linestylecb(ind,a,b)[2],            markevery=50,label=thing)\n        ax[i].set_ylim(ylm)\n        ax[i].set_xlim(xll)\n        ax[i].legend(loc=1)\n        ax[i].set_xlabel(xxlabel)\n        if i%2 == 0:\n            ax[i].set_ylabel('log X')\n    title_str = \"Abundance plot: \"+'t ='+str(title_format%p.header_attr['star_age'])              +' dt ='+str(title_format%p.header_attr['time_step'])              +'model number = '+str(int(p.header_attr['model_number']))\n    f.suptitle(title_str, fontsize=12)\n    f.tight_layout()\n    f.subplots_adjust(left=0.1, bottom=0.1, right=0.95, top=0.9, wspace=0, hspace=0.1)\n    f.savefig('abuprof'+str(int(p.header_attr['model_number'])).zfill(6)+'.png')", "label": 1}
{"index": "gp053237", "code": "def _get_formatted_error(self, error):\n        def bits(n):\n            while n:\n                b = n & (~n+1)\n                yield b\n                n ^= b\n        stsReturn = self.m_objPCANBasic.GetErrorText(error, 0)\n        if stsReturn[0] != PCAN_ERROR_OK:\n            strings = []\n            for b in bits(error):\n                stsReturn = self.m_objPCANBasic.GetErrorText(b, 0)\n                if stsReturn[0] != PCAN_ERROR_OK:\n                    text = \"An error occurred. Error-code's text ({0:X}h) couldn't be retrieved\".format(error)\n                else:\n                    text = stsReturn[1].decode('utf-8', errors='replace')\n                strings.append(text)\n            complete_text = '\\n'.join(strings)\n        else:\n            complete_text = stsReturn[1].decode('utf-8', errors='replace')\n        return complete_text", "contrast": "def get_error_message(err_code):\n    buffer_size = 255\n    buffer = ctypes.create_unicode_buffer(buffer_size)\n    num_chars = ctypes.windll.kernel32.GetErrorTextW(err_code, buffer, buffer_size)\n    if num_chars == 0:\n        return f\"Unable to retrieve error message. Error code: {ctypes.windll.kernel32.GetLastError()}\"\n    else:\n        return buffer.value", "label": 0}
{"index": "gp158505", "code": "def ipc_weights(event, time):\n    if event.all():\n        return numpy.ones(time.shape[0])\n    unique_time, p = kaplan_meier_estimator(~event, time)\n    idx = numpy.searchsorted(unique_time, time[event])\n    Ghat = p[idx]\n    assert (Ghat > 0).all()\n    weights = numpy.zeros(time.shape[0])\n    weights[event] = 1.0 / Ghat\n    return weights", "contrast": "def compute_censoring_weights(event, time):\n    from lifelines import KaplanMeierFitter\n    kmf = KaplanMeierFitter()\n    kmf.fit(time, event, label=\"Kaplan Meier Estimate\")\n    probs_to_not_be_observed_before = kmf.predict(time)\n    observation_probs = 1.0/probs_to_not_be_observed_before\n    observation_probs /= len(observation_probs)\n    weights = np.zeros(len(observation_probs))\n    for i in range(len(observation_probs)):\n        if event[i]:\n            weights[i] = observation_probs[i]\n        else:\n            weights[i] = 1.0 - observation_probs[i]\n    return weights", "label": 0}
{"index": "gp065248", "code": "def validate(method):\n    @wraps(method)\n    def mod_run(self, rinput):\n        self.validate_input(rinput)\n        result = method(self, rinput)\n        self.validate_result(result)\n        return result\n    return mod_run", "contrast": "from functools import wraps\ndef validate_inputs(inputs):\ndef validate_output(output):\ndef decorate_run_method(func):\n    @wraps(func)\n    def wrapper(inputs):\n        if not validate_inputs(inputs):\n            raise ValueError(\"Invalid inputs\")\n        output = func(inputs)\n        if not validate_output(output):\n            raise ValueError(\"Invalid output\")\n        return output\n    return wrapper", "label": 0}
{"index": "gp160689", "code": "def previous_close(self, dt):\n        idx = previous_divider_idx(self.market_closes_nanos, dt.value)\n        return pd.Timestamp(self.market_closes_nanos[idx], tz=UTC)", "contrast": "import pandas as pd\ndef get_previous_close(dt):\n    prev_close = pd.Timestamp(dt.date() - pd.Timedelta(days=1), tz='UTC').normalize() + pd.Timedelta(hours=23, minutes=59, seconds=59)\n    return prev_close", "label": 0}
{"index": "gp248358", "code": "import RPi.GPIO as GPIO\nimport time\ndef calculate_depth():\n    GPIO_TRIGGER = 23\n    GPIO_ECHO = 24\n    GPIO.setup(GPIO_TRIGGER, GPIO.OUT)\n    GPIO.setup(GPIO_ECHO, GPIO.IN)\n    GPIO.output(GPIO_TRIGGER, True)\n    time.sleep(0.00001)\n    GPIO.output(GPIO_TRIGGER, False)\n    StartTime = time.time()\n    StopTime = time.time()\n    while GPIO.input(GPIO_ECHO) == 0:\n        StartTime = time.time()\n    while GPIO.input(GPIO_ECHO) == 1:\n        StopTime = time.time()\n    TimeElapsed = StopTime - StartTime\n    distance = (TimeElapsed * 34300) / 2\n    depth = 30 - distance\n    return depth", "contrast": "def main():\n    trig_pin = 17\n    echo_pin = 27\n    hole_depth = 80  \n    value = sensor.Measurement(trig_pin,\n                               echo_pin\n                               )\n    raw_measurement = value.raw_distance()\n    liquid_depth = value.depth_metric(raw_measurement, hole_depth)\n    print(\"Depth = {} centimeters\".format(liquid_depth))", "label": 1}
{"index": "gp132001", "code": "def filter(self, criteria):\n        if isinstance(criteria, str) or isinstance(criteria, unicode):\n            _criteria = criteria\n            criteria = lambda x: x.get(_criteria)\n        return CoursesList(filter(criteria, self))", "contrast": "def filter_courses(courses, criteria):\n    if isinstance(criteria, str):\n        return [course for course in courses if course.get(criteria)]\n    elif callable(criteria):\n        return [course for course in courses if criteria(course)]\n    else:\n        raise ValueError(\"Criteria must be a string or a function.\")", "label": 0}
{"index": "gp325342", "code": "def solve_toeplitz(T0, TC, TR, Z):\n    N = len(Z)\n    M = N - 1\n    X = [0] * N\n    P = [0] * (M + 1)\n    Q = [0] * (M + 1)\n    G = [0] * (M + 1)\n    P[0] = TC[0] / T0\n    Q[0] = Z[0] / T0\n    for i in range(1, M + 1):\n        G[i] = TR[i - 1] / P[i - 1]\n        P[i] = (TC[i] - G[i] * TC[i - 1]) / (T0 - G[i] * TR[i - 1])\n        Q[i] = (Z[i] - G[i] * Q[i - 1]) / (T0 - G[i] * TR[i - 1])\n    X[M] = Q[M]\n    for i in range(M - 1, -1, -1):\n        X[i] = Q[i] - P[i] * X[i + 1]\n    return X", "contrast": "def TOEPLITZ(T0, TC, TR, Z):\n    assert len(TC)>0\n    assert len(TC)==len(TR)\n    M = len(TC)\n    X = numpy.zeros(M+1,dtype=complex)\n    A = numpy.zeros(M,dtype=complex)\n    B = numpy.zeros(M,dtype=complex)\n    P = T0\n    if P == 0: raise ValueError(\"P must be different from zero\")\n    if P == 0: raise ValueError(\"P must be different from zero\")\n    X[0] = Z[0]/T0 \n    for k in range(0, M):\n        save1 = TC[k]\n        save2 = TR[k]\n        beta = X[0]*TC[k]\n        if k == 0:\n            temp1 = -save1 / P\n            temp2 = -save2 / P\n        else:\n            for j in range(0, k):\n                save1 = save1 + A[j] * TC[k-j-1]\n                save2 = save2 + B[j] * TR[k-j-1]\n                beta = beta + X[j+1] * TC[k-j-1]\n            temp1 = -save1 / P\n            temp2 = -save2/P\n        P = P * (1. - (temp1*temp2))\n        if P <= 0:\n            raise ValueError(\"singular matrix\")\n        A[k] = temp1\n        B[k] = temp2\n        alpha = (Z[k+1]-beta)/P\n        if k == 0: \n            X[k+1] = alpha\n            for j in range(0,k+1):\n                X[j] = X[j] + alpha * B[k-j]\n            continue\n        for j in range(0, k):\n            kj = k-j-1\n            save1 = A[j]\n            A[j] = save1 + temp1 * B[kj] \n            B[kj] = B[kj] + temp2*save1\n        X[k+1] = alpha\n        for j in range(0,k+1):\n            X[j] = X[j] + alpha*B[k-j]\n    return X", "label": 1}
{"index": "gp191042", "code": "def fill_table():\n    table = []\n    table.append({'name': 'John Doe', 'age': 24, 'city': 'Seattle'})\n    table.append({'name': 'Jane Smith', 'age': 32, 'city': 'San Francisco'})\n    table.append({'name': 'Bob Johnson', 'age': 45, 'city': 'New York'})\n    return table", "contrast": "def data_filler_simple_registration(self, number_of_rows, db):\n        try:\n            simple_registration = db\n            for i in range(0, number_of_rows):\n                post_simple_reg = {\n                    \"id\": rnd_id_generator(self),\n                    \"email\": self.faker.safe_email(),\n                    \"password\": self.faker.md5(raw_output=False)\n                }\n                simple_registration.save(post_simple_reg)\n            logger.warning(\n                'simple_registration Commits are successful after write job!',\n                extra=d)\n        except Exception as e:\n            logger.error(e, extra=d)", "label": 1}
{"index": "gp035183", "code": "def pruned(name, user=None, env=None):\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Directory \\'{0}\\' is set to be pruned'.format(\n            name)\n        return ret\n    try:\n        call = __salt__['bower.prune'](dir=name, runas=user, env=env)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Error pruning \\'{0}\\': {1}'.format(name, err)\n        return ret\n    ret['result'] = True\n    if call:\n        ret['comment'] = 'Directory \\'{0}\\' was successfully pruned'.format(name)\n        ret['changes'] = {'old': [], 'new': call}\n    else:\n        ret['comment'] = 'No packages were pruned from directory \\'{0}\\''.format(name)\n    return ret", "contrast": "import os\ndef clean_bower_dir(name, user):\n    os.system(\"sudo -u \" + user + \" bower prune --allow-root\", cwd=name)", "label": 0}
{"index": "gp206781", "code": "import numpy as np\ndef apply_salt_and_pepper_noise(X, v, minimum=None, maximum=None):\n    if minimum is None:\n        minimum = np.min(X)\n    if maximum is None:\n        maximum = np.max(X)\n    num_pixels = X.size\n    num_noisy_pixels = int(num_pixels * v)\n    noisy_pixels_mask = np.random.choice(num_pixels, num_noisy_pixels, replace=False)\n    noisy_pixels = np.zeros(num_pixels)\n    noisy_pixels[noisy_pixels_mask] = 1\n    noisy_pixels = noisy_pixels.reshape(X.shape)\n    X_noisy = X.copy()\n    X_noisy[noisy_pixels == 1] = np.random.choice([minimum, maximum], size=num_noisy_pixels)\n    return X_noisy", "contrast": "def salt_and_pepper_noise(X, v):\n    X_noise = X.copy()\n    n_features = X.shape[1]\n    mn = X.min()\n    mx = X.max()\n    for i, sample in enumerate(X):\n        mask = np.random.randint(0, n_features, v)\n        for m in mask:\n            if np.random.random() < 0.5:\n                X_noise[i][m] = mn\n            else:\n                X_noise[i][m] = mx\n    return X_noise", "label": 1}
{"index": "gp003520", "code": "def session_new(self, **kwargs):\n        path = self._get_path('session_new')\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "contrast": "import requests\ndef generate_session_id(request_token):\n    url = \"https://api.example.com/generate_session_id\"\n    headers = {\"Authorization\": f\"Bearer {request_token}\"}\n    response = requests.get(url, headers=headers)\n    return response.json()", "label": 0}
{"index": "gp165887", "code": "async def addFeedData(self, name, items, seqn=None):\n        async with await self.snap() as snap:\n            snap.strict = False\n            return await snap.addFeedData(name, items, seqn=seqn)", "contrast": "def add_data(name: str, items: list, seqn: tuple) -> int:\n    expected_offset = None\n    return expected_offset", "label": 0}
{"index": "gp310526", "code": "def remove_sister(sister=None):\n    if not self.up:\n        return None\n    sisters = self.up.children\n    if not sisters:\n        return None\n    if sister:\n        if sister in sisters:\n            sisters.remove(sister)\n            return sister\n        else:\n            return None\n    else:\n        first_sister = sisters[0]\n        sisters.remove(first_sister)\n        return first_sister", "contrast": "def remove_sister(self, sister=None):\n        sisters = self.get_sisters()\n        if len(sisters) > 0:\n            if sister is None:\n                sister = sisters.pop(0)\n            return self.up.remove_child(sister)", "label": 1}
{"index": "gp276108", "code": "def get_catalog_yaml_file_name(catalog_name):\n    return catalog_name + '_catalog.yaml'", "contrast": "def catalog_split_yaml(self, **kwargs):\n        kwargs_copy = self.base_dict.copy()\n        kwargs_copy.update(**kwargs)\n        self._replace_none(kwargs_copy)        \n        localpath = NameFactory.catalog_split_yaml_format.format(**kwargs_copy)\n        if kwargs.get('fullpath', False):\n            return self.fullpath(localpath=localpath)\n        return localpath", "label": 1}
{"index": "gp038626", "code": "def halted(name, graceful=True):\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n    zones = __salt__['zoneadm.list'](installed=True)\n    if name in zones:\n        if zones[name]['state'] != 'running':\n            ret['result'] = True\n            ret['comment'] = 'Zone {0} already halted'.format(name)\n        else:\n            if not __opts__['test']:\n                zoneadm_res = __salt__['zoneadm.shutdown'](name) if graceful else __salt__['zoneadm.halt'](name)\n            if __opts__['test'] or zoneadm_res['status']:\n                ret['result'] = True\n                ret['changes'][name] = 'halted'\n                ret['comment'] = 'Zone {0} halted'.format(name)\n            else:\n                ret['result'] = False\n                ret['comment'] = 'Failed to halt {0}'.format(name)\n    else:\n        ret['comment'] = []\n        ret['comment'].append(\n            'The zone {0} is not in the installed state.'.format(name)\n        )\n        for zone in zones:\n            if zones[zone]['uuid'] == name:\n                ret['comment'].append(\n                    'The zone {0} has a uuid of {1}, please use the zone name instead!'.format(\n                        zone,\n                        name,\n                    )\n                )\n        ret['result'] = True\n        ret['comment'] = \"\\n\".join(ret['comment'])\n    return ret", "contrast": "def ensure_zone_halted(name: str, graceful: bool) -> None:\n    if graceful:\n        print(f\"Performing graceful shutdown of zone {name}\")\n    else:\n        print(f\"Halting zone {name} immediately\")", "label": 0}
{"index": "gp283658", "code": "from datetime import datetime\ndef compare_timestamps_file(filepath, timestamp):\n    with open(filepath, 'r') as f:\n        timestamps = [datetime.strptime(line.strip(), '%Y-%m-%d %H:%M:%S.%f') for line in f]\n    timestamp = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S.%f')\n    timestamps.append(timestamp)\n    return max(timestamps)", "contrast": "def _max_timestamps(dirname_full, remove, compare_tstamp):\n    tstamp = _get_timestamp_cached(dirname_full, remove)\n    return max_none((tstamp, compare_tstamp))", "label": 1}
{"index": "gp278430", "code": "def custom_destructor(fragment, adapter):\n    del fragment\n    adapter.fragments.remove(fragment)\n    del self", "contrast": "def destroy(self):\n        fragment = self.fragment\n        if fragment:\n            fragment.setFragmentListener(None)\n            if self.adapter is not None:\n                self.adapter.removeFragment(self.fragment)\n            del self.fragment\n        super(AndroidFragment, self).destroy()", "label": 1}
{"index": "gp055387", "code": "def Kdiag(self, X):\n        vyt = self.variance_Yt\n        vyx = self.variance_Yx\n        lyt = 1./(2*self.lengthscale_Yt)\n        lyx = 1./(2*self.lengthscale_Yx)\n        a = self.a\n        b = self.b\n        c = self.c\n        k1 = (2*lyt )*vyt*vyx\n        k2 = ( - 2*lyx )*vyt*vyx\n        k3 = ( 4*3*lyx**2 )*vyt*vyx\n        Kdiag = np.zeros(X.shape[0])\n        slices = index_to_slices(X[:,-1])\n        for i, ss1 in enumerate(slices):\n            for s1 in ss1:\n                if i==0:\n                    Kdiag[s1]+= vyt*vyx\n                elif i==1:\n                    Kdiag[s1]+= b**2*k1 - 2*a*c*k2 + a**2*k3 + c**2*vyt*vyx\n                else:\n                    raise ValueError(\"invalid input/output index\")\n        return Kdiag", "contrast": "import numpy as np\ndef compute_covariance_diagonal(X):\n    cov_matrix = np.cov(X, rowvar=False)\n    cov_diag = np.diag(cov_matrix)\n    return cov_diag", "label": 0}
{"index": "gp209483", "code": "def rupture_to_element(rup, parent=None):\n    element = Element(parent,\n                      name=str(rup.rupid),\n                      seed=str(rup.seed))\n    for ses, events in rup.events_by_ses.items():\n        segment = Element(element,\n                          name=str(ses))\n        for event in events:\n            _ = Element(segment,\n                        event.name,\n                        attrs={'N026': str(event.magnitude),\n                               'N027': str(event.latitude),\n                               'N028': str(event.longitude),\n                               'N029': str(event.depth)})\n    return element", "contrast": "def rupture_to_element(rup, parent=None):\n    if parent is None:\n        parent = et.Element('root')\n    rup_elem = et.SubElement(parent, rup.typology)\n    elem = et.SubElement(rup_elem, 'stochasticEventSets')\n    n = 0\n    for ses in rup.events_by_ses:\n        eids = rup.events_by_ses[ses]['eid']\n        n += len(eids)\n        ses_elem = et.SubElement(elem, 'SES', id=ses)\n        ses_elem.text = ' '.join(str(eid) for eid in eids)\n    rup_elem.set('id', rup.rupid)\n    rup_elem.set('multiplicity', str(n))\n    sub_elems(rup_elem, rup, 'magnitude',  'strike', 'dip', 'rake')\n    h = rup.hypocenter\n    et.SubElement(rup_elem, 'hypocenter', dict(lon=h.x, lat=h.y, depth=h.z))\n    if rup.is_from_fault_source:\n        mesh_elem = et.SubElement(rup_elem, 'mesh')\n        for i, row in enumerate(rup.lons):\n            for j, col in enumerate(row):\n                node_elem = et.SubElement(mesh_elem, 'node')\n                node_elem.set('row', str(i))\n                node_elem.set('col', str(j))\n                node_elem.set('lon', str(rup.lons[i][j]))\n                node_elem.set('lat', str(rup.lats[i][j]))\n                node_elem.set('depth', str(rup.depths[i][j]))\n        mesh_elem.set('rows', str(i + 1))\n        mesh_elem.set('cols', str(j + 1))\n    elif rup.is_gridded_surface:\n        mesh_elem = et.SubElement(rup_elem, 'mesh')\n        for j, _ in enumerate(rup.lons):\n            node_elem = et.SubElement(mesh_elem, 'node')\n            node_elem.set('row', '0')\n            node_elem.set('col', str(j))\n            node_elem.set('lon', str(rup.lons[j]))\n            node_elem.set('lat', str(rup.lats[j]))\n            node_elem.set('depth', str(rup.depths[j]))\n    else:\n        if rup.is_multi_surface:\n            assert len(rup.lons) % 4 == 0\n            assert len(rup.lons) == len(rup.lats) == len(rup.depths)\n            for offset in range(len(rup.lons) // 4):\n                start = offset * 4\n                end = offset * 4 + 4\n                lons = rup.lons[start:end]  \n                lats = rup.lats[start:end]  \n                depths = rup.depths[start:end]  \n                ps_elem = et.SubElement(\n                    rup_elem, 'planarSurface')\n                top_left, top_right, bottom_left, bottom_right =                    zip(lons, lats, depths)\n                for el_name, corner in (\n                        ('topLeft', top_left),\n                        ('topRight', top_right),\n                        ('bottomLeft', bottom_left),\n                        ('bottomRight', bottom_right)):\n                    corner_elem = et.SubElement(ps_elem, el_name)\n                    corner_elem.set('lon', '%.7f' % corner[0])\n                    corner_elem.set('lat', '%.7f' % corner[1])\n                    corner_elem.set('depth', '%.7f' % corner[2])\n        else:\n            ps_elem = et.SubElement(rup_elem, 'planarSurface')\n            for el_name, corner in (\n                    ('topLeft', rup.top_left_corner),\n                    ('topRight', rup.top_right_corner),\n                    ('bottomLeft', rup.bottom_left_corner),\n                    ('bottomRight', rup.bottom_right_corner)):\n                corner_elem = et.SubElement(ps_elem, el_name)\n                corner_elem.set('lon', '%.7f' % corner[0])\n                corner_elem.set('lat', '%.7f' % corner[1])\n                corner_elem.set('depth', '%.7f' % corner[2])\n    return parent", "label": 1}
{"index": "gp289428", "code": "def merge_device_changes(current_devices: dict, updated_devices: dict) -> dict:\n    merged_changes = {}\n    for device_id, device_info in updated_devices.items():\n        if device_id not in current_devices:\n            merged_changes[device_id] = device_info\n        else:\n            for attr, value in device_info.items():\n                if current_devices[device_id].get(attr) != value:\n                    if device_id not in merged_changes:\n                        merged_changes[device_id] = {attr: value}\n                    else:\n                        merged_changes[device_id][attr] = value\n    return merged_changes", "contrast": "def resolve_updates(orig_list, updated_list):\n        if updated_list is not None and updated_list:\n            if orig_list is None:\n                orig_list = updated_list\n            else:\n                for new_device in updated_list:\n                    was_found = False\n                    for device in orig_list:\n                        if new_device.cid == device.cid:\n                            was_found = True\n                            break\n                    if not was_found:\n                        orig_list.append(new_device)\n                for device in orig_list:\n                    should_remove = True\n                    for new_device in updated_list:\n                        if device.cid == new_device.cid:\n                            should_remove = False\n                            break\n                    if should_remove:\n                        orig_list.remove(device)\n            [device.update() for device in orig_list]\n        return orig_list", "label": 1}
{"index": "gp145387", "code": "def _auth_headers(self):\n        if self.has_cookies():\n            return [(\"Cookie\", _make_cookie_header(list(self.get_cookies().items())))]\n        elif self.basic and (self.username and self.password):\n            token = 'Basic %s' % b64encode((\"%s:%s\" % (self.username, self.password)).encode('utf-8')).decode('ascii')\n            return [(\"Authorization\", token)]\n        elif self.token is _NoAuthenticationToken:\n            return []\n        else:\n            if self.token.startswith('Splunk '):\n                token = self.token\n            else:\n                token = 'Splunk %s' % self.token\n            return [(\"Authorization\", token)]", "contrast": "def get_authentication_headers(context):\n    auth_header_name = \"Authorization\"\n    auth_token = context.get_authentication_token()\n    auth_header_value = f\"Bearer {auth_token}\"\n    headers = [(auth_header_name, auth_header_value)]\n    return headers", "label": 0}
{"index": "gp016874", "code": "def runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False):\n        if partitions is None:\n            partitions = range(rdd._jrdd.partitions().size())\n        mappedRDD = rdd.mapPartitions(partitionFunc)\n        sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n        return list(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))", "contrast": "def runJob(rdd, partitionFunc, partitions=None, allowLocal=False):\n    if partitions:\n        results = rdd.mapPartitionsWithIndex(lambda i, part: partitionFunc(part) if i in partitions else []).collect()\n    else:\n        results = rdd.mapPartitions(partitionFunc).collect()\n    return [elem for sublist in results for elem in sublist]", "label": 0}
{"index": "gp088537", "code": "def get_monomers(self, ligands=True):\n        if ligands and self.ligands:\n            monomers = self._monomers + self.ligands._monomers\n        else:\n            monomers = self._monomers\n        return iter(monomers)", "contrast": "def retrieve_monomers(ampal_obj, ligands=False):\n    monomers = ampal_obj.get_monomers()\n    if ligands:\n        monomers += ampal_obj.get_ligands()\n    return monomers", "label": 0}
{"index": "gp184595", "code": "def send_persisted_messages(websocket):\n    persisted_messages = retrieve_persisted_messages(websocket)\n    for message in persisted_messages:\n        websocket.send(message)\n    clear_persisted_messages(websocket)", "contrast": "def send_persisted_messages(self, websocket):\n        for channel in self._subscription.channels:\n            message = self._connection.get(channel)\n            if message:\n                websocket.send(message)", "label": 1}
{"index": "gp127472", "code": "def classinstances(cls):\n        l = [i for i in cls.allinstances() if type(i) == cls]\n        return l", "contrast": "class JB_Gui:\n    instances = []\n    def __init__(self):\n        self.__class__.instances.append(self)\n    @classmethod\n    def get_all_instances(cls):\n        instance_list = []\n        for instance in cls.instances:\n            if isinstance(instance, cls):\n                instance_list.append(instance)\n        return instance_list", "label": 0}
{"index": "gp138043", "code": "def index_genome_alignment_by_locus(fh, out_fh, verbose=False):\n  bound_iter = functools.partial(genome_alignment_iterator,\n                                 reference_species=\"hg19\", index_friendly=True)\n  hash_func = JustInTimeGenomeAlignmentBlock.build_hash\n  idx = IndexedFile(fh, bound_iter, hash_func)\n  idx.write_index(out_fh, verbose=verbose)", "contrast": "def build_index(coords, ref_genome):\n    index = {}\n    for i, coord in enumerate(coords):\n        key = ref_genome[coord[0]:coord[1]]\n        if key in index:\n            index[key].append(i)\n        else:\n            index[key] = [i]\n    return index", "label": 0}
{"index": "gp302003", "code": "import time\nimport threading\nimport sqlite3\ndef update_and_commit_with_insert(sql):\n    conn = sqlite3.connect('test.db')\n    c = conn.cursor()\n    c.execute(sql)\n    conn.commit()\n    time.sleep(1)  \n    row_id = c.lastrowid\n    c.execute(f\"SELECT * FROM TABLE_NAME WHERE ROW_ID = {row_id}\")\n    record = c.fetchone()\n    conn.close()\n    return record", "contrast": "def update(sql, *args, **kwargs):\n        assert \"update\" in sql.lower(), 'This function requires an update statement, provided: {}'.format(sql)\n        cursor = CoyoteDb.execute_and_commit(sql, *args, **kwargs)\n        last_row_id = cursor.lastrowid\n        return last_row_id", "label": 1}
{"index": "gp292293", "code": "import toml\nfrom typing import Optional\ndef load_config(config_path: Optional[str] = None):\n    if config_path is None:\n        default_config = {\"key1\": \"value1\", \"key2\": \"value2\"}\n        with open('example_config.toml', 'w') as f:\n            toml.dump(default_config, f)\n    else:\n        with open(config_path) as f:\n            config = toml.load(f)\n        return config", "contrast": "def get_or_create_config(path, config):\n    if os.path.isfile(path):\n        with open(path) as fh:\n            _LOG.debug(\"loading config from %s\", os.path.abspath(path))\n            config._inflate(toml.load(fh))\n    else:\n        try:\n            os.makedirs(os.path.dirname(path))\n        except OSError:\n            pass\n        with open(path, \"w\") as fh:\n            toml.dump(config._deflate(), fh)", "label": 1}
{"index": "gp275763", "code": "def create_summed_likelihood(components):\n    from fermipy.like import LikelihoodModel, SummedLikelihood\n    likelihood_objects = []\n    for component in components:\n        likelihood_objects.append(LikelihoodModel(component))\n    summed_likelihood = SummedLikelihood()\n    for likelihood in likelihood_objects:\n        summed_likelihood.addComponent(likelihood)\n    return summed_likelihood", "contrast": "def _create_likelihood(self, srcmdl=None):\n        self._like = SummedLikelihood()\n        for c in self.components:\n            c._create_binned_analysis(srcmdl)\n            self._like.addComponent(c.like)\n        self.like.model = self.like.components[0].model\n        self._fitcache = None\n        self._init_roi_model()", "label": 1}
{"index": "gp167186", "code": "def _parentage(self, categories, levels):\n        if not levels:\n            return tuple(categories)\n        if not levels[0]:\n            return tuple(categories)\n        parent_level, remaining_levels = levels[0], levels[1:]\n        leaf_node = categories[0]\n        parent = parent_level[0]\n        for category in parent_level:\n            if category.idx > leaf_node.idx:\n                break\n            parent = category\n        extended_categories = tuple(categories) + (parent,)\n        return self._parentage(extended_categories, remaining_levels)", "contrast": "def get_ancestors(categories, levels):\n    def find_parent(categories, parent_idx):\n        parent = None\n        for category in categories:\n            if category.idx <= parent_idx:\n                if parent is None or parent.idx < category.idx:\n                    parent = category\n        return parent\n    ancestors = []\n    parent_idx = categories[0].idx\n    for level in levels:\n        parent = find_parent(level, parent_idx)\n        if parent is not None:\n            ancestors.insert(0, parent)\n            parent_idx = parent.idx\n    return tuple(ancestors)", "label": 0}
{"index": "gp026295", "code": "def get_read_options(eventual, transaction_id):\n    if transaction_id is None:\n        if eventual:\n            return datastore_pb2.ReadOptions(\n                read_consistency=datastore_pb2.ReadOptions.EVENTUAL\n            )\n        else:\n            return datastore_pb2.ReadOptions()\n    else:\n        if eventual:\n            raise ValueError(\"eventual must be False when in a transaction\")\n        else:\n            return datastore_pb2.ReadOptions(transaction=transaction_id)", "contrast": "def validate_read_options(eventual: bool, transaction_id: bytes) -> datastore_pb2.ReadOptions:\n    if eventual and transaction_id is not None:\n        raise ValueError(\"Eventual consistency is not compatible with transaction ID\")\n    read_options = datastore_pb2.ReadOptions()\n    read_options.read_consistency = (datastore_pb2.EVENTUAL if eventual\n                                     else datastore_pb2.STRONG)\n    read_options.transaction.CopyFrom(datastore_pb2.TransactionOptions(\n        read_write=datastore_pb2.TransactionOptions.ReadOnly(\n            previous_transaction=transaction_id)))\n    return read_options", "label": 0}
{"index": "gp297038", "code": "def has_full_name(name):\n    parts = name.split()\n    if len(parts) < 2:\n        return False\n    for part in parts:\n        if len(part) == 1 or part[-1] == '.':\n            return False\n    return True", "contrast": "def author_name_contains_fullnames(author_name):\n    def _is_initial(name_part):\n        return len(name_part) == 1 or u'.' in name_part\n    parsed_name = ParsedName(author_name)\n    if len(parsed_name) == 1:\n        return False\n    elif any([_is_initial(name_part) for name_part in parsed_name]):\n        return False\n    return True", "label": 1}
{"index": "gp128337", "code": "def netdevs():\n    with open('/proc/net/dev') as f:\n        net_dump = f.readlines()\n    device_data={}\n    data = namedtuple('data',['rx','tx'])\n    for line in net_dump[2:]:\n        line = line.split(':')\n        if line[0].strip() != 'lo':\n            device_data[line[0].strip()] = data(float(line[1].split()[0])/(1024.0*1024.0), \n                                                float(line[1].split()[8])/(1024.0*1024.0))\n    return device_data", "contrast": "import psutil\ndef get_network_bytes():\n    net_io_counters = psutil.net_io_counters(pernic=True)\n    network_bytes = {}\n    for nic, addrs in psutil.net_if_addrs().items():\n        if nic in net_io_counters:\n            bytes_sent = net_io_counters[nic].bytes_sent\n            bytes_recv = net_io_counters[nic].bytes_recv\n            network_bytes[nic] = {\"RX_bytes\": bytes_recv, \"TX_bytes\": bytes_sent}\n    return network_bytes", "label": 0}
{"index": "gp002614", "code": "def _validate(self, qobj):\n        n_qubits = qobj.config.n_qubits\n        max_qubits = self.configuration().n_qubits\n        if n_qubits > max_qubits:\n            raise BasicAerError('Number of qubits {} '.format(n_qubits) +\n                                'is greater than maximum ({}) '.format(max_qubits) +\n                                'for \"{}\".'.format(self.name()))\n        if hasattr(qobj.config, 'shots') and qobj.config.shots != 1:\n            logger.info('\"%s\" only supports 1 shot. Setting shots=1.',\n                        self.name())\n            qobj.config.shots = 1\n        for experiment in qobj.experiments:\n            name = experiment.header.name\n            if getattr(experiment.config, 'shots', 1) != 1:\n                logger.info('\"%s\" only supports 1 shot. '\n                            'Setting shots=1 for circuit \"%s\".',\n                            self.name(), name)\n                experiment.config.shots = 1\n            for operation in experiment.instructions:\n                if operation.name in ['measure', 'reset']:\n                    raise BasicAerError('Unsupported \"%s\" instruction \"%s\" ' +\n                                        'in circuit \"%s\" ', self.name(),\n                                        operation.name, name)", "contrast": "def semantic_validations(qobj) -> None:\n    for experiment in qobj.experiments:\n        if experiment.config.shots != 1:\n            raise ValueError(\"Semantic Error: Shots must be set to 1\")\n        for op in experiment.instructions:\n            if op.name == 'measure' and op.op.index != 0:\n                raise ValueError(\"Semantic Error: Measurements cannot be in the middle\")", "label": 0}
{"index": "gp132201", "code": "def get_image_dimension(self, url):\n        w_h = (None, None)\n        try:\n            if url.startswith('//'):\n                url = 'http:' + url\n            data = requests.get(url).content\n            im = Image.open(BytesIO(data))\n            w_h = im.size\n        except Exception:\n            logger.warning(\"Error getting image size {}\".format(url), exc_info=True)\n        return w_h", "contrast": "from urllib.request import urlopen\nfrom PIL import ImageFile\ndef get_image_size(url):\n    try:\n        with urlopen(url) as file:\n            parser = ImageFile.Parser()\n            while True:\n                data = file.read(1024)\n                if not data:\n                    break\n                parser.feed(data)\n                if parser.image:\n                    return parser.image.size\n    except:\n        pass\n    return (None, None)", "label": 0}
{"index": "gp321624", "code": "def store_demonstration(demo, demo_memory):\n    demo_memory.append(demo)\n    return demo_memory", "contrast": "def import_demo_experience(self, states, internals, actions, terminal, reward):\n        fetches = self.import_demo_experience_output\n        feed_dict = self.get_feed_dict(\n            states=states,\n            internals=internals,\n            actions=actions,\n            terminal=terminal,\n            reward=reward\n        )\n        self.monitored_session.run(fetches=fetches, feed_dict=feed_dict)", "label": 1}
{"index": "gp263552", "code": "def serialize_inline_query_result_cached_audio(self):\n    serialized_inline_query_result_cached_audio = {\n        'type': 'audio',\n        'audio_file_id': self.audio_file_id,\n        'caption': self.caption,\n        'parse_mode': self.parse_mode,\n        'reply_markup': self.reply_markup,\n    }\n    return serialized_inline_query_result_cached_audio", "contrast": "def to_array(self):\n        array = super(InlineQueryResultCachedAudio, self).to_array()\n        array['type'] = u(self.type)  \n        array['id'] = u(self.id)  \n        array['audio_file_id'] = u(self.audio_file_id)  \n        if self.caption is not None:\n            array['caption'] = u(self.caption)  \n        if self.parse_mode is not None:\n            array['parse_mode'] = u(self.parse_mode)  \n        if self.reply_markup is not None:\n            array['reply_markup'] = self.reply_markup.to_array()  \n        if self.input_message_content is not None:\n            array['input_message_content'] = self.input_message_content.to_array()  \n        return array", "label": 1}
{"index": "gp251195", "code": "def _load_object_from_filesystem(filepath):\n    with open(filepath, 'rb') as f:\n        data = f.read()\n    return data", "contrast": "def load(self):\n        if self.is_persisted:\n            with open(self.file_name, 'rb') as f:\n                self.object_property = pickle.load(f)", "label": 1}
{"index": "gp318976", "code": "def add_tags(tags: List[str]) -> bool:\n    added_tags = []\n    for tag in tags:\n        if tag not in added_tags:\n            added_tags.append(tag)\n        else:\n            return False\n    return True", "contrast": "def _add_tags(self, tags):\n        alltagsadded = True\n        for tag in tags:\n            if not self._add_tag(tag):\n                alltagsadded = False\n        return alltagsadded", "label": 1}
{"index": "gp262734", "code": "import re\ndef gather_team_data(soup, base_url, team_pattern, team, sport):\n    team_data = []\n    team_url = base_url.format(re.findall(team_pattern, team)[0])\n    team_html = requests.get(team_url).content\n    team_soup = BeautifulSoup(team_html, 'html.parser')\n    team_info = team_soup.find_all('div', {'class': 'team-info'})\n    for info in team_info:\n        data = {\n            'team_name': info.find('a', {'class': 'team-name'}).text,\n            'location': info.find('div', {'class': 'team-location'}).text,\n            'record': info.find('div', {'class', 'team-record'}).text,\n            'sport': sport\n        }\n        team_data.append(data)\n    return team_data", "contrast": "def _get_team_info_raw(soup, base_url, team_pattern, team, sport):\n    team_url = None\n    team_name = None\n    for link in soup.find_all('a'):\n        if re.search(team_pattern, link.string):\n            team_name = link.string\n            team_url = base_url.replace('/teams/', link['href'])\n    if team_url is not None and team_name is not None:\n        team_soup = BeautifulSoup(requests.get(team_url).content, 'html.parser')\n        team_info_raw = team_soup.find('div', id='meta').contents[3].get_text().split('\\n')\n        team_info_raw = [x.replace('\\t', '') for x in team_info_raw]\n        team_info_raw = [x.strip() for x in team_info_raw if x != '']\n        team_info_raw[0] = team_name\n        return team_info_raw\n    else:\n        raise errors.TeamNotFoundError(sport, team)", "label": 1}
{"index": "gp152490", "code": "def schoice(self, seq: str, end: int = 10) -> str:\n        return ''.join(self.choice(list(seq))\n                       for _ in range(end))", "contrast": "def choice_func(seq, end):\n    import random\n    return ''.join(random.choice(seq) for _ in range(end))", "label": 0}
{"index": "gp089380", "code": "def children(self) -> NodeList:\n        return NodeList([e for e in self.childNodes\n                         if e.nodeType == Node.ELEMENT_NODE])", "contrast": "def get_child_nodes():\n    child_nodes = [1, 2, 3, 4, 5]\n    return child_nodes", "label": 0}
{"index": "gp094374", "code": "def correlation(P, obs1, obs2=None, times=[1], k=None):\n    M = P.shape[0]\n    T = np.asarray(times).max()\n    if T < M:\n        return correlation_matvec(P, obs1, obs2=obs2, times=times)\n    else:\n        return correlation_decomp(P, obs1, obs2=obs2, times=times, k=k)", "contrast": "def time_correlation(P, obs1, obs2=None, times=None, k=None):\n    if obs2 is None:\n        obs2 = obs1\n    if times is None:\n        times = [0]\n    if k is None:\n        k = min(P.shape)\n    eigvals, eigvecs = np.linalg.eig(P)\n    idx = np.argsort(eigvals)[::-1][:k]\n    eigvals, eigvecs = eigvals[idx], eigvecs[:, idx]\n    correlations = np.zeros_like(times, dtype=np.float)\n    for i, tau in enumerate(times):\n        P_tau = eigvecs.dot(np.diag(np.exp(tau * eigvals))).dot(np.linalg.inv(eigvecs))\n        E1_tau = obs1.dot(P_tau)\n        E2_tau = obs2.dot(P_tau)\n        correlations[i] = np.sum(E1_tau * E2_tau)\n    return correlations", "label": 0}
{"index": "gp086226", "code": "def _parse_join(self, tokens):\n        children = []\n        tokens.pop(0)  \n        tokens.pop(0)  \n        children.append(self._parse_nested_interval(tokens))\n        while tokens[0] == ',':\n            tokens.pop(0)\n            children.append(self._parse_nested_interval(tokens))\n        tokens.pop(0)  \n        chromosome, strand = next((child.chromosome, child.strand) for child in children)\n        start = min(child.start.position for child in children)\n        stop = max(child.stop.position for child in children)\n        parent = NestedGenomicInterval(start, stop, chromosome=chromosome, strand=strand)\n        parent.children = children\n        return parent", "contrast": "def parse_join(join_string):\n    join_args = join_string.strip('join()').split(',')\n    if len(join_args) == 1:\n        return (parse_super_range(join_args[0]), None)\n    else:\n        return (parse_super_range(join_args[0]), parse_super_range(join_args[1]))", "label": 0}
{"index": "gp267776", "code": "def get_input_shard_spec(num_inputs: int, num_shards: int) -> dict:\n    if num_inputs < 1 or num_shards < 1:\n        raise ValueError(\"Number of inputs and shards must be positive integers.\")\n    shard_size = num_inputs // num_shards\n    input_shard_spec = {\n        \"inputs\": []\n    }\n    for shard_index in range(num_shards):\n        start_idx = shard_index * shard_size\n        end_idx = start_idx + shard_size\n        if shard_index == num_shards - 1:  \n            end_idx = num_inputs\n        input_shard_spec[\"inputs\"].append({\n            \"start\": start_idx,\n            \"end\": end_idx\n        })\n    return input_shard_spec", "contrast": "def to_json(self):\n    new_pos = self._blob_reader.tell()\n    if self._has_iterated:\n      new_pos -= 1\n    return {self.BLOB_KEY_PARAM: self._blob_key,\n            self.INITIAL_POSITION_PARAM: new_pos,\n            self.END_POSITION_PARAM: self._end_position}", "label": 1}
{"index": "gp123038", "code": "def subscribe(self, objectID, varIDs=(tc.VAR_ROAD_ID, tc.VAR_LANEPOSITION), begin=0, end=2**31 - 1):\n        Domain.subscribe(self, objectID, varIDs, begin, end)", "contrast": "def subscribe(string:str, integers:list[int], start:int, end:int) -> None:\n    pass", "label": 0}
{"index": "gp056954", "code": "def strip_output(nb):\n    nb.metadata.pop('signature', None)\n    nb.metadata.pop('widgets', None)\n    for cell in _cells(nb):\n        if 'outputs' in cell:\n            cell['outputs'] = []\n        if 'prompt_number' in cell:\n            cell['prompt_number'] = None\n    return nb", "contrast": "def strip_outputs(notebook_obj):\n    for cell in notebook_obj['cells']:\n        if 'outputs' in cell:\n            cell['outputs'] = []\n    return notebook_obj", "label": 0}
{"index": "gp068864", "code": "def get_id_head(self):\n        id_head = None\n        for target_node in self:\n            if target_node.is_head():\n                id_head = target_node.get_id()\n                break\n        return id_head", "contrast": "def get_head_target_id(targets):\n    for target in targets:\n        if target[\"isHead\"]:\n            return target[\"id\"]\n    return None", "label": 0}
{"index": "gp280515", "code": "import numpy as np\nfrom scipy.stats import entropy\ndef jensen_shannon_divergence(p, q):\n    m = (p + q) / 2\n    return (entropy(p, m) + entropy(q, m)) / 2", "contrast": "def js_divergence(p, q):\n    m = .5 * (p+q)\n    js_div = .5*kl_divergence(p, m) + .5*kl_divergence(q, m)\n    return js_div", "label": 1}
{"index": "gp145703", "code": "def _publish_replset(self, data, base_prefix):\n        prefix = base_prefix + ['replset']\n        self._publish_dict_with_prefix(data, prefix)\n        total_nodes = len(data['members'])\n        healthy_nodes = reduce(lambda value, node: value + node['health'],\n                               data['members'], 0)\n        self._publish_dict_with_prefix({\n            'healthy_nodes': healthy_nodes,\n            'total_nodes': total_nodes\n        }, prefix)\n        for node in data['members']:\n            replset_node_name = node[self.config['replset_node_name']]\n            node_name = str(replset_node_name.split('.')[0])\n            self._publish_dict_with_prefix(node, prefix + ['node', node_name])", "contrast": "def publish_replica_set_stats(response):\n    numeric_values = []\n    healthy_nodes = 0\n    total_nodes = 0\n    observed_statuses = {}\n    for key, value in response.items():\n        if isinstance(value, (int, float)):\n            numeric_values.append((key, value))\n        elif key == 'members':\n            total_nodes = value\n            for member in value:\n                if member['health'] == 1:\n                    healthy_nodes += 1\n                observed_statuses[member['_id']] = member['stateStr']\n    print(\"Numeric Values:\")\n    for key, value in numeric_values:\n        print(f\"{key}: {value}\")\n    print(f\"\\nHealthy Nodes: {healthy_nodes}/{total_nodes}\")\n    print(\"\\nObserved Statuses:\")\n    for node_id, status in observed_statuses.items():\n        print(f\"{node_id}: {status}\")", "label": 0}
{"index": "gp299543", "code": "def set_join_clause(query, join):\n    query += \" JOIN \" + join\n    return query", "contrast": "def _set_join(self, query=None):\n        if not query:\n            query = self._query\n        foreign_key = '%s.%s' % (self._related.get_table(), self._second_key)\n        query.join(self._parent.get_table(), self.get_qualified_parent_key_name(), '=', foreign_key)", "label": 1}
{"index": "gp260499", "code": "def draw_box(cb):\n    cb.rect(0, 0, cb.width, cb.height)\n    cb.refresh()", "contrast": "def draw_box(cb, x0, y0, w, h, fg=colors.default_fg, bg=colors.default_bg, h_seps=[], v_seps=[]):\n    w -= 1\n    h -= 1\n    corners = [(x0, y0), (x0 + w, y0), (x0, y0 + h), (x0 + w, y0 + h)]\n    fg = fg()\n    bg = bg()\n    for i, c in enumerate(corners):\n        cb.put(c[0], c[1], BOX_CORNERS[i], fg, bg)\n    for s in h_seps + [0, h]:\n        cb.put(x0 + 1, y0 + s, symbols[\"BOX_HORIZONTAL\"] * (w - 1), fg, bg)\n    for y in range(1, h):\n        for s in v_seps + [0, w]:\n            cb.put(x0 + s, y0 + y, symbols[\"BOX_VERTICAL\"], fg, bg)\n    for s in h_seps:\n        cb.put(x0,     y0 + s, symbols[\"BOX_X_LEFT\"],  fg, bg)\n        cb.put(x0 + w, y0 + s, symbols[\"BOX_X_RIGHT\"], fg, bg)\n    for s in v_seps:\n        cb.put(x0 + s, y0,     symbols[\"BOX_X_TOP\"],    fg, bg)\n        cb.put(x0 + s, y0 + h, symbols[\"BOX_X_BOTTOM\"], fg, bg)", "label": 1}
{"index": "gp327064", "code": "def do_import(self):\n    self.create_tables()\n    rows = self.gen_rows()\n    for row in rows:\n        self.connection.execute(f\"INSERT INTO {self.table_name} VALUES ({','.join(['?' for _ in range(self.num_fields)])})\", row)\n    for hook in self.post_import_hooks:\n        hook(self.connection)\n    for index in self.indexes:\n        self.connection.execute(index)", "contrast": "def import_(self, conn):\n        if self.print_progress:\n            print('Beginning', self.__class__.__name__)\n        self._conn = conn\n        self.create_table(conn)\n        if self.mode in ('all', 'import') and self.fname and self.exists() and self.table not in ignore_tables:\n            self.insert_data(conn)\n        if self.mode in ('all', 'index') and hasattr(self, 'index'):\n            self.create_index(conn)\n        if self.mode in ('all', 'import') and hasattr(self, 'post_import'):\n            self.run_post_import(conn)\n        conn.commit()", "label": 1}
{"index": "gp333881", "code": "def undo_scaling(X, feature_range, truncate=True):\n    minimum = feature_range[0]\n    maximum = feature_range[1]\n    if truncate:\n        X = np.clip(X, minimum, maximum)\n    X_rescaled = (X - minimum) / (maximum - minimum)\n    return X_rescaled", "contrast": "def inverse_transform(self, X):\n        X = check_array(X, copy=self.copy)\n        X -= self.min_\n        X /= self.scale_\n        return X", "label": 1}
{"index": "gp164809", "code": "def _start_execution_in_container(\n            self, args, stdin, stdout, stderr, env, root_dir, cwd, temp_dir,\n            memlimit, memory_nodes,\n            cgroups, output_dir, result_files_patterns, parent_setup_fn,\n            child_setup_fn, parent_cleanup_fn):\n        assert self._use_namespaces\n        if root_dir is None:\n            env.update(self._env_override)\n        args = self._build_cmdline(args, env=env)\n        CHILD_OSERROR = 128\n        CHILD_UNKNOWN_ERROR = 129\n        from_parent, to_grandchild = os.pipe() \n        from_grandchild, to_parent = os.pipe() \n        MARKER_USER_MAPPING_COMPLETED = b'A'\n        MARKER_PARENT_COMPLETED = b'B'\n        if root_dir is None:\n            cwd = os.path.abspath(cwd or os.curdir)\n        else:\n            root_dir = os.path.abspath(root_dir)\n            cwd = os.path.abspath(cwd)\n        def grandchild():\n            try:\n                my_outer_pid = container.get_my_pid_from_procfs()\n                container.mount_proc()\n                container.drop_capabilities()\n                container.reset_signal_handling()\n                child_setup_fn() \n                os.write(to_parent, str(my_outer_pid).encode())\n                received = os.read(from_parent, 1)\n                assert received == MARKER_PARENT_COMPLETED, received\n            finally:\n                os.close(from_parent)\n                os.close(to_parent)\n        def child():\n            try:\n                logging.debug(\"Child: child process of RunExecutor with PID %d started\",\n                              container.get_my_pid_from_procfs())\n                container.block_all_signals()\n                necessary_fds = {sys.stdin, sys.stdout, sys.stderr,\n                    to_parent, from_parent, stdin, stdout, stderr} - {None}\n                container.close_open_fds(keep_files=necessary_fds)\n                try:\n                    if self._container_system_config:\n                        libc.sethostname(container.CONTAINER_HOSTNAME)\n                    if not self._allow_network:\n                        container.activate_network_interface(\"lo\")\n                    received = os.read(from_parent, len(MARKER_USER_MAPPING_COMPLETED))\n                    assert received == MARKER_USER_MAPPING_COMPLETED, received\n                    if root_dir is not None:\n                        self._setup_root_filesystem(root_dir)\n                    else:\n                        self._setup_container_filesystem(\n                            temp_dir,\n                            output_dir if result_files_patterns else None,\n                            memlimit,\n                            memory_nodes)\n                except EnvironmentError as e:\n                    logging.critical(\"Failed to configure container: %s\", e)\n                    return CHILD_OSERROR\n                try:\n                    os.chdir(cwd)\n                except EnvironmentError as e:\n                    logging.critical(\n                        \"Cannot change into working directory inside container: %s\", e)\n                    return CHILD_OSERROR\n                try:\n                    grandchild_proc = subprocess.Popen(args,\n                                        stdin=stdin,\n                                        stdout=stdout, stderr=stderr,\n                                        env=env,\n                                        close_fds=False,\n                                        preexec_fn=grandchild)\n                except (EnvironmentError, RuntimeError) as e:\n                    logging.critical(\"Cannot start process: %s\", e)\n                    return CHILD_OSERROR\n                necessary_capabilities = [libc.CAP_SYS_ADMIN] if result_files_patterns else []\n                container.drop_capabilities(keep=necessary_capabilities)\n                container.close_open_fds(keep_files={sys.stdout, sys.stderr, to_parent, from_parent})\n                if _HAS_SIGWAIT:\n                    grandchild_result = container.wait_for_child_and_forward_all_signals(\n                        grandchild_proc.pid, args[0])\n                else:\n                    container.forward_all_signals_async(grandchild_proc.pid, args[0])\n                    grandchild_result = self._wait_for_process(grandchild_proc.pid, args[0])\n                logging.debug(\"Child: process %s terminated with exit code %d.\",\n                              args[0], grandchild_result[0])\n                if result_files_patterns:\n                    libc.umount(temp_dir.encode())\n                os.write(to_parent, pickle.dumps(grandchild_result))\n                os.close(to_parent)\n                os.read(from_parent, 1)\n                os.close(from_parent)\n                return 0\n            except EnvironmentError as e:\n                logging.exception(\"Error in child process of RunExecutor\")\n                return CHILD_OSERROR\n            except:\n                logging.exception(\"Error in child process of RunExecutor\")\n                return CHILD_UNKNOWN_ERROR\n        try: \n            try:\n                child_pid = container.execute_in_namespace(child, use_network_ns=not self._allow_network)\n            except OSError as e:\n                raise BenchExecException(\n                    \"Creating namespace for container mode failed: \" + os.strerror(e.errno))\n            logging.debug(\"Parent: child process of RunExecutor with PID %d started.\", child_pid)\n            def check_child_exit_code():\n                child_exitcode, unused_child_rusage = self._wait_for_process(child_pid, args[0])\n                child_exitcode = util.ProcessExitCode.from_raw(child_exitcode)\n                logging.debug(\"Parent: child process of RunExecutor with PID %d terminated with %s.\",\n                              child_pid, child_exitcode)\n                if child_exitcode:\n                    if child_exitcode.value:\n                        if child_exitcode.value == CHILD_OSERROR:\n                            raise BenchExecException(\"execution in container failed, check log for details\")\n                        elif child_exitcode.value == CHILD_UNKNOWN_ERROR:\n                            raise BenchExecException(\"unexpected error in container\")\n                        raise OSError(child_exitcode.value, os.strerror(child_exitcode.value))\n                    raise OSError(0, \"Child process of RunExecutor terminated with \" + str(child_exitcode))\n            os.close(from_parent)\n            os.close(to_parent)\n            container.setup_user_mapping(child_pid, uid=self._uid, gid=self._gid)\n            os.write(to_grandchild, MARKER_USER_MAPPING_COMPLETED) \n            try:\n                grandchild_pid = int(os.read(from_grandchild, 10)) \n            except ValueError:\n                check_child_exit_code()\n                assert False, \"Child process of RunExecutor terminated cleanly but did not send expected data.\"\n            logging.debug(\"Parent: executing %s in grand child with PID %d via child with PID %d.\",\n                          args[0], grandchild_pid, child_pid)\n            cgroups.add_task(grandchild_pid)\n            parent_setup = parent_setup_fn()\n            os.write(to_grandchild, MARKER_PARENT_COMPLETED)\n            from_grandchild_copy = os.dup(from_grandchild)\n            to_grandchild_copy = os.dup(to_grandchild)\n        finally:\n            os.close(from_grandchild)\n            os.close(to_grandchild)\n        def wait_for_grandchild():\n            try:\n                received = os.read(from_grandchild_copy, 1024)\n            except OSError as e:\n                if self.PROCESS_KILLED and e.errno == errno.EINTR:\n                    received = os.read(from_grandchild_copy, 1024)\n                else:\n                    raise e\n            exitcode, ru_child = pickle.loads(received)\n            base_path = \"/proc/{}/root\".format(child_pid)\n            parent_cleanup = parent_cleanup_fn(\n                parent_setup, util.ProcessExitCode.from_raw(exitcode), base_path)\n            if result_files_patterns:\n                self._transfer_output_files(\n                    base_path + temp_dir,\n                    cwd,\n                    output_dir,\n                    result_files_patterns)\n            os.close(from_grandchild_copy)\n            os.close(to_grandchild_copy) \n            check_child_exit_code()\n            return exitcode, ru_child, parent_cleanup\n        return grandchild_pid, wait_for_grandchild", "contrast": "import os\ndef execute_in_container(command):\n    if os.geteuid() != 0:\n        print(\"This function must be run as root.\")\n        return\n    container_pid = os.fork()\n    if container_pid == 0:\n        os.unshare(os.CLONE_NEWNS)\n        os.unshare(os.CLONE_NEWPID)\n        os.unshare(os.CLONE_NEWUTS)\n        os.unshare(os.CLONE_NEWIPC)\n        with open('/etc/network/interfaces', 'w') as f:\n            f.write('auto lo\\niface lo inet loopback\\n')\n        os.mount('tmpfs', '/tmp', 'tmpfs')\n        os.setgroups([])\n        os.setresgid(65534, 65534, 65534)\n        os.setresuid(65534, 65534, 65534)\n        os.chdir('/tmp')\n        os.chroot('/tmp')\n        os.system(command)\n    else:\n        container_status = os.waitpid(container_pid, 0)[1]\n        if os.WIFSIGNALED(container_status):\n            print(\"Command terminated with signal\", os.WTERMSIG(container_status))\n        else:\n            print(\"Command exited with status\", os.WEXITSTATUS(container_status))", "label": 0}
{"index": "gp049542", "code": "def RetryUpload(self, job, job_id, error):\n    if self.IsErrorRetryable(error):\n      retry_count = 0\n      sleep_interval = config.CONFIG[\"BigQuery.retry_interval\"]\n      while retry_count < config.CONFIG[\"BigQuery.retry_max_attempts\"]:\n        time.sleep(sleep_interval.seconds)\n        logging.info(\"Retrying job_id: %s\", job_id)\n        retry_count += 1\n        try:\n          response = job.execute()\n          return response\n        except errors.HttpError as e:\n          if self.IsErrorRetryable(e):\n            sleep_interval *= config.CONFIG[\"BigQuery.retry_multiplier\"]\n            logging.exception(\"Error with job: %s, will retry in %s\", job_id,\n                              sleep_interval)\n          else:\n            raise BigQueryJobUploadError(\n                \"Can't retry error code %s. Giving up\"\n                \" on job: %s.\" % (e.resp.status, job_id))\n    else:\n      raise BigQueryJobUploadError(\"Can't retry error code %s. Giving up on \"\n                                   \"job: %s.\" % (error.resp.status, job_id))\n    raise BigQueryJobUploadError(\n        \"Giving up on job:%s after %s retries.\" % (job_id, retry_count))", "contrast": "import time\nclass BigQueryJobUploadError(Exception):\n    pass\ndef retry_upload_job(job, job_id, error):\n    retry_max_attempts = 3\n    retry_delay_sec = 5\n    for i in range(retry_max_attempts):\n        try:\n            return job.upload_from_filename(job_id)\n        except error as e:\n            time.sleep(retry_delay_sec)\n    raise BigQueryJobUploadError('Failed after {} attempts'.format(retry_max_attempts))", "label": 0}
{"index": "gp145464", "code": "def encrypt(self):\n        value = self.parameters.get(\"Plaintext\")\n        if isinstance(value, six.text_type):\n            value = value.encode('utf-8')\n        return json.dumps({\"CiphertextBlob\": base64.b64encode(value).decode(\"utf-8\"), 'KeyId': 'key_id'})", "contrast": "import base64\ndef encrypt(value):\n    encoded = base64.b64encode(value.encode('utf-8'))\n    return encoded\ndef decrypt(value):\n    decoded = base64.b64decode(value).decode('utf-8')\n    return decoded", "label": 0}
{"index": "gp101395", "code": "def is_image(filename):\n    return os.path.isfile(filename) and filename.lower().endswith(ImageExts)", "contrast": "def is_image(filename):\n    image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.bmp']\n    return any(filename.lower().endswith(img_ext) for img_ext in image_extensions)", "label": 0}
{"index": "gp017350", "code": "def _get_list_axis(self, key, axis=None):\n        if axis is None:\n            axis = self.axis or 0\n        try:\n            return self.obj._take(key, axis=axis)\n        except IndexError:\n            raise IndexError(\"positional indexers are out-of-bounds\")", "contrast": "def get_series_values(key, axis=0):\n    return s.iloc[key]", "label": 0}
{"index": "gp197719", "code": "def write_association_to_file(association, output_file):\n    with open(output_file, 'a') as file:\n        file.write(association + '\\n')", "contrast": "def write_assoc(self, assoc):\n        if assoc.get(\"header\", False):\n            return\n        subj = assoc['subject']\n        db, db_object_id = self._split_prefix(subj)\n        rel = assoc['relation']\n        qualifier = rel['id']\n        if assoc['negated']:\n            qualifier = 'NOT|' + qualifier\n        goid = assoc['object']['id']\n        ev = assoc['evidence']\n        evidence = self.ecomap.coderef_to_ecoclass(ev['type'])\n        withfrom = \"|\".join(ev['with_support_from'])\n        reference = \"|\".join(ev['has_supporting_reference'])\n        date = assoc['date']\n        assigned_by = assoc['provided_by']\n        annotation_properties = '' \n        interacting_taxon_id = assoc['interacting_taxon']\n        vals = [db,\n                db_object_id,\n                qualifier,\n                goid,\n                reference,\n                evidence,\n                withfrom,\n                interacting_taxon_id, \n                date,\n                assigned_by,\n                self._extension_expression(assoc['object_extensions']),\n                annotation_properties]\n        self._write_row(vals)", "label": 1}
{"index": "gp140637", "code": "def _file_prefix(\n            self):\n        self.log.info('starting the ``_file_prefix`` method')\n        if self.ra:\n            now = datetime.now()\n            prefix = now.strftime(\"%Y%m%dt%H%M%S%f_tns_conesearch_\")\n        elif self.name:\n            prefix = self.name + \"_tns_conesearch_\"\n        elif self.internal_name:\n            prefix = self.internal_name + \"_tns_conesearch_\"\n        elif self.discInLastDays:\n            discInLastDays = str(self.discInLastDays)\n            now = datetime.now()\n            prefix = now.strftime(\n                discInLastDays + \"d_since_%Y%m%d_tns_conesearch_\")\n        self.log.info('completed the ``_file_prefix`` method')\n        return prefix", "contrast": "def get_file_prefix(search_type):\n    if search_type == 'linear':\n        prefix = 'linear_search_results_'\n    elif search_type == 'binary':\n        prefix = 'binary_search_results_'\n    else:\n        prefix = 'search_results_'\n    return prefix", "label": 0}
{"index": "gp043990", "code": "def swap_dims(self, dims_dict):\n        ds = self._to_temp_dataset().swap_dims(dims_dict)\n        return self._from_temp_dataset(ds)", "contrast": "def swap_dims(dims_dict):\n    return Dataset(data_vars=None, coords=None, attrs=None, compat=None, name=None, indexes=None, explicit_coords=None, encoding=None).rename(dims_dict)", "label": 0}
{"index": "gp326369", "code": "def intermediate_to_config(self):\n    self.config = {'nodes': {}, 'links': {}}\n    for node in self.intermediate_data['nodes']:\n        self.config['nodes'][node['name']] = {'location': node['location']}\n    for link in self.intermediate_data['links']:\n        self.config['links'][(link['src'], link['dst'])] = {'cost': link['cost']}", "contrast": "def to_netjson(self, remove_block=True):\n        result = OrderedDict()\n        intermediate_data = list(self.intermediate_data[self.intermediate_key])\n        for index, block in enumerate(intermediate_data):\n            if self.should_skip_block(block):\n                continue\n            if remove_block:\n                self.intermediate_data[self.intermediate_key].remove(block)\n            result = self.to_netjson_loop(block, result, index + 1)\n        return result", "label": 1}
{"index": "gp277663", "code": "import subprocess\ndef read_sphinx_config(source: str, current_name: str) -> dict:\n    try:\n        sphinx_config = subprocess.run(['sphinx-build', '-b', 'dummy', '-D', f'version={current_name}', source, '-q'],\n                                       capture_output=True, text=True, check=True)\n        config_values = sphinx_config.stdout.split('\\n')\n        config = {}\n        for pair in config_values:\n            if pair:\n                key, value = pair.split('=')\n                config[key] = value\n        return config\n    except subprocess.CalledProcessError:\n        raise HandledError('sphinx-build failed')", "contrast": "def read_config(source, current_name):\n    log = logging.getLogger(__name__)\n    queue = multiprocessing.Queue()\n    config = Config.from_context()\n    with TempDir() as temp_dir:\n        argv = ('sphinx-build', source, temp_dir)\n        log.debug('Running sphinx-build for config values with args: %s', str(argv))\n        child = multiprocessing.Process(target=_read_config, args=(argv, config, current_name, queue))\n        child.start()\n        child.join()  \n        if child.exitcode != 0:\n            log.error('sphinx-build failed for branch/tag while reading config: %s', current_name)\n            raise HandledError\n    config = queue.get()\n    return config", "label": 1}
{"index": "gp084180", "code": "def replace_state_by_id(cls, state_id, state, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._replace_state_by_id_with_http_info(state_id, state, **kwargs)\n        else:\n            (data) = cls._replace_state_by_id_with_http_info(state_id, state, **kwargs)\n            return data", "contrast": "def replace_state_by_id(async: bool, state_id: str, state: State) -> Union[State, Thread]:\n    pass ", "label": 0}
{"index": "gp330721", "code": "def exception_tween_factory(handler, registry):\n    def exception_tween(request):\n        if not request.environ.get('ALLOW_OWS_SERVICE'):\n            raise Exception(\"Access to OWS service is not allowed\")\n        response = handler(request)\n        return response\n    return exception_tween", "contrast": "def ows_security_tween_factory(handler, registry):\n    security = owssecurity_factory(registry)\n    def ows_security_tween(request):\n        try:\n            security.check_request(request)\n            return handler(request)\n        except OWSException as err:\n            logger.exception(\"security check failed.\")\n            return err\n        except Exception as err:\n            logger.exception(\"unknown error\")\n            return OWSNoApplicableCode(\"{}\".format(err))\n    return ows_security_tween", "label": 1}
{"index": "gp055532", "code": "def optimize_auto(self,max_iters=10000,verbose=True):\n        self.Z.fix(warning=False)\n        self.kern.fix(warning=False)\n        self.kern_row.fix(warning=False)\n        self.Zr.fix(warning=False)\n        self.Xr.fix(warning=False)\n        self.optimize(max_iters=int(0.1*max_iters),messages=verbose)\n        self.unfix()\n        self.optimize(max_iters=max_iters,messages=verbose)", "contrast": "def optimize_params(max_iters, verbose):\n    for i in range(max_iters):\n        if verbose:\n            print(f\"Iteration {i+1} of {max_iters}\")", "label": 0}
{"index": "gp272263", "code": "def validate_schema(value, schema):\n    try:\n        validate(value, schema)\n    except ValidationError:\n        raise ValidationError(\"Validation failed\")", "contrast": "def validate(schema, value, noun='value'):\n    errors = schema.errors(value)\n    if errors:\n        error_details = ''\n        for error in errors:\n            if error.pointer:\n                error_details += '  - %s: %s\\n' % (error.pointer, error.message)\n            else:\n                error_details += '  - %s\\n' % error.message\n        raise ValidationError('Invalid %s:\\n%s' % (noun, error_details))", "label": 1}
{"index": "gp279743", "code": "def get_deserializer(format):\n    if format == 'json':\n        return json.loads\n    elif format == 'yaml':\n        return yaml.safe_load\n    elif format == 'pickle':\n        return pickle.load\n    else:\n        raise ValueError(f'Invalid format: {format}')", "contrast": "def get_deserializer(serializer_format):\n    if serializer_format == Format.JSON:\n        return _deserialize_json\n    if serializer_format == Format.PICKLE:\n        return _deserialize_pickle", "label": 1}
{"index": "gp177579", "code": "import azure.mgmt.resource.resources as resources\nfrom azure.common.credentials import ServicePrincipalCredentials\nfrom azure.mgmt.resource.exceptions import ResourceNotFound\ndef try_delete_resource_group(subscription_id, resource_group_name, credentials):\n    resource_client = resources.ResourceManagementClient(credentials, subscription_id)\n    try:\n        resource_client.resource_groups.delete(resource_group_name)\n        print(\"Resource group '{}' has been deleted.\".format(resource_group_name))\n    except ResourceNotFound:\n        print(\"Resource group '{}' was not found.\".format(resource_group_name))", "contrast": "def _checkIfClusterExists(self):\n        ansibleArgs = {\n            'resgrp': self.clusterName,\n            'region': self._zone\n        }\n        try:\n            self.callPlaybook(self.playbook['check-cluster'], ansibleArgs, wait=True)\n        except RuntimeError:\n            logger.info(\"The cluster could not be created. Try deleting the cluster if it already exits.\")\n            raise", "label": 1}
{"index": "gp253932", "code": "def write_to_socket(frame_data: str) -> None:\n    socket.sendall(frame_data.encode())", "contrast": "def write_to_socket(self, frame_data):\n        self._wr_lock.acquire()\n        try:\n            total_bytes_written = 0\n            bytes_to_send = len(frame_data)\n            while total_bytes_written < bytes_to_send:\n                try:\n                    if not self.socket:\n                        raise socket.error('connection/socket error')\n                    bytes_written = (\n                        self.socket.send(frame_data[total_bytes_written:])\n                    )\n                    if bytes_written == 0:\n                        raise socket.error('connection/socket error')\n                    total_bytes_written += bytes_written\n                except socket.timeout:\n                    pass\n                except socket.error as why:\n                    if why.args[0] in (EWOULDBLOCK, EAGAIN):\n                        continue\n                    self._exceptions.append(AMQPConnectionError(why))\n                    return\n        finally:\n            self._wr_lock.release()", "label": 1}
{"index": "gp284380", "code": "from random import randint\nfrom datetime import datetime, timedelta\ndef generate_data(days=30):\n    now = datetime.now()\n    data = [{'x': (now - timedelta(days=i)).timestamp(), 'y': randint(50, 200)} for i in range(days)]\n    total = []\n    count = 0\n    for day in data:\n        count += day['y']\n        total.append(count)\n    return data, total", "contrast": "def newlogins(sessions):\n    if not sessions:\n        return [], []\n    users = {}\n    dates = {}\n    for session in sessions:\n        user = session.user\n        date = session.started_at.strftime(\"%Y/%m/%d\")\n        if user not in users:\n            users[user] = date\n            if date not in dates:\n                dates[date] = [user]\n            else:\n                dates[date].append(user)\n    data = []\n    total = []\n    previous = 0\n    for date in sorted(dates.keys()):\n        date_epoch = unix_time_millis(datetime.strptime(date, \"%Y/%m/%d\"))\n        data.append({\"x\": date_epoch, \"y\": len(dates[date])})\n        previous += len(dates[date])\n        total.append({\"x\": date_epoch, \"y\": previous})\n    return data, total", "label": 1}
{"index": "gp091009", "code": "def encode(text, orig_coding):\r\n    if orig_coding == 'utf-8-bom':\r\n        return BOM_UTF8 + text.encode(\"utf-8\"), 'utf-8-bom'\r\n    coding = get_coding(text)\r\n    if coding:\r\n        try:\r\n            return text.encode(coding), coding\r\n        except (UnicodeError, LookupError):\r\n            raise RuntimeError(\"Incorrect encoding (%s)\" % coding)\r\n    if (orig_coding and orig_coding.endswith('-default') or\r\n       orig_coding.endswith('-guessed')):\r\n        coding = orig_coding.replace(\"-default\", \"\")\r\n        coding = orig_coding.replace(\"-guessed\", \"\")\r\n        try:\r\n            return text.encode(coding), coding\r\n        except (UnicodeError, LookupError):\r\n            pass\r\n    try:\r\n        return text.encode('ascii'), 'ascii'\r\n    except UnicodeError:\r\n        pass\r\n    return text.encode('utf-8'), 'utf-8'", "contrast": "def encode_text(text: str, orig_coding: str) -> Tuple[str, str]:\n    encoded = text.encode(orig_coding)\n    return encoded.decode(), orig_coding", "label": 0}
{"index": "gp163071", "code": "def _sorted_keys(self, keys, startkey, reverse=False):\n        tuple_key = lambda t: t[::-1]\n        if reverse:\n            tuple_cmp = lambda t: t[::-1] > startkey[::-1]\n        else:\n            tuple_cmp = lambda t: t[::-1] < startkey[::-1]\n        searchkeys = sorted(keys, key=tuple_key, reverse=reverse)\n        searchpos = sum(1 for _ in ifilter(tuple_cmp, searchkeys))\n        searchkeys = searchkeys[searchpos:] + searchkeys[:searchpos]\n        for key in searchkeys:\n            yield key", "contrast": "def sorted_key_generator(keys, startkey, reverse=False):\n    sorted_keys = sorted(keys, reverse=reverse)\n    index = sorted_keys.index(startkey)\n    for key in sorted_keys[index:]:\n        yield key", "label": 0}
{"index": "gp301171", "code": "def empty_label(duration):\n    return {\n        'start': 0,\n        'end': duration,\n        'value': None\n    }", "contrast": "def empty(self, duration):\n        ann = super(DynamicLabelTransformer, self).empty(duration)\n        ann.append(time=0, duration=duration, value=None)\n        return ann", "label": 1}
{"index": "gp122590", "code": "def cybox_defined_object_in_fact_term_handler(self, enrichment, fact, attr_info, add_fact_kargs):\n        add_fact_kargs['fact_term_name'] = self.RE_DEFINED_OBJECT.sub('Properties', fact['term'])\n        return True", "contrast": "def update_xml_properties(xml_file):\n    try:\n        import xml.etree.ElementTree as ET\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n        nodes = root.findall(\".//Defined_Object\")\n        for node in nodes:\n            node.tag = 'Properties'\n        tree.write(xml_file)\n    except:\n        print('Error updating the XML file.')", "label": 0}
{"index": "gp221833", "code": "import requests\ndef send_request(request, **kwargs):\n    if 'session' in kwargs:\n        session = kwargs['session']\n        kwargs.pop('session')\n    else:\n        session = requests.Session()\n    allowed_kwargs = {'cookies', 'verify', 'timeout', 'allow_redirects', 'proxies', 'cert'}\n    kwargs = {k:v for k,v in kwargs.items() if k in allowed_kwargs}\n    return session.request(method=request.method, url=request.url, **kwargs)", "contrast": "def send(self, request, **kwargs):\n        requests_kwargs = self._configure_send(request, **kwargs)\n        return super(RequestsHTTPSender, self).send(request, **requests_kwargs)", "label": 1}
{"index": "gp084623", "code": "def get_stripe_gateway_by_id(cls, stripe_gateway_id, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._get_stripe_gateway_by_id_with_http_info(stripe_gateway_id, **kwargs)\n        else:\n            (data) = cls._get_stripe_gateway_by_id_with_http_info(stripe_gateway_id, **kwargs)\n            return data", "contrast": "def get_stripe_gateway_by_id(stripe_gateway_id: str, async: bool = False) -> 'StripeGateway':\n    url = '/stripeGateways/{stripeGatewayId}'\n    url = url.replace('{format}', 'json')\n    url = url.replace('{' + 'stripeGatewayId' + '}', str(stripe_gateway_id))\n    queryParams = {}\n    headerParams = {}\n    formParams = {}\n    return self.api_client.call_api(\n            url, 'GET',\n            headerParams=headerParams,\n            queryParams=queryParams,\n            postBody=postBody,\n            async=async)['response']", "label": 0}
{"index": "gp219713", "code": "def parse_unlock_result(result):\n    if result.startswith(\"OK\"):\n        return True\n    else:\n        raise pyhsm.exception.YHSM_CommandFailed(\"YubiHSM failed to unlock key storage\")", "contrast": "def parse_result(self, data):\n        fmt = \"B\"\n        self.status, = struct.unpack(fmt, data)\n        if self.status == pyhsm.defines.YSM_STATUS_OK:\n            return True\n        else:\n            raise pyhsm.exception.YHSM_CommandFailed(pyhsm.defines.cmd2str(self.command), self.status)", "label": 1}
{"index": "gp053211", "code": "def lifter(cepstra, L=22):\n    if L > 0:\n        nframes,ncoeff = numpy.shape(cepstra)\n        n = numpy.arange(ncoeff)\n        lift = 1 + (L/2.)*numpy.sin(numpy.pi*n/L)\n        return lift*cepstra\n    else:\n        return cepstra", "contrast": "import numpy as np\ndef apply_lifter(cepstra, L=22):\n    if L <= 0:\n        return cepstra\n    nframes, ncoeff = np.shape(cepstra)\n    lift = 1 + (L / 2.) * np.sin(np.pi * np.arange(ncoeff) / L)\n    return lift * cepstra", "label": 0}
{"index": "gp307444", "code": "def call_from_base_type(value):\n    if isinstance(value, _BaseValue):\n        return value._from_base_type()\n    else:\n        return value", "contrast": "def _opt_call_from_base_type(self, value):\n    if isinstance(value, _BaseValue):\n      value = self._call_from_base_type(value.b_val)\n    return value", "label": 1}
{"index": "gp129217", "code": "def make_date(dt, date_parser=parse_date):\n    if not dt:\n        return datetime.date(1970, 1, 1)\n    if isinstance(dt, basestring):\n        dt = date_parser(dt)\n    try:\n        dt = dt.timetuple()[:3]\n    except:\n        dt = tuple(dt)[:3]\n    return datetime.date(*dt)", "contrast": "import datetime\nimport numpy as np\ndef make_date(dt):\n    if dt is None or dt == '':\n        return datetime.date(1970, 1, 1)\n    if isinstance(dt, str):\n        dt = datetime.datetime.strptime(dt, '%I:%M %p').time()\n    elif isinstance(dt, (datetime.datetime, np.datetime64)):\n        dt = dt.time()        \n    elif isinstance(dt, datetime.time):\n        pass\n    else:\n        raise ValueError('Unsupported date/time format: {}'.format(type(dt)))\n    return dt", "label": 0}
{"index": "gp190666", "code": "import torch.nn as nn\ndef create_network():\n    return nn.Sequential(\n        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2),\n        nn.Flatten(),\n        nn.Linear(in_features=32*8*8, out_features=10)\n    )", "contrast": "def createDenseCNNModel(self):\n    model = nn.Sequential(\n      nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels[0],\n                kernel_size=self.kernel_size[0], stride=self.stride[0],\n                padding=self.padding[0]),\n      nn.MaxPool2d(kernel_size=2),\n      nn.ReLU(),\n      nn.Conv2d(in_channels=self.out_channels[0], out_channels=self.out_channels[1],\n                kernel_size=self.kernel_size[1], stride=self.stride[1],\n                padding=self.padding[1]),\n      nn.MaxPool2d(kernel_size=2),\n      nn.ReLU(),\n      Flatten(),\n      nn.Linear(self.cnn_output_len[1], self.n),\n      nn.ReLU(),\n      nn.Linear(self.n, self.output_size),\n      nn.LogSoftmax(dim=1)\n    )\n    model.to(self.device)\n    if torch.cuda.device_count() > 1:\n      model = torch.nn.DataParallel(model)\n    return model", "label": 1}
{"index": "gp243334", "code": "def unlock_next_button():\n    clear_aggregation_layers()\n    enable_next_button()\n@QtCore.pyqtSlot()\ndef on_radiobutton_activated():\n    unlock_next_button()", "contrast": "def on_rbAggLayerNoAggregation_toggled(self):\n        self.parent.aggregation_layer = None\n        self.parent.pbnNext.setEnabled(True)", "label": 1}
{"index": "gp148272", "code": "def get_document(project_id, knowledge_base_id, document_id):\n    import dialogflow_v2beta1 as dialogflow\n    client = dialogflow.DocumentsClient()\n    document_path = client.document_path(project_id, knowledge_base_id,\n                                         document_id)\n    response = client.get_document(document_path)\n    print('Got Document:')\n    print(' - Display Name: {}'.format(response.display_name))\n    print(' - Knowledge ID: {}'.format(response.name))\n    print(' - MIME Type: {}'.format(response.mime_type))\n    print(' - Knowledge Types:')\n    for knowledge_type in response.knowledge_types:\n        print('    - {}'.format(KNOWLEDGE_TYPES[knowledge_type]))\n    print(' - Source: {}\\n'.format(response.content_uri))", "contrast": "from google.cloud import dialogflow_v2beta1 as dialogflow\ndef get_document(project_id, knowledge_base_id, document_id):\n    client = dialogflow.DocumentsClient()\n    document_path = client.document_path(\n        project_id, knowledge_base_id, document_id\n    )\n    document = client.get_document(request={\"name\": document_path})\n    return document", "label": 0}
{"index": "gp151380", "code": "def get_preconfigured_ssl_connection(\n            self,\n            override_ssl_version: Optional[OpenSslVersionEnum] = None,\n            ssl_verify_locations: Optional[str] = None,\n            should_use_legacy_openssl: Optional[bool] = None,\n    ) -> SslConnection:\n        if override_ssl_version is not None:\n            final_ssl_version = override_ssl_version\n            openssl_cipher_string = None\n        else:\n            final_ssl_version = self.highest_ssl_version_supported\n            openssl_cipher_string = self.openssl_cipher_string_supported\n        if should_use_legacy_openssl is not None:\n            openssl_cipher_string = None\n        if self.client_auth_credentials is not None:\n            should_ignore_client_auth = False\n        else:\n            should_ignore_client_auth = True\n            if self.client_auth_requirement == ClientAuthenticationServerConfigurationEnum.REQUIRED:\n                should_ignore_client_auth = False\n        ssl_connection = SslConnectionConfigurator.get_connection(\n            ssl_version=final_ssl_version,\n            server_info=self,\n            openssl_cipher_string=openssl_cipher_string,\n            ssl_verify_locations=ssl_verify_locations,\n            should_use_legacy_openssl=should_use_legacy_openssl,\n            should_ignore_client_auth=should_ignore_client_auth,\n        )\n        return ssl_connection", "contrast": "import ssl\ndef get_ssl_connection(server_name):\n    context = ssl.create_default_context()\n    context.check_hostname = False\n    context.verify_mode = ssl.CERT_NONE\n    connection = context.wrap_socket(socket.socket(), server_hostname=server_name)\n    return connection", "label": 0}
{"index": "gp107129", "code": "def delete_attachment(cls, session, attachment):\n        return super(Conversations, cls).delete(\n            session,\n            attachment,\n            endpoint_override='/attachments/%s.json' % attachment.id,\n            out_type=Attachment,\n        )", "contrast": "def delete_attachment(session, attachment):\n    url = f\"https://docsapi.helpscout.net/v1/attachments/{attachment.id}\"\n    response = session.delete(url)\n    response.raise_for_status()", "label": 0}
{"index": "gp009364", "code": "def resize_height(image, size, resample=Image.LANCZOS):\n    try:\n        height = size[1]\n    except:\n        height = size\n    img_format = image.format\n    img = image.copy()\n    img_size = img.size\n    if img_size[1] == height:\n        return image\n    new_width = int(math.ceil((height / img_size[1]) * img_size[0]))\n    img.thumbnail((new_width, height), resample)\n    img.format = img_format\n    return img", "contrast": "from PIL import Image\ndef resize_image(image, size):\n    return image.resize(size)", "label": 0}
{"index": "gp286397", "code": "def finish_and_upload_report(stats):\n    stats.update()\n    if configuration.uploading_enabled:\n        stats.upload_report()\n        stats.save_reports()\n    elif not configuration.uploading_enabled and not configuration.uploading_disabled:\n        if user_wants_uploading():\n            configuration.uploading_enabled = True\n            finish_and_upload_report(stats)\n        else:\n            stats.save_report()\n    else:\n        stats.save_report()", "contrast": "def submit(self, info, *flags):\n        if not self.recording:\n            return\n        env_val = os.environ.get(self.env_var, '').lower()\n        if env_val not in (None, '', '1', 'on', 'enabled', 'yes', 'true'):\n            self.status = Stats.DISABLED_ENV\n            self.notes = None\n            return\n        if self.notes is None:\n            raise ValueError(\"This report has already been submitted\")\n        all_info, self.notes = self.notes, None\n        all_info.extend(self._to_notes(info))\n        for flag in flags:\n            flag(self, all_info)\n        now = time.time()\n        secs = int(now)\n        msecs = int((now - secs) * 1000)\n        all_info.insert(0, ('date', '%d.%d' % (secs, msecs)))\n        if self.user_id:\n            all_info.insert(1, ('user', self.user_id))\n        logger.debug(\"Generated report:\\n%r\", (all_info,))\n        def generator():\n            for key, value in all_info:\n                yield _encode(key) + b':' + _encode(value) + b'\\n'\n        filename = 'report_%d_%d.txt' % (secs, msecs)\n        if not self.sending:\n            fullname = os.path.join(self.location, filename)\n            with open(fullname, 'wb') as fp:\n                for l in generator():\n                    fp.write(l)\n            sys.stderr.write(self.prompt.prompt)\n            return\n        old_reports = [f for f in os.listdir(self.location)\n                       if f.startswith('report_')]\n        old_reports.sort()\n        old_reports = old_reports[:4]  \n        for old_filename in old_reports:\n            fullname = os.path.join(self.location, old_filename)\n            try:\n                with open(fullname, 'rb') as fp:\n                    r = requests.post(self.drop_point, data=fp.read(),\n                                      timeout=1, verify=self.ssl_verify)\n                    r.raise_for_status()\n            except Exception as e:\n                logger.warning(\"Couldn't upload %s: %s\", old_filename, str(e))\n                break\n            else:\n                logger.info(\"Submitted report %s\", old_filename)\n                os.remove(fullname)\n        try:\n            r = requests.post(self.drop_point, data=b''.join(generator()),\n                              timeout=1, verify=self.ssl_verify)\n        except requests.RequestException as e:\n            logger.warning(\"Couldn't upload report: %s\", str(e))\n            fullname = os.path.join(self.location, filename)\n            with open(fullname, 'wb') as fp:\n                for l in generator():\n                    fp.write(l)\n        else:\n            try:\n                r.raise_for_status()\n                logger.info(\"Submitted report\")\n            except requests.RequestException as e:\n                logger.warning(\"Server rejected report: %s\", str(e))", "label": 1}
{"index": "gp151423", "code": "def login(self, username='0000', userid=0, password=None):\n  if password and len(password) > 20:\n\t\t\tself.logger.error('password longer than 20 characters received')\n   raise Exception('password longer than 20 characters, login failed')\n  self.send(C1218LogonRequest(username, userid))\n  data = self.recv()\n  if data != b'\\x00':\n\t\t\tself.logger.warning('login failed, username and user id rejected')\n   return False\n  if password is not None:\n\t\t\tself.send(C1218SecurityRequest(password))\n   data = self.recv()\n   if data != b'\\x00':\n\t\t\t\tself.logger.warning('login failed, password rejected')\n    return False\n  self.logged_in = True\n  return True", "contrast": "def login_to_device(username: str, userid: int, password: str) -> bool:\n    if len(username) > 10 or len(password) > 20 or not (0 <= userid <= 65535):\n        return False\n    return True", "label": 0}
{"index": "gp043452", "code": "def remove_keywords_from_dict(self, keyword_dict):\n        for clean_name, keywords in keyword_dict.items():\n            if not isinstance(keywords, list):\n                raise AttributeError(\"Value of key {} should be a list\".format(clean_name))\n            for keyword in keywords:\n                self.remove_keyword(keyword)", "contrast": "def remove_keywords_from_dict(keyword_dict):\n    for key, value in keyword_dict.items():\n        if not isinstance(value, list):\n            raise AttributeError(f\"Value for '{key}' in keyword_dict should be a list.\")\n        keyword_dict[key] = [v for v in value if v != key]\n    return keyword_dict", "label": 0}
{"index": "gp160864", "code": "def astensor(array: TensorLike) -> BKTensor:\n    array = np.asarray(array, dtype=CTYPE)\n    return array", "contrast": "import tensorflow as tf\nimport numpy as np\ndef convert_numpy_to_tensor(arr: np.ndarray) -> tf.Tensor:\n    tensor = tf.convert_to_tensor(arr)\n    return tensor", "label": 0}
{"index": "gp190828", "code": "def compute_cnn_max_pool_width(input_width, num_conv_layers, num_pool_layers, filter_width=3, pool_width=2):\n    width = input_width\n    for i in range(num_conv_layers):\n        width = width - filter_width + 1\n    for i in range(num_pool_layers):\n        width = (width - pool_width) // pool_width + 1\n    return width", "contrast": "def computeMaxPool(input_width):\n  wout = math.floor((input_width + 2 * PADDING - KERNEL_SIZE) / STRIDE + 1)\n  return int(math.floor(wout / 2.0))", "label": 1}
{"index": "gp027100", "code": "def batch_augment(x, func, device='/CPU:0'):\n  with tf.device(device):\n    return tf.map_fn(func, x)", "contrast": "import torch\ndef apply_augmentation(x, func, device):\n    x = x.to(device)\n    augmented_x = []\n    for image in x:\n        augmented_x.append(func(image))\n    return torch.stack(augmented_x)", "label": 0}
{"index": "gp118944", "code": "def file_add(self, ev, paths):\n        log = self._params.get('log', self._discard)\n        if not isinstance(paths, list):\n            paths = [paths]\n        for path in paths:\n            if path not in self._file_event_map:\n                self._watch_files.add(path)\n                self._file_event_map[path] = {}\n            self._file_event_map[path][ev.get_key()] = ev\n        log.debug(\"Added event key %r, action %r to path%s: %s\", ev.get_key(), ev._handler_name, ses(len(paths)), paths)", "contrast": "def register_event(path: str, task: object, action: str) -> None:\n    if not hasattr(register_event, 'registry'):\n        register_event.registry = {}\n    registry = register_event.registry\n    path_registry = registry.setdefault(path, {})\n    task_action = path_registry.get(task)\n    if task_action is None:\n        path_registry[task] = action\n    elif isinstance(task_action, list):\n        task_action.append(action)\n    else:\n        path_registry[task] = [task_action, action]", "label": 0}
{"index": "gp079810", "code": "def _update_status(self, sub_job_num=None):\n        job_id = '%s.%s' % (self.cluster_id, sub_job_num) if sub_job_num else str(self.cluster_id)\n        format = ['-format', '\"%d\"', 'JobStatus']\n        cmd = 'condor_q {0} {1} && condor_history {0} {1}'.format(job_id, ' '.join(format))\n        args = [cmd]\n        out, err = self._execute(args, shell=True, run_in_job_dir=False)\n        if err:\n            log.error('Error while updating status for job %s: %s', job_id, err)\n            raise HTCondorError(err)\n        if not out:\n            log.error('Error while updating status for job %s: Job not found.', job_id)\n            raise HTCondorError('Job not found.')\n        out = out.replace('\\\"', '').split('\\n')\n        status_code = 0\n        for status_code_str in out:\n            try:\n                status_code = int(status_code_str.strip())\n            except:\n                pass\n        log.info('Job %s status: %d', job_id, status_code)\n        key = CONDOR_JOB_STATUSES[status_code]\n        return key", "contrast": "def get_workflow_status() -> str:\n    pass", "label": 0}
{"index": "gp231093", "code": "def send_blocks(data, length, method, sock_type):\n    if sock_type == socket.SOCK_STREAM:\n        sent_bytes = 0\n        while len(data) > 0:\n            block = data[:length]\n            sent_bytes += method(block)\n            data = data[length:]\n        return sent_bytes\n    elif sock_type == socket.SOCK_DGRAM:\n        sent_bytes = 0\n        for i in range(0, len(data), length):\n            block = data[i:i+length]\n            sent_bytes += method(block)\n        return sent_bytes\n    else:\n        raise ValueError(\"Unknown socket type\")", "contrast": "def write_to_stream(stream, response, block_len=1024):\n        try:\n            i_start = 0\n            i_end = block_len\n            while True:\n                if i_end > len(response):\n                    stream(response[i_start:])\n                    break\n                stream(response[i_start:i_end])\n                i_start = i_end\n                i_end += block_len\n        except (socket.timeout, socket.error) as exception:\n            logger.debug('Error sending response to the client: %s', exception)", "label": 1}
{"index": "gp011513", "code": "def open_in_browser(doc, encoding=None):\n    import os\n    import webbrowser\n    import tempfile\n    if not isinstance(doc, etree._ElementTree):\n        doc = etree.ElementTree(doc)\n    handle, fn = tempfile.mkstemp(suffix='.html')\n    f = os.fdopen(handle, 'wb')\n    try:\n        doc.write(f, method=\"html\", encoding=encoding or doc.docinfo.encoding or \"UTF-8\")\n    finally:\n        f.close()\n    url = 'file://' + fn.replace(os.path.sep, '/')\n    print(url)\n    webbrowser.open(url)", "contrast": "import webbrowser\nimport tempfile\ndef open_html_in_browser(html_content):\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n        filename = f.name\n        f.write(html_content)\n    webbrowser.open('file://' + filename)", "label": 0}
{"index": "gp033771", "code": "def list_datacenters_via_proxy(datacenter_names=None, service_instance=None):\n    if not datacenter_names:\n        dc_refs = salt.utils.vmware.get_datacenters(service_instance,\n                                                    get_all_datacenters=True)\n    else:\n        dc_refs = salt.utils.vmware.get_datacenters(service_instance,\n                                                    datacenter_names)\n    return [{'name': salt.utils.vmware.get_managed_object_name(dc_ref)}\n            for dc_ref in dc_refs]", "contrast": "def list_datacenters_via_proxy(prox, datacenter_names=None, service_instance=None):\n    try:\n        si = __get_service_instance_via_proxy(prox, service_instance)\n        content = si.RetrieveContent()\n        datacenters = __get_datacenters(content, datacenter_names)\n        ret_list = []\n        for dc in datacenters:\n            dc_name = dc.name\n            dc_moref = dc._moId\n            dc_dict = {'name': dc_name,\n                       'moref': dc_moref}\n            ret_list.append(dc_dict)\n        return ret_list\n    except Exception as exc:\n        return {'error': \"{0}\".format(exc)}", "label": 0}
{"index": "gp288483", "code": "from typing import Any, Callable\ndef method_caller(method_name: str) -> Callable[..., Any]:\n    def call_method(obj: Any, *args: Any, **kwargs: Any) -> Any:\n        return getattr(obj, method_name)(*args, **kwargs)\n    return call_method", "contrast": "def method_caller(method_name, *args, **kwargs):\n def call_method(target):\n\t\tfunc = getattr(target, method_name)\n  return func(*args, **kwargs)\n return call_method", "label": 1}
{"index": "gp086590", "code": "def getImportFromObjects(node):\n    somenames = [x.asname for x in node.names if x.asname]\n    othernames = [x.name for x in node.names if not x.asname]\n    return somenames+othernames", "contrast": "import_ast = {\n    'node1': {\n        'imports': ['module1', 'module2'],\n        'objects': {\n            'item1': 'module1',\n            'item2': 'module2'\n        }\n    },\n    'node2': {\n        'imports': ['module3'],\n        'objects': {\n            'item3': 'module3'\n        }\n    }\n}\ndef get_imported_objects(node, import_ast):\n    imported_objects = []\n    for imp in import_ast[node]['imports']:\n        for obj in import_ast[node]['objects']:\n            if import_ast[node]['objects'][obj] == imp:\n                imported_objects.append(obj)\n    return imported_objects", "label": 0}
{"index": "gp032211", "code": "def write_cron_file(user, path):\n    if _check_instance_uid_match(user) or __grains__.get('os_family') in ('Solaris', 'AIX'):\n        return __salt__['cmd.retcode'](_get_cron_cmdstr(path),\n                                       runas=user,\n                                       python_shell=False) == 0\n    else:\n        return __salt__['cmd.retcode'](_get_cron_cmdstr(path, user),\n                                       python_shell=False) == 0", "contrast": "import os\ndef write_cron_file(user, file_path):\n    try:\n        os.system(f\"crontab -u {user} {file_path}\")\n        return True\n    except Exception:\n        return False", "label": 0}
{"index": "gp205565", "code": "def equivalent_contigs(contig_dict):\n    equivalent_contig_sets = []\n    for contigs in contig_dict.values():\n        is_new_set = True\n        for equal_set in equivalent_contig_sets:\n            if equal_set == contigs:\n                is_new_set = False\n                break\n        if is_new_set:\n            equivalent_contig_sets.append(contigs)\n    return equivalent_contig_sets", "contrast": "def _get_identical_contigs(self, hits_dict):\n        equivalent_contigs = []\n        for qry_name, containing in hits_dict.items():\n            equivalent = set()\n            for containing_name in containing:\n                if containing_name in hits_dict and qry_name in hits_dict[containing_name]:\n                    equivalent.add(containing_name)\n                    equivalent.add(qry_name)\n            if len(equivalent):\n                equivalent_contigs.append(equivalent)\n                equivalent_contigs = self._collapse_list_of_sets(equivalent_contigs)\n        return equivalent_contigs", "label": 1}
{"index": "gp317998", "code": "from gevent import monkey\nfrom socketio.server import SocketIOServer\ndef start_socketio_server():\n    monkey.patch_all()  \n    server = SocketIOServer(('0.0.0.0', 8080), app)\n    server.serve_forever()", "contrast": "def serve_forever(django=False):\n    logger = getLogger(\"irc.dispatch\")\n    logger.setLevel(settings.LOG_LEVEL)\n    logger.addHandler(StreamHandler())\n    app = IRCApplication(django)\n    server = SocketIOServer((settings.HTTP_HOST, settings.HTTP_PORT), app)\n    print \"%s [Bot: %s] listening on %s:%s\" % (\n        settings.GNOTTY_VERSION_STRING,\n        app.bot.__class__.__name__,\n        settings.HTTP_HOST,\n        settings.HTTP_PORT,\n    )\n    server.serve_forever()", "label": 1}
{"index": "gp004002", "code": "def option_attrname(self, opt, optdict=None):\n        if optdict is None:\n            optdict = self.get_option_def(opt)\n        return optdict.get(\"dest\", opt.replace(\"-\", \"_\"))", "contrast": "def get_config_attribute(config, opt):\n    return config[opt]", "label": 0}
{"index": "gp109911", "code": "def wordvalue(word):\n    total = 0\n    for i in enumerate(word):\n        total += letternum(word[i[0]])\n    return total", "contrast": "def get_alphabetical_value(word):\n    return sum(ord(letter.lower()) - 96 for letter in word)", "label": 0}
{"index": "gp139883", "code": "def authorize(self, api_token=None, username=None, password=None):\n        access_token = None\n        if username and password:\n            auth = ExistAuthBasic(username, password)\n            auth.authorize()\n            if auth.token:\n                access_token = auth.token['access_token']\n        elif api_token:\n            access_token = api_token\n        else:\n            auth = ExistAuth(self.client_id, self.client_secret, 'code', self.redirect_uri)\n            auth.browser_authorize()\n            if auth.token:\n                access_token = auth.token['access_token']\n        if access_token:\n            self.write_config(access_token)\n        else:\n            print('ERROR: We were unable to authorize to use the Exist API.')", "contrast": "import cherrypy\ndef authorize_user():\n    class AuthServer(object):\n        @cherrypy.expose\n        def authorize(self):\n            raise cherrypy.HTTPRedirect(\"https://example.com/login\")\n        @cherrypy.expose\n        def callback(self, code=None):\n            with open(\"config.ini\", \"w\") as config:\n                config.write(f\"access_token={access_token}\\n\")\n                config.write(f\"refresh_token={refresh_token}\\n\")\n                config.write(f\"expires_at={expires_at}\\n\")\n            raise cherrypy.HTTPRedirect(\"https://example.com/success\")\n    cherrypy.quickstart(AuthServer())", "label": 0}
{"index": "gp279201", "code": "def add_category_tags(tags):\n    for tag in tags:\n        try:\n            existing_tag = smc.elements.other.Category(tag)\n        except:\n            print(\"Category tag not found, creating new tag\")\n            new_tag = smc.elements.other.Category.create(tag)\n    return None", "contrast": "def add_category(self, category):\n        assert isinstance(category, list), 'Category input was expecting list.'\n        from smc.elements.other import Category\n        for tag in category:\n            category = Category(tag)\n            try:\n                category.add_element(self.href)\n            except ElementNotFound:\n                Category.create(name=tag)\n                category.add_element(self.href)", "label": 1}
{"index": "gp242177", "code": "from werkzeug.routing import BuildError\ndef inject_x_forwarded_port(url_adapter, request_headers):\n    try:\n        x_forwarded_port = int(request_headers.get('X-Forwarded-Port', ''))\n    except ValueError:\n        raise BuildError('Invalid X-Forwarded-Port header value')\n    if x_forwarded_port:\n        server_name = request_headers.get('X-Forwarded-Server', '')\n        script_name = request_headers.get('X-Forwarded-Script-Name', '')\n        path_info = request_headers.get('X-Forwarded-Path', '')\n        query_string = request_headers.get('X-Forwarded-QueryString', '')\n        url_adapter.server_name = server_name\n        url_adapter.script_name = script_name\n        url_adapter.path_info = path_info\n        url_adapter.query_string = query_string\n        url_adapter.url_scheme = 'https' if x_forwarded_port == 443 else 'http'\n        url_adapter.server_port = x_forwarded_port", "contrast": "def use_forwarded_port(graph):\n    context = _request_ctx_stack.top\n    if _request_ctx_stack is None:\n        return None\n    forwarded_host = graph.config.port_forwarding.get(\"host\")\n    forwarded_port = request.headers.get(\"X-Forwarded-Port\")\n    if not forwarded_port and not forwarded_host:\n        return None\n    if \":\" in context.url_adapter.server_name:\n        server_host, server_port = context.url_adapter.server_name.split(\":\", 1)\n    else:\n        server_host = context.url_adapter.server_name\n        server_port = 443 if context.url_adapter.url_scheme == \"https\" else 80\n    if forwarded_host:\n        server_name = forwarded_host\n    elif server_port:\n        server_name = \"{}:{}\".format(server_host, forwarded_port)\n    else:\n        server_name = \"{}:{}\".format(server_host, server_port)\n    context.url_adapter.server_name = server_name\n    return server_name", "label": 1}
{"index": "gp271895", "code": "import openpyxl\ndef read_xlsx_to_list(filepath):\n    wb = openpyxl.load_workbook(filepath)\n    ws = wb.active\n    data_list = []\n    keys = [cell.value for cell in ws[1]]\n    for row in ws.iter_rows(min_row=2, values_only=True):\n        data_list.append({keys[i]: row[i] for i in range(len(keys))})\n    return data_list", "contrast": "def _read_xlsx_table(path):\n    workbook = pyxl.load_workbook(path)\n    worksheet = workbook.active\n    table = helpers.sheet_to_table(worksheet)\n    return table", "label": 1}
{"index": "gp095374", "code": "def po_to_unicode(po_obj):\n    po_text = po_obj.__str__()\n    if type(po_text) != types.UnicodeType:\n        po_text = po_text.decode('utf-8')\n    return po_text", "contrast": "import polib\ndef po_to_unicode(po_obj):\n    unicode_string = ''\n    if isinstance(po_obj, polib.POFile):\n        for entry in po_obj:\n            if entry.msgid_plural:\n                unicode_string += '{}\\nmsgid {}\\nmsgid_plural {}\\nmsgstr[0] {}\\n\\n'.format(\n                    entry.comment, entry.msgid, entry.msgid_plural, entry.msgstr[0])\n                for index, string in enumerate(entry.msgstr[1:], start=1):\n                    unicode_string += 'msgstr[{}] {}\\n'.format(index, string)\n            else:\n                unicode_string += '{}\\nmsgid {}\\nmsgstr {}\\n\\n'.format(\n                    entry.comment, entry.msgid, entry.msgstr)\n    elif isinstance(po_obj, polib.POEntry):\n        if po_obj.msgid_plural:\n            unicode_string += 'msgid {}\\nmsgid_plural {}\\nmsgstr[0] {}\\n\\n'.format(\n                po_obj.msgid, po_obj.msgid_plural, po_obj.msgstr[0])\n            for index, string in enumerate(po_obj.msgstr[1:], start=1):\n                unicode_string += 'msgstr[{}] {}\\n'.format(index, string)\n        else:\n            unicode_string += 'msgid {}\\nmsgstr {}\\n\\n'.format(po_obj.msgid, po_obj.msgstr)\n    else:\n        raise ValueError('Invalid po_obj type. Must be polib.PoFile or polib.PoEntry.')\n    return unicode_string", "label": 0}
{"index": "gp166645", "code": "def get_dataframe():\n        names = []\n        [names.extend(line.split()) for line in CONTROL_VARIABLE_LINES]\n        defaults = []\n        [defaults.extend(line.split()) for line in CONTROL_DEFAULT_LINES]\n        types, required,cast_defaults,formats = [],[],[],[]\n        for name,default in zip(names,defaults):\n            if '[' in name or ']' in name:\n                required.append(False)\n            else:\n                required.append(True)\n            v,t,f = ControlData._parse_value(default)\n            types.append(t)\n            formats.append(f)\n            cast_defaults.append(v)\n        return pandas.DataFrame({\"name\":names,\"type\":types,\n                                     \"value\":cast_defaults,\"required\":required,\n                                    \"format\":formats})", "contrast": "import pandas as pd\ndef get_control_section_dataframe():\n    df = pd.DataFrame({\n        'Control Section': [1, 2, 3, 4, 5],\n        'Parameter 1': [10, 20, 30, 40, 50],\n        'Parameter 2': [100, 200, 300, 400, 500],\n        'Parameter 3': [1000, 2000, 3000, 4000, 5000]\n    })\n    return df", "label": 0}
{"index": "gp182794", "code": "def find_host(end_point, metadata, inet_address=None):\n    for host in metadata:\n        if inet_address:\n            if host.broadcast_rpc_address == inet_address:\n                return host\n        elif host.endpoint == end_point:\n            return host\n    return None", "contrast": "def get_host(self, endpoint_or_address):\n        if not isinstance(endpoint_or_address, EndPoint):\n            return self._get_host_by_address(endpoint_or_address)\n        return self._hosts.get(endpoint_or_address)", "label": 1}
{"index": "gp327751", "code": "import numpy as np\nfrom scipy import special\ndef sinusoidAWGN(x, SNRdB):\n    P = np.var(x)\n    SNR = 10** (SNRdB/10)\n    N0 = P / SNR\n    w = np.sqrt(N0/2) * np.random.randn(len(x))\n    y = x + w\n    return y", "contrast": "def sinusoidAWGN(x,SNRdB):\n    x_pwr = np.var(x)\n    noise = np.sqrt(x_pwr/10**(SNRdB/10.))*np.random.randn(len(x));\n    return x + noise", "label": 1}
{"index": "gp119315", "code": "def load_context(ctx_path, ctx_type, scm=None):\n    if ctx_path == '-':\n        return loads(sys.stdin.read(), ac_parser=ctx_type, ac_schema=scm)\n    return load(ctx_path, ac_parser=ctx_type, ac_schema=scm)", "contrast": "import anyconfig\nimport json\ndef validate_context(ctx_path, ctx_type, scm):\n    if ctx_path == '-':\n        ctx_file = json.load(sys.stdin)\n    else:\n        with open(ctx_path, 'r') as file:\n            if ctx_type == 'json':\n                ctx_file = json.load(file)\n    validator = anyconfig.validate(ctx_file, scm)\n    return validator", "label": 0}
{"index": "gp068662", "code": "def get_dict_tokens_for_termid(self, term_id):\n        if self.dict_tokens_for_tid is None:\n            self.dict_tokens_for_tid = {}\n            for term in self.get_terms():\n                self.dict_tokens_for_tid[term.get_id()] = term.get_span().get_span_ids()\n        return self.dict_tokens_for_tid.get(term_id,[])", "contrast": "def get_token_span(term_id):\n    token_ids = term_id_to_token_ids_dict.get(term_id,[])\n    spans = []\n    for token_id in token_ids:\n        spans.append(token_id_to_span_dict.get(token_id))\n    return spans", "label": 0}
{"index": "gp320201", "code": "def get_max_item(tree):\n    if not tree:\n        raise ValueError('Tree is empty')\n    max_key = max(tree.keys())\n    return tree[max_key]", "contrast": "def max_item(self):\n        if self.is_empty():\n            raise ValueError(\"Tree is empty\")\n        node = self._root\n        while node.right is not None:\n            node = node.right\n        return node.key, node.value", "label": 1}
{"index": "gp207938", "code": "def find_info_by_confidence(info_dict, key, confidence):\n    if key not in info_dict:\n        return None\n    if info_dict[key]['confidence'] >= confidence:\n        return info_dict[key]['info']\n    else:\n        return None", "contrast": "def get(self, key, confidence=0):\n        if key not in self.info:\n            return None\n        conf, value = self.info.get(key)\n        if conf >= confidence:\n            return value\n        return None", "label": 1}
{"index": "gp063997", "code": "def filter_geometry(queryset, **filters):\n    fieldname = geo_field(queryset).name\n    query = {'%s__%s' % (fieldname, k): v for k, v in filters.items()}\n    return queryset.filter(**query)", "contrast": "def spatial_lookup_filter(**kwargs):\n    spatial_lookup_map = {\n        'bbcontains': 'bbcontains',\n        'bboverlaps': 'bboverlaps',\n        'contained': 'contained',\n        'contains': 'contains',\n        'disjoint': 'disjoint',\n        'equals': 'equals',\n        'intersects': 'intersects',\n        'overlaps': 'overlaps',\n        'same_as': 'same_as',\n        'touches': 'touches',\n        'within': 'within',\n    }\n    lookup = ''\n    for key, value in kwargs.items():\n        lookup_type = spatial_lookup_map.get(key)\n        if lookup_type:\n            if lookup:\n                lookup += '__'\n            lookup += lookup_type\n    return lookup, value", "label": 0}
{"index": "gp152945", "code": "def prt_num_sig(self, prt=sys.stdout, alpha=0.05):\n        ctr = self.get_num_sig(alpha)\n        prt.write(\"{N:6,} TOTAL: {TXT}\\n\".format(N=len(self.nts), TXT=\" \".join([\n            \"FDR({FDR:4})\".format(FDR=ctr['FDR']),\n            \"Bonferroni({B:4})\".format(B=ctr['Bonferroni']),\n            \"Benjamini({B:4})\".format(B=ctr['Benjamini']),\n            \"PValue({P:4})\".format(P=ctr['PValue']),\n            os.path.basename(self.fin_davidchart)])))", "contrast": "def print_significant_GO_terms(go_terms):\n    significant_terms = 0\n    for term in go_terms:\n        if term.startswith(\"GO:\") and float(term.split(\":\")[1]) < 0.01:\n            significant_terms += 1\n    print(\"Number of significant GO terms:\", significant_terms)", "label": 0}
{"index": "gp276946", "code": "def get_folder_details(file_id: str) -> File:\n    pass", "contrast": "def get_file_by_id(self, file_id):\n        return self._create_item_response(\n            self.data_service.get_file(file_id),\n            File\n        )", "label": 1}
{"index": "gp057897", "code": "def _drop_empty_props(self, item):\n        if isinstance(item, list):\n            return [self._drop_empty_props(i) for i in item]\n        if isinstance(item, dict):\n            return {\n                k: self._drop_empty_props(v)\n                for k, v in item.items() if v != ''\n            }\n        return item", "contrast": "def remove_empty_props(nested_dict):\n    def inner(d):\n        return {\n            k: inner(v) if isinstance(v, dict) else v\n            for k, v in d.items() if v != \"\" and inner(v) != {}\n        }\n    return inner(nested_dict)", "label": 0}
{"index": "gp286071", "code": "def get_vault_token(app_id, user_id):\n    token = determine_token(app_id, user_id) \n    return token", "contrast": "def app_token(vault_client, app_id, user_id):\n    resp = vault_client.auth_app_id(app_id, user_id)\n    if 'auth' in resp and 'client_token' in resp['auth']:\n        return resp['auth']['client_token']\n    else:\n        raise aomi.exceptions.AomiCredentials('invalid apptoken')", "label": 1}
{"index": "gp299267", "code": "def get_full_group_path(group):\n    full_path = \"\"\n    current_group = group\n    while current_group:\n        if full_path:\n            full_path = current_group + \"/\" + full_path\n        else:\n            full_path = current_group\n        current_group = parent_group(current_group)\n    return full_path", "contrast": "def path(self):\n        if self.dataset is self:\n            return ''\n        else:  \n            return self.dataset.path + '/' + self.name", "label": 1}
{"index": "gp177022", "code": "import networkx as nx\ndef generate_dummy_graph(graph):\n    pos = nx.spring_layout(graph)\n    nx.set_node_attributes(graph, pos, name='pos')\n    nodes_380 = []\n    nodes_220 = []\n    for node in graph.nodes.data():\n        if '380' in node[0]:\n            nodes_380.append(node[0])\n        elif '220' in node[0]:\n            nodes_220.append(node[0])\n    for node in nodes_380:\n        nodes = []\n        for neighbor in graph.neighbors(node):\n            if '220' in neighbor:\n                nodes.append(neighbor)\n        for neighbor in nodes:\n            graph.add_edge(node, neighbor)\n        graph.remove_node(node)\n    return graph", "contrast": "def generate_dummy_graph(network):\n    graph = pypsa.descriptors.OrderedGraph()\n    graph.add_nodes_from([bus for bus in network.buses.index if bus not in buses_to_split])\n    for node in graph.nodes():\n        graph.node[node][\"pos\"] = np.array(network.buses.loc[node,[\"x\",\"y\"]],dtype=float)\n    return graph", "label": 1}
{"index": "gp282151", "code": "import serial.tools.list_ports\nimport pandas as pd\ndef list_available_com_ports():\n    com_ports = [(port.device, port.description) for port in serial.tools.list_ports.comports()]\n    com_ports_df = pd.DataFrame(com_ports, columns=['port', 'descriptor']).set_index('port')\n    return com_ports_df", "contrast": "def _comports():\n    return (pd.DataFrame(list(map(list, serial.tools.list_ports.comports())),\n                         columns=['port', 'descriptor', 'hardware_id'])\n            .set_index('port'))", "label": 1}
{"index": "gp124698", "code": "def rebuild():\n    drop_db()\n    create_db()\n    if StrictVersion(django.get_version()) < StrictVersion('1.7'):\n        local('python{} manage.py syncdb --all --noinput'.format(\n            PYTHON_VERSION))\n        local('python{} manage.py migrate --fake'.format(PYTHON_VERSION))\n    else:\n        local('python{} manage.py migrate'.format(PYTHON_VERSION))", "contrast": "from django.core.management import call_command\ndef reset_db():\n    call_command('reset_db', interactive=False)\n    call_command('migrate')", "label": 0}
{"index": "gp025341", "code": "def peek(self, size: int) -> memoryview:\n        assert size > 0\n        try:\n            is_memview, b = self._buffers[0]\n        except IndexError:\n            return memoryview(b\"\")\n        pos = self._first_pos\n        if is_memview:\n            return typing.cast(memoryview, b[pos : pos + size])\n        else:\n            return memoryview(b)[pos : pos + size]", "contrast": "def get_view(buffer, size):\n    return buffer[:size]", "label": 0}
{"index": "gp234582", "code": "import requests\ndef send_http_request():\n    response = requests.get('http://satellite-url/wait_new_conf')\n    if response.status_code == 200:\n        return True\n    else:\n        return False", "contrast": "def wait_new_conf(self):\n        logger.debug(\"Wait new configuration for %s, %s %s\", self.name, self.alive, self.reachable)\n        return self.con.get('_wait_new_conf')", "label": 1}
{"index": "gp198501", "code": "import glob\ndef expand_paths(path_list, pattern):\n    expanded_paths = []\n    for path in path_list:\n        for file_path in glob.glob(path + '/' + pattern):\n            expanded_paths.append(file_path)\n    return expanded_paths", "contrast": "def extract_files(files):\n    expanded_files = []\n    legal_extensions = [\".md\", \".txt\", \".rtf\", \".html\", \".tex\", \".markdown\"]\n    for f in files:\n        if os.path.isdir(f):\n            for dir_, _, filenames in os.walk(f):\n                for filename in filenames:\n                    fn, file_extension = os.path.splitext(filename)\n                    if file_extension in legal_extensions:\n                        joined_file = os.path.join(dir_, filename)\n                        expanded_files.append(joined_file)\n        else:\n            expanded_files.append(f)\n    return expanded_files", "label": 1}
{"index": "gp091366", "code": "def extractbpflags(calpath, deststream):\n    tb = util.tools.table()\n    tb.open(b(os.path.join(calpath, 'ANTENNA')))\n    antnames = tb.getcol(b'NAME')\n    tb.close()\n    tb.open(b(calpath))\n    try:\n        t = tb.getkeyword(b'VisCal')\n    except RuntimeError:\n        raise PKError('no \"VisCal\" keyword in %s; it doesn\\'t seem to be a '\n                       'bandpass calibration table', calpath)\n    if t != 'B Jones':\n        raise PKError('table %s doesn\\'t seem to be a bandpass calibration '\n                       'table; its type is \"%s\"', calpath, t)\n    def emit(antidx, spwidx, chanstart, chanend):\n        print(\"antenna='%s&*' spw='%d:%d~%d' reason='BANDPASS_FLAGGED'\" %              (antnames[antidx], spwidx, chanstart, chanend), file=deststream)\n    for row in range(tb.nrows()):\n        ant = tb.getcell(b'ANTENNA1', row)\n        spw = tb.getcell(b'SPECTRAL_WINDOW_ID', row)\n        flag = tb.getcell(b'FLAG', row)\n        sqflag = ~((~flag).prod(axis=0, dtype=np.bool))\n        runstart = None\n        for i in range(sqflag.size):\n            if sqflag[i]:\n                if runstart is None:\n                    runstart = i\n            elif runstart is not None:\n                emit(ant, spw, runstart, i - 1)\n                runstart = None\n        if runstart is not None:\n            emit(ant, spw, runstart, i)\n    tb.close()", "contrast": "def make_flags_file(calpath:str, deststream:typing.IO):\n    with open(calpath, 'rb') as f:\n        while True:\n            chunk = f.read(1024)\n            if not chunk:\n                break\n            deststream.write(chunk)", "label": 0}
{"index": "gp110281", "code": "def export(self, location):\n        url, rev = self.get_url_rev()\n        logger.notify('Exporting svn repository %s to %s' % (url, location))\n        logger.indent += 2\n        try:\n            if os.path.exists(location):\n                rmtree(location)\n            call_subprocess(\n                [self.cmd, 'export', url, location],\n                filter_stdout=self._filter, show_stdout=False)\n        finally:\n            logger.indent -= 2", "contrast": "import subprocess\ndef export_svn(svn_url, destination_path):\n    subprocess.check_call(['svn', 'export', svn_url, destination_path])", "label": 0}
{"index": "gp130687", "code": "def NoExclusions(self):\n        if len(self.start_bounds) + len(self.target_rs) + len(self.ignored_rs) == 0:\n            return BoundaryCheck.chrom == -1\n        return False", "contrast": "def check_exclusion_criteria():\n    return True", "label": 0}
{"index": "gp067192", "code": "def discharge_token(self, username):\n        url = '{}discharge-token-for-user?username={}'.format(\n            self.url, quote(username))\n        logging.debug('Sending identity info to {}'.format(url))\n        response = make_request(url, method='GET', timeout=self.timeout)\n        try:\n            macaroon = response['DischargeToken']\n            json_macaroon = json.dumps(macaroon)\n        except (KeyError, UnicodeDecodeError) as err:\n            raise InvalidMacaroon(\n                'Invalid macaroon from discharger: {}'.format(err.message))\n        return base64.urlsafe_b64encode(\"[{}]\".format(\n            json_macaroon).encode('utf-8'))", "contrast": "import base64\ndef discharge_token(username):\n    try:\n        response_token = 'example_token_generated_for_user_' + username\n        encoded_token = base64.b64encode(response_token.encode('utf-8'))\n        return encoded_token.decode('utf-8')\n    except:\n        raise ServerError('An error occurred in the request process.')", "label": 0}
{"index": "gp128801", "code": "def rollback(self):\n        self._state_machine.transition_to_rollback()\n        for action in reversed(self._executed_actions):\n            try:\n                self.execute_with_retries(action, lambda a: a.rollback())\n            except:  \n                pass  \n        self._state_machine.transition_to_rollback_complete()", "contrast": "def call_rollback(actions):\n    for action in actions:\n        action.rollback()", "label": 0}
{"index": "gp226603", "code": "def image_provider(filename):\n    with open(filename, 'rb') as f:\n        img_data = f.read()\n    img = Image.open(io.BytesIO(img_data))\n    if img.format not in ['JPEG', 'PNG']:\n        raise ValueError('Image format not supported')\n    if img.width > 1920 or img.height > 1080:\n        raise ValueError('Image too big')", "contrast": "def setOverlayFromFile(self, ulOverlayHandle, pchFilePath):\n        fn = self.function_table.setOverlayFromFile\n        result = fn(ulOverlayHandle, pchFilePath)\n        return result", "label": 1}
{"index": "gp254216", "code": "def is_file_like(obj):\n    try:\n        obj.read()\n    except AttributeError:\n        return False\n    return True", "contrast": "def io_check(*args, func=None):\n    func = func or inspect.stack()[2][3]\n    for var in args:\n        if not isinstance(var, io.IOBase):\n            name = type(var).__name__\n            raise IOObjError(\n                f'Function {func} expected file-like object, {name} got instead.')", "label": 1}
{"index": "gp155453", "code": "def get_atlas_zonefile_data( zonefile_hash, zonefile_dir, check=True ):\n    zonefile_path = atlas_zonefile_path(zonefile_dir, zonefile_hash)\n    zonefile_path_legacy = atlas_zonefile_path_legacy(zonefile_dir, zonefile_hash)\n    for zfp in [zonefile_path, zonefile_path_legacy]:\n        if not os.path.exists( zfp ):\n            continue\n        if check:\n            res = _read_atlas_zonefile(zfp, zonefile_hash)\n        else:\n            res = _read_atlas_zonefile(zfp, None)\n        if res:\n            return res\n    return None", "contrast": "import pickle\nimport os\ndef get_cached_zone_file():\n    if os.path.exists('cached_zone_file.pkl'):\n        with open('cached_zone_file.pkl', 'rb') as f:\n            return pickle.load(f)\n    else:\n        return None", "label": 0}
{"index": "gp007395", "code": "def _resolve_ctx(rules):\n    if not rules:\n        raise ResolveError(\"Missing node definition.\")\n    if len(rules) == 1 and rules[0][0] is None:\n        return rules[0][1]\n    if any(r[0] is None for r in rules):\n        raise ResolveError(\"Multiple definition, multiple ieml object provided for the same node.\")\n    if any(not isinstance(r[0], Path) for r in rules):\n        raise ResolveError(\"Must have only path instance.\")\n    r0 = rules[0]\n    types = _inferred_types(*r0)\n    for r in rules[1:]:\n        types = types.intersection(_inferred_types(*r))\n    if not types:\n        raise ResolveError(\"No definition, no type inferred on rules list.\")\n    if len(types) > 1:\n        raise ResolveError(\"Multiple definition, multiple type inferred on rules list.\")\n    type = next(types.__iter__())\n    if type == Topic:\n        error, deps = _build_deps_topic(rules)\n        if error:\n            return\n        flexing = None\n        if deps['f']:\n            flexing = deps['f']\n        if not deps['r']:\n            raise ResolveError(\"No root for the topic node.\")\n        return topic(deps['r'], flexing)\n    if type == Text:\n        error, deps = _build_deps_text(rules)\n        if error:\n            return\n        return text(deps)\n    if type in (Theory, Fact):\n        error, deps = _build_deps_tree_graph(rules)\n        if error:\n            return\n        if type == Fact:\n            clauses = []\n            for s, a, m in deps:\n                clauses.append((s, a, m))\n            return fact(clauses)\n        else:\n            clauses = []\n            for s, a, m in deps:\n                clauses.append((s, a, m))\n            return theory(clauses)\n    raise ResolveError(\"Invalid type inferred %s\"%type.__name__)", "contrast": "def resolve_context_and_build_ieml(rules):\n    return ieml_element", "label": 0}
{"index": "gp025773", "code": "def utilization(self):\n        class GpuUtilizationInfo(Structure):\n            _fields_ = [\n                ('gpu', c_uint),\n                ('memory', c_uint),\n            ]\n        c_util = GpuUtilizationInfo()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetUtilizationRates\")(self.hnd, byref(c_util)))\n        return {'gpu': c_util.gpu, 'memory': c_util.memory}", "contrast": "import pynvml\ndef utilization():\n    pynvml.nvmlInit()\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    gpu_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n    gpu_memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    return {'gpu': gpu_utilization.gpu, 'memory': gpu_memory.percent}\n    pynvml.nvmlShutdown()", "label": 0}
{"index": "gp224981", "code": "def fetch_agent(agent_id):\n    return agent", "contrast": "def get_agent(self, agent_id):\n        url = 'agents/%s' % agent_id\n        return Agent(**self._api._get(url))", "label": 1}
{"index": "gp014059", "code": "def odd_even(self):\n        return self.select(lambda Z, N: (Z % 2) and not(N % 2), name=self.name)", "contrast": "def odd_even_nuclei(table):\n    odd_nuclei = []\n    even_nuclei = []\n    for row in table:\n        if row[0] % 2 == 0:\n            even_nuclei.append(row)\n        else:\n            odd_nuclei.append(row)\n    return odd_nuclei, even_nuclei", "label": 0}
{"index": "gp004546", "code": "def default(cls) -> 'PrecalculatedTextMeasurer':\n        if cls._default_cache is not None:\n            return cls._default_cache\n        if pkg_resources.resource_exists(__name__, 'default-widths.json.xz'):\n            import lzma\n            with pkg_resources.resource_stream(__name__,\n                                               'default-widths.json.xz') as f:\n                with lzma.open(f, \"rt\") as g:\n                    cls._default_cache = PrecalculatedTextMeasurer.from_json(\n                        cast(TextIO, g))\n                    return cls._default_cache\n        elif pkg_resources.resource_exists(__name__, 'default-widths.json'):\n            with pkg_resources.resource_stream(__name__,\n                                               'default-widths.json') as f:\n                cls._default_cache = PrecalculatedTextMeasurer.from_json(\n                    io.TextIOWrapper(f, encoding='utf-8'))\n                return cls._default_cache\n        else:\n            raise ValueError('could not load default-widths.json')", "contrast": "def get_default_precalculated_text_measurer():\n    return PrecalculatedTextMeasurer()", "label": 0}
{"index": "gp051260", "code": "def add_to(\n            self, to_email, global_substitutions=None, is_multiple=False, p=0):\n        if isinstance(to_email, list):\n            for email in to_email:\n                if isinstance(email, str):\n                    email = To(email, None)\n                if isinstance(email, tuple):\n                    email = To(email[0], email[1])\n                self._set_emails(email, global_substitutions, is_multiple, p)\n        else:\n            if isinstance(to_email, str):\n                to_email = To(to_email, None)\n            if isinstance(to_email, tuple):\n                to_email = To(to_email[0], to_email[1])\n            if isinstance(to_email, Email):\n                p = to_email.personalization\n            self._set_emails(to_email, global_substitutions, is_multiple, p)", "contrast": "def add_to_to_personalization(to_emails, global_substitutions=None, is_multiple=False, p=None):\n    if p is None:\n        p = Personalization()\n    if isinstance(to_emails, str):\n        to_emails = [to_emails]\n    if isinstance(to_emails, tuple):\n        to_emails = list(to_emails)\n    if is_multiple:\n        for email in to_emails:\n            new_p = Personalization()\n            new_p.add_to(To(email))\n            if global_substitutions:\n                for key, value in global_substitutions.items():\n                    new_p.add_global_substitution(key, value)\n            if isinstance(p, int):\n                message.personalizations[p] = new_p\n            else:\n                p.add_substitution(new_p)\n    else:\n        new_to = To(to_emails)\n        if global_substitutions:\n            for key, value in global_substitutions.items():\n                p.add_global_substitution(key, value)\n        p.add_to(new_to)\n    return p", "label": 0}
{"index": "gp158624", "code": "def to_gremlin(self):\n        self.validate()\n        template_data = {\n            'direction': self.direction,\n            'edge_name': self.edge_name,\n            'inverse_direction': 'in' if self.direction == 'out' else 'out'\n        }\n        return (u'collectMany{{entry -> entry.{direction}_{edge_name}'\n                u'.collect{{edge -> edge.{inverse_direction}V.next()}}}}'\n                .format(**template_data))", "contrast": "def gremlin_representation(block):\n    return str(block).encode('unicode_escape').decode('utf-8')", "label": 0}
{"index": "gp125057", "code": "def log_iter(self, maxentries=None, template=None, **kwargs):\n        template = str(template or self.template)\n        op = self.log(n=maxentries, template=template, **kwargs)\n        if not op:\n            return\n        print(op)\n        for l in itersplit(op, self.lsep):\n            l = l.strip()\n            if not l:\n                continue\n            try:\n                yield self._parselog(l,)\n            except Exception:\n                log.error(\"%s %r\" % (str(self), l))\n                raise\n        return", "contrast": "def parse_repository_log_command(log_output):\n    for log_entry in log_output.split('\\n'):\n        if not log_entry:\n            continue\n        parts = log_entry.split('\\t')\n        yield parts[0], parts[1], parts[2], parts[3], parts[4], parts[5], parts[6]", "label": 0}
{"index": "gp261592", "code": "def get_id(server_and_prefix: str, identifier: str) -> str:\n    return f\"{server_and_prefix}:{identifier}\"", "contrast": "def id(self):\n        id = ''\n        if (self.server_and_prefix is not None and\n                self.server_and_prefix != ''):\n            id += self.server_and_prefix + '/'\n        if (self.identifier is not None):\n            id += self.identifier\n        return id", "label": 1}
{"index": "gp110228", "code": "def duplicates(base, items):\n    for item in items:\n        if item.similarity(base) and not item.equality(base):\n            yield item", "contrast": "def similar_items(base, items):\n    def similarity(item):\n        return abs(item - base)\n    sorted_items = sorted(items, key=similarity)\n    for item in sorted_items:\n        if item != base:\n            yield item", "label": 0}
{"index": "gp135317", "code": "def pitch(ax, ay, az):\n    import numpy\n    return numpy.arctan(ax, numpy.sqrt(ay**2+az**2))", "contrast": "def get_pitch_angle(ax, ay, az):\n    import numpy as np\n    pitch = np.arctan2(-ax, np.sqrt(ay * ay + az * az))\n    return pitch", "label": 0}
{"index": "gp000940", "code": "def _propagate_mean(mean, linop, dist):\n  return linop.matmul(mean) + dist.mean()[..., tf.newaxis]", "contrast": "import numpy as np\ndef propagate_mean(mu, A, b):\n    transformed_mean = np.dot(A, mu) + b\n    return transformed_mean", "label": 0}
{"index": "gp087492", "code": "def traverse(proto_file):\n    def _collapse_comments(comments):\n        return '\\n'.join(\n            [c.strip() for c in (comments[\"leading_comments\"] + comments[\"trailing_comments\"]).split('\\n')])\n    def _traverse(package, items, tree):\n        for item_index, item in enumerate(items):\n            item = convert_protodef_to_editable(item)\n            if item_index in tree:\n                comments = tree[item_index]\n                if \"leading_comments\" in comments or \"trailing_comments\" in comments:\n                    item.comment = _collapse_comments(comments)\n                    del comments[\"leading_comments\"]\n                    del comments[\"trailing_comments\"]\n                if item.kind is EnumDescriptorProto:\n                    if 2 in comments: \n                        for k in comments[2]:\n                            value_comment = comments[2][k]\n                            if value_comment != {}:\n                                item.value[k].comment = _collapse_comments(value_comment)\n                elif item.kind is DescriptorProto:\n                    if 2 in comments: \n                        for k in comments[2]:\n                            field_comment = comments[2][k]\n                            if field_comment != {}:\n                                item.field[k].comment = _collapse_comments(field_comment)\n                elif item.kind is ServiceDescriptorProto:\n                    if 2 in comments: \n                        for k in comments[2]:\n                            method_comment = comments[2][k]\n                            if method_comment != {}:\n                                item.method[k].comment = _collapse_comments(method_comment)\n                else:\n                    raise Exception, item.kind\n            yield item, package\n            if item.kind is DescriptorProto:\n                for enum in item.enum_type:\n                    yield enum, package\n                for nested in item.nested_type:\n                    nested_package = package + \".\" + item.name\n                    for nested_item, np in _traverse(nested_package, [nested], tree[item_index]):\n                        yield nested_item, np\n    tree = collections.defaultdict(collections.defaultdict)\n    for loc in proto_file.source_code_info.location:\n        if loc.leading_comments or loc.trailing_comments:\n            place = tree\n            for p in loc.path:\n                if not place.has_key(p):\n                    place[p] = collections.defaultdict(collections.defaultdict)\n                place = place[p]\n            place[\"leading_comments\"] = loc.leading_comments\n            place[\"trailing_comments\"] = loc.trailing_comments\n    if set(tree.keys()).difference(set([4, 5, 6, 7, 8])) != set():\n        raise Exception, tree\n    return {\"types\":\n        list(itertools.chain(\n            _traverse(proto_file.package, proto_file.service, tree[6]), \n            _traverse(proto_file.package, proto_file.enum_type, tree[5]), \n            _traverse(proto_file.package, proto_file.message_type, tree[4]), \n        )),\n        \"file\": [\"\".join(x.leading_detached_comments) for x in proto_file.source_code_info.location if len(x.leading_detached_comments) > 0]\n    }", "contrast": "from google.protobuf.descriptor_pb2 import FileDescriptorProto\ndef flatten_tree(proto_file: FileDescriptorProto) -> dict:\n    flattened_tree = {'messages': [], 'enums': []}\n    def traverse(path, items, is_enum=False):\n        for i, item in enumerate(items):\n            item_path = path[:]\n            item_path.append(i)\n            if 'source_code_info' in item:\n                for location in item['source_code_info'].location:\n                    if location.leading_comments:\n                        comment = location.leading_comments.strip()\n                        if comment:\n                            msg_path = item_path[:-1]\n                            msg_name = items[msg_path[-1]].name\n                            msg = {'path': msg_path, 'name': msg_name, 'comment': comment}\n                            if is_enum:\n                                flattened_tree['enums'].append(msg)\n                            else:\n                                flattened_tree['messages'].append(msg)\n            if 'nested_type' in item:\n                traverse(item_path, item['nested_type'])\n            if 'enum_type' in item:\n                traverse(item_path, item['enum_type'], True)\n    traverse([], proto_file.message_type)\n    return flattened_tree", "label": 0}
{"index": "gp238714", "code": "def get_child_node(inst_name):\n    for child in self.children:\n        if child.name == inst_name:\n            return child\n    return None", "contrast": "def get_child_by_name(self, inst_name):\n        child_inst = self.inst.get_child_by_name(inst_name)\n        if child_inst is None:\n            return None\n        return Node._factory(child_inst, self.env, self)", "label": 1}
{"index": "gp089072", "code": "def reference(self, install, upgrade):\n        self.template(78)\n        print(\"| Total {0} {1} installed and {2} {3} upgraded\".format(\n            len(install), self.pkg(len(install)),\n            len(upgrade), self.pkg(len(upgrade))))\n        self.template(78)\n        for installed, upgraded in itertools.izip_longest(install, upgrade):\n            if upgraded:\n                print(\"| Package {0} upgraded successfully\".format(upgraded))\n            if installed:\n                print(\"| Package {0} installed successfully\".format(installed))\n        self.template(78)\n        print(\"\")", "contrast": "import subprocess\ndef reference_list():\n    installed_packages = subprocess.check_output(['pip', 'freeze']).decode('utf-8').strip().split('\\n')\n    upgraded_packages = subprocess.check_output(['pip', 'list', '--upgrade']).decode('utf-8').strip().split('\\n')\n    reference_list = []\n    for package in installed_packages:\n        name, version = package.split('==')\n        reference_list.append({'name': name, 'version': version, 'status': 'installed'})\n    for package in upgraded_packages:\n        name, version = package.split()\n        reference_list.append({'name': name, 'version': version, 'status': 'upgraded'})\n    return reference_list", "label": 0}
{"index": "gp190679", "code": "def normalize_sequence(sequence, considerDimensions):\n    mean = []\n    for i in considerDimensions:\n        subSeq = [sequence[j][i] for j in range(len(sequence))]\n        mean.append(sum(subSeq)/len(subSeq))\n    for i in range(len(sequence)):\n        for j in considerDimensions:\n            sequence[i][j] -= mean[considerDimensions.index(j)]\n    return sequence", "contrast": "def normalizeSequence(sequence):\n  seq = np.array(sequence).astype('float64')\n  meanSeq = np.mean(seq)\n  stdSeq = np.std(seq)\n  seq = (seq - np.mean(seq)) / np.std(seq)\n  sequence = seq.tolist()\n  return sequence, meanSeq, stdSeq", "label": 1}
{"index": "gp059887", "code": "def adaptive_knn_graph(traj_dist,k): \n    adj_mat = np.zeros_like(traj_dist,dtype=float) \n    knn=(np.transpose(np.argsort(traj_dist,0))) \n    for i in range(0,(traj_dist.shape[0])): \n        adj_mat[i,knn[i,range(1,k)]]=traj_dist[i,knn[i,range(1,k)]]\n    return adj_mat", "contrast": "def adj_matrix(traj_dist, k):\n    n = traj_dist.shape[0]\n    adj_mat = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i+1, n):\n            dist = traj_dist[i][j]\n            if dist <= k:\n                adj_mat[i][j] = 1\n                adj_mat[j][i] = 1\n    return adj_mat", "label": 0}
{"index": "gp052744", "code": "def plot_compare(self, other_plotter, legend=True):\n        import matplotlib.lines as mlines\n        plt = self.get_plot()\n        data_orig = self.bs_plot_data()\n        data = other_plotter.bs_plot_data()\n        band_linewidth = 1\n        for i in range(other_plotter._nb_bands):\n            for d in range(len(data_orig['distances'])):\n                plt.plot(data_orig['distances'][d],\n                         [e[str(Spin.up)][i] for e in data['energy']][d],\n                         'c-', linewidth=band_linewidth)\n                if other_plotter._bs.is_spin_polarized:\n                    plt.plot(data_orig['distances'][d],\n                             [e[str(Spin.down)][i] for e in data['energy']][d],\n                             'm--', linewidth=band_linewidth)\n        if legend:\n            handles = [mlines.Line2D([], [], linewidth=2,\n                                     color='b', label='bs 1 up'),\n                       mlines.Line2D([], [], linewidth=2,\n                                     color='r', label='bs 1 down',\n                                     linestyle=\"--\"),\n                       mlines.Line2D([], [], linewidth=2,\n                                     color='c', label='bs 2 up'),\n                       mlines.Line2D([], [], linewidth=2,\n                                     color='m', linestyle=\"--\",\n                                     label='bs 2 down')]\n            plt.legend(handles=handles)\n        return plt", "contrast": "import matplotlib.pyplot as plt\ndef plot_two_bandstructures(bandstructure1, bandstructure2):\n    data1 = bandstructure1.as_dict()\n    data2 = bandstructure2.as_dict()\n    ymin = min([data1['energy'][ispin][iband][-1] for ispin in range(data1['nspin'])\n                                                      for iband in range(data1['nb_bands'])])\n    ymax = max([data1['energy'][ispin][iband][0] for ispin in range(data1['nspin'])\n                                                     for iband in range(data1['nb_bands'])])\n    color = 'blue'\n    for ispin in range(data1['nspin']):\n        for iband in range(data1['nb_bands']):\n            plt.plot(data1['distances'], data1['energy'][ispin][iband], color=color)\n    color = 'red'\n    for ispin in range(data2['nspin']):\n        for iband in range(data2['nb_bands']):\n            plt.plot(data2['distances'], data2['energy'][ispin][iband], color=color)\n    plt.xlim(data1['distances'][0], data1['distances'][-1])\n    plt.ylim(ymin, ymax)\n    plt.xlabel('Wave Vector')\n    plt.ylabel('Energy')\n    plt.show()", "label": 0}
{"index": "gp300919", "code": "def delete_rows(indexes):\n    df.drop(index=df.index[indexes], inplace=True)", "contrast": "def delete(self, indexes):\n        indexes = [indexes] if not isinstance(indexes, (list, blist)) else indexes\n        if all([isinstance(i, bool) for i in indexes]):  \n            if len(indexes) != len(self._index):\n                raise ValueError('boolean indexes list must be same size of existing indexes')\n            indexes = [i for i, x in enumerate(indexes) if x]\n        else:\n            indexes = [sorted_index(self._index, x) for x in indexes] if self._sort                else [self._index.index(x) for x in indexes]\n        indexes = sorted(indexes, reverse=True)  \n        for i in indexes:\n            del self._data[i]\n        for i in indexes:\n            del self._index[i]", "label": 1}
{"index": "gp113973", "code": "def of_think(self, think):\n        return self._compute(\n            duration=think.duration,\n            after=self.continuation)", "contrast": "import time\ndef simulate_worker_process(time_in_seconds):\n    time.sleep(time_in_seconds)", "label": 0}
{"index": "gp068993", "code": "def find_element_by_jquery(browser, selector):\n    elements = find_elements_by_jquery(browser, selector)\n    if not elements:\n        raise AssertionError(\"No matching element found.\")\n    if len(elements) > 1:\n        raise AssertionError(\"Multiple matching elements found.\")\n    return elements[0]", "contrast": "from bs4 import BeautifulSoup\ndef find_html_element(html, selector):\n    soup = BeautifulSoup(html, 'html.parser')\n    element = soup.select_one(selector)\n    return element", "label": 0}
{"index": "gp218969", "code": "def check_order(run_orders):\n    current_order = run_orders[0]\n    for order in run_orders[1:]:\n        if current_order > order:\n            return False\n        elif current_order == order:\n            continue\n        else:\n            current_order = order\n    return True", "contrast": "def check_dependee_order(depender, dependee, dependee_id):\n shutit_global.shutit_global_object.yield_to_draw()\n if dependee.run_order > depender.run_order:\n\t\treturn 'depender module id:\\n\\n' + depender.module_id + '\\n\\n(run order: ' + str(depender.run_order) + ') ' + 'depends on dependee module_id:\\n\\n' + dependee_id + '\\n\\n(run order: ' + str(dependee.run_order) + ') ' + 'but the latter is configured to run after the former'\n return ''", "label": 1}
{"index": "gp189049", "code": "import os\ndef ensure_directory_does_not_exist(directory):\n    if os.path.exists(directory):\n        os.rmdir(directory)\n    return True", "contrast": "def ensure_dir_does_not_exist(*args):\n    path = os.path.join(*args)\n    if os.path.isdir(path):\n        shutil.rmtree(path)", "label": 1}
{"index": "gp156143", "code": "def overlaps(self, other):\n        return self.network in other or self.broadcast in other or (\n            other.network in self or other.broadcast in self)", "contrast": "def is_partly_contained(self, other):\n    return any(x in other for x in self)", "label": 0}
{"index": "gp239642", "code": "def is_excluded_dir(directory, exclude_list):\n    for exclude_dir in exclude_list:\n        if directory.startswith(exclude_dir):\n            return True\n    return False", "contrast": "def is_excluded(root, excludes):\n    for exclude in excludes:\n        if fnmatch(root, exclude):\n            return True\n    return False", "label": 1}
{"index": "gp300361", "code": "def apply_quaternion_rotation(quaternion, vector):\n    r = [quaternion[3], quaternion[0], quaternion[1], quaternion[2]]\n    p = [0] + vector\n    conj_q = [r[0], -r[1], -r[2], -r[3]]\n    result = quaternion_multiply(quaternion_multiply(r, p), conj_q)\n    return result[1:]", "contrast": "def quaternion_rotation(quat, vector):\n    dp = np.dot(quat[1:], vector)\n    cos = (2*quat[0]*quat[0] - 1)\n    return np.array([\n        2 * (quat[0] * (quat[2] * vector[2] - quat[3] * vector[1]) + quat[1] * dp) + cos * vector[0],\n        2 * (quat[0] * (quat[3] * vector[0] - quat[1] * vector[2]) + quat[2] * dp) + cos * vector[1],\n        2 * (quat[0] * (quat[1] * vector[1] - quat[2] * vector[0]) + quat[3] * dp) + cos * vector[2]\n    ], float)", "label": 1}
{"index": "gp081729", "code": "def pop_event(self):\n    with self.lock:\n      if not self.events:\n        raise ValueError('no events queued')\n      return self.events.popleft()", "contrast": "def pop_event(queue):\n    if len(queue) == 0:\n        raise ValueError('There is no event queued')\n    else:\n        return queue.pop(0)", "label": 0}
{"index": "gp321745", "code": "def concatenate_criteria(criterion):\n    criteria = []\n    for key, value in criterion.items():\n        if isinstance(value, dict):\n            sub_criteria = concatenate_criteria(value).split('__')\n            for sub_criterion in sub_criteria:\n                criteria.append(key + '__' + sub_criterion)\n        else:\n            criteria.append(key)\n    return '__'.join(sorted(criteria))", "contrast": "def name(self):\n        names = (criterion.name() for criterion in self._criteria)\n        return '__'.join(sorted(names))", "label": 1}
{"index": "gp171525", "code": "def get_nth_value(tag, nth=1):\n    tag_value_list = [tag_value for (tag_num, tag_value) in some_message if tag_num == tag]\n    if len(tag_value_list) < nth:\n        return None\n    return tag_value_list[nth-1]", "contrast": "def get(self, tag, nth=1):\n        tag = fix_tag(tag)\n        nth = int(nth)\n        for t, v in self.pairs:\n            if t == tag:\n                nth -= 1\n                if nth == 0:\n                    return v\n        return None", "label": 1}
{"index": "gp219702", "code": "import logging\ndef log_client_host(client_host):\n    logger = logging.getLogger(__name__)\n    logger.info(\"Client host: %s\", client_host, extra={'client_host': client_host})", "contrast": "def my_address_string(self):\n        addr = getattr(self, 'client_address', ('', None))[0]\n        if addr in self.proxy_ips:\n            return self.headers.getheader('x-forwarded-for', addr)\n        return addr", "label": 1}
{"index": "gp100153", "code": "def all_variables(self):\n        variables = []\n        variables.extend(self.free_variables)\n        variables.append(self.independent['symbol'])\n        variables.extend([ param['symbol'] for param in self.fitting_parameters ])\n        variables.extend([ param['symbol'] for param in self.fixed_parameters ])\n        variables.extend([ const['symbol'] for const in self.constants ])\n        symbols = []\n        for variable in variables:\n            if isinstance(variable, str):\n                symbols.append(self.model.symbol(variable))\n            else:\n                symbols.append(variable)\n        return tuple(symbols)", "contrast": "def get_all_symbols(fit):\n    return tuple(fit.free_variables + fit.independent + fit.fitting_parameters + fit.fixed_parameters + fit.constants)", "label": 0}
{"index": "gp153205", "code": "def _get_ntgpadnt(self, ver, add_ns):\n        hdrs = self.gpad_columns[ver]\n        if add_ns:\n            hdrs = hdrs + ['NS']\n        return cx.namedtuple(\"ntgpadobj\", hdrs)", "contrast": "from collections import namedtuple\ndef create_namedtuple(annotation):\n    Annotation = namedtuple('Annotation', ['name', 'value'])\n    ann = Annotation(name=annotation[0], value=annotation[1])\n    return ann", "label": 0}
{"index": "gp310168", "code": "def attribute_probability(node, attribute):\n    return probability", "contrast": "def get_value_prob(self, attr_name, value):\n        if attr_name not in self._attr_value_count_totals:\n            return\n        n = self._attr_value_counts[attr_name][value]\n        d = self._attr_value_count_totals[attr_name]\n        return n/float(d)", "label": 1}
{"index": "gp108574", "code": "def _get_select_commands(self, source, tables):\n        row_queries = {tbl: self.select_all(tbl, execute=False) for tbl in\n                       tqdm(tables, total=len(tables), desc='Getting {0} select queries'.format(source))}\n        for tbl, command in row_queries.items():\n            if isinstance(command, str):\n                row_queries[tbl] = [command]\n        return [(tbl, cmd) for tbl, cmds in row_queries.items() for cmd in cmds]", "contrast": "def create_select_queries(source, tables):\n    commands = {}\n    for table in tables:\n         commands[table] = f\"SELECT * FROM {source}.{table};\"\n    return commands", "label": 0}
{"index": "gp222493", "code": "def execute_command(command):\n    if command == 'help':\n        print('This is the help text')\n    else:\n        pass ", "contrast": "def execute(self):\n        if self.args and self.argument(0) == \"help\":\n            self.error(self.usage() + \"\\n\\n\" + self.help())\n            return False\n        return True", "label": 1}
{"index": "gp258138", "code": "from habanero import Crossref\nimport random\ndef random_dois(sample=10, **kwargs):\n    if sample > 100:\n        sample = 100\n    cr = Crossref()\n    query = \"works?sample=%s&filter=doi\" % sample\n    for key in kwargs:\n        query += \"&%s=%s\" % (key, kwargs[key])\n    data = cr._make_request(query)\n    return random.sample(data[\"message\"][\"result\"], sample)", "contrast": "def random_dois(self, sample = 10, **kwargs):\n        res = request(self.mailto, self.base_url, \"/works/\", None,\n            None, None, None, None, sample, None,\n            None, None, None, True, None, None, None, **kwargs)\n        return [ z['DOI'] for z in res['message']['items'] ]", "label": 1}
{"index": "gp106515", "code": "def FunctionalGroupColorMapping(maptype='jet', reverse=False):\n    small_color = '#f76ab4'\n    nucleophilic_color = '#ff7f00'\n    hydrophobic_color = '#12ab0d'\n    aromatic_color = '#84380b'\n    acidic_color = '#e41a1c'\n    amide_color = '#972aa8'\n    basic_color = '#3c58e5'\n    mapping_d = {'G':small_color, 'A':small_color,\n                 'S':nucleophilic_color, 'T':nucleophilic_color, 'C':nucleophilic_color,\n                 'V':hydrophobic_color, 'L':hydrophobic_color, 'I':hydrophobic_color, 'M':hydrophobic_color, 'P':hydrophobic_color,\n                 'F':aromatic_color, 'Y':aromatic_color, 'W':aromatic_color,\n                 'D':acidic_color, 'E':acidic_color,\n                 'H':basic_color, 'K':basic_color, 'R':basic_color,\n                 'N':amide_color, 'Q':amide_color,\n                 '*':'#000000'}\n    return (None, mapping_d, None)", "contrast": "def amino_acid_group_colors(maptype=None, reverse=False):\n    aa_colors = {\n        'hydrophobic': '#FCA000',\n        'basic': '#0000FF',\n        'acidic': '#FF1493',\n        'polar': '#29B4B4',\n        'cysteine': '#FFD700',\n        'aromatic': '#A020F0',\n        'glycine': '#FFFFFF',\n        'proline': '#C0C0C0'\n    }\n    return aa_colors", "label": 0}
{"index": "gp012714", "code": "def draw_string(font, text, x, y, width=None, height=None, align=Alignment.left, vertical_align=VerticalAlignment.baseline):\n    style = Style(font)\n    run = GlyphRun(style, text)\n    glyph_layout = GlyphLayout([run], x, y, width, height, align, vertical_align)\n    draw_glyph_layout(glyph_layout)", "contrast": "def draw_text(font, text, x, y):\n    font_surface = font.render(text, True, (0, 0, 0))\n    screen.blit(font_surface, (x, y))", "label": 0}
{"index": "gp219317", "code": "from django.contrib.admin.options import ModelAdmin\ndef is_modeladmin_derived(node):\n    return issubclass(node.__class__, ModelAdmin)", "contrast": "def is_model_admin_subclass(node):\n    if node.name[-5:] != 'Admin' or isinstance(node.parent, ClassDef):\n        return False\n    return node_is_subclass(node, 'django.contrib.admin.options.ModelAdmin')", "label": 1}
{"index": "gp247724", "code": "def normalize_to_bytes(data):\n    if isinstance(data, str):\n        return data.encode()\n    elif isinstance(data, bytes):\n        return data\n    else:\n        raise TypeError(\"Input data must be a string or bytes object\")", "contrast": "def to_bytes(data):\n    if isinstance(data, six.string_types) and not isinstance(data, bytes):\n        return codecs.encode(data, TEXT_ENCODING)\n    return data", "label": 1}
{"index": "gp144488", "code": "def functions(self):\n        it = ffi.lib.LLVMPY_ModuleFunctionsIter(self)\n        return _FunctionsIterator(it, dict(module=self))", "contrast": "import types\ndef get_functions(module):\n    for _, value in module.__dict__.items():\n        if isinstance(value, types.FunctionType):\n            yield value.__code__", "label": 0}
{"index": "gp093209", "code": "def set_index(self, keys, drop=True, append=False,\n                  inplace=False, verify_integrity=False):\n        if drop is True:\n            try:\n                assert type(keys) is not str\n                dropped_cols = set(keys)\n            except (TypeError, AssertionError):\n                dropped_cols = set([keys])\n        if not self._required_cols <= (set(self.columns) - set(dropped_cols)):\n            raise PhysicalMeaning('You drop a column that is needed to '\n                                  'be a physical meaningful description '\n                                  'of a molecule.')\n        if inplace:\n            self._frame.set_index(keys, drop=drop, append=append,\n                                  inplace=inplace,\n                                  verify_integrity=verify_integrity)\n        else:\n            new = self._frame.set_index(keys, drop=drop, append=append,\n                                        inplace=inplace,\n                                        verify_integrity=verify_integrity)\n            return self.__class__(new, _metadata=self._metadata,\n                                  metadata=self.metadata)", "contrast": "def set_dataframe_index(dataframe, columns):\n    return dataframe.set_index(columns)", "label": 0}
{"index": "gp299205", "code": "from IPython.display import HTML\ndef show_preview(size):\n    options = {\n        'small': '100px',\n        'med': '250px',\n        'thumb': '50px',\n        'full': '100%'\n    }\n    img_html = f'<img src=\"preview.png\" style=\"width:{options[size]}\">'\n    display(HTML(img_html))", "contrast": "def show_images(self, size=\"small\"):\n        d = dict(small=256, med=512, thumb=100, full=1024)\n        try:\n            width = d[size]\n        except KeyError:\n            print(\"Allowed keys:\", d.keys())\n            return\n        img_urls = [i._get_img_url(size) for i in self.obsids]\n        imagesList = \"\".join(\n            [\n                \"<img style='width: {0}px; margin: 0px; float: \"\n                \"left; border: 1px solid black;' \"\n                \"src='{1}' />\".format(width, s)\n                for s in img_urls\n            ]\n        )\n        display(HTML(imagesList))", "label": 1}
{"index": "gp004410", "code": "def delete_lower(script, layer_num=None):\n    if layer_num is None:\n        layer_num = script.current_layer()\n    if layer_num != 0:\n        change(script, 0)\n    for i in range(layer_num):\n        delete(script, 0)\n    return None", "contrast": "def delete_layers_below(layer_num, meshlab_project_file):\n    with open(meshlab_project_file, 'r') as f:\n        proj_str = f.read()\n    start_idx = proj_str.find(\"<Layer \")\n    new_proj_str = proj_str[:start_idx]\n    new_proj_str += proj_str[start_idx:].split(f'<Layer ID=\"{layer_num}\"')[0]\n    new_proj_str += f'<Layer ID=\"{layer_num}\"'\n    with open(meshlab_project_file, 'w') as f:\n        f.write(new_proj_str)", "label": 0}
{"index": "gp004398", "code": "def mesh2fc(script, all_visible_layers=False):\n    filter_xml = ''.join([\n        '  <filter name=\"Transfer Color: Mesh to Face\">\\n',\n        '    <Param name=\"allVisibleMesh\" ',\n        'value=\"%s\" ' % str(all_visible_layers).lower(),\n        'description=\"Apply to all Meshes\" ',\n        'type=\"RichBool\" ',\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None", "contrast": "faces, otherwise the filter applies color mapping only to the faces of the active mesh.\n    Returns:\n        The created filter.\n    '''\n    def transfer_mesh_colors_to_face_colors(script, all_visible_layers=True):\n        filter = script.createFilter(\"geometry.colors.transferMeshColorsToFaceColors\")\n        filter.setEnabled(True)\n        filter.setBoolProperty(\"all_visible_layers\", all_visible_layers)\n        return filter", "label": 0}
{"index": "gp269106", "code": "import subprocess\ndef globus_task_event_list_executor(task_id):\n    command = ['globus', 'task-event-list', task_id]\n    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if result.returncode != 0:\n        raise Exception(f\"Command execution failed with error: {result.stderr.decode('utf-8')}\")\n    return result.stdout.decode('utf-8')", "contrast": "def task_event_list(task_id, limit, filter_errors, filter_non_errors):\n    client = get_client()\n    if filter_errors and filter_non_errors:\n        raise click.UsageError(\"Cannot filter by both errors and non errors\")\n    elif filter_errors:\n        filter_string = \"is_error:1\"\n    elif filter_non_errors:\n        filter_string = \"is_error:0\"\n    else:\n        filter_string = \"\"\n    event_iterator = client.task_event_list(\n        task_id, num_results=limit, filter=filter_string\n    )\n    def squashed_json_details(x):\n        is_json = False\n        try:\n            loaded = json.loads(x[\"details\"])\n            is_json = True\n        except ValueError:\n            loaded = x[\"details\"]\n        if is_json:\n            return json.dumps(loaded, separators=(\",\", \":\"), sort_keys=True)\n        else:\n            return loaded.replace(\"\\n\", \"\\\\n\")\n    formatted_print(\n        event_iterator,\n        fields=(\n            (\"Time\", \"time\"),\n            (\"Code\", \"code\"),\n            (\"Is Error\", \"is_error\"),\n            (\"Details\", squashed_json_details),\n        ),\n        json_converter=iterable_response_to_dict,\n    )", "label": 1}
{"index": "gp196283", "code": "class ValidationError(Exception):\n    pass\ndef validate_value(obj, key, validation_fun):\n    if key in obj:\n        try:\n            validation_fun(obj[key])\n        except Exception as e:\n            raise ValidationError(f\"Validation error for {key} with value '{obj[key]}': {str(e)}\")\n    for value in obj.values():\n        if isinstance(value, dict):\n            validate_value(value, key, validation_fun)\n        elif isinstance(value, list):\n            for item in value:\n                if isinstance(item, dict):\n                    validate_value(item, key, validation_fun)", "contrast": "def validate_all_values_for_key(obj, key, validation_fun):\n    for vkey, value in obj.items():\n        if vkey == key:\n            validation_fun(value)\n        elif isinstance(value, dict):\n            validate_all_values_for_key(value, key, validation_fun)", "label": 1}
{"index": "gp069735", "code": "def parse_int_string(int_string: str) -> List[int]:\n    cleaned = \" \".join(int_string.strip().split())\n    cleaned = cleaned.replace(\" - \", \"-\")\n    cleaned = cleaned.replace(\",\", \" \")\n    tokens = cleaned.split(\" \")\n    indices: Set[int] = set()\n    for token in tokens:\n        if \"-\" in token:\n            endpoints = token.split(\"-\")\n            if len(endpoints) != 2:\n                LOG.info(f\"Dropping '{token}' as invalid - weird range.\")\n                continue\n            start = int(endpoints[0])\n            end = int(endpoints[1]) + 1\n            indices = indices.union(indices, set(range(start, end)))\n        else:\n            try:\n                indices.add(int(token))\n            except ValueError:\n                LOG.info(f\"Dropping '{token}' as invalid - not an int.\")\n    return list(indices)", "contrast": "def unique_integers(string):\n    numbers = []\n    for group in string.split():\n        if '-' in group:\n            start, end = group.split('-')\n            numbers.extend(range(int(start), int(end)+1))\n        else:\n            try:\n                number = int(group)\n                numbers.append(number)\n            except ValueError:\n                pass\n    return list(set(numbers))", "label": 0}
{"index": "gp195145", "code": "import numpy as np\ndef invert_matrix(W, copy=True):\n    if copy:\n        W = W.copy()\n    for row in range(W.shape[0]):\n        for col in range(W.shape[1]):\n            W[row][col] = 1 / W[row][col]\n    return W", "contrast": "def invert(W, copy=True):\n    if copy:\n        W = W.copy()\n    E = np.where(W)\n    W[E] = 1. / W[E]\n    return W", "label": 1}
{"index": "gp210742", "code": "def append_pending_symbol_to_annotations(context):\n    context['annotations'].append(context['pending_symbol'])\n    return None", "contrast": "def set_annotation(self):\n        assert self.pending_symbol is not None\n        assert not self.value\n        annotations = (_as_symbol(self.pending_symbol, is_symbol_value=False),)  \n        self.annotations = annotations if not self.annotations else self.annotations + annotations\n        self.ion_type = None\n        self.pending_symbol = None  \n        self.quoted_text = False\n        self.line_comment = False\n        self.is_self_delimiting = False\n        return self", "label": 1}
{"index": "gp095721", "code": "def patch(self, delta):\n        with (tempfile.NamedTemporaryFile(prefix='.sync',\n              suffix=os.path.basename(self.path),\n              dir=os.path.dirname(self.path), delete=False)) as output:\n            try:\n                with open(self.path, 'rb') as reference:\n                    r = librsync.patch(reference, delta, output)\n                    os.rename(output.name, self.path)\n                    return r\n            finally:\n                try:\n                    os.remove(output.name)\n                except OSError as e:\n                    if e.errno != errno.ENOENT:\n                        raise", "contrast": "def apply_remote_delta_to_local_file(remote_delta: bytearray, local_file_path: str) -> None:\n    with open(local_file_path, 'r+b') as file:\n        file_content = bytearray(file.read())\n        file_content = apply_delta(file_content, remote_delta)\n        file.seek(0)\n        file.write(file_content)", "label": 0}
{"index": "gp096629", "code": "def make_cmd_invocation(invocation, args, kwargs):\n    if not invocation.endswith('/'):\n        invocation += '/'\n    if not invocation.startswith('/'):\n        invocation = '/' + invocation\n    cmd = invocation\n    for arg in args:\n        cmd += str(arg) + \"/\"\n    rendered_kwargs = []\n    for k, v in kwargs.items():\n        rendered_kwargs.append(\"--%s=%s\" % (k,v))\n    return ['./giotto-cmd', cmd] + rendered_kwargs", "contrast": "def make_cmd_invocation(program_path, args_list, kwargs_dict):\n    invocation = ['./giotto-cmd', program_path]\n    for arg in args_list:\n        invocation.append(arg)\n    for key, value in kwargs_dict.items():\n        invocation.append('--{}={}'.format(key, value))\n    return invocation", "label": 0}
{"index": "gp205836", "code": "import tqdm\ndef create_progress_bar(iterable, desc='', total=None):\n    return tqdm.tqdm(iterable, desc=desc, total=total, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')", "contrast": "def progress_bar(**kwargs):\n    tqdm_kw = {\n        'desc': 'Processing',\n        'file': sys.stdout,\n        'bar_format': TQDM_BAR_FORMAT,\n    }\n    tqdm_kw.update(kwargs)\n    pbar = tqdm(**tqdm_kw)\n    if not pbar.disable:\n        pbar.desc = pbar.desc.rstrip(': ')\n        pbar.refresh()\n    return pbar", "label": 1}
{"index": "gp236343", "code": "def set_callback_verbosity(callback):\n    def wrapper(*args, **kwargs):\n        return callback(*args, **kwargs)\n    wrapper.wants_verbosity = True\n    return wrapper", "contrast": "def pass_verbosity(f):\n    def new_func(*args, **kwargs):\n        kwargs['verbosity'] = click.get_current_context().verbosity\n        return f(*args, **kwargs)\n    return update_wrapper(new_func, f)", "label": 1}
{"index": "gp259333", "code": "def meets_search_space_restrictions(instance, search_space):\n    for key, value in instance.items():\n        if key not in search_space or value not in search_space[key]:\n            return False\n    return True", "contrast": "def check_restrictions(restrictions, element, keys, verbose):\n    params = OrderedDict(zip(keys, element))\n    for restrict in restrictions:\n        if not eval(replace_param_occurrences(restrict, params)):\n            if verbose:\n                print(\"skipping config\", get_instance_string(params), \"reason: config fails restriction\")\n            return False\n    return True", "label": 1}
{"index": "gp088188", "code": "def get_api_docs(routes):\n    routes = map(_get_tuple_from_route, routes)\n    documentation = []\n    for url, rh, methods in sorted(routes, key=lambda a: a[0]):\n        if issubclass(rh, APIHandler):\n            documentation.append(_get_route_doc(url, rh, methods))\n    documentation = (\n        \"**This documentation is automatically generated.**\\n\\n\" +\n        \"**Output schemas only represent `data` and not the full output; \" +\n        \"see output examples and the JSend specification.**\\n\" +\n        \"\\n<br>\\n<br>\\n\".join(documentation)\n    )\n    return documentation", "contrast": "def generate_api_docs(routes: list) -> str:\n    docs = ''\n    for route in routes:\n        url, handler = route\n        docstring = handler.__doc__\n        if docstring:\n            sig = inspect.signature(handler)\n            args = sig.parameters\n            returns = handler.__annotations__.get('return', None)\n            method = handler.__module__.split('.', 1)[-1]\n            if hasattr(handler, 'schema'):\n                schema = handler.schema\n            else:\n                schema = None\n            docs += f\"### {method.upper()}: {url}\\n\\n\"\n            docs += f\"{docstring.strip()}\\n\\n\"\n            if schema:\n                docs += f\"", "label": 0}
{"index": "gp063762", "code": "def predict_mhcii_binding(job, peptfile, allele, univ_options, mhcii_options):\n    job.fileStore.logToMaster('Running mhcii on %s:%s' % (univ_options['patient'], allele))\n    work_dir = job.fileStore.getLocalTempDir()\n    input_files = {\n        'peptfile.faa': peptfile}\n    input_files = get_files_from_filestore(job, input_files, work_dir, docker=True)\n    parameters = [mhcii_options['pred'],\n                  allele,\n                  input_files['peptfile.faa']]\n    with open('/'.join([work_dir, 'predictions.tsv']), 'w') as predfile:\n        docker_call(tool='mhcii', tool_parameters=parameters, work_dir=work_dir,\n                    dockerhub=univ_options['dockerhub'], outfile=predfile, interactive=True)\n    run_netMHCIIpan = True\n    with open(predfile.name, 'r') as predfile:\n        for line in predfile:\n            if not line.startswith('HLA'):\n                continue\n            if line.strip().split('\\t')[5] == 'NetMHCIIpan':\n                break\n            elif line.strip().split('\\t')[5] == 'Sturniolo':\n                predictor = 'Sturniolo'\n            else:\n                predictor = 'Consensus'\n            run_netMHCIIpan = False\n            break\n    if run_netMHCIIpan:\n        NetMHCIIpan = job.addChildJobFn(predict_netmhcii_binding, peptfile, allele, univ_options,\n                                        disk='10G')\n        return NetMHCIIpan.rv()\n    else:\n        output_file = job.fileStore.writeGlobalFile(predfile.name)\n        return output_file, predictor", "contrast": "def predict_MHC_binding(YY, ALLELE):\n    PREDFILE = \"path/to/prediction/file\"\n    PREDICTOR = \"Consensus\" \n    return PREDFILE, PREDICTOR", "label": 0}
{"index": "gp229416", "code": "import requests\ndef get_pending_txns(server_url):\n    try:\n        response = requests.get(f\"{server_url}/api/transactions/pending\")\n        return len(response.json())\n    except requests.exceptions.RequestException as e:\n        print(f\"Error occurred while fetching pending transactions: {e}\")\n        return None", "contrast": "def pending_transactions(server):\n    namecoind = NamecoindClient(server, NAMECOIND_PORT,\n                                NAMECOIND_USER, NAMECOIND_PASSWD)\n    reply = namecoind.listtransactions(\"\", 10000)\n    counter = 0\n    for i in reply:\n        if i['confirmations'] == 0:\n            counter += 1\n    return counter", "label": 1}
{"index": "gp234985", "code": "import numpy as np\ndef check_time_and_freq(ref_time: np.ndarray, ref_freqs: list[np.ndarray], est_time: np.ndarray, est_freqs: list[np.ndarray]):\n    if not isinstance(ref_time, np.ndarray):\n        return False\n    for freq in ref_freqs:\n        if not isinstance(freq, np.ndarray):\n            return False\n    if not isinstance(est_time, np.ndarray):\n        return False\n    for freq in est_freqs:\n        if not isinstance(freq, np.ndarray):\n            return False\n    return True", "contrast": "def validate(ref_time, ref_freqs, est_time, est_freqs):\n    util.validate_events(ref_time, max_time=MAX_TIME)\n    util.validate_events(est_time, max_time=MAX_TIME)\n    if ref_time.size == 0:\n        warnings.warn(\"Reference times are empty.\")\n    if ref_time.ndim != 1:\n        raise ValueError(\"Reference times have invalid dimension\")\n    if len(ref_freqs) == 0:\n        warnings.warn(\"Reference frequencies are empty.\")\n    if est_time.size == 0:\n        warnings.warn(\"Estimated times are empty.\")\n    if est_time.ndim != 1:\n        raise ValueError(\"Estimated times have invalid dimension\")\n    if len(est_freqs) == 0:\n        warnings.warn(\"Estimated frequencies are empty.\")\n    if ref_time.size != len(ref_freqs):\n        raise ValueError('Reference times and frequencies have unequal '\n                         'lengths.')\n    if est_time.size != len(est_freqs):\n        raise ValueError('Estimate times and frequencies have unequal '\n                         'lengths.')\n    for freq in ref_freqs:\n        util.validate_frequencies(freq, max_freq=MAX_FREQ, min_freq=MIN_FREQ,\n                                  allow_negatives=False)\n    for freq in est_freqs:\n        util.validate_frequencies(freq, max_freq=MAX_FREQ, min_freq=MIN_FREQ,\n                                  allow_negatives=False)", "label": 1}
{"index": "gp205409", "code": "def remove_redundant_segments(lst):\n    result = []\n    for item in lst:\n        if not result or item != result[-1]:\n            result.append(item)\n    return result", "contrast": "def reduce(self) -> None:\n        idx = 0\n        while idx < len(self):\n            if idx > 0 and                    self[idx - 1].type == 'text' and self[idx].type == 'text':\n                self[idx - 1].data['text'] += self[idx].data['text']\n                del self[idx]\n            else:\n                idx += 1", "label": 1}
{"index": "gp181932", "code": "def increment_byte_sequence(inp: bytes) -> bytes:\n    num = int.from_bytes(inp, byteorder='little', signed=False)\n    num += 1\n    return num.to_bytes((num.bit_length() + 7) // 8, byteorder='little', signed=False)", "contrast": "def sodium_increment(inp):\n    ensure(isinstance(inp, bytes),\n           raising=exc.TypeError)\n    ln = len(inp)\n    buf = ffi.new(\"unsigned char []\", ln)\n    ffi.memmove(buf, inp, ln)\n    lib.sodium_increment(buf, ln)\n    return ffi.buffer(buf, ln)[:]", "label": 1}
{"index": "gp077631", "code": "def get_pdb_id(self):\n        if self.pdb_id:\n            return self.pdb_id\n        else:\n            header = self.parsed_lines[\"HEADER\"]\n            assert(len(header) <= 1)\n            if header:\n                self.pdb_id = header[0][62:66]\n                return self.pdb_id\n        return None", "contrast": "class PDBFile:\n    def __init__(self, file_path: str, pdb_id: str = None):\n        self.file_path = file_path\n        self.pdb_id = pdb_id\n    def get_pdb_id(self):\n        if self.pdb_id:\n            return self.pdb_id\n        else:\n            with open(self.file_path) as f:\n                for line in f:\n                    if line.startswith('HEADER'):\n                        header_info = line.split()\n                        for info in header_info:\n                            if len(info) == 4 and info.isalnum():\n                                return info\n                        break\n            return None", "label": 0}
{"index": "gp194330", "code": "def update_income_dist(AggShkDstn, IncomeDstn):\n    for j in range(IncomeDstn[0].size):\n        for i in range(AggShkDstn[0].size):\n            PermShkVals_new = IncomeDstn[1][j] * AggShkDstn[1][i]\n            TranShkVals_new = IncomeDstn[2][j] * AggShkDstn[2][i]\n            Weight_new = IncomeDstn[0][j] * AggShkDstn[0][i]\n            IncomeDstn[0][i * IncomeDstn[0].size + j] = Weight_new\n            IncomeDstn[1][i * IncomeDstn[0].size + j] = PermShkVals_new\n            IncomeDstn[2][i * IncomeDstn[0].size + j] = TranShkVals_new\n    return None", "contrast": "def addAggShkDstn(self,AggShkDstn):\n        if len(self.IncomeDstn[0]) > 3:\n            self.IncomeDstn = self.IncomeDstnWithoutAggShocks\n        else:\n            self.IncomeDstnWithoutAggShocks = self.IncomeDstn\n        self.IncomeDstn = [combineIndepDstns(self.IncomeDstn[t],AggShkDstn) for t in range(self.T_cycle)]", "label": 1}
{"index": "gp274189", "code": "def update_state(kept_indices, new_indices, new_state_vec, new_state_cov, new_noise_var):\n    new_kept_indices = dict(kept_indices)\n    index_offset = len(kept_indices)\n    for new_index, state_index in zip(new_indices, range(index_offset, index_offset + len(new_indices))):\n        new_kept_indices[new_index] = state_index\n    new_state_vec = np.concatenate((state_vec, new_state_vec), axis=0)\n    new_state_cov = np.concatenate((state_cov, new_state_cov), axis=0)\n    new_noise_var = np.concatenate((noise_var, new_noise_var))\n    return new_kept_indices, new_state_vec, new_state_cov, new_noise_var", "contrast": "def add_features(self, kept_indices, new_indices,\n                     new_state_vec, new_state_cov, new_noise_var):\n        assert len(kept_indices) == len(self.state_vec)\n        assert len(new_indices) == len(new_state_vec)\n        assert len(new_indices) == len(new_state_cov)\n        assert len(new_indices) == len(new_noise_var)\n        if self.has_cached_obs_vec:\n            del self.obs_vec\n        if self.has_cached_predicted_state_vec:\n            del self.predicted_obs_vec\n        nfeatures = len(kept_indices) + len(new_indices)\n        next_state_vec = np.zeros((nfeatures, self.state_len))\n        next_state_cov = np.zeros((nfeatures, self.state_len, self.state_len))\n        next_noise_var = np.zeros((nfeatures, self.state_len))\n        if len(kept_indices) > 0:\n            next_state_vec[kept_indices] = self.state_vec\n            next_state_cov[kept_indices] = self.state_cov\n            next_noise_var[kept_indices] = self.noise_var\n            if len(self.state_noise_idx) > 0:\n                self.state_noise_idx = kept_indices[self.state_noise_idx]\n        if len(new_indices) > 0:\n            next_state_vec[new_indices] = new_state_vec\n            next_state_cov[new_indices] = new_state_cov\n            next_noise_var[new_indices] = new_noise_var\n        self.state_vec = next_state_vec\n        self.state_cov = next_state_cov\n        self.noise_var = next_noise_var", "label": 1}
{"index": "gp135337", "code": "def stepback(self, append=False):\n        if append:\n            data = self._buffer[self._index - 1]\n            self._buffer.append(data)\n        else:\n            self._index -= 1", "contrast": "def stepback_buffer(buffer, append=False):\n    if not append:\n        return buffer[:-1]\n    else:\n        return buffer[::-1]", "label": 0}
{"index": "gp128814", "code": "def _update_pathway_definitions(crosstalk_corrected_index_map,\n                                gene_row_names,\n                                pathway_column_names):\n    corrected_pathway_definitions = {}\n    for pathway_index, gene_indices in crosstalk_corrected_index_map.items():\n        pathway = pathway_column_names[pathway_index]\n        genes = set([gene_row_names[index] for index in list(gene_indices)])\n        corrected_pathway_definitions[pathway] = genes\n    return corrected_pathway_definitions", "contrast": "def convert_mapping(mapping, pathway_names, gene_names):\n    converted_mapping = {}\n    for pathway_id, gene_ids in mapping.items():\n        pathway_name = pathway_names.get(pathway_id, None)\n        converted_gene_names = []\n        for gene_id in gene_ids:\n            gene_name = gene_names.get(gene_id, None)\n            if gene_name:\n                converted_gene_names.append(gene_name)\n        if pathway_name:\n            converted_mapping[pathway_name] = converted_gene_names\n    return converted_mapping", "label": 0}
{"index": "gp089718", "code": "def _to_dict(self):\n        return dict(area= self.area._to_dict(),\n                    earthquakes = [q._to_dict() for q in self.earthquakes],\n                    title = self.title)", "contrast": "class MyClass:\n    def __init__(self, attribute1, attribute2):\n        self.attribute1 = attribute1\n        self.attribute2 = attribute2\n    def to_dict(self):\n        return {'attribute1': self.attribute1, 'attribute2': self.attribute2}", "label": 0}
{"index": "gp018372", "code": "def uniform_int(low:int, high:int, size:Optional[List[int]]=None)->IntOrTensor:\n    return random.randint(low,high) if size is None else torch.randint(low,high+1,size)", "contrast": "import random\nfrom typing import Union, List\ndef generate_int_or_tensor(size: Union[int, List[int]], low: int, high: int) -> Union[int, List[int]]:\n    if isinstance(size, int):\n        return [random.randint(low, high) for _ in range(size)]\n    else:\n        return [[random.randint(low, high) for _ in range(size[i])] for i in range(len(size))]", "label": 0}
{"index": "gp019723", "code": "def _encode_gif(images, fps):\n  writer = WholeVideoWriter(fps)\n  writer.write_multi(images)\n  return writer.finish()", "contrast": "from moviepy.video.io.ImageSequenceClip import ImageSequenceClip\nimport numpy as np\ndef encode_images_to_gif(images, fps):\n    if isinstance(images, list):\n        images = np.array(images)\n    clip = ImageSequenceClip(list(images), fps=fps)\n    gif_string = clip.write_gif('temp.gif', fps=fps)\n    with open('temp.gif', 'rb') as f:\n        gif_bytes = f.read()\n    return gif_bytes", "label": 0}
{"index": "gp081984", "code": "def xcoord(self):\n        v = next(self.raw_data.psy.iter_base_variables)\n        return self.decoder.get_x(v, coords=self.data.coords)", "contrast": "def x_coordinate(x):\n    return x", "label": 0}
{"index": "gp091779", "code": "def expr_order_key(expr):\n    if hasattr(expr, '_order_key'):\n        return expr._order_key\n    try:\n        if isinstance(expr.kwargs, OrderedDict):\n            key_vals = expr.kwargs.values()\n        else:\n            key_vals = [expr.kwargs[key] for key in sorted(expr.kwargs)]\n        return KeyTuple((expr.__class__.__name__, ) +\n                        tuple(map(expr_order_key, expr.args)) +\n                        tuple(map(expr_order_key, key_vals)))\n    except AttributeError:\n        return str(expr)", "contrast": "def default_order_key(expr):\n    if isinstance(expr, str):\n        return expr.lower(),  \n    elif isinstance(expr, (int, float)):\n        return expr,\n    elif isinstance(expr, tuple):\n        return tuple(default_order_key(e) for e in expr)\n    elif isinstance(expr, list):\n        return [default_order_key(e) for e in expr]\n    elif isinstance(expr, dict):\n        return {k: default_order_key(v) for k, v in expr.items()}\n    else:\n        return str(expr).lower(),", "label": 0}
{"index": "gp182648", "code": "def get_hash(blockbase):\n    block_hash = hash(blockbase)\n    return block_hash", "contrast": "def Hash(self):\n        if not self.__hash:\n            hashdata = self.RawData()\n            ba = bytearray(binascii.unhexlify(hashdata))\n            hash = bin_dbl_sha256(ba)\n            self.__hash = UInt256(data=hash)\n        return self.__hash", "label": 1}
{"index": "gp144523", "code": "def display(self, image):\n        assert(image.mode == self.mode)\n        assert(image.size == self.size)\n        image = self.preprocess(image)\n        if self.framebuffer.redraw_required(image):\n            left, top, right, bottom = self.framebuffer.inflate_bbox()\n            width = right - left\n            height = bottom - top\n            buf = bytearray(width * height >> 1)\n            self._set_position(top, right, bottom, left)\n            self._populate(buf, self.framebuffer.getdata())\n            self.data(list(buf))", "contrast": "from PIL import Image\ndef render_to_greyscale_OLED(image):\n    if image.mode == '1' or image.mode == 'L':\n        return image\n    elif image.mode == 'RGB':\n        gray_image = Image.new('L', image.size)\n        for i in range(image.size[0]):\n            for j in range(image.size[1]):\n                pixel = image.getpixel((i, j))\n                gray_value = int(0.299 * pixel[0] + 0.587 * pixel[1] + 0.114 * pixel[2])\n                gray_image.putpixel((i, j), gray_value)\n        return gray_image\n    else:\n        raise ValueError('Unsupported image mode')", "label": 0}
{"index": "gp325539", "code": "def disconnect(silent=False):\n    if not silent:\n        packet = {'type': 'disconnect'}\n        send_packet(packet)\n    for job in jobs.values():\n        job.stop()\n    jobs.clear()\n    current_app.socketio.server.disconnect(current_app.client_namespace)", "contrast": "def disconnect(self, silent=False):\n        if not silent:\n            packet = {\"type\": \"disconnect\",\n                      \"endpoint\": self.ns_name}\n            self.socket.send_packet(packet)\n        try:\n            self.socket.remove_namespace(self.ns_name)\n        finally:\n            self.kill_local_jobs()", "label": 1}
{"index": "gp264248", "code": "def filter_listeners_with_hooks(svc_event, listeners):\n    filtered_listeners = []\n    for listener in listeners:\n        if hasattr(listener, 'hooks') and svc_event.event_type in listener.hooks:\n            filtered_listeners.append(listener)\n    return filtered_listeners", "contrast": "def _filter_with_hooks(self, svc_event, listeners):\n        svc_ref = svc_event.get_service_reference()\n        hook_refs = self._registry.find_service_references(\n            SERVICE_EVENT_LISTENER_HOOK\n        )\n        if hook_refs:\n            ctx_listeners = {}\n            for listener in listeners:\n                context = listener.bundle_context\n                ctx_listeners.setdefault(context, []).append(listener)\n            shrinkable_ctx_listeners = ShrinkableMap(\n                {\n                    context: ShrinkableList(value)\n                    for context, value in ctx_listeners.items()\n                }\n            )\n            for hook_ref in hook_refs:\n                if not svc_ref == hook_ref:\n                    hook_bundle = hook_ref.get_bundle()\n                    hook_svc = self._registry.get_service(hook_bundle, hook_ref)\n                    if hook_svc is not None:\n                        try:\n                            hook_svc.event(svc_event, shrinkable_ctx_listeners)\n                        except:\n                            self._logger.exception(\n                                \"Error calling EventListenerHook\"\n                            )\n                        finally:\n                            self._registry.unget_service(hook_bundle, hook_ref)\n            ret_listeners = set()\n            for bnd_listeners in shrinkable_ctx_listeners.values():\n                ret_listeners.update(bnd_listeners)\n            return ret_listeners\n        return listeners", "label": 1}
{"index": "gp249374", "code": "def save_modified_batch(server_context, assay_id, batch):\n    server_context.data_client.save_assay_batch(\n        assay_id=assay_id,\n        batch=batch,\n    )", "contrast": "def save_batch(server_context, assay_id, batch):\n    result = save_batches(server_context, assay_id, [batch])\n    if result is not None:\n        return result[0]\n    return None", "label": 1}
{"index": "gp275115", "code": "import numpy as np\nfrom stwcs.wcsutil import HSTWCS\ndef in_science_area(img: np.ndarray, wcs: HSTWCS, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    ra, dec = wcs.all_pix2world(x, y, 0)  \n    inside_img = np.logical_not(np.isnan(img[0]))  \n    inside_bounds = (ra >= wcs.wcs.crval[0]) & (ra <= wcs.wcs.crval[0]+wcs.wcs.cdelt[0]*img.shape[2]) &                    (dec >= wcs.wcs.crval[1]) & (dec <= wcs.wcs.crval[1]+wcs.wcs.cdelt[1]*img.shape[1])  \n    inside_science = np.zeros(x.shape, dtype=bool)  \n    for i in range(inside_science.size):\n        if inside_bounds[i]:\n            xpix, ypix = wcs.all_world2pix(ra[i], dec[i], 0)  \n            if 0 <= int(ypix) < img.shape[1] and 0 <= int(xpix) < img.shape[2]:\n                if np.isnan(img[0][int(ypix),int(xpix)]) == False:\n                    inside_science[i] = True  \n    return x[inside_science], y[inside_science]  ", "contrast": "def within_footprint(img, wcs, x, y):\n    if hasattr(wcs, 'naxis1'):\n        naxis1 = wcs.naxis1\n        naxis2 = wcs.naxis2\n    elif hasattr(wcs, 'pixel_shape'):\n        naxis1, naxis2 = wcs.pixel_shape\n    else:\n        naxis1 = wcs._naxis1\n        naxis2 = wcs._naxis2\n    maskx = np.bitwise_or(x < 0, x > naxis1)\n    masky = np.bitwise_or(y < 0, y > naxis2)\n    mask = ~np.bitwise_or(maskx, masky)\n    x = x[mask]\n    y = y[mask]\n    img_mask = create_image_footprint(img, wcs, border=1.0)\n    inmask = np.where(img_mask[y.astype(np.int32), x.astype(np.int32)])[0]\n    x = x[inmask]\n    y = y[inmask]\n    return x, y", "label": 1}
{"index": "gp015911", "code": "def nice(self):\n        msg = \"this property is deprecated; use Process.get_nice() method instead\"\n        warnings.warn(msg, category=DeprecationWarning, stacklevel=2)\n        return self.get_nice()", "contrast": "import os\ndef get_set_process_nice(pid=None, priority=None):\n    if pid is None:\n        pid = os.getpid()\n    if priority is not None:\n        os.nice(priority)\n        return\n    return os.nice(0)", "label": 0}
{"index": "gp199062", "code": "import hashlib\ndef change_avatar(new_avatar=None):\n    if new_avatar is None:\n        new_avatar = hashlib.sha256(bytes(str(time.time()), encoding='utf-8')).hexdigest()\n    else:\n        new_avatar = str(new_avatar)\n    print(\"Avatar changed to:\", new_avatar)", "contrast": "def change(self, inpt, hashfun=DEFAULT_HASHFUN):\n        self.img = self.__create_image(inpt, hashfun)", "label": 1}
{"index": "gp139025", "code": "def _filter_disabled_regions(contents):\n    contents = list(contents)\n    in_backticks = False\n    contents_len = len(contents)\n    index = 0\n    while index < contents_len:\n        character = contents[index]\n        if character == \"`\":\n            if ((index + 2) < contents_len and\n                    \"\".join(contents[index:index + 3]) == \"```\"):\n                in_backticks = not in_backticks\n                index += 3\n                continue\n        if in_backticks:\n            contents[index] = \" \"\n        index += 1\n    return \"\".join(contents)", "contrast": "def filter_backtick_regions(regions):\n    filtered_regions = []\n    for region in regions:\n        if '`' not in region:\n            filtered_regions.append(region)\n    return filtered_regions", "label": 0}
{"index": "gp266897", "code": "import numpy as np\ndef bootstrap_parameters(Ss, ipar=True, nb=1000):\n    T = []\n    V = []\n    for S in Ss:\n        eigenvalues, eigenvectors = np.linalg.eig(S)\n        idx = np.argsort(-eigenvalues)\n        T.append(eigenvalues[idx])\n        V.append(eigenvectors[:, idx])\n    Tmean = np.mean(T, axis=0)\n    Vmean = np.mean(V, axis=0)\n    Taus = []\n    Vs = []\n    for _ in range(nb):\n        if ipar:\n            S = np.diag(Tmean).dot(Vmean).dot(np.diag(Tmean)).dot(Vmean.T)\n        else:\n            S = np.zeros_like(Ss[0])\n            for i in range(len(S)):\n                S[i, i] = Tmean[i]\n            for i in range(len(S)):\n                for j in range(i+1, len(S)):\n                    u = np.random.normal(0, 1)\n                    S[i, j] = u\n                    S[j, i] = u\n        eigenvalues, eigenvectors = np.linalg.eig(S)\n        idx = np.argsort(-eigenvalues)\n        Taus.append(eigenvalues[idx])\n        Vs.append(eigenvectors[:, idx])\n    return Tmean, Vmean, Taus, Vs", "contrast": "def s_boot(Ss, ipar=0, nb=1000):\n    Ss = np.array(Ss)\n    npts = Ss.shape[0]\n    nf, Sigma, avs = sbar(Ss)\n    Tmean, Vmean = doseigs(avs)  \n    Taus, Vs = [], []  \n    for k in range(int(float(nb))):  \n        BSs = apseudo(Ss, ipar, Sigma)\n        nf, sigma, avbs = sbar(BSs)  \n        tau, Vdirs = doseigs(avbs)  \n        Taus.append(tau)\n        Vs.append(Vdirs)\n    return Tmean, Vmean, Taus, Vs", "label": 1}
{"index": "gp134377", "code": "def _safe_getmodule(o):\n    from inspect import getmodule\n    try:\n        return getmodule(o)\n    except: \n        msg.err(\"_safe_getmodule: {}\".format(o), 2)\n        pass", "contrast": "def get_module(o):\n    return o.__class__.__module__", "label": 0}
{"index": "gp198046", "code": "def _print(self, *data, sep=' ', end='\\n', file=None):\n    if file is None:\n        file = self.channel\n    print(*data, sep=sep, end=end, file=file)", "contrast": "def _print(self, *data, **kw):\n        sep = kw.pop('sep', ' ')\n        end = kw.pop('end', '\\n')\n        _ = kw.pop('file', None)\n        assert not kw, 'Too many keyword-only arguments'\n        data = sep.join(map(str, data))\n        self._chan.write(data + end)", "label": 1}
{"index": "gp037385", "code": "def rm_token(opts, tok):\n    redis_client = _redis_client(opts)\n    if not redis_client:\n        return\n    try:\n        redis_client.delete(tok)\n        return {}\n    except Exception as err:\n        log.warning('Could not remove token %s: %s', tok, err)", "contrast": "def remove_token(opts, tok):\n    try:\n        return {}\n    except:\n        return None", "label": 0}
{"index": "gp103832", "code": "def get_synidx(self, cellindex):\n        cell = self.cellsim(cellindex, return_just_cell=True)\n        synidx = {}\n        for i, X in enumerate(self.X):\n            synidx[X] = self.fetchSynIdxCell(cell=cell,\n                                             nidx=self.k_yXL[:, i],\n                                             synParams=self.synParams.copy())\n        return synidx", "contrast": "import numpy as np\ndef draw_synapse_locations(cellindex):\n    np.random.seed(POPULATIONSEED + cellindex)\n    synidx = dict()\n    return synidx", "label": 0}
{"index": "gp285488", "code": "class Expression:\n    def __init__(self, value):\n        self.value = value\n    def __repr__(self):\n        return f\"{self.value}\"\n    def __eq__(self, other):\n        return type(self) == type(other) and self.value == other.value\n    def __ne__(self, other):\n        return not self.__eq__(other)\n    def __hash__(self):\n        return hash((type(self), self.value))\n    @classmethod\n    def create_spl_expression(cls, value):\n        if isinstance(value, cls):\n            return cls(value.value)\n        return cls(value)", "contrast": "def expression(value):\n        if isinstance(value, Expression):\n            return Expression(value._type, value._value)\n        if hasattr(value, 'spl_json'):\n            sj = value.spl_json()\n            return Expression(sj['type'], sj['value'])\n        return Expression('splexpr', value)", "label": 1}
{"index": "gp268773", "code": "def set_client_certificate(self, client_certificate):\n    self.client_certificate = client_certificate", "contrast": "def client_certificate(self, client_certificate):\n        if client_certificate is None:\n            raise ValueError(\"Invalid value for `client_certificate`, must not be `None`\")\n        if client_certificate is not None and len(client_certificate) > 3000:\n            raise ValueError(\"Invalid value for `client_certificate`, length must be less than or equal to `3000`\")\n        self._client_certificate = client_certificate", "label": 1}
{"index": "gp252112", "code": "def get_modified_lines(filename, extra_data, commit):\n    if extra_data is None:\n        return None\n    modified_lines = []\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n        for line_num, line in enumerate(lines):\n            if commit == extra_data.get((filename, line_num + 1)):\n                modified_lines.append(line)\n    return modified_lines if modified_lines else None", "contrast": "def modified_lines(filename, extra_data, commit=None):\n    if extra_data is None:\n        return []\n    if extra_data != 'M':\n        return None\n    command = ['hg', 'diff', '-U', '0']\n    if commit:\n        command.append('--change=%s' % commit)\n    command.append(filename)\n    diff_lines = subprocess.check_output(command).split(\n        os.linesep.encode('utf-8'))\n    diff_line_numbers = utils.filter_lines(\n        diff_lines,\n        br'@@ -\\d+,\\d+ \\+(?P<start_line>\\d+),(?P<lines>\\d+) @@',\n        groups=('start_line', 'lines'))\n    modified_line_numbers = []\n    for start_line, lines in diff_line_numbers:\n        start_line = int(start_line)\n        lines = int(lines)\n        modified_line_numbers.extend(range(start_line, start_line + lines))\n    return modified_line_numbers", "label": 1}
{"index": "gp103511", "code": "def get(self, criteria=None, offset=None, limit=None):\n        if criteria is None and limit is None:\n            return self._get_all()\n        elif limit is not None and limit == 1:\n            return self.get_one(criteria)\n        else:\n            return self._get_with_criteria(criteria, offset=offset, limit=limit)", "contrast": "def get(items, criteria=None):\n    if criteria:\n        return [item for item in items if criteria(item)]\n    else:\n        return items", "label": 0}
{"index": "gp188083", "code": "def formatted_filenames(filenames):\n    formatted_files = {}\n    for filename in filenames:\n        if filename.endswith('.txt'):\n            formatted_files[filename] = filename.upper()\n        elif filename.endswith('.csv'):\n            formatted_files[filename] = filename.lower()\n        elif filename.endswith('.docx'):\n            formatted_files[filename] = filename.title()\n        else:\n            formatted_files[filename] = filename\n    return formatted_files", "contrast": "def get_formatted_files(self, net):\n        idx = -1\n        if (\n                self.event_name is not None and\n                net.history\n        ):\n            for i, v in enumerate(net.history[:, self.event_name]):\n                if v:\n                    idx = i\n        return {\n            \"f_params\": self._format_target(net, self.f_params, idx),\n            \"f_optimizer\": self._format_target(net, self.f_optimizer, idx),\n            \"f_history\": self.f_history_,\n            \"f_pickle\": self._format_target(net, self.f_pickle, idx)\n        }", "label": 1}
{"index": "gp162868", "code": "def parameters(self):\n        return self.block_tl, self.block_br, self.rows, self.cols, self.cells", "contrast": "def get_selection_params(self):\n    return (self.block_tl, self.block_br, self.rows, self.cols, self.cells)", "label": 0}
{"index": "gp003148", "code": "def get_property_descriptions(self):\n        return {k: v.as_property_description()\n                for k, v in self.properties.items()}", "contrast": "def get_properties(thing):\n    properties = {}\n    for prop in dir(thing):\n        if not prop.startswith(\"__\"):\n            properties[prop] = getattr(thing, prop).__doc__\n    return properties", "label": 0}
{"index": "gp109241", "code": "def get_next_appointment(self, appointment_group_ids=None):\r\n        path = {}\r\n        data = {}\r\n        params = {}\r\n        \"\"\"List of ids of appointment groups to search.\"\"\"\r\n        if appointment_group_ids is not None:\r\n            params[\"appointment_group_ids\"] = appointment_group_ids\r\n        self.logger.debug(\"GET /api/v1/appointment_groups/next_appointment with query params: {params} and form data: {data}\".format(params=params, data=data, **path))\r\n        return self.generic_request(\"GET\", \"/api/v1/appointment_groups/next_appointment\".format(**path), data=data, params=params, all_pages=True)", "contrast": "def get_next_appointment():\n    appointments = ['2021-10-01', '2021-10-05', '2021-10-06', '2021-10-10']\n    now = datetime.now()\n    next_appointment = []\n    for appointment in appointments:\n        appointment_date = datetime.strptime(appointment, \"%Y-%m-%d\")\n        if appointment_date > now:\n            next_appointment.append(appointment)\n            break\n    return next_appointment", "label": 0}
{"index": "gp133936", "code": "def _check_forward_mode_input_dict(self, var_tbl: dict) -> int:\n        T: int = 1\n        for var_name in var_tbl:\n            val = var_tbl[var_name]\n            if isinstance(val, scalar_instance_types):\n                t = 1\n            elif isinstance(val, np.ndarray):\n                t = self._calc_T_var(val)\n            else:\n                raise ValueError(f'val={val} in var_tbl; {type(val)} not a recognized value type.')\n            if t > 1 and T == 1:\n                T = t\n            elif t not in (1,T):\n                raise ValueError(f'Bound variable {var_name} has inconsistent shape')\n        return T", "contrast": "def check_input_dict_shape(input_dict):\n    T = None\n    for key in input_dict.keys():\n        if isinstance(input_dict[key], (int, float)):\n            if T is None:\n                T = 1\n            else:\n                T *= 1\n        elif isinstance(input_dict[key], (list, tuple)):\n            if T is None:\n                T = len(input_dict[key])\n            else:\n                T *= len(input_dict[key])\n        else:\n            raise ValueError(f\"Invalid shape for {key}\")\n    return T", "label": 0}
{"index": "gp066924", "code": "def create_budget(self, wallet_name, model_uuid, limit):\n        request = {\n            'model': model_uuid,\n            'limit': limit,\n        }\n        return make_request(\n            '{}wallet/{}/budget'.format(self.url, wallet_name),\n            method='POST',\n            body=request,\n            timeout=self.timeout,\n            client=self._client)", "contrast": "def create_budget(wallet_name, model_uuid, limit):\n    try:\n        response = make_request('POST', 'plans/', {'walletName': wallet_name, 'modelUuid': model_uuid, 'limit': limit})\n        return response['success']\n    except ServerError as e:\n        raise e", "label": 0}
{"index": "gp020619", "code": "def cv_squared(x):\n  epsilon = 1e-10\n  float_size = tf.to_float(tf.size(x)) + epsilon\n  mean = tf.reduce_sum(x) / float_size\n  variance = tf.reduce_sum(tf.squared_difference(x, mean)) / float_size\n  return variance / (tf.square(mean) + epsilon)", "contrast": "import tensorflow as tf\ndef squared_coefficient_variation(x):\n    if tf.size(x) == 0:\n        return 0\n    else:\n        mean = tf.reduce_mean(x)\n        mean_diff = x - mean\n        mean_diff_squared = tf.square(mean_diff)\n        variance = tf.reduce_mean(mean_diff_squared)\n        coefficient_variation = tf.divide(tf.sqrt(variance + 1e-8), mean + 1e-8)\n        squared_coefficient_variation = tf.square(coefficient_variation)\n        return squared_coefficient_variation", "label": 0}
{"index": "gp151125", "code": "def flatten_phases_and_groups(phases_or_groups):\n  if isinstance(phases_or_groups, PhaseGroup):\n    phases_or_groups = [phases_or_groups]\n  ret = []\n  for phase in phases_or_groups:\n    if isinstance(phase, PhaseGroup):\n      ret.append(phase.flatten())\n    elif isinstance(phase, collections.Iterable):\n      ret.extend(flatten_phases_and_groups(phase))\n    else:\n      ret.append(phase_descriptor.PhaseDescriptor.wrap_or_copy(phase))\n  return ret", "contrast": "def flatten_list(nested_list):\n    flat_list = []\n    for element in nested_list:\n        if isinstance(element, list):\n            flat_list += flatten_list(element)\n        else:\n            flat_list.append(element)\n    return flat_list", "label": 0}
{"index": "gp320928", "code": "def remove_rows(idsToRemove):\n    for idx in sorted(idsToRemove, reverse=True):\n        del data[idx]", "contrast": "def removeIds(self, idsToRemove):\n    rowsToRemove = [k for k, rowID in enumerate(self._categoryRecencyList)                    if rowID in idsToRemove]\n    self._removeRows(rowsToRemove)", "label": 1}
{"index": "gp215836", "code": "import inspect\nclass ChainedMinimizer:\n    def __init__(self, *minimizers):\n        self.minimizers = minimizers\n    def execute(self, x, **kwargs):\n        result = x\n        for minimizer in self.minimizers:\n            result = minimizer(result, **kwargs)\n        return result\n    def signature(self):\n        parameters = [inspect.Parameter('x', inspect.Parameter.POSITIONAL_OR_KEYWORD)]\n        for minimizer in self.minimizers:\n            parameters.append(inspect.Parameter(minimizer.__name__, inspect.Parameter.POSITIONAL_OR_KEYWORD))\n        return inspect.Signature(parameters)", "contrast": "def _make_signature(self):\n        name = lambda x: x.__class__.__name__\n        count = Counter(\n            [name(minimizer) for minimizer in self.minimizers]\n        ) \n        parameters = []\n        for minimizer in reversed(self.minimizers):\n            if count[name(minimizer)] == 1:\n                param_name = name(minimizer)\n            else:\n                param_name = '{}_{}'.format(name(minimizer), count[name(minimizer)])\n            count[name(minimizer)] -= 1\n            parameters.append(\n                inspect_sig.Parameter(\n                    param_name,\n                    kind=inspect_sig.Parameter.KEYWORD_ONLY,\n                    default={}\n                )\n            )\n        return inspect_sig.Signature(parameters=reversed(parameters))", "label": 1}
{"index": "gp275764", "code": "def generate_model_maps(model_name=None):\n    if model_name is not None:\n        pass\n    else:\n        pass", "contrast": "def generate_model(self, model_name=None):\n        for i, c in enumerate(self._components):\n            c.generate_model(model_name=model_name)", "label": 1}
{"index": "gp083116", "code": "def stamp_title(kb_app: kb,\n                sphinx_app: Sphinx,\n                doctree: doctree):\n    resources = sphinx_app.env.resources\n    confdir = sphinx_app.confdir\n    source = PurePath(doctree.attributes['source'])\n    docname = str(source.relative_to(confdir)).split('.rst')[0]\n    resource = resources.get(docname)\n    if resource:\n        title = get_rst_title(doctree)\n        resource.title = title", "contrast": "def walk_tree(tree):\n    if 'title' in tree:\n        title = tree['title']\n        if isinstance(title, dict):\n            tree['title'] = title.get('text', '')\n        elif isinstance(title, list):\n            for i in title:\n                walk_tree(i)\n    for k, v in tree.items():\n        if isinstance(v, dict):\n            walk_tree(v)\n        elif isinstance(v, list):\n            for i in v:\n                if isinstance(i, dict):\n                    walk_tree(i)\n    return tree", "label": 0}
{"index": "gp286934", "code": "import subprocess\ndef pull_docker_images():\n    docker_images = [\"image_1\", \"image_2\", \"image_3\"]\n    for image in docker_images:\n        try:\n            subprocess.check_call([\"docker\", \"pull\", image])\n        except subprocess.CalledProcessError as e:\n            print(\"Pulling image failed with error:\", e)", "contrast": "def post_register_hook(self, verbosity=1):\n        if not getattr(settings, 'FLOW_DOCKER_DONT_PULL', False):\n            call_command('list_docker_images', pull=True, verbosity=verbosity)", "label": 1}
{"index": "gp118623", "code": "def get_conv(bits, bin_point, signed=False, scaling=1.0):\n    conversion_t = {}\n    conversion_t[\"bits\"] = bits\n    conversion_t[\"bin_point\"] = bin_point\n    conversion_t[\"signed\"] = signed\n    conversion_t[\"scaling\"] = scaling\n    conversion_t[\"dec_step\"] = 1.0 / (2 ** bin_point)\n    conversion_t[\"dec_mask\"] = sum([2 ** i for i in range(bin_point)])\n    if bits == 8:\n        conversion_t[\"fmt\"] = \"B\"\n    elif bits == 16:\n        conversion_t[\"fmt\"] = \"H\"\n    elif bits == 32:\n        conversion_t[\"fmt\"] = \"I\"\n    else:\n        raise ConversionError(\"numer of bits not supported: \" + str(bits))\n    if signed:\n        _get_signed_params(conversion_t)\n    else:\n        _get_unsigned_params(conversion_t)\n    return conversion_t", "contrast": "def create_conversion_structure(bits, bin_point, signed, scaling=None):\n    conversion_structure = {\n        'bits': bits,\n        'bin_point': bin_point,\n        'signed': signed\n    }\n    if scaling:\n        conversion_structure['scaling'] = scaling\n    return conversion_structure", "label": 0}
{"index": "gp287461", "code": "def boolean_grammar(name: str) -> str:\n    return f\"{name}: 'Y' | 'N'\"", "contrast": "def boolean(name=None):\n    if name is None:\n        name = 'Boolean Field'\n    field = pp.Regex('[YN]')\n    field.setParseAction(lambda b: _to_boolean(b[0]))\n    field.setName(name)\n    return field", "label": 1}
{"index": "gp293202", "code": "import hashlib\ndef generate_hash_functions(num_functions):\n    def murmur3_64(key, seed=0):\n        m = 0xc6a4a7935bd1e995\n        r = 47\n        h = seed ^ (len(key) * m)\n        for i in range(0, len(key) // 8):\n            k = int.from_bytes(key[i*8:(i+1)*8], byteorder='little')\n            k *= m\n            k ^= k >> r\n            k *= m\n            h ^= k\n            h *= m\n        if len(key) % 8 >= 4:\n            k = int.from_bytes(key[-4:], byteorder='little')\n            k *= m\n            k ^= k >> r\n            k *= m\n            h ^= k\n            h *= m\n            key = key[:-(len(key) % 8)]\n        if len(key) % 8 > 0:\n            k = int.from_bytes(key[-8:], byteorder='little')\n            k *= m\n            k ^= k >> r\n            k *= m\n            h ^= k\n            h *= m\n        h ^= h >> r\n        h *= m\n        h ^= h >> r\n        return h\n    hashes = []\n    for i in range(num_functions):\n        def new_hash(key, seed=i+1):\n            return murmur3_64(key, seed=seed)\n        hashes.append(new_hash)\n    return hashes", "contrast": "def generate_hashfunctions(nbr_bits, nbr_slices):\n    def _make_hashfuncs(key):\n        if isinstance(key, text_type):\n            key = key.encode('utf-8')\n        else:\n            key = str(key)\n        rval = []\n        current_hash = 0\n        for i in range(nbr_slices):\n            seed = current_hash\n            current_hash = hash64(key, seed)\n            rval.append(current_hash % nbr_bits)\n        return rval\n    return _make_hashfuncs", "label": 1}
{"index": "gp019745", "code": "def get_policy(observations, hparams, action_space):\n  if not isinstance(action_space, gym.spaces.Discrete):\n    raise ValueError(\"Expecting discrete action space.\")\n  obs_shape = common_layers.shape_list(observations)\n  (frame_height, frame_width) = obs_shape[2:4]\n  if hparams.policy_problem_name == \"dummy_policy_problem_ttt\":\n    tf.logging.info(\"Using DummyPolicyProblemTTT for the policy.\")\n    policy_problem = tic_tac_toe_env.DummyPolicyProblemTTT()\n  else:\n    tf.logging.info(\"Using DummyPolicyProblem for the policy.\")\n    policy_problem = DummyPolicyProblem(action_space, frame_height, frame_width)\n  trainer_lib.add_problem_hparams(hparams, policy_problem)\n  hparams.force_full_predict = True\n  model = registry.model(hparams.policy_network)(\n      hparams, tf.estimator.ModeKeys.TRAIN\n  )\n  try:\n    num_target_frames = hparams.video_num_target_frames\n  except AttributeError:\n    num_target_frames = 1\n  features = {\n      \"inputs\": observations,\n      \"input_action\": tf.zeros(obs_shape[:2] + [1], dtype=tf.int32),\n      \"input_reward\": tf.zeros(obs_shape[:2] + [1], dtype=tf.int32),\n      \"targets\": tf.zeros(obs_shape[:1] + [num_target_frames] + obs_shape[2:]),\n      \"target_action\": tf.zeros(\n          obs_shape[:1] + [num_target_frames, 1], dtype=tf.int32),\n      \"target_reward\": tf.zeros(\n          obs_shape[:1] + [num_target_frames, 1], dtype=tf.int32),\n      \"target_policy\": tf.zeros(\n          obs_shape[:1] + [num_target_frames] + [action_space.n]),\n      \"target_value\": tf.zeros(\n          obs_shape[:1] + [num_target_frames])\n  }\n  with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    t2t_model.create_dummy_vars()\n    (targets, _) = model(features)\n  return (targets[\"target_policy\"][:, 0, :], targets[\"target_value\"][:, 0])", "contrast": "def get_policy_network(observations, hparams, action_space):\n    logits = \n    value = \n    return logits, value", "label": 0}
{"index": "gp311379", "code": "import datetime\ndef copy_timestamp(time_string):\n    try:\n        dt = datetime.datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S.%f%z')\n    except ValueError:\n        dt = datetime.datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S.%f')\n        dt = dt.replace(tzinfo=datetime.timezone.utc)\n    return dt.timestamp()", "contrast": "def CopyFromDateTimeString(self, time_string):\n    date_time_values = self._CopyDateTimeFromString(time_string)\n    year = date_time_values.get('year', 0)\n    month = date_time_values.get('month', 0)\n    day_of_month = date_time_values.get('day_of_month', 0)\n    hours = date_time_values.get('hours', 0)\n    minutes = date_time_values.get('minutes', 0)\n    seconds = date_time_values.get('seconds', 0)\n    microseconds = date_time_values.get('microseconds', 0)\n    timestamp = self._GetNumberOfSecondsFromElements(\n        year, month, day_of_month, hours, minutes, seconds)\n    timestamp *= definitions.MILLISECONDS_PER_SECOND\n    if microseconds:\n      milliseconds, _ = divmod(\n          microseconds, definitions.MILLISECONDS_PER_SECOND)\n      timestamp += milliseconds\n    self._timestamp = timestamp\n    self.is_local_time = False", "label": 1}
{"index": "gp208562", "code": "def present_proof(proofRequest):\n    return proof, revealed_attributes", "contrast": "async def presentProof(self, proofRequest: ProofRequest) -> FullProof:\n        claims, requestedProof = await self._findClaims(proofRequest)\n        proof = await self._prepareProof(claims, proofRequest.nonce, requestedProof)\n        return proof", "label": 1}
{"index": "gp192295", "code": "def get_form_kwargs(**kwargs):\n    return kwargs", "contrast": "def get_form_kwargs(self):\n        kwargs = {\"user\": self.request.user, \"initial\": self.get_initial()}\n        if self.request.method in [\"POST\", \"PUT\"]:\n            kwargs.update({\n                \"data\": self.request.POST,\n                \"files\": self.request.FILES,\n            })\n        return kwargs", "label": 1}
{"index": "gp070865", "code": "def syscal_save_to_config_txt(filename, configs, spacing=1):\n    print('Number of measurements: ', configs.shape[0])\n    number_of_electrodes = configs.max().astype(int)\n    with open(filename, 'w') as fid:\n        _syscal_write_electrode_coords(fid, spacing, number_of_electrodes)\n        _syscal_write_quadpoles(fid, configs.astype(int))", "contrast": "import numpy as np\ndef write_syscal_config(filename: str, configs: np.ndarray) -> None:\n    with open(filename, 'w') as f:\n        for cfg in configs:\n            f.write(f'{cfg[0]:.3f}\\t{cfg[1]:.3f}\\t{cfg[2]:.3f}\\t{cfg[3]:.3f}\\n')", "label": 0}
{"index": "gp034040", "code": "def describe(DomainName,\n             region=None, key=None, keyid=None, profile=None):\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        domain = conn.describe_elasticsearch_domain_config(DomainName=DomainName)\n        if domain and 'DomainConfig' in domain:\n            domain = domain['DomainConfig']\n            keys = ('ElasticsearchClusterConfig', 'EBSOptions', 'AccessPolicies',\n                    'SnapshotOptions', 'AdvancedOptions')\n            return {'domain': dict([(k, domain.get(k, {}).get('Options')) for k in keys if k in domain])}\n        else:\n            return {'domain': None}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "contrast": "import boto3\ndef describe_domain(domain_name):\n    es = boto3.client('es')\n    response = es.describe_elasticsearch_domain(DomainName=domain_name)\n    return response", "label": 0}
{"index": "gp245008", "code": "import json\nfrom django.core.serializers.json import DjangoJSONEncoder\ndef encode_to_json(data):\n    return json.dumps(data, cls=DjangoJSONEncoder)", "contrast": "def serialize_instance(instance):\n    ret = dict([(k, v)\n                for k, v in instance.__dict__.items()\n                if not k.startswith('_')])\n    return json.loads(json.dumps(ret, cls=DjangoJSONEncoder))", "label": 1}
{"index": "gp292864", "code": "def validate_data(name, data):\n    if not isinstance(data, dict):\n        raise ValidationError(f\"{name} should be a dictionary.\")", "contrast": "def validate(self, name, data):\n        super().validate(name, data)\n        if self.value is not None:\n            try:\n                self.value = self.query.get(self.lookup_field == self.value)\n            except (AttributeError, ValueError, peewee.DoesNotExist):\n                raise ValidationError('related', field=self.lookup_field.name, values=self.value)", "label": 1}
{"index": "gp207251", "code": "def _reparentUnions(self):\n    for ns in self.namespaces:\n        for union in ns.unions:\n            ns._removeNode(union)\n        for pyclass in ns.classes:\n            pyclass._reparentUnions()\n            for union in pyclass.unions:\n                ns._removeNode(union)\n                pyclass._addNodeToSelf(union)\n        for pyfunc in ns.functions:\n            pyfunc._reparentUnions()\n            for union in pyfunc.unions:\n                ns._removeNode(union)\n                pyfunc._addNodeToSelf(union)\n            for node in pyfunc.nodes:\n                node._reparent(ns)\n        for struct in ns.structs:\n            struct._reparentUnions()\n            for union in struct.unions:\n                ns._removeNode(union)\n                struct._addNodeToSelf(union)", "contrast": "def reparentUnions(self):\n        removals = []\n        for u in self.unions:\n            parts = u.name.split(\"::\")\n            if len(parts) >= 2:\n                parent_name = \"::\".join(p for p in parts[:-1])\n                reparented  = False\n                for node in itertools.chain(self.class_like, self.namespaces):\n                    if node.name == parent_name:\n                        node.children.append(u)\n                        u.parent = node\n                        reparented = True\n                        break\n                if reparented:\n                    removals.append(u)\n                else:\n                    utils.verbose_log(\n                        \"The union {0} has '::' in its name, but no parent was found!\".format(u.name),\n                        utils.AnsiColors.BOLD_RED\n                    )\n        for rm in removals:\n            self.unions.remove(rm)", "label": 1}
{"index": "gp068141", "code": "def console(loop, log):\n    parser = argparse.ArgumentParser(description=console.__doc__)\n    parser.add_argument('--host', default='127.0.0.1', help='IP or FQDN of AVR')\n    parser.add_argument('--port', default='14999', help='Port of AVR')\n    parser.add_argument('--verbose', '-v', action='count')\n    args = parser.parse_args()\n    if args.verbose:\n        level = logging.DEBUG\n    else:\n        level = logging.INFO\n    logging.basicConfig(level=level)\n    def log_callback(message):\n        log.info('Callback invoked: %s' % message)\n    host = args.host\n    port = int(args.port)\n    log.info('Connecting to Anthem AVR at %s:%i' % (host, port))\n    conn = yield from anthemav.Connection.create(\n        host=host, port=port, loop=loop, update_callback=log_callback)\n    log.info('Power state is '+str(conn.protocol.power))\n    conn.protocol.power = True\n    log.info('Power state is '+str(conn.protocol.power))\n    yield from asyncio.sleep(10, loop=loop)\n    log.info('Panel brightness (raw) is '+str(conn.protocol.panel_brightness))\n    log.info('Panel brightness (text) is '+str(conn.protocol.panel_brightness_text))", "contrast": "import socket\ndef connect_and_show_events(host, port, verbose=False):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.connect((host, port))\n            if verbose:\n                print(\"Connection established with the receiver.\")\n            while True:\n                data = s.recv(1024)\n                if verbose:\n                    print(\"Received: \", str(data.decode()))\n                if not data:\n                    break\n    except socket.error as err:\n        print(\"Error: \", err)", "label": 0}
{"index": "gp065584", "code": "def rotate(self, angle, direction='z', axis=None):\r\n        return Space(self).rotate(angle, direction, axis)[0]", "contrast": "def rotate_place(place, angle, direction, axis=None):\n    import numpy as np\n    from pyny import Place\n    if axis is None:\n        axis = place.centroid()[:2]\n    if len(axis) == 2:\n        axis.append(0)\n    if direction == 'x':\n        axis_index = 0\n    elif direction == 'y':\n        axis_index = 1\n    else:\n        axis_index = 2\n    rotation_matrix = np.zeros((3,3))\n    rotation_matrix[(axis_index+1)%3, axis_index] = 1\n    rotation_matrix[axis_index, (axis_index+1)%3] = -1\n    rotation_matrix[(axis_index+2)%3, (axis_index+2)%3] = 1\n    angle_sin = np.sin(angle)\n    angle_cos = np.cos(angle)\n    rotation_matrix[(axis_index+1)%3, (axis_index+1)%3] = angle_cos\n    rotation_matrix[(axis_index+1)%3, (axis_index+2)%3] = -angle_sin\n    rotation_matrix[(axis_index+2)%3, (axis_index+1)%3] = angle_sin\n    rotation_matrix[(axis_index+2)%3, (axis_index+2)%3] = angle_cos\n    points = np.array(place.points)\n    points -= np.array(axis)\n    rotated_points = np.dot(rotation_matrix, points.T).T\n    rotated_points += np.array(axis)\n    return Place(rotated_points.tolist())", "label": 0}
{"index": "gp160826", "code": "def finish(self, end='\\n', dirty=False):\n        if not dirty:\n            self.end_time = datetime.now()\n            self.update(self.max_value, force=True)\n        StdRedirectMixin.finish(self, end=end)\n        ResizableMixin.finish(self)\n        ProgressBarBase.finish(self)", "contrast": "def finish_progressbar(end='\\n', dirty=False):\n    if not dirty:\n        sys.stdout.write('\\n')\n    sys.stdout.write(end)\n    sys.stdout.flush()\n    sys.stdout.buffer.flush()", "label": 0}
{"index": "gp270892", "code": "import os\ndef get_file_size(filepath, compressed=False):\n    if compressed:\n        return os.path.getsize(filepath + '.zip')\n    else:\n        return os.path.getsize(filepath)", "contrast": "def _file_size(file_path, uncompressed=False):\n    _, ext = os.path.splitext(file_path)\n    if uncompressed:\n        if ext in {\".gz\", \".gzip\"}:\n            with gzip.GzipFile(file_path, mode=\"rb\") as fp:\n                try:\n                    fp.seek(0, os.SEEK_END)\n                    return fp.tell()\n                except ValueError:\n                    fp.seek(0)\n                    while len(fp.read(8192)) != 0:\n                        pass\n                    return fp.tell()\n        elif ext in {\".bz\", \".bz2\", \".bzip\", \".bzip2\"}:\n            with bz2.BZ2File(file_path, mode=\"rb\") as fp:\n                fp.seek(0, os.SEEK_END)\n                return fp.tell()\n    return os.path.getsize(file_path)", "label": 1}
{"index": "gp289675", "code": "import random\ndef random_step_self_loop():\n    return random.choice([0, 1])", "contrast": "def pagerank_lazy_push(s, r, w_i, a_i, push_node, rho, lazy):\n    A = rho*r[push_node]\n    B = (1-rho)*(1 - lazy)*r[push_node]\n    C = (1-rho)*lazy*(r[push_node])\n    s[push_node] += A\n    r[push_node] = C\n    r[a_i] += B * w_i", "label": 1}
{"index": "gp316967", "code": "class Population:\n    def __init__(self):\n        self.individuals = []\n    def add_individual(self, individual):\n        self.individuals.append(individual)\n    def get_individuals(self):\n        return self.individuals", "contrast": "def population(self):\n        try:\n            return self._p\n        except AttributeError:\n            self._p = self._population_class(base=self,\n                                             tournament_size=self._tournament_size,\n                                             classifier=self.classifier,\n                                             labels=self._labels,\n                                             es_extra_test=self.es_extra_test,\n                                             popsize=self._popsize,\n                                             random_generations=self._random_generations,\n                                             negative_selection=self._negative_selection)\n            return self._p", "label": 1}
{"index": "gp164474", "code": "def to_sky(self, wcs, origin=_DEFAULT_WCS_ORIGIN, mode=_DEFAULT_WCS_MODE):\n        return SkyCoord.from_pixel(\n            xp=self.x, yp=self.y, wcs=wcs,\n            origin=origin, mode=mode,\n        )", "contrast": "from astropy.coordinates import SkyCoord\ndef pixcoord_to_skycoord(pixcoord, wcs):\n    return SkyCoord.from_pixel(pixcoord.x, pixcoord.y, wcs)", "label": 0}
{"index": "gp163296", "code": "def hash_data(salt, value):\n        msg = \"UserIdHasher is deprecated; use satosa.util.hash_data instead.\"\n        _warnings.warn(msg, DeprecationWarning)\n        return util.hash_data(salt, value)", "contrast": "import hashlib\ndef hash_with_salt(salt: str, value: str) -> str:\n    salted_value = salt + value\n    hash_obj = hashlib.sha512()\n    hash_obj.update(salted_value.encode())\n    return hash_obj.hexdigest()", "label": 0}
{"index": "gp035383", "code": "def network_interface_get_effective_route_table(name, resource_group, **kwargs):\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        nic = netconn.network_interfaces.get_effective_route_table(\n            network_interface_name=name,\n            resource_group_name=resource_group\n        )\n        nic.wait()\n        tables = nic.result()\n        tables = tables.as_dict()\n        result = tables['value']\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n    return result", "contrast": "def get_route_tables_for_nic(name, resource_group):\n    result = {}\n    return result", "label": 0}
{"index": "gp174048", "code": "from typing import List\nfrom project import Task\ndef profile_task_status(task: Task, status: str) -> List[str]:\n    return [s for s in task.status if s == status]", "contrast": "def Sample(self, task, status):\n    sample_time = time.time()\n    sample = '{0:f}\\t{1:s}\\t{2:s}\\n'.format(\n        sample_time, task.identifier, status)\n    self._WritesString(sample)", "label": 1}
{"index": "gp056847", "code": "def cloud_cover_to_irradiance_liujordan(self, cloud_cover, **kwargs):\n        solar_position = self.location.get_solarposition(cloud_cover.index)\n        dni_extra = get_extra_radiation(cloud_cover.index)\n        airmass = self.location.get_airmass(cloud_cover.index)\n        transmittance = self.cloud_cover_to_transmittance_linear(cloud_cover,\n                                                                 **kwargs)\n        irrads = liujordan(solar_position['apparent_zenith'],\n                           transmittance, airmass['airmass_absolute'],\n                           dni_extra=dni_extra)\n        irrads = irrads.fillna(0)\n        return irrads", "contrast": "import pvlib\ndef estimate_irradiance(cloud_cover):\n    transmittance = pvlib.ForecastModel.cloud_cover_to_transmittance_linear(cloud_cover)\n    solar_position = pvlib.solarposition.get_solarposition(time,latitude,longitude)\n    dni_extra = pvlib.irradiance.get_extra_radiation(time)\n    airmass = pvlib.atmosphere.get_relative_airmass(solar_position['apparent_zenith'])\n    pressure = pvlib.atmosphere.alt2pres(altitude)\n    am_abs = pvlib.atmosphere.get_absolute_airmass(airmass,pressure)\n    tl = pvlib.clearsky.lookup_linke_turbidity(time,latitude,longitude)\n    cs = pvlib.clearsky.ineichen(solar_position['apparent_zenith'],am_abs,tl,dni_extra=dni_extra,altitude=altitude)\n    effective_angles = pvlib.irradiance.get_effective_irradiance(solar_position['zenith'],solar_position['azimuth'],aoi, aoi_limit, cs['dni'], cs['ghi'], cs['dhi'], model='haydavies')\n    poa = pvlib.irradiance.poa_irradiance(effective_angles['aoi'], effective_angles['apparent_zenith'], effective_angles['azimuth'], \n                                          dni, ghi, dhi, dni_extra=dni_extra, model='haydavies', airmass_absolute=am_abs, \n                                          pressure=pressure, dni_extra_tracker=dni_extra_tracker, surface_tilt=surface_tilt, \n                                          surface_azimuth=surface_azimuth, albedo=albedo, \n                                          model_perez='allsitescomposite1990')\n    return poa", "label": 0}
{"index": "gp185818", "code": "import numpy as np\ndef derivative(func, delta=1e-7):\n    def derivative_func(x):\n        return (func(x + delta) - func(x)) / delta\n    return np.vectorize(derivative_func)", "contrast": "def derivative_factory(name):\n    if name == 'sin':\n        def derivative(self, point):\n            return MultiplyOperator(cos(self.domain)(point))\n    elif name == 'cos':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(-sin(self.domain)(point))\n    elif name == 'tan':\n        def derivative(self, point):\n            return MultiplyOperator(1 + self(point) ** 2)\n    elif name == 'sqrt':\n        def derivative(self, point):\n            return MultiplyOperator(0.5 / self(point))\n    elif name == 'square':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(2.0 * point)\n    elif name == 'log':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(1.0 / point)\n    elif name == 'exp':\n        def derivative(self, point):\n            return MultiplyOperator(self(point))\n    elif name == 'reciprocal':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(-self(point) ** 2)\n    elif name == 'sinh':\n        def derivative(self, point):\n            point = self.domain.element(point)\n            return MultiplyOperator(cosh(self.domain)(point))\n    elif name == 'cosh':\n        def derivative(self, point):\n            return MultiplyOperator(sinh(self.domain)(point))\n    else:\n        derivative = Operator.derivative\n    return derivative", "label": 1}
{"index": "gp234481", "code": "def add_event_handler(action):\n    actions_list.append(action) ", "contrast": "def add_event_handler(self, action):\n        if action.uuid in self.actions:\n            logger.info(\"Already existing event handler: %s\", action)\n            return\n        self.actions[action.uuid] = action\n        self.nb_event_handlers += 1", "label": 1}
{"index": "gp019316", "code": "def get_longest_non_repeat_v1(string):\n    if string is None:\n        return 0, ''\n    sub_string = ''\n    dict = {}\n    max_length = 0\n    j = 0\n    for i in range(len(string)):\n        if string[i] in dict:\n            j = max(dict[string[i]], j)\n        dict[string[i]] = i + 1\n        if i - j + 1 > max_length:\n            max_length = i - j + 1\n            sub_string = string[j: i + 1]\n    return max_length, sub_string", "contrast": "def find_longest_substring(input_str):\n    if not input_str:\n        return 0, \"\"\n    max_len = 0\n    sub_str = \"\"\n    for i in range(len(input_str)):\n        seen_chars = set()\n        j = i\n        while j < len(input_str) and input_str[j] not in seen_chars:\n            seen_chars.add(input_str[j])\n            j += 1\n        if j-i > max_len:\n            max_len = j-i\n            sub_str = input_str[i:j]\n    return max_len, sub_str", "label": 0}
{"index": "gp080776", "code": "def unload(self):\n        self.is_loaded = False\n        for evt in self._event_cache:\n            self.context.unregister(\n                evt['event'], evt['callback'], evt['selector'])\n        self._event_cache = {}", "contrast": "def on_unload_view(self):\n    for handler in self.event_handlers:\n        self.unsubscribe(handler)", "label": 0}
{"index": "gp304678", "code": "import numpy as np\ndef std(dur1, dur2, dur3, n1, n2, n3):\n    durations = np.array([dur1, dur2, dur3])\n    counts = np.array([n1, n2, n3])\n    std = np.sqrt(np.sum((durations - np.average(durations, weights=counts))**2 * counts) / np.sum(counts))\n    return std", "contrast": "def exampleSignals(std=1, dur1=1, dur2=3, dur3=0.2,\r\n                          n1=0.5, n2=0.5, n3=2):\r\n    np.random.seed(123)\r\n    t = np.linspace(0, 10, 100)\r\n    f0 = _flux(t, n1, dur1, std, offs=0)\r\n    f1 = _flux(t, n2, dur2, std, offs=0)\r\n    f2 = _flux(t, n3, dur3, std, offs=0)\r\n    return t,f0,f1,f2", "label": 1}
{"index": "gp033281", "code": "def _get_tcpip_interface_info(interface):\n    base_information = _get_base_interface_info(interface)\n    if base_information['ipv4']['requestmode'] == 'static':\n        settings = _load_config(interface.name, ['IP_Address', 'Subnet_Mask', 'Gateway', 'DNS_Address'])\n        base_information['ipv4']['address'] = settings['IP_Address']\n        base_information['ipv4']['netmask'] = settings['Subnet_Mask']\n        base_information['ipv4']['gateway'] = settings['Gateway']\n        base_information['ipv4']['dns'] = [settings['DNS_Address']]\n    elif base_information['up']:\n        base_information['ipv4']['address'] = interface.sockaddrToStr(interface.addr)\n        base_information['ipv4']['netmask'] = interface.sockaddrToStr(interface.netmask)\n        base_information['ipv4']['gateway'] = '0.0.0.0'\n        base_information['ipv4']['dns'] = _get_dns_info()\n        with salt.utils.files.fopen('/proc/net/route', 'r') as route_file:\n            pattern = re.compile(r'^{interface}\\t[0]{{8}}\\t([0-9A-Z]{{8}})'.format(interface=interface.name),\n                                 re.MULTILINE)\n            match = pattern.search(route_file.read())\n            iface_gateway_hex = None if not match else match.group(1)\n        if iface_gateway_hex is not None and len(iface_gateway_hex) == 8:\n            base_information['ipv4']['gateway'] = '.'.join([str(int(iface_gateway_hex[i:i + 2], 16))\n                                                            for i in range(6, -1, -2)])\n    return base_information", "contrast": "import socket\nimport psutil\ndef interface_details(interface_name):\n    interfaces = psutil.net_if_addrs()\n    if interface_name in interfaces.keys():\n        interface = interfaces[interface_name]\n        for info in interface:\n            if info.family == socket.AF_INET:\n                ip_address = info.address\n                netmask = info.netmask\n        mac_address = interfaces[interface_name][0].address\n        return {'interface': interface_name, 'ipv4_address': ip_address, 'netmask': netmask, 'mac_address': mac_address}\n    else:\n        return 'Invalid interface provided'", "label": 0}
{"index": "gp284709", "code": "def get_sentence(offset: int) -> str:\n    sentences = [\"This is the first sentence.\",\n                 \"This is the second sentence.\",\n                 \"This is the third sentence.\",\n                 \"This is the fourth sentence.\",\n                 \"This is the fifth sentence.\"]\n    return sentences[offset]", "contrast": "def get_sentence(self, offset: int) -> BioCSentence or None:\n        for sentence in self.sentences:\n            if sentence.offset == offset:\n                return sentence\n        return None", "label": 1}
{"index": "gp279500", "code": "def put_file(self, key, file_obj):\n    try:\n        self._backend.put_file(key, file_obj)\n    finally:\n        self._cache.pop(key, None)", "contrast": "def put_file(self, key, file):\n        try:\n            return self._dstore.put_file(key, file)\n        finally:\n            self.cache.delete(key)", "label": 1}
{"index": "gp264551", "code": "def test_update_state(component_state, dependency_states):\n    for dependency in dependency_states:\n        if not dependency:\n            return False\n    return not component_state", "contrast": "def check_lifecycle(self):\n        with self._lock:\n            was_valid = self.state == StoredInstance.VALID\n            can_validate = self.state not in (\n                StoredInstance.VALIDATING,\n                StoredInstance.VALID,\n            )\n            handlers_valid = self.__safe_handlers_callback(\n                \"is_valid\", break_on_false=True\n            )\n            if was_valid and not handlers_valid:\n                self.invalidate(True)\n            elif (\n                can_validate and handlers_valid and self._ipopo_service.running\n            ):\n                self.validate(True)", "label": 1}
{"index": "gp189961", "code": "def eda_parameter_to_string(param_value, bool_is_str=False, str_quote_style='\"'):\n    if isinstance(param_value, bool):\n        if bool_is_str:\n            return \"true\" if param_value else \"false\"\n        else:\n            return int(param_value)\n    elif isinstance(param_value, str):\n        return str_quote_style + param_value + str_quote_style\n    else:\n        return str(param_value)", "contrast": "def jinja_filter_param_value_str(value, str_quote_style=\"\", bool_is_str=False):\n    if (type(value) == bool) and not bool_is_str:\n        if (value) == True:\n            return '1'\n        else:\n            return '0'\n    elif type(value) == str or ((type(value) == bool) and bool_is_str):\n        return str_quote_style + str(value) + str_quote_style\n    else:\n        return str(value)", "label": 1}
{"index": "gp281540", "code": "def create_new_transition(from_state_id, from_outcome, to_state_id, to_outcome, transition_id=None):\n    if from_state_id is None or to_state_id is None:\n        raise AttributeError(\"Invalid state id\")\n    if from_outcome is None or to_outcome is None:\n        raise AttributeError(\"Invalid outcome\")\n    new_transition_id = 1 \n    return new_transition_id", "contrast": "def create_transition(self, from_state_id, from_outcome, to_state_id, to_outcome, transition_id):\n        if from_state_id is not None:\n            if from_state_id == self.state_id:\n                from_state = self\n            else:\n                from_state = self.states[from_state_id]\n        if from_outcome is not None:\n            if from_outcome in from_state.outcomes:\n                if to_outcome is not None:\n                    if to_outcome in self.outcomes:  \n                        self.transitions[transition_id] =                            Transition(from_state_id, from_outcome, to_state_id, to_outcome, transition_id, self)\n                    else:\n                        raise AttributeError(\"to_state does not have outcome %s\", to_outcome)\n                else:  \n                    self.transitions[transition_id] =                        Transition(from_state_id, from_outcome, to_state_id, to_outcome, transition_id, self)\n            else:\n                raise AttributeError(\"from_state does not have outcome %s\", from_state)\n        else:\n            self.transitions[transition_id] =                Transition(None, None, to_state_id, to_outcome, transition_id, self)\n        self._transitions_cv.acquire()\n        self._transitions_cv.notify_all()\n        self._transitions_cv.release()\n        return transition_id", "label": 1}
{"index": "gp144512", "code": "def element_type(self):\n        if not self.is_pointer:\n            raise ValueError(\"Type {} is not a pointer\".format(self))\n        return TypeRef(ffi.lib.LLVMPY_GetElementType(self))", "contrast": "def pointed_type(tp):\n    if not isinstance(tp, type):\n        raise TypeError('tp must be a type.')\n    if not hasattr(tp, '__name__'):\n        raise ValueError('tp must have a name.')\n    if not tp.__name__.endswith('*'):\n        raise ValueError('tp must be a pointer type.')\n    return type(tp.__name__[:-1])", "label": 0}
{"index": "gp289485", "code": "import heapq\ndef dijkstra(start, edges, distances):\n    distance_dict = {node: float('inf') for node in range(len(distances))}\n    distance_dict[start] = 0\n    heap = [(0, start)]\n    while heap:\n        current_distance, current_node = heapq.heappop(heap)\n        if current_distance > distance_dict[current_node]:\n            continue\n        for neighbor, weight in edges[current_node]:\n            distance = current_distance + distances[current_node][neighbor]\n            if distance < distance_dict[neighbor]:\n                distance_dict[neighbor] = distance\n                heapq.heappush(heap, (distance, neighbor))\n    return distance_dict", "contrast": "def graph_distances(start, edges, distances):\n    adj = {x: [] for x in range(len(distances))}\n    for n1, n2 in edges:\n        adj[n1].append(n2)\n        adj[n2].append(n1)\n    to_visit = []\n    new_dist = {}\n    for n in adj[start]:\n        heapq.heappush(to_visit, (distances[start, n], n))\n    while to_visit:\n        d, next_node = heapq.heappop(to_visit)\n        if next_node not in new_dist:\n            new_dist[next_node] = d\n        for n in adj[next_node]:\n            if n not in new_dist:\n                heapq.heappush(to_visit, (d + distances[next_node, n], n))\n    return new_dist", "label": 1}
{"index": "gp264330", "code": "def get_instances_by_factory(factory_name):\n    instances = []\n    for obj in stored_objects:\n        if obj.factory == factory_name:\n            instances.append(obj)\n    return instances", "contrast": "def __get_stored_instances(self, factory_name):\n        with self.__instances_lock:\n            return [\n                stored_instance\n                for stored_instance in self.__instances.values()\n                if stored_instance.factory_name == factory_name\n            ]", "label": 1}
{"index": "gp089962", "code": "def set_plugin_directories(self, paths, except_blacklisted=True):\n        self.directory_manager.set_directories(paths, except_blacklisted)", "contrast": "import os\nclass Plugin():\n    def __init__(self):\n        self.plugin_directories = set()\n    def set_plugin_directories(self, directories, except_blacklisted=False, blacklisted=set()):\n        if hasattr(directories, '__iter__'):\n            directories = set(directories)\n        else:\n            directories = {directories}\n        working_dir = os.getcwd()\n        directories = {os.path.abspath(os.path.join(working_dir, directory)) for directory in directories}\n        if except_blacklisted:\n            directories = directories - blacklisted\n        self.plugin_directories = directories", "label": 0}
{"index": "gp087003", "code": "def make_gpg_home(appname, config_dir=None):\n    assert is_valid_appname(appname)\n    config_dir = get_config_dir( config_dir )\n    path = os.path.join( config_dir, \"gpgkeys\", appname )\n    if not os.path.exists(path):\n        os.makedirs( path, 0700 )\n    else:\n        os.chmod( path, 0700 )\n    return path", "contrast": "import os\ndef make_gpg_keyring_dir(app_name):\n    path = os.path.expanduser(f\"~/.{app_name}/gnupg\")\n    os.makedirs(path, exist_ok=True)\n    return path", "label": 0}
{"index": "gp097380", "code": "def _get_statements(self, lines, start):\n        result = []\n        statement = []\n        nocomment = [l.split(\"!\")[0] for l in lines]\n        length = 0\n        for i in range(len(nocomment)):\n            line = nocomment[i].strip()\n            linenum = start + i\n            length += len(lines[i]) + 1\n            if len(line) == 0 or line[-1] != \"&\":\n                statement.append(line)\n                result.append((linenum-len(statement)+1, \n                               \" \".join(statement), length))\n                statement = []\n                length = 0\n            else:\n                statement.append(line[:len(line)-1])\n        return result", "contrast": "def fortran_statements(lines, start):\n    statements = []\n    statement = ''\n    original_length = 0\n    line_num = start\n    continuation = False\n    for line in lines[start:]:\n        if not continuation:\n            statement = ''\n            original_length = 0\n        line_label = str(line)\n        stripped_line = line.rstrip()\n        if not stripped_line:\n            continuation = False\n            continue\n        if stripped_line.startswith(('c', 'C', '*')):\n            continuation = False\n            statements.append((line_num, line_label, original_length))\n            line_num += 1\n            continue\n        if stripped_line.endswith('&'):\n            continuation = True\n            statement += stripped_line[:-1].rstrip()\n            original_length += len(line)\n            line_num += 1\n            continue\n        continuation = False\n        statement += stripped_line\n        original_length += len(line)\n        statements.append((line_num, statement, original_length))\n        line_num += 1\n    return statements", "label": 0}
{"index": "gp308951", "code": "def read_topology(source):\n    topology = {}\n    with open(source, 'r') as f:\n        pass\n    return topology", "contrast": "def read(self, stream):\n        def read_it(stream):\n            bytes = stream.read()\n            transportIn = TMemoryBuffer(bytes)\n            protocolIn = TBinaryProtocol.TBinaryProtocol(transportIn)\n            topology = StormTopology()\n            topology.read(protocolIn)\n            return topology\n        if isinstance(stream, six.string_types):\n            with open(stream, 'rb') as f:\n                return read_it(f)\n        else:\n            return read_it(stream)", "label": 1}
{"index": "gp089338", "code": "def getAttribute(self, attr: str) -> _AttrValueType:\n        if attr == 'class':\n            if self.classList:\n                return self.classList.toString()\n            return None\n        attr_node = self.getAttributeNode(attr)\n        if attr_node is None:\n            return None\n        return attr_node.value", "contrast": "def get_attribute_as_string(node, attr):\n    if hasattr(node, attr):\n        return str(getattr(node, attr))\n    else:\n        return None", "label": 0}
{"index": "gp328507", "code": "import subprocess\ndef host_exists(host):\n    result = subprocess.run(['ping', '-c', '1', '-W', '1', host], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    if result.returncode == 0:\n        return True\n    else:\n        return False", "contrast": "def is_present(self, host=None):\n        r = self.local_renderer\n        r.env.host = host or self.genv.host_string\n        ret = r._local(\"getent hosts {host} | awk '{{ print $1 }}'\", capture=True) or ''\n        if self.verbose:\n            print('ret:', ret)\n        ret = ret.strip()\n        if self.verbose:\n            print('Host %s %s present.' % (r.env.host, 'IS' if bool(ret) else 'IS NOT'))\n        ip = ret\n        ret = bool(ret)\n        if not ret:\n            return False\n        r.env.ip = ip\n        with settings(warn_only=True):\n            ret = r._local('ping -c 1 {ip}', capture=True) or ''\n        packet_loss = re.findall(r'([0-9]+)% packet loss', ret)\n        ip_accessible = packet_loss and int(packet_loss[0]) < 100\n        if self.verbose:\n            print('IP %s accessible: %s' % (ip, ip_accessible))\n        return bool(ip_accessible)", "label": 1}
{"index": "gp099628", "code": "def location_filter(files_with_tags, location, radius):\r\n    on_location = dict()\r\n    for f, tags in files_with_tags.items():\r\n        if 'GPS GPSLatitude' in tags:\r\n            try:\r\n                lat = convert_to_decimal(str(tags['GPS GPSLatitude']))\r\n                long = convert_to_decimal(str(tags['GPS GPSLongitude']))\r\n            except ValueError:\r\n                print('{0} has invalid gps info'.format(f))\r\n            try:\r\n                if haversine(lat, long, location['lat'], location['long']) < radius:\r\n                    on_location[f] = tags\r\n            except InvalidCoordinate:\r\n                print('{0} has invalid gps info'.format(f))\r\n    return on_location", "contrast": "from typing import List, Tuple\ndef get_photos_within_radius(photos: List[Tuple[float, float]], center: Tuple[float, float], radius: float) -> List[Tuple[float, float]]:\n    result = []\n    for photo in photos:\n        distance = ((center[0] - photo[0])**2 + (center[1] - photo[1])**2)**0.5\n        if distance <= radius:\n            result.append(photo)\n    return result", "label": 0}
{"index": "gp292890", "code": "def get_correspondance(degenerate_list: list, nondegenerate_list: list) -> dict:\n    return dict(zip(degenerate_list, nondegenerate_list))", "contrast": "def calculate_iI_correspondence(omega):\n    Ne = len(omega[0])\n    om = omega[0][0]\n    correspondence = []\n    I = 0\n    for i in range(Ne):\n        if omega[i][0] != om:\n            om = omega[i][0]\n            I += 1\n        correspondence += [(i+1, I+1)]\n    Nnd = I+1\n    def I_nd(i):\n        return correspondence[i-1][1]\n    def i_d(I):\n        for i in range(Ne):\n            if correspondence[i][1] == I:\n                return correspondence[i][0]\n    return i_d, I_nd, Nnd", "label": 1}
{"index": "gp308794", "code": "def notUnique(iterable, reportMax=None):\n    from collections import Counter\n    c = Counter(iterable)\n    for elem, count in c.most_common(reportMax):\n        if count == 1:\n            break\n        yield elem", "contrast": "def notUnique(iterable, reportMax=INF):\n    hash = {}\n    n=0\n    if reportMax < 1:\n        raise ValueError(\"`reportMax` must be >= 1 and is %r\" % reportMax)\n    for item in iterable:\n        count = hash[item] = hash.get(item, 0) + 1\n        if count > 1:\n            yield item\n            n += 1\n            if n >= reportMax:\n                return", "label": 1}
{"index": "gp009078", "code": "def listen(self):\n        count = 0\n        self._start()\n        while True:\n            result = self._socket.recv_pyobj()\n            if isinstance(result, tuple):\n                request, data = result\n            else:\n                request = result\n                data = None\n            if request == self.SPACE:\n                if self.queue.qsize() + count < self.queue_maxsize:\n                    self._socket.send_string(self.SPACE_AVAILABLE)\n                    count += 1\n                else:\n                    self._socket.send_string(self.SPACE_NOT_AVAILABLE)\n            elif request == self.PING:\n                self._socket.send_string(self.PONG)\n            elif request == self.DATA:\n                self._socket.send_string(self.STORING)\n                self.queue.put(data)\n                count -= 1\n            elif request == self.DONE:\n                self._socket.send_string(ZMQServer.CLOSED)\n                self.queue.put(('DONE', [], {}))\n                self._close()\n                break\n            else:\n                raise RuntimeError('I did not understand your request %s' % request)", "contrast": "def handle_client_request(request_type, data=None):\n    if request_type == 1:\n        pass\n    elif request_type == 2:\n        pass\n    elif request_type == 3:\n        if handle_client_request(1):\n            pass\n        else:\n            return False\n    elif request_type == 4:\n        pass\n    else:\n        raise ValueError(\"Invalid request type\") ", "label": 0}
{"index": "gp295178", "code": "def convert_to_string(commands):\n    return \"\\n\".join(commands)", "contrast": "def parse_setup(options: Union[List, str]) -> str:\n    if isinstance(options, str):\n        return options\n    return \"\\n\".join(options)", "label": 1}
{"index": "gp160681", "code": "def clear_calendars(self):\n        self._calendars.clear()\n        self._calendar_factories.clear()\n        self._aliases.clear()", "contrast": "def deregister_all_calendars():\n    for cal in calendar.calendar_list:\n        calendar.deregister_calendar(cal)", "label": 0}
{"index": "gp239752", "code": "def list_registered_hooks():\n    hook_order = []\n    for hook_name in sorted(hook_registry.keys()):\n        hook = hook_registry[hook_name]\n        for registered_hook in hook:\n            if registered_hook not in hook_order:\n                hook_order.append(registered_hook)\n    return hook_order", "contrast": "def hooks(ctx):\n    from ..hooks.run_hooks_hook import RunHooksHook\n    bundles = _get_bundles(ctx.obj.data['env'])\n    hooks = RunHooksHook(None).collect_from_bundles(bundles)\n    print_table(('Hook Name',\n                 'Default Bundle Module',\n                 'Bundle Module Override Attr',\n                 'Description'),\n                [(hook.name,\n                 hook.bundle_module_name or '(None)',\n                 hook.bundle_override_module_name_attr or '(None)',\n                 format_docstring(hook.__doc__) or '(None)') for hook in hooks])", "label": 1}
{"index": "gp170174", "code": "def run_command(session, check_perm=True, dry=False):\n    if check_perm and not session.has_permission():\n        raise PermissionError(\"User doesn't have permission to run this command\")\n    if dry:\n        if session.check_prerequisites():\n            return True\n        else:\n            return False\n    else:\n        session.run()\n        return True", "contrast": "async def run(self, session, *,\n                  check_perm: bool = True,\n                  dry: bool = False) -> bool:\n        has_perm = await self._check_perm(session) if check_perm else True\n        if self.func and has_perm:\n            if dry:\n                return True\n            if session.current_arg_filters is not None and                    session.current_key is not None:\n                arg = session.current_arg\n                config = session.bot.config\n                for f in session.current_arg_filters:\n                    try:\n                        res = f(arg)\n                        if isinstance(res, Awaitable):\n                            res = await res\n                        arg = res\n                    except ValidateError as e:\n                        if config.MAX_VALIDATION_FAILURES > 0:\n                            session.state['__validation_failure_num'] =                                session.state.get(\n                                    '__validation_failure_num', 0) + 1\n                            if session.state['__validation_failure_num'] >=                                    config.MAX_VALIDATION_FAILURES:\n                                session.finish(render_expression(\n                                    config.TOO_MANY_VALIDATION_FAILURES_EXPRESSION\n                                ), **session._current_send_kwargs)\n                        failure_message = e.message\n                        if failure_message is None:\n                            failure_message = render_expression(\n                                config.DEFAULT_VALIDATION_FAILURE_EXPRESSION\n                            )\n                        session.pause(failure_message,\n                                      **session._current_send_kwargs)\n                session.state[session.current_key] = arg\n            else:\n                if self.args_parser_func:\n                    await self.args_parser_func(session)\n                if session.current_key is not None and                        session.current_key not in session.state:\n                    session.state[session.current_key] = session.current_arg\n            await self.func(session)\n            return True\n        return False", "label": 1}
{"index": "gp128847", "code": "def get_facts_by_name_and_value(api_url=None, fact_name=None, fact_value=None, verify=False, cert=list()):\n    return utils._make_api_request(api_url, '/facts/{0}/{1}'.format(fact_name, fact_value), verify, cert)", "contrast": "import requests\ndef get_facts_by_name_and_value(api_url, fact_name, fact_value):\n    url = api_url + \"/facts/\" + fact_name + \"/\" + fact_value\n    response = requests.get(url)\n    return response.json()", "label": 0}
{"index": "gp082022", "code": "def _re_flatten(p):\n    if '(' not in p: return p\n    return re.sub(r'(\\\\*)(\\(\\?P<[^>]*>|\\((?!\\?))',\n        lambda m: m.group(0) if len(m.group(1)) % 2 else m.group(1) + '(?:', p)", "contrast": "import re\ndef turn_capturing_groups_to_noncapturing(pattern):\n    return re.sub(r\"(\\()(\\?P<\\w+>)?\", r\"\\1?:\", pattern)", "label": 0}
{"index": "gp304827", "code": "import cv2\nimport numpy as np\ndef polar_to_cartesian(polar_array):\n    height, width, _ = polar_array.shape\n    center_x = width/2\n    center_y = height/2\n    max_radius = min(height, width)/2\n    x_map = np.zeros((height, width), dtype=np.float32)\n    y_map = np.zeros((height, width), dtype=np.float32)\n    for x in range(width):\n        for y in range(height):\n            radius, angle = polar_array[y, x]\n            angle_rad = angle*np.pi/180\n            x_cart = center_x + radius*np.sin(angle_rad)\n            y_cart = center_y + radius*np.cos(angle_rad)\n            x_map[y, x] = x_cart/width\n            y_map[y, x] = y_cart/height\n    cartesian_array = np.zeros((height, width, 2), dtype=np.float32)\n    cartesian_array[:, :, 0] = x_map\n    cartesian_array[:, :, 1] = y_map\n    return cv2.remap(polar_array, cartesian_array, None, cv2.INTER_LINEAR)", "contrast": "def polarToLinear(img, shape=None, center=None, maps=None,\r\n                  interpolation=cv2.INTER_AREA,\r\n                  borderValue=0, borderMode=cv2.BORDER_REFLECT, **opts):\r\n    if maps is None:\r\n        mapY, mapX = polarToLinearMaps(img.shape[:2], shape, center)\r\n    else:\r\n        mapY, mapX = maps\r\n    o = {'interpolation': interpolation,\r\n         'borderValue': borderValue,\r\n         'borderMode': borderMode}\r\n    o.update(opts)\r\n    return cv2.remap(img, mapY, mapX, **o)", "label": 1}
{"index": "gp009740", "code": "def db_stats(self):\n        data = dict(action='db-stats')\n        jsondata = self._api_request(params=data)\n        stats = DBStats(total_clicks=int(jsondata['db-stats']['total_clicks']),\n                        total_links=int(jsondata['db-stats']['total_links']))\n        return stats", "contrast": "import requests\ndef get_db_stats():\n    url = 'https://example.com/db_stats'\n    response = requests.get(url)\n    response.raise_for_status()\n    db_stats = response.json()\n    return db_stats", "label": 0}
{"index": "gp002721", "code": "def topological_nodes(self):\n        return nx.lexicographical_topological_sort(self._multi_graph,\n                                                   key=lambda x: str(x.qargs))", "contrast": "def topological_sort(graph):\n    incoming_edges = {v: 0 for v in graph}\n    for v in graph:\n        for w in graph[v]:\n            incoming_edges[w] += 1\n    queue = [v for v in incoming_edges if incoming_edges[v] == 0]\n    while queue:\n        v = queue.pop(0)\n        yield v\n        for w in graph[v]:\n            incoming_edges[w] -= 1\n            if incoming_edges[w] == 0:\n                queue.append(w)", "label": 0}
{"index": "gp198606", "code": "def start_serialized_action(logger, task_id):\n    return eliot.start_action(logger=logger, action_type='serialized-action', task_id=task_id)", "contrast": "def continue_task(cls, logger=None, task_id=_TASK_ID_NOT_SUPPLIED):\n        if task_id is _TASK_ID_NOT_SUPPLIED:\n            raise RuntimeError(\"You must supply a task_id keyword argument.\")\n        if isinstance(task_id, bytes):\n            task_id = task_id.decode(\"ascii\")\n        uuid, task_level = task_id.split(\"@\")\n        action = cls(\n            logger, uuid, TaskLevel.fromString(task_level),\n            \"eliot:remote_task\")\n        action._start({})\n        return action", "label": 1}
{"index": "gp137022", "code": "def process_check(pool,model,field,version):\n  try: syncable=model[field]\n  except pg.FieldError: return ['?field']\n  if not isinstance(syncable,syncschema.Syncable): return ['here',None,syncable]\n  if syncable.version()>version: \n    return ['here',syncable.version(),syncable.generate()] \n  elif syncable.version()==version: return ['ok',version]\n  elif syncable.version()<version: return ['upload',syncable.version()]\n  else: raise RuntimeError(\"shouldn't get here\")", "contrast": "def do_check_helper(version):\n    if version is None:\n        return None\n    else:\n        return \"Helper for do_check, version {0}\".format(version)", "label": 0}
{"index": "gp188869", "code": "import requests\nimport time\nRATE_LIMIT_DELAY = 5  \ndef call_zendesk_api(http_method, url, **kwargs):\n    response = requests.request(http_method, url, **kwargs)\n    if response.status_code == 429:  \n        retry_time = int(response.headers.get('Retry-After', RATE_LIMIT_DELAY))\n        time.sleep(retry_time)\n        response = requests.request(http_method, url, **kwargs)\n    response.raise_for_status()\n    try:\n        return response.json()\n    except ValueError:\n        raise ValueError(\"Invalid JSON response from Zendesk API\")", "contrast": "def _call_api(self, http_method, url, **kwargs):\n        log.debug(\"{}: {} - {}\".format(http_method.__name__.upper(), url, kwargs))\n        if self.ratelimit is not None:\n            response = self._ratelimit(http_method=http_method, url=url, **kwargs)\n        else:\n            response = http_method(url, **kwargs)\n        if response.status_code == 429:\n            while 'retry-after' in response.headers and int(response.headers['retry-after']) > 0:\n                retry_after_seconds = int(response.headers['retry-after'])\n                log.warning(\n                    \"Waiting for requested retry-after period: %s seconds\" % retry_after_seconds\n                )\n                while retry_after_seconds > 0:\n                    retry_after_seconds -= 1\n                    self.check_ratelimit_budget(1)\n                    log.debug(\"    -> sleeping: %s more seconds\" % retry_after_seconds)\n                    sleep(1)\n                response = http_method(url, **kwargs)\n        self._check_response(response)\n        self._update_callsafety(response)\n        return response", "label": 1}
{"index": "gp079280", "code": "def validate(self, value):\n        try:\n            self._choice = IPAddress(value)\n        except (ValueError, AddrFormatError):\n            self.error_message = '%s is not a valid IP address.' % value\n            return False\n        if self._choice.is_netmask():\n            return True\n        else:\n            self.error_message = '%s is not a valid IP netmask.' % value\n            return False", "contrast": "def is_valid_netmask(netmask):\n    octets = netmask.split('.')\n    if len(octets) != 4:\n        return False\n    for octet in octets:\n        try:\n            octet = int(octet)\n            if octet < 0 or octet > 255:\n                return False\n        except ValueError:\n            return False\n    for i in range(1, 4):\n        if octets[i] < octets[i-1]:\n            return False\n    return True", "label": 0}
{"index": "gp184460", "code": "def set_path(dicts, keys, v):\n    if len(keys) == 1:\n        if keys[0] in dicts:\n            if isinstance(dicts[keys[0]], list):\n                dicts[keys[0]].append(v)\n            else:\n                dicts[keys[0]] = [dicts[keys[0]], v]\n        else:\n            dicts[keys[0]] = [v]\n    else:\n        if keys[0] not in dicts:\n            dicts[keys[0]] = dict()\n        set_path(dicts[keys[0]], keys[1:], v)", "contrast": "def set_path(dicts, keys, v):\n    for key in keys[:-1]:\n        dicts = dicts.setdefault(key, dict())\n    dicts = dicts.setdefault(keys[-1], list())\n    dicts.append(v)", "label": 1}
{"index": "gp160771", "code": "def head_request(\n            self,\n            alias,\n            uri,\n            headers=None,\n            allow_redirects=None,\n            timeout=None):\n        session = self._cache.switch(alias)\n        redir = False if allow_redirects is None else allow_redirects\n        response = self._head_request(session, uri, headers, redir, timeout)\n        logger.info('Head Request using : alias=%s, uri=%s, headers=%s, \\\n        allow_redirects=%s ' % (alias, uri, headers, redir))\n        return response", "contrast": "import requests\ndef send_head_request(alias, uri, allow_redirects=False, headers=None):\n    session = requests.Session()\n    session_object = session.cache.get(alias)\n    if session_object:\n        response = session_object.head(uri, allow_redirects=allow_redirects, headers=headers)\n        return response\n    else:\n        print(f\"No session object found with alias '{alias}' in the cache.\")", "label": 0}
{"index": "gp005714", "code": "def record_consumption_rate(self, amt, time_at_consumption):\n        if self._last_time is None:\n            self._last_time = time_at_consumption\n            self._current_rate = 0.0\n            return\n        self._current_rate = self._calculate_exponential_moving_average_rate(\n            amt, time_at_consumption)\n        self._last_time = time_at_consumption", "contrast": "def record_consumption_rate(amt: int, time_at_consumption: float):\n    consumption_rate = amt / time_at_consumption\n    return consumption_rate", "label": 0}
{"index": "gp274913", "code": "def get_mandate(payment):\n    return mandate", "contrast": "def mandate(self):\n        return self.client.customer_mandates.with_parent_id(self.customer_id).get(self.mandate_id)", "label": 1}
{"index": "gp326968", "code": "import datetime\nimport matplotlib.pyplot as plt\nimport gtfspy\ndef plot_dates(G: gtfspy.GTFS, ax=None, highlight_dates=[], highlight_date_labels=[], show=False):\n    date_ranges = []\n    for service in G.services:\n        date_ranges.append([service.start_date, service.end_date])\n    date_ranges.sort()\n    current_date = datetime.datetime.strptime(date_ranges[0][0], \"%Y%m%d\")\n    end_date = datetime.datetime.strptime(date_ranges[-1][1], \"%Y%m%d\")\n    dates = []\n    while current_date <= end_date:\n        dates.append(current_date)\n        current_date += datetime.timedelta(days=1)\n    x = []\n    y = []\n    for date in dates:\n        count = 0\n        for service in G.services:\n            if date.strftime(\"%Y%m%d\") in service.active_dates:\n                count += 1\n        x.append(date)\n        y.append(count)\n    ax = ax or plt.axes()\n    ax.plot(x, y)\n    if highlight_dates:\n        for date, label in zip(highlight_dates, highlight_date_labels):\n            if isinstance(date, str):\n                date = datetime.datetime.strptime(date, \"%Y%m%d\")\n            ax.axvline(x=date, c='r', ls='--', lw=1)\n            ax.text(date, ax.get_ylim()[1], label, fontsize=8, ha='center', va='top', rotation=45)\n    if show:\n        plt.show()\n    return ax", "contrast": "def plot_trip_counts_per_day(G, ax=None, highlight_dates=None, highlight_date_labels=None, show=False):\n    daily_trip_counts = G.get_trip_counts_per_day()\n    if ax is None:\n        _fig, ax = plt.subplots()\n    daily_trip_counts[\"datetime\"] = pandas.to_datetime(daily_trip_counts[\"date_str\"])\n    daily_trip_counts.plot(\"datetime\", \"trip_counts\", kind=\"line\", ax=ax, marker=\"o\", color=\"C0\", ls=\":\",\n                           label=\"Trip counts\")\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"Trip counts per day\")\n    if highlight_dates is not None:\n        assert isinstance(highlight_dates, list)\n        if highlight_date_labels is not None:\n            assert isinstance(highlight_date_labels, list)\n            assert len(highlight_dates) == len(highlight_date_labels), \"Number of highlight date labels do not match\"\n        else:\n            highlight_date_labels = [None] * len(highlight_dates)\n        for i, (highlight_date, label) in enumerate(zip(highlight_dates, highlight_date_labels)):\n            color = \"C\" + str(int(i % 8 + 1))\n            highlight_date = pandas.to_datetime(highlight_date)\n            ax.axvline(highlight_date, color=color, label=label)\n    ax.legend(loc=\"best\")\n    ax.grid()\n    if show:\n        plt.show()\n    return ax", "label": 1}
{"index": "gp327638", "code": "def compute_angular_mass_within_ellipse(major_axis, units_luminosity, exposure_time):\n    return profiles.mass_profiles.angualr_mass_within_ellipse(major_axis, units_luminosity, exposure_time)", "contrast": "def mass_within_ellipse_in_units(self, major_axis, unit_mass='angular', kpc_per_arcsec=None, critical_surface_density=None):\n        if self.has_mass_profile:\n            return sum(map(lambda p: p.mass_within_ellipse_in_units(major_axis=major_axis, unit_mass=unit_mass,\n                                                                    kpc_per_arcsec=kpc_per_arcsec,\n                                                                    critical_surface_density=critical_surface_density),\n                           self.mass_profiles))\n        else:\n            return None", "label": 1}
{"index": "gp121517", "code": "def _send(self, message):\n        result = self._talk.put(message)\n        if not result:\n            self._logger.error('Failed to send \"%s\"' % message)\n        return result", "contrast": "def return_response_message():\n    return \"Response message to client\"", "label": 0}
{"index": "gp153259", "code": "def get_init(self, filename=\"__init__.py\"):\n        import ast\n        with open(filename) as init_file:\n            module = ast.parse(init_file.read())\n        itr = lambda x: (ast.literal_eval(node.value) for node in ast.walk(module)            if isinstance(node, ast.Assign) and node.targets[0].id == x)\n        try:\n            return next(itr(\"__author__\")),                   next(itr(\"__email__\")),                   next(itr(\"__license__\")),                   next(itr(\"__version__\"))\n        except StopIteration:\n            raise ValueError(\"One of author, email, license, or version\"\n                        \" cannot be found in {}\".format(filename))", "contrast": "def get_package_info(package_name):\n    try:\n        import pkg_resources\n        package_info = pkg_resources.get_distribution(package_name)\n        return {'version': package_info.version, \n                'author': package_info.author, \n                'license': package_info.license,\n                'description': package_info.get_metadata('DESCRIPTION'),\n                'keywords': package_info.keywords,\n                'classifiers': package_info.classifiers,\n                'provides': package_info.provides\n                }\n    except Exception as e:\n        print(str(e))\n        return {}", "label": 0}
{"index": "gp291103", "code": "def get_file_contents_as_memoryview(file_path):\n    with open(file_path, 'rb') as f:\n        contents = memoryview(f.read())\n    return contents", "contrast": "def get_file(hash):\n    stmt = _get_sql('get-file.sql')\n    args = dict(hash=hash)\n    with db_connect() as db_conn:\n        with db_conn.cursor() as cursor:\n            cursor.execute(stmt, args)\n            try:\n                file, _ = cursor.fetchone()\n            except TypeError:\n                raise FileNotFound(hash)\n    return memoryview(file[:])", "label": 1}
{"index": "gp085362", "code": "def update_ips(self):\n        self.ips = self._cloud_provider.get_ips(self.instance_id)\n        return self.ips[:]", "contrast": "import time\ndef get_instance_ips():\n    public_ip = None\n    private_ip = None\n    timeout = 60 \n    cloud_provider_response = call_cloud_provider_api()\n    if cloud_provider_response:\n        if \"public_ip\" in cloud_provider_response:\n            public_ip = cloud_provider_response[\"public_ip\"]\n        if \"private_ip\" in cloud_provider_response:\n            private_ip = cloud_provider_response[\"private_ip\"]\n        while not public_ip and timeout > 0:\n            time.sleep(5) \n            cloud_provider_response = call_cloud_provider_api()\n            if \"public_ip\" in cloud_provider_response:\n                public_ip = cloud_provider_response[\"public_ip\"]\n            timeout -= 5\n    return public_ip, private_ip\ndef call_cloud_provider_api():\n    return {\"public_ip\": \"1.2.3.4\", \"private_ip\": \"192.168.1.1\"}", "label": 0}
{"index": "gp195999", "code": "import random\ndef predict_next_action(state_key, next_action_list):\n    max_q_value = float(\"-inf\")\n    max_action = None\n    for action in next_action_list:\n        if q_table[state_key][action] > max_q_value:\n            max_q_value = q_table[state_key][action]\n            max_action = action\n    max_actions = [action for action in next_action_list if q_table[state_key][action] == max_q_value]\n    next_action = random.choice(max_actions)\n    return next_action", "contrast": "def predict_next_action(self, state_key, next_action_list):\n        if self.q_df is not None:\n            next_action_q_df = self.q_df[self.q_df.state_key == state_key]\n            next_action_q_df = next_action_q_df[next_action_q_df.action_key.isin(next_action_list)]\n            if next_action_q_df.shape[0] == 0:\n                return random.choice(next_action_list)\n            else:\n                if next_action_q_df.shape[0] == 1:\n                    max_q_action = next_action_q_df[\"action_key\"].values[0]\n                else:\n                    next_action_q_df = next_action_q_df.sort_values(by=[\"q_value\"], ascending=False)\n                    max_q_action = next_action_q_df.iloc[0, :][\"action_key\"]\n                return max_q_action\n        else:\n            return random.choice(next_action_list)", "label": 1}
{"index": "gp077994", "code": "def generate_pymol_session(self, pymol_executable = 'pymol', settings = {}):\n        if not self.fixed:\n            self.fix()\n        b = BatchBuilder(pymol_executable = pymol_executable)\n        for s in self.structures:\n            s.add_residues_of_interest(self.get_differing_atom_residue_ids(s.structure_name))\n        PSE_files = b.run(MultiStructureBuilder, [self.structures], settings = settings)\n        return PSE_files[0], b.PSE_scripts[0]", "contrast": "def generate_pymol_session(scaffold_structure, model_structure, design_structure):\n    pymol_session = f'''\n    # Load structures\n    load {scaffold_structure}, Scaffold\n    load {model_structure}, Model\n    load {design_structure}, Design\n    # Create the required selections\n    select scaffold, Scaffold\n    select model, Model\n    select design, Design\n    # Color the structures\n    color gray, scaffold\n    color teal, model\n    color red, design\n    # Set the representations\n    hide everything\n    show cartoon, scaffold\n    show sticks, model\n    show sticks, design and not resn hoh\n    # Align the structures\n    align model, scaffold\n    # Save the session\n    save session.pse\n    quit\n    '''\n    return pymol_session, \" \".join(pymol_session.split())", "label": 0}
{"index": "gp061863", "code": "def cookie(\n        url,\n        name,\n        value,\n        expires=None):\n    u = urlparse(url)\n    domain = u.hostname\n    if '.' not in domain and not _is_ip_addr(domain):\n        domain += \".local\"\n    port = str(u.port) if u.port is not None else None\n    secure = u.scheme == 'https'\n    if expires is not None:\n        if expires.tzinfo is not None:\n            raise ValueError('Cookie expiration must be a naive datetime')\n        expires = (expires - datetime(1970, 1, 1)).total_seconds()\n    return http_cookiejar.Cookie(\n        version=0,\n        name=name,\n        value=value,\n        port=port,\n        port_specified=port is not None,\n        domain=domain,\n        domain_specified=True,\n        domain_initial_dot=False,\n        path=u.path,\n        path_specified=True,\n        secure=secure,\n        expires=expires,\n        discard=False,\n        comment=None,\n        comment_url=None,\n        rest=None,\n        rfc2109=False,\n    )", "contrast": "import datetime\nfrom http.cookiejar import Cookie\ndef create_cookie(name: str, value: str, url: str, expires: datetime.datetime = None) -> Cookie:\n    if expires is not None:\n        expires = int((expires - datetime.datetime(1970, 1, 1)).total_seconds())\n    cookie = Cookie(\n        version=0,\n        name=name,\n        value=value,\n        port=None,\n        port_specified=False,\n        domain='',\n        domain_specified=False,\n        domain_initial_dot=False,\n        path=url,\n        path_specified=True,\n        secure=False,\n        expires=expires,\n        discard=True,\n        comment=None,\n        comment_url=None,\n        rest=None,\n        rfc2109=False\n    )\n    return cookie", "label": 0}
{"index": "gp122003", "code": "def done(message):\n    def done(value, _context, **_params):\n        return Done(value, message)\n    return done", "contrast": "def deleted_response_builder(message):\n    return {'status': 'deleted', 'message': message}", "label": 0}
{"index": "gp231065", "code": "def set_cmake_flags(details):\n    custom_flags = details.get_custom_cmake_flags()\n    if custom_flags:\n        cmake_file_path = details.get_cmake_file_path()\n        with open(cmake_file_path, 'r') as fd:\n            content = fd.read()\n        content = content.replace('##CUSTOM_CMAKE_FLAGS##', custom_flags)\n        with open(cmake_file_path, 'w') as fd:\n            fd.write(content)", "contrast": "def processLibraryDetails(details):\n  for includeDir in details.includeDirs:\n\t\t\tfor pattern in CUSTOM_FLAGS_FOR_INCLUDE_DIRS:\n\t\t\t\tif pattern in includeDir:\n\t\t\t\t\tflag = '-D' + CUSTOM_FLAGS_FOR_INCLUDE_DIRS[pattern] + '=' + includeDir\n     details.cmakeFlags.append(flag)\n  for lib in details.libs:\n\t\t\tfilename = os.path.basename(lib)\n   (name, ext) = os.path.splitext(filename)\n   libName = name.replace('lib', '') if name.startswith('lib') else name\n   libName = libName.rstrip('_-1234567890')\n   if libName in CUSTOM_FLAGS_FOR_LIBS:\n\t\t\t\tflag = '-D' + CUSTOM_FLAGS_FOR_LIBS[libName] + '=' + lib\n    details.cmakeFlags.append(flag)", "label": 1}
{"index": "gp051683", "code": "def event_handlers(self):\n        if self.is_flow:\n            return self._event_handlers\n        try:\n            return self._event_handlers\n        except AttributeError:\n            return self.flow._event_handlers", "contrast": "def registered_handlers(self):\n    if self.handlers:\n        return self.handlers\n    elif self.flow:\n        return self.flow.handlers\n    else:\n        return []", "label": 0}
{"index": "gp117759", "code": "def get_comments_by_genus_type(self, comment_genus_type):\n        collection = JSONClientValidated('commenting',\n                                         collection='Comment',\n                                         runtime=self._runtime)\n        result = collection.find(\n            dict({'genusTypeId': str(comment_genus_type)},\n                 **self._view_filter())).sort('_id', DESCENDING)\n        return objects.CommentList(result, runtime=self._runtime, proxy=self._proxy)", "contrast": "def get_comments_by_genus_type(self, comment_genus_type):\n    if not comment_genus_type:\n        raise NullArgument('comment_genus_type is null')\n    return CommentList() ", "label": 0}
{"index": "gp105230", "code": "def _get_clstr_outfile(self):\n        if self.Parameters['-o'].isOn():\n            return ''.join([self.Parameters['-o'].Value, '.clstr'])\n        else:\n            raise ValueError, \"No output file specified\"", "contrast": "import os\ndef absolute_path_to_clstr_outfile(filepath: str) -> str:\n    return os.path.abspath(filepath)", "label": 0}
{"index": "gp173765", "code": "def create_signature_scanner(specification_store):\n    scanner = pysigscan.scanner()\n    for specification in specification_store:\n        scanner.add_signature(specification.signature, specification.offset, specification.description)\n    return scanner", "contrast": "def CreateSignatureScanner(cls, specification_store):\n    scanner_object = pysigscan.scanner()\n    for format_specification in specification_store.specifications:\n      for signature in format_specification.signatures:\n        pattern_offset = signature.offset\n        if pattern_offset is None:\n          signature_flags = pysigscan.signature_flags.NO_OFFSET\n        elif pattern_offset < 0:\n          pattern_offset *= -1\n          signature_flags = pysigscan.signature_flags.RELATIVE_FROM_END\n        else:\n          signature_flags = pysigscan.signature_flags.RELATIVE_FROM_START\n        scanner_object.add_signature(\n            signature.identifier, pattern_offset, signature.pattern,\n            signature_flags)\n    return scanner_object", "label": 1}
{"index": "gp002184", "code": "def __trim_beats(localscore, beats, trim):\n    smooth_boe = scipy.signal.convolve(localscore[beats],\n                                       scipy.signal.hann(5),\n                                       'same')\n    if trim:\n        threshold = 0.5 * ((smooth_boe**2).mean()**0.5)\n    else:\n        threshold = 0.0\n    valid = np.argwhere(smooth_boe > threshold)\n    return beats[valid.min():valid.max()]", "contrast": "def final_post_processing(data):\n    while data[0] == 'beat':\n        del data[0]\n    while data[-1] == 'beat':\n        del data[-1]\n    return data", "label": 0}
{"index": "gp310935", "code": "def monolithic_job():\n    data = download_data()\n    converted_data = convert_data(data)\n    transformed_data = transform_data(converted_data)\n    upload_data(transformed_data)", "contrast": "def download_run_and_upload(job, master_ip, inputs, spark_on_toil):\n    master_ip = MasterAddress(master_ip)\n    bam_name = inputs.sample.split('://')[-1].split('/')[-1]\n    sample_name = \".\".join(os.path.splitext(bam_name)[:-1])\n    hdfs_subdir = sample_name + \"-dir\"\n    if inputs.run_local:\n        inputs.local_dir = job.fileStore.getLocalTempDir()\n        if inputs.native_adam_path is None:\n            hdfs_dir = \"/data/\"\n        else:\n            hdfs_dir = inputs.local_dir\n    else:\n        inputs.local_dir = None\n        hdfs_dir = \"hdfs://{0}:{1}/{2}\".format(master_ip, HDFS_MASTER_PORT, hdfs_subdir)\n    try:\n        hdfs_prefix = hdfs_dir + \"/\" + sample_name\n        hdfs_bam = hdfs_dir + \"/\" + bam_name\n        hdfs_snps = hdfs_dir + \"/\" + inputs.dbsnp.split('://')[-1].split('/')[-1]\n        if not inputs.run_local:\n            download_data(job, master_ip, inputs, inputs.dbsnp, inputs.sample, hdfs_snps, hdfs_bam)\n        else:\n            copy_files([inputs.sample, inputs.dbsnp], inputs.local_dir)\n        adam_input = hdfs_prefix + \".adam\"\n        adam_snps = hdfs_dir + \"/snps.var.adam\"\n        adam_convert(job, master_ip, inputs, hdfs_bam, hdfs_snps, adam_input, adam_snps, spark_on_toil)\n        adam_output = hdfs_prefix + \".processed.bam\"\n        adam_transform(job, master_ip, inputs, adam_input, adam_snps, hdfs_dir, adam_output, spark_on_toil)\n        out_file = inputs.output_dir + \"/\" + sample_name + inputs.suffix + \".bam\"\n        if not inputs.run_local:\n            upload_data(job, master_ip, inputs, adam_output, out_file, spark_on_toil)\n        else:\n            local_adam_output = \"%s/%s.processed.bam\" % (inputs.local_dir, sample_name)\n            move_files([local_adam_output], inputs.output_dir)\n        remove_file(master_ip, hdfs_subdir, spark_on_toil)\n    except:\n        remove_file(master_ip, hdfs_subdir, spark_on_toil)\n        raise", "label": 1}
{"index": "gp226070", "code": "import logging\ndef log_batch(log_data):\n    for log in log_data:\n        timestamp = log['time']\n        message = log['message']\n        level = log['level']\n        attachment = log['attachment']\n        attachment_name = attachment['name']\n        attachment_data = attachment['data']\n        attachment_mime = attachment['mime']\n        logger = logging.getLogger()\n        logger.setLevel(level.upper())\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n        file_handler = logging.FileHandler(f'{timestamp}.log')\n        file_handler.setLevel(level.upper())\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n        if isinstance(attachment_data, str):\n            logger.log(level.upper(), message, extra={'attachment_name': attachment_name, 'attachment_mime': attachment_mime, 'attachment_content': attachment_data})\n        else:\n            logger.log(level.upper(), message, extra={'attachment_name': attachment_name, 'attachment_mime': attachment_mime}, stack_info=True)", "contrast": "def log_batch(self, log_data):\n        url = uri_join(self.base_url, \"log\")\n        attachments = []\n        for log_item in log_data:\n            log_item[\"item_id\"] = self.stack[-1]\n            attachment = log_item.get(\"attachment\", None)\n            if \"attachment\" in log_item:\n                del log_item[\"attachment\"]\n            if attachment:\n                if not isinstance(attachment, collections.Mapping):\n                    attachment = {\"data\": attachment}\n                name = attachment.get(\"name\", str(uuid.uuid4()))\n                log_item[\"file\"] = {\"name\": name}\n                attachments.append((\"file\", (\n                    name,\n                    attachment[\"data\"],\n                    attachment.get(\"mime\", \"application/octet-stream\")\n                )))\n        files = [(\n            \"json_request_part\", (\n                None,\n                json.dumps(log_data),\n                \"application/json\"\n            )\n        )]\n        files.extend(attachments)\n        from reportportal_client import POST_LOGBATCH_RETRY_COUNT\n        for i in range(POST_LOGBATCH_RETRY_COUNT):\n            try:\n                r = self.session.post(\n                    url=url,\n                    files=files,\n                    verify=self.verify_ssl\n                )\n            except KeyError:\n                if i < POST_LOGBATCH_RETRY_COUNT - 1:\n                    continue\n                else:\n                    raise\n            break\n        logger.debug(\"log_batch - Stack: %s\", self.stack)\n        logger.debug(\"log_batch response: %s\", r.text)\n        return _get_data(r)", "label": 1}
{"index": "gp303144", "code": "def create_access_token_entry(access_token):\n    return True", "contrast": "def save_token(self, access_token):\n        access_token_id = self.execute(self.create_access_token_query,\n                                       access_token.client_id,\n                                       access_token.grant_type,\n                                       access_token.token,\n                                       access_token.expires_at,\n                                       access_token.refresh_token,\n                                       access_token.refresh_expires_at,\n                                       access_token.user_id)\n        for key, value in list(access_token.data.items()):\n            self.execute(self.create_data_query, key, value,\n                         access_token_id)\n        for scope in access_token.scopes:\n            self.execute(self.create_scope_query, scope, access_token_id)\n        return True", "label": 1}
{"index": "gp306772", "code": "def calculate_good_piece_size(size):\n    if size <= 0:\n        return 0\n    elif size <= 100:\n        return 10\n    elif size <= 1000:\n        return 100\n    else:\n        return 1000", "contrast": "def calc_piece_size(size, min_piece_size=20, max_piece_size=29, max_piece_count=1000):\n    logger.debug('Calculating piece size for %i' % size)\n    for i in range(min_piece_size, max_piece_size): \n        if size / (2**i) < max_piece_count:\n            break\n    return 2**i", "label": 1}
{"index": "gp258516", "code": "import pint\ndef infer_unit_entity(unit: str) -> str:\n    ureg = pint.UnitRegistry(autoconvert_offset_to_baseunit=True, auto_reduce_dimensions=True)\n    entity = str(ureg.parse_expression(unit).dimensionality)\n    return entity.split('[')[0].strip()", "contrast": "def get_entity_from_dimensions(dimensions, text):\n    new_dimensions = [{'base': l.NAMES[i['base']].entity.name,\n                       'power': i['power']} for i in dimensions]\n    final_dimensions = sorted(new_dimensions, key=lambda x: x['base'])\n    key = l.get_key_from_dimensions(final_dimensions)\n    try:\n        if clf.USE_CLF:\n            ent = clf.disambiguate_entity(key, text)\n        else:\n            ent = l.DERIVED_ENT[key][0]\n    except IndexError:\n        logging.debug(u'\\tCould not find entity for: %s', key)\n        ent = c.Entity(name='unknown', dimensions=new_dimensions)\n    return ent", "label": 1}
{"index": "gp274926", "code": "def cancel_subscription(subscription):\n    subscription.status = 'canceled'\n    return subscription", "contrast": "def delete(self, subscription_id, data=None):\n        if not subscription_id or not subscription_id.startswith(self.RESOURCE_ID_PREFIX):\n            raise IdentifierError(\n                \"Invalid subscription ID: '{id}'. A subscription ID should start with '{prefix}'.\".format(\n                    id=subscription_id, prefix=self.RESOURCE_ID_PREFIX)\n            )\n        result = super(CustomerSubscriptions, self).delete(subscription_id, data)\n        return self.get_resource_object(result)", "label": 1}
{"index": "gp222154", "code": "def output_info(data):\n    for key, value in data.items():\n        print(key + ':', value)", "contrast": "def output_sub_generic(gandi, data, output_keys, justify=10):\n    for key in output_keys:\n        if key in data:\n            output_sub_line(gandi, key, data[key], justify)", "label": 1}
{"index": "gp023100", "code": "def _reader(self, name, stream, outbuf):\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            s = s.decode('utf-8').rstrip()\n            outbuf.append(s)\n            logger.debug('%s: %s' % (name, s))\n        stream.close()", "contrast": "def read_subprocess_stream(name, stream, outbuf):\n    for line in iter(stream.readline, b''):\n        outbuf.append(line.decode('utf-8'))", "label": 0}
{"index": "gp237074", "code": "from typing import Tuple\nfrom device_adapter import AbstractDeviceAdapter, DeviceServerError, DeviceAdapterError\ndef send_script(client_id: str, conn_string: str, script: bytes) -> Tuple[int, str]:\n    try:\n        adapter = AbstractDeviceAdapter(conn_string)\n        response = adapter.send_script(client_id, script)\n        return response\n    except DeviceServerError as e:\n        raise e\n    except DeviceAdapterError as e:\n        raise e", "contrast": "async def send_script(self, client_id, conn_string, script):\n        conn_id = self._client_connection(client_id, conn_string)\n        await self.adapter.send_script(conn_id, script)", "label": 1}
{"index": "gp177082", "code": "def map_paths(source_path, destination_path, date_for_cleaning):\n    mapped_path = destination_path + date_for_cleaning\n    return mapped_path", "contrast": "def process_IN_MOVED_TO(self, raw_event):\n        watch_ = self._watch_manager.get_watch(raw_event.wd)\n        path_ = watch_.path\n        dst_path = os.path.normpath(os.path.join(path_, raw_event.name))\n        mv_ = self._mv_cookie.get(raw_event.cookie)\n        to_append = {'cookie': raw_event.cookie}\n        if mv_ is not None:\n            self._mv[mv_[0]] = (dst_path, datetime.now())\n            to_append['src_pathname'] = mv_[0]\n        elif (raw_event.mask & IN_ISDIR and watch_.auto_add and\n              not watch_.exclude_filter(dst_path)):\n            self._watch_manager.add_watch(dst_path, watch_.mask,\n                                          proc_fun=watch_.proc_fun,\n                                          rec=True, auto_add=True,\n                                          exclude_filter=watch_.exclude_filter)\n        return self.process_default(raw_event, to_append)", "label": 1}
{"index": "gp098249", "code": "def monitor_instances(self, instance_ids):\n        params = {}\n        self.build_list_params(params, instance_ids, 'InstanceId')\n        return self.get_list('MonitorInstances', params,\n                             [('item', InstanceInfo)], verb='POST')", "contrast": "import boto.ec2\ndef enable_cloudwatch_monitoring(instance_id):\n    conn = boto.ec2.connect_to_region('us-east-1')\n    reservations = conn.get_all_instances(instance_ids=instance_id)\n    instances = [i for r in reservations for i in r.instances]\n    return conn.monitor_instances(instances)", "label": 0}
{"index": "gp321163", "code": "def print_float_array(aState, maxCols):\n    for i in range(min(maxCols, len(aState))):\n        print(\"{:.2f}\".format(aState[i]), end=\" \")\n    print()", "contrast": "def printColConfidence(self, aState, maxCols = 20):\n    def formatFPRow(var):\n      s = ''\n      for c in range(min(maxCols, self.numberOfCols)):\n        if c > 0 and c % 10 == 0:\n          s += '   '\n        s += ' %5.3f' % var[c]\n      s += ' '\n      return s\n    print formatFPRow(aState)", "label": 1}
{"index": "gp164700", "code": "def search_line(line, search, searchtype):\n    if searchtype == 're' or searchtype == 'word':\n        return re.search(search, line)  \n    elif searchtype == 'pos':\n        return searcher.search_out(line, search)\n    elif searchtype == 'hyper':\n        return searcher.hypernym_search(line, search)", "contrast": "def search_line(line, search_term):\n    if search_term in line:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp240068", "code": "def lookup_widget(pos):\n    return pos.get('widget', None)", "contrast": "def _confirm_pos(self, pos):\n        candidate = None\n        if self._get_node(self._treelist, pos) is not None:\n            candidate = pos\n        return candidate", "label": 1}
{"index": "gp070842", "code": "def popvalue(self, k, d=None):\n        if k not in self._col_dict:\n            return d\n        value = self._col_dict.pop(k)\n        self._col_list.remove(value)\n        return value", "contrast": "def popvalue(D, k, d=None):\n    try:\n        v = D.pop(k)\n    except KeyError:\n        if d is None:\n            raise\n        else:\n            v = d\n    return v", "label": 0}
{"index": "gp088199", "code": "def coroutine(func, replace_callback=True):\n    if TORNADO_MAJOR != 4:\n        wrapper = gen.coroutine(func)\n    else:\n        wrapper = gen.coroutine(func, replace_callback)\n    wrapper.__argspec_args = inspect.getargspec(func).args\n    return wrapper", "contrast": "from tornado import gen\ndef tornado_coroutine_compat(func):\n    @gen.coroutine\n    def wrapper(*args, **kwargs):\n        return (yield func(*args, **kwargs))\n    wrapper.__argspec_args = func.__code__.co_varnames[:func.__code__.co_argcount]\n    return wrapper", "label": 0}
{"index": "gp271956", "code": "def get_key_hash(key):\n    return hash(key)", "contrast": "def get_hash(key: str) -> int:\n    return int(hashlib.sha1(key.encode('utf8')).hexdigest(), 16) % 4294967295", "label": 1}
{"index": "gp175419", "code": "def find_file_handler(requirements, filename_info, file_handlers):\n    if requirements not in file_handlers:\n        raise KeyError('No handler for the given requirements is available.')\n    if file_handlers[requirements](filename_info):\n        return file_handlers[requirements]\n    else:\n        raise RuntimeError('Handler for the given requirements is available but it doesn\\'t match the filename info.')", "contrast": "def find_required_filehandlers(self, requirements, filename_info):\n        req_fh = []\n        filename_info = set(filename_info.items())\n        if requirements:\n            for requirement in requirements:\n                for fhd in self.file_handlers[requirement]:\n                    if set(fhd.filename_info.items()).issubset(filename_info):\n                        req_fh.append(fhd)\n                        break\n                else:\n                    raise RuntimeError(\"No matching requirement file of type \"\n                                       \"{}\".format(requirement))\n        return req_fh", "label": 1}
{"index": "gp257268", "code": "def label_list(label_list, value):\n    if isinstance(value, str):\n        label_list.append(value)\n    elif isinstance(value, int):\n        label_list.append(str(value))\n    else:\n        raise TypeError(\"Value must be a string or integer\")\n    return label_list", "contrast": "def p_label_list_list(p):\n    p[0] = p[1]\n    entry = check_and_make_label(p[3], p.lineno(3))\n    p[1].append(entry)", "label": 1}
{"index": "gp084716", "code": "def follow_cf(save, Uspan, target_cf, nup, n_tot=5.0, slsp=None):\n    if slsp == None:\n        slsp = Spinon(slaves=6, orbitals=3, avg_particles=n_tot,\n                       hopping=[0.5]*6, populations = np.asarray([n_tot]*6)/6)\n    zet, lam, mu, mean_f = [], [], [], []\n    for co in Uspan:\n        print('U=', co, 'del=', target_cf)\n        res=root(targetpop, nup[-1],(co,target_cf,slsp, n_tot))\n        print(res.x)\n        if res.x>nup[-1]: break\n        nup.append(res.x)\n        slsp.param['populations']=population_distri(nup[-1])\n        mean_f.append(slsp.mean_field())\n        zet.append(slsp.quasiparticle_weight())\n        lam.append(slsp.param['lambda'])\n        mu.append(orbital_energies(slsp.param, zet[-1]))\n    case = save.createGroup('cf={}'.format(target_cf))\n    varis = st.setgroup(case)\n    st.storegroup(varis, Uspan[:len(zet)], zet, lam, mu, nup[1:],target_cf,mean_f)", "contrast": "def quasiparticle_weight(N):\n    return 1 / (1 + N)", "label": 0}
{"index": "gp187794", "code": "import requests\ndef check_and_visit_login_url(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        print(\"Login URL is working fine!\")\n    else:\n        print(\"Error accessing the login URL.\")", "contrast": "def visit_loginurl(self):\n        url = self.config[\"loginurl\"]\n        if not url:\n            return\n        user, password = self.config.get_user_password(url)\n        session = requests.Session()\n        response = session.get(url)\n        cgiuser = self.config[\"loginuserfield\"]\n        cgipassword = self.config[\"loginpasswordfield\"]\n        form = formsearch.search_form(response.content, cgiuser, cgipassword,\n              encoding=response.encoding)\n        form.data[cgiuser] = user\n        form.data[cgipassword] = password\n        for key, value in self.config[\"loginextrafields\"].items():\n            form.data[key] = value\n        formurl = urlparse.urljoin(url, form.url)\n        response = session.post(formurl, data=form.data)\n        self.cookies = session.cookies\n        if len(self.cookies) == 0:\n            raise LinkCheckerError(\"No cookies set by login URL %s\" % url)", "label": 1}
{"index": "gp035944", "code": "def destination_absent(name, server=None):\n    ret = {'name': name, 'result': None, 'comment': None, 'changes': {}}\n    jms_ret = _do_element_absent(name, 'admin_object_resource', {}, server)\n    if not jms_ret['error']:\n        if __opts__['test'] and jms_ret['delete']:\n            ret['comment'] = 'JMS Queue set to be deleted'\n        elif jms_ret['delete']:\n            ret['result'] = True\n            ret['comment'] = 'JMS Queue deleted'\n        else:\n            ret['result'] = True\n            ret['comment'] = 'JMS Queue doesn\\'t exist'\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Error: {0}'.format(jms_ret['error'])\n    return ret", "contrast": "import os\ndef ensure_destination_doesnt_exist(name):\n    if os.path.exists(name):\n        raise Exception(f\"JMS Destination '{name}' already exists\")\n    else:\n        print(f\"JMS Destination '{name}' doesn't exist\")", "label": 0}
{"index": "gp211100", "code": "def add_api_call_to_cache(api_name, key, value):\n    cache = {}\n    if api_name in cache:\n        cache[api_name][key] = value\n    else:\n        cache[api_name] = {}\n        cache[api_name][key] = value\n    return cache", "contrast": "def lookup_value(self, api_name, key):\n        if api_name in self._cache:\n            return self._cache[api_name].get(key, None)\n        return None", "label": 1}
{"index": "gp313829", "code": "def load_genD(params_file_name):\n    with open(params_file_name, 'r') as f:\n        lines = f.readlines()\n    genD = []\n    for line in lines:\n        if line.startswith(\"D_\"):\n            line = line.strip().split()\n            genD.append([line[0], line[1]])\n    return genD", "contrast": "def read_igor_D_gene_parameters(params_file_name):\n    params_file = open(params_file_name, 'r')\n    D_gene_info = {}\n    in_D_gene_sec = False\n    for line in params_file:\n        if line.startswith('#GeneChoice;D_gene;'):\n            in_D_gene_sec = True\n        elif in_D_gene_sec:\n            if line[0] == '%':\n                split_line = line[1:].split(';')\n                D_gene_info[split_line[0]] = [split_line[1] , int(split_line[2])]\n            else:\n                break\n    params_file.close()\n    genD = [[]]*len(D_gene_info.keys())\n    for D_gene in D_gene_info.keys():\n        genD[D_gene_info[D_gene][1]] = [D_gene, D_gene_info[D_gene][0]]\n    return genD", "label": 1}
{"index": "gp240454", "code": "def set_canvas_format(format):\n    if format == 'png':\n        canvas_format = 'png'\n    elif format == 'jpeg':\n        canvas_format = 'jpeg'\n    else:\n        raise ValueError(\"Invalid canvas format. Must be 'png' or 'jpeg'.\")", "contrast": "def set_html5_canvas_format(self, fmt):\n        fmt = fmt.lower()\n        if fmt not in ('jpeg', 'png'):\n            raise ValueError(\"Format must be one of {jpeg|png} not '%s'\" % (\n                fmt))\n        settings = self.get_settings()\n        settings.set(html5_canvas_format=fmt)", "label": 1}
{"index": "gp004054", "code": "def classe(self, name):\n        for klass in self.classes():\n            if klass.node.name == name:\n                return klass\n        raise KeyError(name)", "contrast": "def get_class_by_name(class_name):\n    try:\n        return globals()[class_name]\n    except KeyError:\n        raise KeyError(f\"No such class found: {class_name}\")", "label": 0}
{"index": "gp083764", "code": "def hittime(cls, timestamp=None, age=None, milliseconds=None):\n        if isinstance(timestamp, (int, float)):\n            return int(Time.milliseconds_offset(Time.from_unix(timestamp, milliseconds=milliseconds)))\n        if isinstance(timestamp, datetime.datetime):\n            return int(Time.milliseconds_offset(timestamp))\n        if isinstance(age, (int, float)):\n            return int(age * 1000) + (milliseconds or 0)", "contrast": "import time\ndef get_hit_offset(hit_timestamp):\n    current_time_milliseconds = int(round(time.time() * 1000))\n    return current_time_milliseconds - hit_timestamp", "label": 0}
{"index": "gp188235", "code": "def split_attrgetter_atom_tokens(attrgetter_atom_tokens):\n    attr_or_method_name = attrgetter_atom_tokens[0]\n    if len(attrgetter_atom_tokens) > 1:\n        method_args_or_none_if_attr = tuple(attrgetter_atom_tokens[1:])\n    else:\n        method_args_or_none_if_attr = None\n    return (attr_or_method_name, method_args_or_none_if_attr)", "contrast": "def attrgetter_atom_split(tokens):\n    if len(tokens) == 1:  \n        return tokens[0], None\n    elif len(tokens) >= 2 and tokens[1] == \"(\":  \n        if len(tokens) == 2:  \n            return tokens[0], \"\"\n        elif len(tokens) == 3:  \n            return tokens[0], tokens[2]\n        else:\n            raise CoconutInternalException(\"invalid methodcaller literal tokens\", tokens)\n    else:\n        raise CoconutInternalException(\"invalid attrgetter literal tokens\", tokens)", "label": 1}
{"index": "gp212477", "code": "def get_volume_per_neurite(neurite_collection):\n    return [volume/len(neurite_collection) for volume in neurite_collection]", "contrast": "def total_volume_per_neurite(neurites, neurite_type=NeuriteType.all):\n    return list(sum(s.volume for s in n.iter_sections())\n                for n in iter_neurites(neurites, filt=is_type(neurite_type)))", "label": 1}
{"index": "gp028077", "code": "def relaxNGValidatePushElement(self, doc, elem):\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlRelaxNGValidatePushElement(self._o, doc__o, elem__o)\n        return ret", "contrast": "def push_to_relaxng_stack(start, stack):\n    stack.append(start)\n    return stack", "label": 0}
{"index": "gp016294", "code": "def load_default_config(ipython_dir=None):\n    if ipython_dir is None:\n        ipython_dir = get_ipython_dir()\n    profile_dir = os.path.join(ipython_dir, 'profile_default')\n    cl = PyFileConfigLoader(default_config_file_name, profile_dir)\n    try:\n        config = cl.load_config()\n    except ConfigFileNotFound:\n        config = Config()\n    return config", "contrast": "from IPython import get_ipython\ndef load_default_config():\n    ip = get_ipython()\n    if ip is not None:\n        ip.magic('%config IPCompleter.use_jedi=False')\n        ip.magic('%config InlineBackend.figure_format=\"retina\"')\n    else:\n        print(\"IPython shell not found. Please run this command in an IPython shell.\")", "label": 0}
{"index": "gp097827", "code": "def ne(self, other, ranks=None):\n        ranks = ranks or DEFAULT_RANKS\n        if isinstance(other, Card):\n            if ranks.get(\"suits\"):\n                return (\n                    ranks[\"values\"][self.value] !=\n                    ranks[\"values\"][other.value] or\n                    ranks[\"suits\"][self.suit] !=\n                    ranks[\"suits\"][other.suit]\n                )\n            else:\n                return ranks[self.value] != ranks[other.value]\n        else:\n            return False", "contrast": "def compare_cards(card, other, ranks):\n    return ranks[card.get_rank()] != ranks[other.get_rank()]", "label": 0}
{"index": "gp058001", "code": "def _wrap_response(self, status=None, **kwargs):\n        kwargs['status'] = status if status is not None else self._status.OK\n        return kwargs", "contrast": "def wrap_status(status='OK', **kwargs):\n    return {'status': status, **kwargs}", "label": 0}
{"index": "gp173511", "code": "def ExtractTerminalServerClientEvents(parser_mediator, registry_key):\n    event_data = {}\n    event_data['Terminal Server Client Name'] = registry_key.name\n    event_data['Product ID'] = registry_key.GetValueByName('ProductId').data\n    event_data['Company Name'] = registry_key.GetValueByName('CompanyName').data\n    event_data['File Version'] = registry_key.GetValueByName('FileVersion').data\n    event_data['Product Version'] = registry_key.GetValueByName('ProductVersion').data\n    event_data['Client Name'] = registry_key.GetValueByName('ClientName').data\n    event_data['Client Build Number'] = registry_key.GetValueByName('ClientBuildNumber').data\n    event_data['Client DLL Name'] = registry_key.GetValueByName('ClientDll').data\n    event_data['Last Successful Connection Time'] = registry_key.GetValueByName('LastSuccessfulConnectTime').data\n    event_data['Last Connection Time'] = registry_key.GetValueByName('LastConnectTime').data\n    event_data['Connection Statistics'] = registry_key.GetValueByName('ConnectionStatistics').data\n    event_data['Connection Bar Data'] = registry_key.GetValueByName('ConnectionBarData').data\n    event_data['Negotiated Protocol'] = registry_key.GetValueByName('NegotiatedProtocol').data\n    event_data['Client Address'] = registry_key.GetValueByName('ClientAddress').data\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "contrast": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    mru_values_dict = {}\n    for subkey in registry_key.GetSubkeys():\n      username_value = subkey.GetValueByName('UsernameHint')\n      if (username_value and username_value.data and\n          username_value.DataIsString()):\n        username = username_value.GetDataAsObject()\n      else:\n        username = 'N/A'\n      mru_values_dict[subkey.name] = username\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = subkey.path\n      event_data.offset = subkey.offset\n      event_data.regvalue = {'Username hint': username}\n      event_data.source_append = self._SOURCE_APPEND\n      event = time_events.DateTimeValuesEvent(\n          subkey.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = mru_values_dict\n    event_data.source_append = self._SOURCE_APPEND\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "label": 1}
{"index": "gp165075", "code": "def createDaemon():\n   try:\n      pid = os.fork()\n   except OSError, e:\n      raise Exception, \"%s [%d]\" % (e.strerror, e.errno)\n   if (pid == 0):   \n      os.setsid()\n      try:\n         pid = os.fork()    \n      except OSError, e:\n         raise Exception, \"%s [%d]\" % (e.strerror, e.errno)\n      if (pid == 0):    \n         os.chdir(WORKDIR)\n         os.umask(UMASK)\n      else:\n         os._exit(0)    \n   else:\n      os._exit(0)   \n   import resource      \n   maxfd = resource.getrlimit(resource.RLIMIT_NOFILE)[1]\n   if (maxfd == resource.RLIM_INFINITY):\n      maxfd = MAXFD\n   os.open(REDIRECT_TO, os.O_RDWR)  \n   os.dup2(0, 1)            \n   os.dup2(0, 2)            \n   return(0)", "contrast": "Here's one way to implement the function:\nimport os\nimport sys\ndef detach_process_from_terminal():\n    pid = os.fork()\n    if pid > 0:\n        sys.exit(0)\n    os.setsid()\n    pid = os.fork()\n    if pid > 0:\n        sys.exit(0)\n    os.chdir('/')\n    for fd in range(0, 3):\n        try:\n            os.close(fd)\n        except OSError:\n            pass\n    os.open('/dev/null', os.O_RDWR)  \n    os.dup2(0, 1)  \n    os.dup2(0, 2)  ", "label": 0}
{"index": "gp083064", "code": "def _invertMapping(mapping):\n    invertedMapping = ddict(set)\n    for key, values in viewitems(mapping):\n        for value in values:\n            invertedMapping[value].add(key)\n    return invertedMapping", "contrast": "def convert_mapping(mapping):\n    inverted_mapping = {}\n    for key, values in mapping.items():\n        for value in values:\n            if value not in inverted_mapping:\n                inverted_mapping[value] = set()\n            inverted_mapping[value].add(key)\n    return inverted_mapping", "label": 0}
{"index": "gp269429", "code": "from datetime import datetime\nfrom flask import request, make_response\nfrom werkzeug.exceptions import NotModified, UnprocessableEntity\ndef get_record(pid, record, read_permission_factory):\n    if not read_permission_factory.can():\n        raise UnprocessableEntity(\"Unable to read record\")\n    etag = record.get_etag()\n    last_modified = record.get_last_modified()\n    try:\n        modified_since = datetime.strptime(request.headers['If-Modified-Since'], '%a, %d %b %Y %H:%M:%S %Z')\n        if modified_since >= last_modified:\n            raise NotModified\n    except (KeyError, ValueError):\n        pass\n    if request.headers.get('If-None-Match') == etag:\n        raise NotModified\n    response = make_response(record.get_data())\n    response.headers['Content-Type'] = record.get_content_type()\n    response.headers['ETag'] = etag\n    response.headers['Last-Modified'] = last_modified.strftime('%a, %d %b %Y %H:%M:%S %Z')\n    response.headers['Link'] = '<{}>; rel=\"self\"'.format(request.url)\n    return response", "contrast": "def get(self, pid, record, **kwargs):\n        etag = str(record.revision_id)\n        self.check_etag(str(record.revision_id))\n        self.check_if_modified_since(record.updated, etag=etag)\n        return self.make_response(\n            pid, record, links_factory=self.links_factory\n        )", "label": 1}
{"index": "gp174359", "code": "async def read_packet_id(reader):\n    packet_id_bytes = await reader.readexactly(2)\n    packet_id = int.from_bytes(packet_id_bytes, byteorder='big', signed=False)\n    return packet_id", "contrast": "def decode_packet_id(reader) -> int:\n    packet_id_bytes = yield from read_or_raise(reader, 2)\n    packet_id = unpack(\"!H\", packet_id_bytes)\n    return packet_id[0]", "label": 1}
{"index": "gp257870", "code": "def find_or_generate_getter(object_type, property_name, private_property_name):\n    try:\n        getter_fn = getattr(object_type, 'get_' + property_name)\n        if getter_fn.__code__.co_argcount == 1:\n            return getter_fn\n    except AttributeError:\n        pass\n    def getter(self):\n        return getattr(self, private_property_name)\n    return getter", "contrast": "def _get_getter_fun(object_type,           \n                    parameter,             \n                    private_property_name  \n                    ):\n    property_name = parameter.name\n    overridden_getters = getmembers(object_type, predicate=_has_annotation(__GETTER_OVERRIDE_ANNOTATION, property_name))\n    if len(overridden_getters) > 0:\n        if len(overridden_getters) > 1:\n            raise DuplicateOverrideError('Getter is overridden more than once for attribute name : ' + property_name)\n        getter_fun = overridden_getters[0][1]\n        s = signature(getter_fun)\n        if not ('self' in s.parameters.keys() and len(s.parameters.keys()) == 1):\n            raise IllegalGetterSignatureException('overridden getter must only have a self parameter, found ' +\n                                                  str(len(s.parameters.items()) - 1) + ' for function ' + str(\n                getter_fun.__qualname__))\n        property_obj = property(getter_fun)\n    else:\n        def autoprops_generated_getter(self):\n            return getattr(self, private_property_name)\n        getter_fun = autoprops_generated_getter\n        try:\n            annotations = getter_fun.__annotations__\n        except AttributeError:\n            pass\n        else:\n            annotations['return'] = parameter.annotation  \n    return getter_fun", "label": 1}
{"index": "gp165316", "code": "def i2c_write_request(self, address, args):\n        task = asyncio.ensure_future(self.core.i2c_write_request(address, args))\n        self.loop.run_until_complete(task)", "contrast": "import smbus\ndef write_i2c(address, *args):\n    bus = smbus.SMBus(1)\n    bus.write_i2c_block_data(address, 0, list(args))", "label": 0}
{"index": "gp269789", "code": "def create_chunked_ending():\n    return b\"\\r\\n0\\r\\n\\r\\n\"", "contrast": "def create_chunked_body_end(trailers=None):\n    chunk = []\n    chunk.append('0\\r\\n')\n    if trailers:\n        for name, value in trailers:\n            chunk.append(name)\n            chunk.append(': ')\n            chunk.append(value)\n            chunk.append('\\r\\n')\n    chunk.append('\\r\\n')\n    return s2b(''.join(chunk))", "label": 1}
{"index": "gp103032", "code": "def _update_version(connection, version):\n    if connection.engine.name == 'sqlite':\n        connection.execute('PRAGMA user_version = {}'.format(version))\n    elif connection.engine.name == 'postgresql':\n        connection.execute(DDL('CREATE SCHEMA IF NOT EXISTS {};'.format(POSTGRES_SCHEMA_NAME)))\n        connection.execute(DDL('CREATE SCHEMA IF NOT EXISTS {};'.format(POSTGRES_PARTITION_SCHEMA_NAME)))\n        connection.execute('CREATE TABLE IF NOT EXISTS {}.user_version(version INTEGER NOT NULL);'\n                           .format(POSTGRES_SCHEMA_NAME))\n        if connection.execute('SELECT * FROM {}.user_version;'.format(POSTGRES_SCHEMA_NAME)).fetchone():\n            connection.execute('UPDATE {}.user_version SET version = {};'\n                               .format(POSTGRES_SCHEMA_NAME, version))\n        else:\n            connection.execute('INSERT INTO {}.user_version (version) VALUES ({})'\n                               .format(POSTGRES_SCHEMA_NAME, version))\n    else:\n        raise DatabaseMissingError('Do not know how to migrate {} engine.'\n                                   .format(connection.engine.driver))", "contrast": "def update_version(connection, version):\n    query = \"UPDATE migrations SET version = :version\"\n    connection.execute(query, version=version)", "label": 0}
{"index": "gp065951", "code": "def _parse_methods(cls, list_string):\n        if list_string is None:\n            return APIServer.DEFAULT_METHODS\n        json_list = list_string.replace(\"'\", '\"')\n        return json.loads(json_list)", "contrast": "import json\ndef get_http_methods():\n    http_methods = [\"GET\", \"HEAD\", \"POST\", \"PUT\", \"DELETE\", \"CONNECT\", \"OPTIONS\", \"TRACE\", \"PATCH\"]\n    return json.dumps(http_methods)", "label": 0}
{"index": "gp132554", "code": "def _randomize_speed(base_speed: int, sigma: int = None) -> int:\n        if sigma is None:\n            int_sigma = int(base_speed / 4)\n        else:\n            int_sigma = sigma\n        val = MissionWeather._gauss(base_speed, int_sigma)\n        if val < 0:\n            return 0\n        return min(val, 50)", "contrast": "import random\nfrom math import exp, sqrt\ndef create_variation(base_speed, sigma):\n    return base_speed * random.gauss(1, sigma)", "label": 0}
{"index": "gp058241", "code": "def download_feed_posts(self, max_count: int = None, fast_update: bool = False,\n                            post_filter: Optional[Callable[[Post], bool]] = None) -> None:\n        self.context.log(\"Retrieving pictures from your feed...\")\n        count = 1\n        for post in self.get_feed_posts():\n            if max_count is not None and count > max_count:\n                break\n            name = post.owner_username\n            if post_filter is not None and not post_filter(post):\n                self.context.log(\"<pic by %s skipped>\" % name, flush=True)\n                continue\n            self.context.log(\"[%3i] %s \" % (count, name), end=\"\", flush=True)\n            count += 1\n            with self.context.error_catcher('Download feed'):\n                downloaded = self.download_post(post, target=':feed')\n                if fast_update and not downloaded:\n                    break", "contrast": "from instaloader import Instaloader\ndef download_user_feed(max_count=20, fast_update=True, post_filter=lambda post: post.viewer_has_liked):\n    loader = Instaloader()\n    loader.load_session_from_file('USER')\n    loader.download_feed_posts(max_count=max_count, fast_update=fast_update, post_filter=post_filter)", "label": 0}
{"index": "gp160522", "code": "def _ModifyInterface(\n      self, interface_config, config_key, config_value, replace=False):\n    config_entry = '%s=%s' % (config_key, config_value)\n    if not open(interface_config).read().count(config_key):\n      with open(interface_config, 'a') as config:\n        config.write('%s\\n' % config_entry)\n    elif replace:\n      for line in fileinput.input(interface_config, inplace=True):\n        print(re.sub(r'%s=.*' % config_key, config_entry, line.rstrip()))", "contrast": "import configparser\ndef write_config_value(interface_config, config_key, config_value, replace):\n    parser = configparser.ConfigParser()\n    parser.read(interface_config)\n    if not parser.has_section(config_key):\n        parser.add_section(config_key)\n    if replace or not parser.has_option(config_key, config_value):\n        parser.set(config_key, config_value)\n        with open(interface_config, 'w') as config_file:\n            parser.write(config_file)", "label": 0}
{"index": "gp139798", "code": "def _in_batches_cmdidx(cmd_array):\n        in_batches_cmdidx_dict = {}\n        for cmdidx, tok in enumerate(cmd_array):\n            mat = BatchCommand.in_batches_pat.match(tok)\n            if mat:\n                batch_idx = int(mat.group(1))\n                if batch_idx in in_batches_cmdidx_dict:\n                    raise IndexError(\n                        'IN_BATCH%d is used multiple times in command below, while IN_BATCH0 - IN_BATCH%d must be used:%s$ %s' %\n                        (batch_idx, len(in_batches_cmdidx_dict) - 1, os.linesep, list2cmdline(cmd_array)))\n                in_batches_cmdidx_dict[batch_idx] = cmdidx\n        in_batches_cmdidx = []\n        for batch_idx in range(len(in_batches_cmdidx_dict)):\n            try:\n                cmdidx = in_batches_cmdidx_dict[batch_idx]\n                in_batches_cmdidx.append(cmdidx)\n            except KeyError:\n                raise IndexError('IN_BATCH%d is not found in command below, while IN_BATCH0 - IN_BATCH%d must be used:%s$ %s' %\n                                 (batch_idx, len(in_batches_cmdidx_dict) - 1, os.linesep, list2cmdline(cmd_array)))\n        return tuple(in_batches_cmdidx)", "contrast": "def get_in_batch_indices(cmd_array):\n    in_batch_count = 0\n    in_batch_indices = []\n    for i, cmd in enumerate(cmd_array):\n        if cmd.startswith(\"IN_BATCH\"):\n            expected = \"IN_BATCH{}\".format(in_batch_count)\n            if cmd != expected:\n                raise IndexError\n            in_batch_count += 1\n            in_batch_indices.append(i)\n    return tuple(in_batch_indices)", "label": 0}
{"index": "gp334916", "code": "def generate_policy(allowed_conditions, denied_conditions):\n    policy = {\n        'Version': '2012-10-17',\n        'Statement': [\n            {\n                'Effect': 'Allow',\n                'Action': '*',\n                'Resource': '*',\n                'Condition': allowed_conditions\n            },\n            {\n                'Effect': 'Deny',\n                'Action': '*',\n                'Resource': '*',\n                'Condition': denied_conditions\n            }\n        ]\n    }\n    return policy", "contrast": "def build(self):\n        if ((self.allowMethods is None or len(self.allowMethods) == 0) and\n                (self.denyMethods is None or len(self.denyMethods) == 0)):\n            raise NameError('No statements defined for the policy')\n        policy = {\n            'principalId': self.principal_id,\n            'policyDocument': {\n                'Version': self.version,\n                'Statement': []\n            }\n        }\n        policy['policyDocument']['Statement'].extend(\n            self._get_effect_statement('Allow', self.allowMethods))\n        policy['policyDocument']['Statement'].extend(\n            self._get_effect_statement('Deny', self.denyMethods))\n        return policy", "label": 1}
{"index": "gp018680", "code": "def res_block(nf, dense:bool=False, norm_type:Optional[NormType]=NormType.Batch, bottle:bool=False, **conv_kwargs):\n    norm2 = norm_type\n    if not dense and (norm_type==NormType.Batch): norm2 = NormType.BatchZero\n    nf_inner = nf \n    return SequentialEx(conv_layer(nf, nf_inner, norm_type=norm_type, **conv_kwargs),\n                      conv_layer(nf_inner, nf, norm_type=norm2, **conv_kwargs),\n                      MergeLayer(dense))", "contrast": "def resnet_block(nf, conv_layer, conv_kwargs):\n    block = []\n    block += [conv_layer(nf, **conv_kwargs), nn.ReLU()]\n    block += [conv_layer(nf, **conv_kwargs), nn.ReLU()]\n    return nn.Sequential(*block)", "label": 0}
{"index": "gp336646", "code": "import os\nimport configparser\ndef files_to_ini(directory, stringio, base=None, exclude=None, include=None):\n    config = configparser.ConfigParser()\n    for root, dirs, files in os.walk(directory):\n        for filename in files:\n            if exclude and fnmatch.fnmatch(filename, exclude):\n                continue\n            if include and not fnmatch.fnmatch(filename, include):\n                continue\n            filepath = os.path.join(root, filename)\n            hash_filename = filename\n            if base:\n                hash_filename = base.strip(\"/\") + \"/\" + hash_filename\n            config[hash_filename] = {\"hash\": hashlib.sha1(open(filepath, 'rb').read()).hexdigest()}\n    config.write(stringio)", "contrast": "def DumpDirHashToStringIO(directory, stringio, base='', exclude=None, include=None):\n    import fnmatch\n    import os\n    files = [(os.path.join(directory, i), i) for i in os.listdir(directory)]\n    files = [i for i in files if os.path.isfile(i[0])]\n    for fullname, filename in files:\n        if include is not None:\n            if not fnmatch.fnmatch(fullname, include):\n                continue\n        if exclude is not None:\n            if fnmatch.fnmatch(fullname, exclude):\n                continue\n        md5 = Md5Hex(fullname)\n        if base:\n            stringio.write('%s/%s=%s\\n' % (base, filename, md5))\n        else:\n            stringio.write('%s=%s\\n' % (filename, md5))", "label": 1}
{"index": "gp221949", "code": "import sys\nimport os\ndef set_environment():\n    os.environ['SCOOP_MAIN_MODULE'] = __file__\n    sys.argv[0] = __file__\n    sys.path.append(os.path.dirname(__file__))\n    import scoop.MAIN_MODULE", "contrast": "def setupEnvironment(self=None):\n        sys.path.append(os.path.dirname(os.path.abspath(scoop.MAIN_MODULE)))\n        sys.argv = sys.argv[:1]\n        if self:\n            sys.argv += self.args.args\n        try:\n            if scoop.IS_ORIGIN:\n                _ = open(scoop.MAIN_MODULE, 'r')\n                user_module = None\n            else:\n                user_module = importFunction(\n                    \"SCOOP_WORKER\",\n                    scoop.MAIN_MODULE,\n                )\n        except FileNotFoundError as e:\n            sys.stderr.write('{0}\\nFile: {1}\\nIn path: {2}\\n'.format(\n                    str(e),\n                    scoop.MAIN_MODULE,\n                    sys.path[-1],\n                )\n            )\n            sys.stderr.flush()\n            sys.exit(-1)\n        globs = {}\n        try:\n            attrlist = user_module.__all__\n        except AttributeError:\n            attrlist = dir(user_module)\n        for attr in attrlist:\n            globs[attr] = getattr(user_module, attr)\n        if self and scoop.IS_ORIGIN:\n            return {}\n        elif self:\n            return globs\n        return user_module", "label": 1}
{"index": "gp188014", "code": "def reset_auth():\n    if RFID.auth:\n        stop_crypto()", "contrast": "def deauth(self):\n        self.method = None\n        self.key = None\n        self.last_auth = None\n        if self.debug:\n            print(\"Changing auth key and method to None\")\n        if self.rfid.authed:\n            self.rfid.stop_crypto()\n            if self.debug:\n                print(\"Stopping crypto1\")", "label": 1}
{"index": "gp306738", "code": "import importlib\ndef get_class_from_name(class_name, data, base_class='facsimile.base.Facsimile'):\n    module_name, class_name = class_name.rsplit('.', 1)\n    module = importlib.import_module(module_name)\n    class_obj = getattr(module, class_name)(data)\n    if base_class:\n        base_module, base_class = base_class.rsplit('.', 1)\n        base_module = importlib.import_module(base_module)\n        base_class = getattr(base_module, base_class)\n        for attr in dir(base_class):\n            if not attr.startswith('__'):\n                setattr(class_obj, attr, getattr(base_class, attr))\n    return class_obj", "contrast": "def get_cls(project_name, project_data):\n    if project_name:\n        cls = getattr(facsimile.base, project_data.get('class', 'Facsimile'))\n        cls.name = project_name\n    else:\n        cls = facsimile.base.Facsimile\n    return cls", "label": 1}
{"index": "gp127771", "code": "def auto_load_model_menu(self):\n        from trionyx.trionyx.apps import BaseConfig\n        order = 0\n        for app in apps.get_app_configs():\n            if not isinstance(app, BaseConfig) or getattr(app, 'no_menu', False):\n                continue\n            app_path = app.name.split('.')[-1]\n            model_order = 0\n            for model in app.get_models():\n                config = models_config.get_config(model)\n                if config.menu_exclude:\n                    continue\n                menu_icon = None\n                menu_path = '{}/{}'.format(app_path, config.model_name)\n                if config.menu_root:\n                    order += 10\n                    menu_order = order\n                    menu_icon = config.menu_icon\n                    menu_path = config.model_name\n                else:\n                    model_order += 10\n                    menu_order = model_order\n                self.add_item(\n                    path=menu_path,\n                    name=config.menu_name if config.menu_name else model._meta.verbose_name_plural.capitalize(),\n                    order=config.menu_order if config.menu_order else menu_order,\n                    icon=menu_icon,\n                    url=reverse(\n                        \"trionyx:model-list\",\n                        kwargs={\n                            'app': model._meta.app_label,\n                            'model': model._meta.model_name,\n                        }\n                    )\n                )\n            if model_order > 0:\n                order += 10\n                self.add_item(\n                    path=app_path,\n                    name=getattr(app, 'menu_name', app.verbose_name),\n                    icon=getattr(app, 'menu_icon', None),\n                    order=getattr(app, 'menu_order', order),\n                )", "contrast": "def auto_load_models():\n    from trionyx.config import ModelConfig\n    configs = ModelConfig.get_all_configs()\n    menu_entries = []\n    for config in configs:\n        entry = dict(\n            menu_name=config.get(\"menu_name\", \"\"),\n            menu_icon=config.get(\"menu_icon\", \"\"),\n            menu_order=config.get(\"menu_order\", \"\"),\n        )\n        menu_entries.append(entry)\n    return menu_entries", "label": 0}
{"index": "gp175452", "code": "import logging\ndef get_logger(name):\n    logger = logging.getLogger(name)\n    if not logger.handlers:\n        logger.addHandler(logging.NullHandler())\n    return logger", "contrast": "def get_logger(name):\n    if not hasattr(logging.Logger, 'trace'):\n        logging.addLevelName(TRACE_LEVEL, 'TRACE')\n        def trace(self, message, *args, **kwargs):\n            if self.isEnabledFor(TRACE_LEVEL):\n                self._log(TRACE_LEVEL, message, args, **kwargs)\n        logging.Logger.trace = trace\n    log = logging.getLogger(name)\n    if not log.handlers:\n        log.addHandler(logging.NullHandler())\n    return log", "label": 1}
{"index": "gp132227", "code": "def _json_clean(d):\n    result = {}\n    compkeys = {}\n    for k, v in d.items():\n        if not isinstance(k, tuple):\n            result[k] = v\n        else:\n            key = \"c.{}\".format(id(k))\n            result[key] = v\n            compkeys[key] = k\n    return (result, compkeys)", "contrast": "def clean_dict(d):\n    cleaned_dict = {}\n    for key, value in d.items():\n        if isinstance(key, tuple):\n            cleaned_key = str(key)\n        else:\n            cleaned_key = key\n        if isinstance(value, dict):\n            cleaned_dict[cleaned_key] = clean_dict(value)\n        else:\n            cleaned_dict[cleaned_key] = value\n    return cleaned_dict", "label": 0}
{"index": "gp008756", "code": "def register_link(self, link):\n        keys = tuple((ref, link.initial_hook_value) for ref in link.hook_references)\n        for k in keys:\n            if k in self._record_hooks:\n                link.set_target(target_record=self._record_hooks[k].target_record)\n                break\n        else:\n            for k in keys:\n                if k in self._table_hooks:\n                    link.set_target(target_table=self._table_hooks[k])\n                    break\n            else:\n                field_descriptor = link.source_record.get_field_descriptor(link.source_index)\n                raise FieldValidationError(\n                    f\"No object found with any of given references : {keys}. \"\n                    f\"{field_descriptor.get_error_location_message(link.initial_hook_value)}\"\n                )\n        if link.source_record not in self._links_by_source:\n            self._links_by_source[link.source_record] = set()\n        self._links_by_source[link.source_record].add(link)\n        if link.target not in self._links_by_target:\n            self._links_by_target[link.target] = set()\n        self._links_by_target[link.target].add(link)", "contrast": "def check_record_and_index(source_record, index):\n    if source_record is not None and index is not None:\n        return True\n    else:\n        return False", "label": 0}
{"index": "gp286924", "code": "from django.core.paginator import Paginator\ndef chunked_delete(queryset, chunk_size=1000):\n    paginator = Paginator(queryset, chunk_size)\n    for page_num in paginator.page_range:\n        page = paginator.page(page_num)\n        for obj in page.object_list.iterator():\n            obj.delete()", "contrast": "def _delete_chunked(queryset, chunk_size=500):\n        while True:\n            with transaction.atomic():\n                offset = queryset.order_by('pk')[:chunk_size].count()\n                if not offset:\n                    break\n                last_instance = queryset.order_by('pk')[offset - 1]\n                queryset.filter(pk__lte=last_instance.pk).delete()", "label": 1}
{"index": "gp253257", "code": "import json\nfrom molotov import request\nasync def request_json(*args, **kwargs):\n    response = await request(*args, **kwargs)\n    return json.loads(response.text)", "contrast": "def json_request(endpoint, verb='GET', session_options=None, **options):\n    req = functools.partial(_request, endpoint, verb, session_options,\n                            json=True, **options)\n    return _run_in_fresh_loop(req)", "label": 1}
{"index": "gp252363", "code": "from typing import List, Callable\ndef chain_functions(functions: List[Callable]) -> Callable:\n    def chained_function(value):\n        for f in functions:\n            value = f(value)\n        return value\n    return chained_function", "contrast": "def chain_functions(functions):\n    functions = list(functions)\n    if not functions:\n        return _no_op\n    elif len(functions) == 1:\n        return functions[0]\n    else:\n        return partial(reduce, lambda res, f: f(res), functions)", "label": 1}
{"index": "gp190366", "code": "def get_network_envelope(N, periodic):\n    envelope = []\n    if periodic:\n        envelope = [1] * N\n    else:\n        for i in range(N):\n            if i <= N // 2:\n                envelope.append(float((i + 1) / (N // 2 + 1)))\n            else:\n                envelope.append(float((N - i) / (N // 2 + 1)))\n    return envelope", "contrast": "def create_envelope(periodic,N):\n  kappa = 0.3; \n  a0 = 30;    \n  if periodic==0:\n      A = np.zeros(N)\n      for m in range(N):\n          r = np.abs(m-N/2);\n          if r<kappa*N:\n              A[m] = 1\n          else:\n              A[m] = np.exp(-a0*((r-kappa*N)/((1-kappa)*N))**2)\n  else:\n      A = np.ones((1,N));\n  return A", "label": 1}
{"index": "gp115121", "code": "def set_distribute_compositions(self, distribute_comps):\n        if self.get_distribute_compositions_metadata().is_read_only():\n            raise errors.NoAccess()\n        if not self._is_valid_boolean(distribute_comps):\n            raise errors.InvalidArgument()\n        self._my_map['distributeCompositions'] = distribute_comps", "contrast": "def set_distribution_rights(distribute_comps):\n    if not isinstance(distribute_comps, bool):\n        raise InvalidArgument(\"distribute_comps is invalid\")\n    distribute_verbatim = True\n    return distribute_verbatim", "label": 0}
{"index": "gp248565", "code": "def is_valid_prefix(path: str) -> bool:\n    if path.startswith(\"/\") and path.endswith(\"/\"):\n        return True\n    else:\n        return False", "contrast": "def is_prefix(cls, path):\n        lagofile = paths.Paths(path).prefix_lagofile()\n        return os.path.isfile(lagofile)", "label": 1}
{"index": "gp176214", "code": "def set_realized_target_count(run_tracker, count):\n    run_tracker.daemon_stats['realized_target_count'] = count", "contrast": "def _set_affected_target_count_in_runtracker(self):\n    target_count = len(self.build_graph)\n    self.run_tracker.pantsd_stats.set_affected_targets_size(target_count)\n    return target_count", "label": 1}
{"index": "gp146296", "code": "def _load_file(self, filename):\n        filename = os.path.abspath(os.path.expanduser(filename))\n        if not os.path.isfile(filename):\n            raise Exception('File %s does not exist' % filename)\n        ext = vtki.get_ext(filename)\n        if ext == '.ply':\n            reader = vtk.vtkPLYReader()\n        elif ext == '.stl':\n            reader = vtk.vtkSTLReader()\n        elif ext == '.vtk':\n            reader = vtk.vtkPolyDataReader()\n        elif ext == '.vtp':\n            reader = vtk.vtkXMLPolyDataReader()\n        elif ext == '.obj':\n            reader = vtk.vtkOBJReader()\n        else:\n            raise TypeError('Filetype must be either \"ply\", \"stl\", \"vtk\", \"vtp\", or \"obj\".')\n        reader.SetFileName(filename)\n        reader.Update()\n        self.ShallowCopy(reader.GetOutput())\n        if not np.any(self.points):\n            raise AssertionError('Empty or invalid file')", "contrast": "import os\nimport trimesh\ndef load_mesh_from_file(filename):\n    extension = os.path.splitext(filename)[1].lower()\n    valid_extensions = ['.ply', '.stl', '.vtk']\n    if extension not in valid_extensions:\n        raise ValueError(f'Invalid file type: {extension}')\n    try:\n        mesh = trimesh.load(filename)\n    except ValueError as e:\n        raise ValueError(f'Could not load mesh from file {filename}: {e}')\n    return mesh", "label": 0}
{"index": "gp075538", "code": "def sign(user_id, user_type=None, today=None, session=None):\n    if session is None:\n        session = Session()\n    else:\n        session = session\n    if today is None:\n        today = date.today()\n    else:\n        today = today\n    user = (\n        session\n        .query(User)\n        .filter(User.user_id == user_id)\n        .one_or_none()\n    )\n    if user:\n        signed_in_entries = (\n            user\n            .entries\n            .filter(Entry.date == today)\n            .filter(Entry.time_out.is_(None))\n            .all()\n        )\n        if not signed_in_entries:\n            new_entry = sign_in(user, user_type=user_type)\n            session.add(new_entry)\n            status = Status(\n                valid=True,\n                in_or_out='in',\n                user_name=get_user_name(user),\n                user_type=new_entry.user_type,\n                entry=new_entry\n            )\n        else:\n            for entry in signed_in_entries:\n                signed_out_entry = sign_out(entry)\n                session.add(signed_out_entry)\n                status = Status(\n                    valid=True,\n                    in_or_out='out',\n                    user_name=get_user_name(user),\n                    user_type=signed_out_entry.user_type,\n                    entry=signed_out_entry\n                )\n        session.commit()\n    else:\n        raise UnregisteredUser(\n            '{} not registered. Please register at the front desk.'.format(\n                user_id\n            )\n        )\n    logger.debug(status)\n    return status", "contrast": "from datetime import datetime\nfrom collections import namedtuple\nStatus = namedtuple('Status', ['success', 'message'])\ndef sign_user(user_id, user_type=None, today=None, session=None):\n    if not isinstance(user_id, int):\n        return Status(False, 'Invalid user id')\n    if today is None:\n        today = datetime.now().date()\n    if session is None:\n        session = get_session()\n    user = session.query(User).filter(User.id == user_id).first()\n    if user is None:\n        return Status(False, 'User does not exist with given id')\n    if user.signed_in:\n        user.signed_in = False\n        session.commit()\n        return Status(True, 'User signed out successfully')\n    else:\n        user.signed_in = True\n        user.last_login = today\n        session.commit()\n        return Status(True, 'User signed in successfully')", "label": 0}
{"index": "gp084381", "code": "def census(self, *scales):\n        params = {'mode': 'score+rank+rrank+prank+prrank'}\n        if scales:\n            params['scale'] = '+'.join(str(x) for x in scales)\n        @api_query('census', **params)\n        async def result(_, root):\n            return [\n                CensusScaleCurrent(scale_elem)\n                for scale_elem in root.find('CENSUS')\n            ]\n        return result(self)", "contrast": "def census(*scales):\n    if not scales:\n        scales = [81]\n    url = f\"https://www.nationstates.net/cgi-bin/api.cgi?q=census;scale={','.join(map(str, scales))}\"\n    response = requests.get(url)\n    root = ET.fromstring(response.text)\n    return ApiQuery([CensusScaleCurrent(child) for child in root.findall(\".//SCALE\")])", "label": 0}
{"index": "gp306591", "code": "def post_to_all_outputs(*args, text=''):\n    return new_record", "contrast": "def send(\n            self,\n            *args: str,\n            text: str=None,\n    ) -> IterationRecord:\n        if text is not None:\n            final_text = text\n        else:\n            if len(args) == 0:\n                raise BotSkeletonException((\"Please provide text either as a positional arg or \"\n                                            \"as a keyword arg (text=TEXT)\"))\n            else:\n                final_text = args[0]\n        record = IterationRecord(extra_keys=self.extra_keys)\n        for key, output in self.outputs.items():\n            if output[\"active\"]:\n                self.log.info(f\"Output {key} is active, calling send on it.\")\n                entry: Any = output[\"obj\"]\n                output_result = entry.send(text=final_text)\n                record.output_records[key] = output_result\n            else:\n                self.log.info(f\"Output {key} is inactive. Not sending.\")\n        self.history.append(record)\n        self.update_history()\n        return record", "label": 1}
{"index": "gp046450", "code": "def find_pids(self, name, search_string, exact_match, ignore_ad=True):\n        if not self.should_refresh_pid_cache(name):\n            return self.pid_cache[name]\n        ad_error_logger = self.log.debug\n        if not ignore_ad:\n            ad_error_logger = self.log.error\n        refresh_ad_cache = self.should_refresh_ad_cache(name)\n        matching_pids = set()\n        for proc in psutil.process_iter():\n            if not refresh_ad_cache and proc.pid in self.ad_cache:\n                continue\n            found = False\n            for string in search_string:\n                try:\n                    if string == 'All':\n                        found = True\n                    if exact_match:\n                        if os.name == 'nt':\n                            if proc.name().lower() == string.lower():\n                                found = True\n                        else:\n                            if proc.name() == string:\n                                found = True\n                    else:\n                        cmdline = proc.cmdline()\n                        if os.name == 'nt':\n                            lstring = string.lower()\n                            if re.search(lstring, ' '.join(cmdline).lower()):\n                                found = True\n                        else:\n                            if re.search(string, ' '.join(cmdline)):\n                                found = True\n                except psutil.NoSuchProcess:\n                    self.log.warning('Process disappeared while scanning')\n                except psutil.AccessDenied as e:\n                    ad_error_logger('Access denied to process with PID {}'.format(proc.pid))\n                    ad_error_logger('Error: {}'.format(e))\n                    if refresh_ad_cache:\n                        self.ad_cache.add(proc.pid)\n                    if not ignore_ad:\n                        raise\n                else:\n                    if refresh_ad_cache:\n                        self.ad_cache.discard(proc.pid)\n                    if found:\n                        matching_pids.add(proc.pid)\n                        break\n        self.pid_cache[name] = matching_pids\n        self.last_pid_cache_ts[name] = time.time()\n        if refresh_ad_cache:\n            self.last_ad_cache_ts[name] = time.time()\n        return matching_pids", "contrast": "import psutil\ndef find_pids(search_string):\n    pids = set()\n    for process in psutil.process_iter(['pid', 'name']):\n        if search_string in process.info['name']:\n            pids.add(process.info['pid'])\n    return pids", "label": 0}
{"index": "gp285581", "code": "def merge_streams(streamSet):\n    result = []\n    for stream in streamSet:\n        result.extend(stream)\n    return result", "contrast": "def union(self, streamSet):\n        if(not isinstance(streamSet,set)) :\n            raise TypeError(\"The union operator parameter must be a set object\")\n        if(len(streamSet) == 0):\n            return self        \n        op = self.topology.graph.addOperator(\"$Union$\")\n        op.addInputPort(outputPort=self.oport)\n        for stream in streamSet:\n            op.addInputPort(outputPort=stream.oport)\n        oport = op.addOutputPort(schema=self.oport.schema)\n        return Stream(self.topology, oport)", "label": 1}
{"index": "gp151033", "code": "def read_until_close(self, timeout_ms=None):\n    while True:\n      try:\n        yield self.read(timeout_ms=timeout_ms)\n      except usb_exceptions.AdbStreamClosedError:\n        break", "contrast": "def yield_data_until_closed(timeout_ms):\n    start_time = time.monotonic()\n    while True:\n        if timeout_ms is not None:\n            elapsed_time = (time.monotonic() - start_time) * 1000\n            time_left_ms = timeout_ms - elapsed_time\n            if time_left_ms <= 0:\n                raise AdbTimeoutError('Timeout waiting for data')\n            else:\n                timeout = PolledTimeout.from_millis(time_left_ms)\n        else:\n            timeout = None\n        try:\n            data = self.read(timeout)\n            if data:\n                yield data\n            else:\n                return\n        except AdbConnectionResetError as e:\n            raise AdbConnectionResetError(f'Read failed: {e!r}')\n        except AdbTimeoutError as e:\n            raise AdbTimeoutError(f'Read timed out: {e!r}')\n        except IOError as e:\n            raise AdbError(f'Read failed: {e!r}') from e", "label": 0}
{"index": "gp275976", "code": "import numpy as np\ndef make_wcs_mapping(hpx, wcs):\n    ny, nx = wcs.array_shape\n    ipixs = np.full((ny, nx), -1, dtype=int)\n    mult_val = np.ones((ny, nx))\n    npix = (ny, nx)\n    for iy in range(ny):\n        for ix in range(nx):\n            lon, lat, _ = wcs.pixel_to_world_values(ix + 0.5, iy + 0.5, 0)\n            ipix = hpx.lonlat_to_healpix(lon, lat)\n            ipixs[iy, ix] = ipix\n    return ipixs, mult_val, npix", "contrast": "def make_hpx_to_wcs_mapping_centers(hpx, wcs):\n    npix = (int(wcs.wcs.crpix[0] * 2), int(wcs.wcs.crpix[1] * 2))\n    mult_val = np.ones(npix).T.flatten()\n    sky_crds = hpx.get_sky_coords()\n    pix_crds = wcs.wcs_world2pix(sky_crds, 0).astype(int)\n    ipixs = -1 * np.ones(npix, int).T.flatten()\n    pix_index = npix[1] * pix_crds[0:, 0] + pix_crds[0:, 1]\n    if hpx._ipix is None:\n        for ipix, pix_crd in enumerate(pix_index):\n            ipixs[pix_crd] = ipix\n    else:\n        for pix_crd, ipix in zip(pix_index, hpx._ipix):\n            ipixs[pix_crd] = ipix\n    ipixs = ipixs.reshape(npix).T.flatten()\n    return ipixs, mult_val, npix", "label": 1}
{"index": "gp284536", "code": "def send_apdu(apdu_to_send):\n    return response_apdu", "contrast": "def InternalSendApdu(self, apdu_to_send):\n    response = None\n    if not self.use_legacy_format:\n      response = apdu.ResponseApdu(self.transport.SendMsgBytes(\n          apdu_to_send.ToByteArray()))\n      if response.sw1 == 0x67 and response.sw2 == 0x00:\n        self.use_legacy_format = True\n        return self.InternalSendApdu(apdu_to_send)\n    else:\n      response = apdu.ResponseApdu(self.transport.SendMsgBytes(\n          apdu_to_send.ToLegacyU2FByteArray()))\n    return response", "label": 1}
{"index": "gp112565", "code": "def itemfreq(inlist):\n    scores = pstat.unique(inlist)\n    scores.sort()\n    freq = []\n    for item in scores:\n        freq.append(inlist.count(item))\n    return zip(scores, freq)", "contrast": "def litemfreq(inlist):\n    freq_dict = {}\n    for score in inlist:\n        if score in freq_dict:\n            freq_dict[score] += 1\n        else:\n            freq_dict[score] = 1\n    freq_table = [[score, freq_dict[score]] for score in freq_dict]\n    freq_table.sort()\n    freq_table.append(['Total', len(inlist)])\n    return freq_table", "label": 0}
{"index": "gp305929", "code": "def create_tweet_listener():\n    class TweetListener(tweepy.StreamListener):\n        def on_status(self, status):\n            print(status.text)\n    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n    auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n    listener = TweetListener()\n    stream = tweepy.Stream(auth=auth, listener=listener)\n    return stream", "contrast": "def construct_listener(outfile=None):\n    if outfile is not None:\n        if os.path.exists(outfile):\n            raise IOError(\"File %s already exists\" % outfile)\n        outfile = open(outfile, 'wb')\n    return PrintingListener(out=outfile)", "label": 1}
{"index": "gp312439", "code": "def munin_plugin_fetch():\n    measured_values = [1, 2, 3, 4, 5]  \n    for value in measured_values:\n        print(value)", "contrast": "def fetch(self):\n        self.retrieveVals()\n        for parent_name in self._graphNames:\n            graph = self._graphDict[parent_name]\n            if self.isMultigraph:\n                print \"multigraph %s\" % self._getMultigraphID(parent_name)\n            print self._formatVals(graph.getVals())\n            print\n        if (self.isMultigraph and self._nestedGraphs \n            and self._subgraphDict and self._subgraphNames):\n            for (parent_name, subgraph_names) in self._subgraphNames.iteritems():\n                for graph_name in subgraph_names:\n                    graph = self._subgraphDict[parent_name][graph_name]\n                    print \"multigraph %s\" % self.getMultigraphID(parent_name, \n                                                                 graph_name)\n                    print self._formatVals(graph.getVals())\n                    print\n        return True", "label": 1}
{"index": "gp165172", "code": "def send(self):\n        for name, array in iteritems(self):\n            shader_id = c_int(0)\n            gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM, byref(shader_id))\n            if shader_id.value == 0:\n                raise UnboundLocalError(\"\"\"Shader not bound to OpenGL context--uniform cannot be sent.\n                ------------ Tip -------------\n                with ratcave.default_shader:\n                    mesh.draw()\n                ------------------------------\n                \"\"\")\n            try:\n                loc, shader_id_for_array = array.loc\n                if shader_id.value != shader_id_for_array:\n                    raise Exception('Uniform location bound to a different shader')\n            except (AttributeError, Exception) as e:\n                array.loc = (gl.glGetUniformLocation(shader_id.value, name.encode('ascii')), shader_id.value)\n            if array.ndim == 2:  \n                try:\n                    pointer = array.pointer\n                except AttributeError:\n                    array.pointer = array.ctypes.data_as(POINTER(c_float * 16)).contents\n                    pointer = array.pointer\n                gl.glUniformMatrix4fv(array.loc[0], 1, True, pointer)\n            else:\n                sendfun = self._sendfuns[array.dtype.kind][len(array) - 1]  \n                sendfun(array.loc[0], *array)", "contrast": "def send_uniforms_to_graphics_card(uniforms_dict):\n    for key, value in uniforms_dict.items():\n        send_uniform_variable_to_graphics_card(key, value)", "label": 0}
{"index": "gp085331", "code": "def list_all_categories(cls, **kwargs):\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._list_all_categories_with_http_info(**kwargs)\n        else:\n            (data) = cls._list_all_categories_with_http_info(**kwargs)\n            return data", "contrast": "def list_categories(async=False, page=None, size=None, sort=None):\n    if async:\n        thread = api.list_all_categories(async=True)\n        result = thread.get()\n        return result\n    else:\n        pass", "label": 0}
{"index": "gp293017", "code": "def isCLIExecutable(filename):\ndef getCLIExecutables(baseDir):\n    paths = []\n    for file in os.listdir(baseDir):\n        path = os.path.join(baseDir, file)\n        if isCLIExecutable(path):\n            paths.append(path)\n    return paths", "contrast": "def listCLIExecutables(baseDir):\n    return [path for path in glob.glob(os.path.join(os.path.normpath(baseDir), '*'))\n            if isCLIExecutable(path)]", "label": 1}
{"index": "gp277526", "code": "from xml.etree.ElementTree import Element, ElementTree\nfrom typing import Union\nclass SubSection:\n    ALLOWED_SUBSECTIONS = {'subsection1', 'subsection2', 'subsection3'}\n    def __init__(self, md_ref: Union[str, None], md_wrap: Union[str, None]):\n        self.md_ref = md_ref\n        self.md_wrap = md_wrap\n    @staticmethod\n    def from_xml(root: Union[Element, ElementTree]) -> 'SubSection':\n        if root.tag not in SubSection.ALLOWED_SUBSECTIONS:\n            raise exceptions.ParseError(\"root's tag is not in allowed sub-sections\")\n        if root[0].tag != 'mdRef' and root[0].tag != 'mdWrap':\n            raise exceptions.ParseError(\"the first child of root is not mdRef or mdWrap\")\n        return SubSection(root[0].get('href'), root[0].text if root[0].tag == 'mdWrap' else None)", "contrast": "def parse(cls, root):\n        subsection = root.tag.replace(utils.lxmlns(\"mets\"), \"\", 1)\n        if subsection not in cls.ALLOWED_SUBSECTIONS:\n            raise exceptions.ParseError(\n                \"SubSection can only parse elements with tag in %s with METS namespace\"\n                % (cls.ALLOWED_SUBSECTIONS,)\n            )\n        section_id = root.get(\"ID\")\n        created = root.get(\"CREATED\", \"\")\n        status = root.get(\"STATUS\", \"\")\n        child = root[0]\n        if child.tag == utils.lxmlns(\"mets\") + \"mdWrap\":\n            mdwrap = MDWrap.parse(child)\n            obj = cls(subsection, mdwrap, section_id)\n        elif child.tag == utils.lxmlns(\"mets\") + \"mdRef\":\n            mdref = MDRef.parse(child)\n            obj = cls(subsection, mdref, section_id)\n        else:\n            raise exceptions.ParseError(\n                \"Child of %s must be mdWrap or mdRef\" % subsection\n            )\n        obj.created = created\n        obj.status = status\n        return obj", "label": 1}
{"index": "gp194138", "code": "def interpolate_timestamps(timestamps):\n    interpolated_timestamps = []\n    for i in range(len(timestamps)):\n        if i > 0 and timestamps[i] == timestamps[i-1]:\n            diff = timestamps[i+1] - timestamps[i]\n            interpolated_timestamps.append(timestamps[i] + diff/2)\n        else:\n            interpolated_timestamps.append(timestamps[i])\n    return interpolated_timestamps", "contrast": "def interpolate_timestamp(capture_times):\n    timestamps = []\n    num_file = len(capture_times)\n    time_dict = OrderedDict()\n    if num_file < 2:\n        return capture_times\n    time_dict = OrderedDict()\n    for i, t in enumerate(capture_times):\n        if t not in time_dict:\n            time_dict[t] = {\n                \"count\": 0,\n                \"pointer\": 0\n            }\n            interval = 0\n            if i != 0:\n                interval = (t - capture_times[i - 1]).total_seconds()\n                time_dict[capture_times[i - 1]][\"interval\"] = interval\n        time_dict[t][\"count\"] += 1\n    if len(time_dict) >= 2:\n        time_dict[time_dict.keys()[-1]\n                  ][\"interval\"] = time_dict[time_dict.keys()[-2]][\"interval\"]\n    else:\n        time_dict[time_dict.keys()[0]][\"interval\"] = time_dict[time_dict.keys()[\n            0]][\"count\"] * 1.\n    for t in capture_times:\n        d = time_dict[t]\n        s = datetime.timedelta(\n            seconds=d[\"pointer\"] * d[\"interval\"] / float(d[\"count\"]))\n        updated_time = t + s\n        time_dict[t][\"pointer\"] += 1\n        timestamps.append(updated_time)\n    return timestamps", "label": 1}
{"index": "gp292104", "code": "class intrange:\n    def __init__(self, start, end):\n        self.start = start\n        self.end = end\n    def endsbefore(self, other):\n        if isinstance(other, int):\n            return self.end <= other\n        elif isinstance(other, intrange):\n            return self.end <= other.end\n        else:\n            raise TypeError(\"other must be an integer or an intrange object\")", "contrast": "def endsbefore(self, other):\n        if self.is_valid_range(other):\n            if self.upper == other.upper:\n                return not self.upper_inc or other.upper_inc\n            elif self.upper_inf:\n                return False\n            elif other.upper_inf:\n                return True\n            else:\n                return self.upper <= other.upper\n        elif self.is_valid_scalar(other):\n            return self.upper <= other\n        else:\n            raise TypeError(\n                \"Unsupported type to test for ends before '{}'\".format(\n                    other.__class__.__name__))", "label": 1}
{"index": "gp105978", "code": "def get_breakpoint_graph(stream, merge_edges=True):\n        result = BreakpointGraph()\n        current_genome = None\n        fragment_data = {}\n        for line in stream:\n            line = line.strip()\n            if len(line) == 0:\n                continue\n            if GRIMMReader.is_genome_declaration_string(data_string=line):\n                current_genome = GRIMMReader.parse_genome_declaration_string(data_string=line)\n                fragment_data = {}\n            elif GRIMMReader.is_comment_string(data_string=line):\n                if GRIMMReader.is_comment_data_string(string=line):\n                    path, (key, value) = GRIMMReader.parse_comment_data_string(comment_data_string=line)\n                    if len(path) > 0 and path[0] == \"fragment\":\n                        add_to_dict_with_path(destination_dict=fragment_data, key=key, value=value, path=path)\n                else:\n                    continue\n            elif current_genome is not None:\n                parsed_data = GRIMMReader.parse_data_string(data_string=line)\n                edges = GRIMMReader.get_edges_from_parsed_data(parsed_data=parsed_data)\n                for v1, v2 in edges:\n                    edge_specific_data = {\n                        \"fragment\": {\n                            \"forward_orientation\": (v1, v2)\n                        }\n                    }\n                    edge = BGEdge(vertex1=v1, vertex2=v2, multicolor=Multicolor(current_genome), data=deepcopy(fragment_data))\n                    edge.update_data(source=edge_specific_data)\n                    result.add_bgedge(bgedge=edge,\n                                      merge=merge_edges)\n        return result", "contrast": "from typing import IO, List\nfrom bg import BreakpointGraph\ndef transform_gene_order_data(merge_edges: bool, stream: IO[str]) -> BreakpointGraph:\n    gene_order_data = [line.strip() for line in stream if line.strip()]\n    block_sizes = [len(block) for block in gene_order_data]\n    block_ends = [sum(block_sizes[:i]) + size for i, size in enumerate(block_sizes)]\n    adjacencies = []\n    for i in range(len(gene_order_data) - 1):\n        adjacencies.append((block_ends[i], -block_ends[i+1]))\n        adjacencies.append((block_ends[i+1], -block_ends[i]))\n    return BreakpointGraph(adjacencies, merge_edges)", "label": 0}
{"index": "gp261399", "code": "def remove_keys(dct, *keys):\n    new_dct = dct.copy()\n    for key in keys:\n        new_dct.pop(key, None)\n    return new_dct", "contrast": "def _delete_keys(dct, keys):\n    c = deepcopy(dct)\n    assert isinstance(keys, list)\n    for k in keys:\n        c.pop(k)\n    return c", "label": 1}
{"index": "gp164839", "code": "def main(argv=None):\n    if argv is None:\n        argv = sys.argv\n    parser = argparse.ArgumentParser(\n        fromfile_prefix_chars='@',\n        description=\n        \"\"\"Execute a command with resource limits and measurements.\n           Command-line parameters can additionally be read from a file if file name prefixed with '@' is given as argument.\n           Part of BenchExec: https://github.com/sosy-lab/benchexec/\"\"\")\n    resource_args = parser.add_argument_group(\"optional arguments for resource limits\")\n    resource_args.add_argument(\"--memlimit\", type=util.parse_memory_value, metavar=\"BYTES\",\n        help=\"memory limit in bytes\")\n    resource_args.add_argument(\"--timelimit\", type=util.parse_timespan_value, metavar=\"SECONDS\",\n        help=\"CPU time limit in seconds\")\n    resource_args.add_argument(\"--softtimelimit\", type=util.parse_timespan_value, metavar=\"SECONDS\",\n        help='\"soft\" CPU time limit in seconds (command will be send the TERM signal at this time)')\n    resource_args.add_argument(\"--walltimelimit\", type=util.parse_timespan_value, metavar=\"SECONDS\",\n        help='wall time limit in seconds (default is CPU time limit plus a few seconds)')\n    resource_args.add_argument(\"--cores\", type=util.parse_int_list, metavar=\"N,M-K\",\n        help=\"list of CPU cores to use\")\n    resource_args.add_argument(\"--memoryNodes\", type=util.parse_int_list, metavar=\"N,M-K\",\n        help=\"list of memory nodes to use\")\n    io_args = parser.add_argument_group(\"optional arguments for run I/O\")\n    io_args.add_argument(\"--input\", metavar=\"FILE\",\n        help=\"name of file used as stdin for command \"\n            \"(default: /dev/null; use - for stdin passthrough)\")\n    io_args.add_argument(\"--output\", default=\"output.log\", metavar=\"FILE\",\n        help=\"name of file where command output (stdout and stderr) is written\")\n    io_args.add_argument(\"--maxOutputSize\", type=util.parse_memory_value, metavar=\"BYTES\",\n        help=\"shrink output file to approximately this size if necessary \"\n            \"(by removing lines from the middle of the output)\")\n    io_args.add_argument(\"--filesCountLimit\", type=int, metavar=\"COUNT\",\n        help=\"maximum number of files the tool may write to (checked periodically, counts only files written in container mode or to temporary directories, only supported with --no-tmpfs)\")\n    io_args.add_argument(\"--filesSizeLimit\", type=util.parse_memory_value, metavar=\"BYTES\",\n        help=\"maximum size of files the tool may write (checked periodically, counts only files written in container mode or to temporary directories, only supported with --no-tmpfs)\")\n    io_args.add_argument(\"--skip-cleanup\", action=\"store_false\", dest=\"cleanup\",\n        help=\"do not delete files created by the tool in temp directory\")\n    container_args = parser.add_argument_group(\"optional arguments for run container\")\n    container_on_args = container_args.add_mutually_exclusive_group()\n    container_on_args.add_argument(\"--container\", action='store_true',\n        help=\"force isolation of run in container (future default starting with BenchExec 2.0)\")\n    container_on_args.add_argument(\"--no-container\", action='store_true',\n        help=\"disable use of containers for isolation of runs (current default)\")\n    containerexecutor.add_basic_container_args(container_args)\n    containerexecutor.add_container_output_args(container_args)\n    environment_args = parser.add_argument_group(\"optional arguments for run environment\")\n    environment_args.add_argument(\"--require-cgroup-subsystem\", action=\"append\", default=[], metavar=\"SUBSYSTEM\",\n        help=\"additional cgroup system that should be enabled for runs \"\n            \"(may be specified multiple times)\")\n    environment_args.add_argument(\"--set-cgroup-value\", action=\"append\", dest=\"cgroup_values\", default=[],\n        metavar=\"SUBSYSTEM.OPTION=VALUE\",\n        help=\"additional cgroup values that should be set for runs (e.g., 'cpu.shares=1000')\")\n    environment_args.add_argument(\"--dir\", metavar=\"DIR\",\n        help=\"working directory for executing the command (default is current directory)\")\n    environment_args.add_argument(\"--user\", metavar=\"USER\",\n        help=\"execute tool under given user account (needs password-less sudo setup, \"\n            \"not supported in combination with --container)\")\n    baseexecutor.add_basic_executor_options(parser)\n    options = parser.parse_args(argv[1:])\n    baseexecutor.handle_basic_executor_options(options, parser)\n    if options.container:\n        if options.user is not None:\n            sys.exit(\"Cannot use --user in combination with --container.\")\n        container_options = containerexecutor.handle_basic_container_args(options, parser)\n        container_output_options = containerexecutor.handle_container_output_args(options, parser)\n        if container_options['container_tmpfs'] and (options.filesCountLimit or options.filesSizeLimit):\n            parser.error(\"Files-count limit and files-size limit are not supported if tmpfs is used in container. Use --no-tmpfs to make these limits work or disable them (typically they are unnecessary if a tmpfs is used).\")\n    else:\n        container_options = {}\n        container_output_options = {}\n        if options.user is not None:\n            logging.warning(\n                \"Executing benchmarks at another user with --user is deprecated and may be removed in the future. \"\n                \"Consider using the container mode instead for isolating runs \"\n                \"(cf. https://github.com/sosy-lab/benchexec/issues/215).\")\n        elif not options.no_container:\n            logging.warning(\n                \"Neither --container or --no-container was specified, \"\n                \"not using containers for isolation of runs. \"\n                \"Either specify --no-container to silence this warning, \"\n                \"or specify --container to use containers for better isolation of runs \"\n                \"(this will be the default starting with BenchExec 2.0). \"\n                \"Please read https://github.com/sosy-lab/benchexec/blob/master/doc/container.md \"\n                \"for more information.\")\n    env = {}\n    if len(options.args) == 1 and options.args[0].startswith(\"{\"):\n        data = eval(options.args[0])\n        options.args = data[\"args\"]\n        env = data.get(\"env\", {})\n        options.debug = data.get(\"debug\", options.debug)\n        if \"maxLogfileSize\" in data:\n            try:\n                options.maxOutputSize = int(data[\"maxLogfileSize\"]) * _BYTE_FACTOR * _BYTE_FACTOR \n            except ValueError:\n                options.maxOutputSize = util.parse_memory_value(data[\"maxLogfileSize\"])\n    if options.input == '-':\n        stdin = sys.stdin\n    elif options.input is not None:\n        if options.input == options.output:\n            parser.error(\"Input and output files cannot be the same.\")\n        try:\n            stdin = open(options.input, 'rt')\n        except IOError as e:\n            parser.error(e)\n    else:\n        stdin = None\n    cgroup_subsystems = set(options.require_cgroup_subsystem)\n    cgroup_values = {}\n    for arg in options.cgroup_values:\n        try:\n            key, value = arg.split(\"=\", 1)\n            subsystem, option = key.split(\".\", 1)\n            if not subsystem or not option:\n                raise ValueError()\n        except ValueError:\n            parser.error(\n                'Cgroup value \"{}\" has invalid format, needs to be \"subsystem.option=value\".'\n                    .format(arg))\n        cgroup_values[(subsystem, option)] = value\n        cgroup_subsystems.add(subsystem)\n    executor = RunExecutor(user=options.user, cleanup_temp_dir=options.cleanup,\n                           additional_cgroup_subsystems=list(cgroup_subsystems),\n                           use_namespaces=options.container, **container_options)\n    def signal_handler_kill(signum, frame):\n        executor.stop()\n    signal.signal(signal.SIGTERM, signal_handler_kill)\n    signal.signal(signal.SIGINT,  signal_handler_kill)\n    formatted_args = \" \".join(map(util.escape_string_shell, options.args))\n    logging.info('Starting command %s', formatted_args)\n    if options.container and options.output_directory and options.result_files:\n        logging.info('Writing output to %s and result files to %s',\n                     util.escape_string_shell(options.output),\n                     util.escape_string_shell(options.output_directory))\n    else:\n        logging.info('Writing output to %s', util.escape_string_shell(options.output))\n    try:\n        result = executor.execute_run(\n                            args=options.args,\n                            output_filename=options.output,\n                            stdin=stdin,\n                            hardtimelimit=options.timelimit,\n                            softtimelimit=options.softtimelimit,\n                            walltimelimit=options.walltimelimit,\n                            cores=options.cores,\n                            memlimit=options.memlimit,\n                            memory_nodes=options.memoryNodes,\n                            cgroupValues=cgroup_values,\n                            environments=env,\n                            workingDir=options.dir,\n                            maxLogfileSize=options.maxOutputSize,\n                            files_count_limit=options.filesCountLimit,\n                            files_size_limit=options.filesSizeLimit,\n                            **container_output_options)\n    finally:\n        if stdin:\n            stdin.close()\n    executor.check_for_new_files_in_home()\n    exit_code = util.ProcessExitCode.from_raw(result['exitcode'])\n    def print_optional_result(key, unit=''):\n        if key in result:\n            print(key + \"=\" + str(result[key]).replace(\"'u\", '') + unit)\n    print_optional_result('terminationreason')\n    print(\"exitcode=\" + str(exit_code.raw))\n    if exit_code.value is not None:\n        print(\"returnvalue=\" + str(exit_code.value))\n    if exit_code.signal is not None:\n        print(\"exitsignal=\" + str(exit_code.signal))\n    print(\"walltime=\" + str(result['walltime']) + \"s\")\n    print(\"cputime=\" + str(result['cputime']) + \"s\")\n    for key in sorted(result.keys()):\n        if key.startswith('cputime-'):\n            print(\"{}={:.9f}s\".format(key, result[key]))\n    print_optional_result('memory')\n    print_optional_result('blkio-read', 'B')\n    print_optional_result('blkio-write', 'B')\n    energy = intel_cpu_energy.format_energy_results(result.get('cpuenergy'))\n    for energy_key, energy_value in energy.items():\n        print('{}={}J'.format(energy_key, energy_value))", "contrast": "import argparse\nfrom benchexec import runexecutor\ndef main():\n    parser = argparse.ArgumentParser(description='A simple command-line interface for BenchExec runexecutor module')\n    parser.add_argument('xml_file', type=str, help='Path to the XML file containing the benchmark definition')\n    parser.add_argument('tool', type=str, help='Name of the tool to run')\n    parser.add_argument('output_dir', type=str, help='Path to the output directory')\n    args = parser.parse_args()\n    run_set = runexecutor.RunSet(args.xml_file, args.tool, args.output_dir)\n    run_executor = runexecutor.RunExecutor()\n    run_executor.execute_run_set(run_set)\nif __name__ == '__main__':\n    main()", "label": 0}
{"index": "gp012529", "code": "def state(self):\n        return {'c': self.c, 's0': self.s0, 's1': self.s1, 's2': self.s2}", "contrast": "def get_internal_state():\n    return internal_state", "label": 0}
